<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />
<title>Task3_DQN_RL</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<style type="text/css">
    /*!
*
* Twitter Bootstrap
*
*/
/*!
 * Bootstrap v3.3.7 (http://getbootstrap.com)
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 */
/*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */
html {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block;
  vertical-align: baseline;
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a {
  background-color: transparent;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit;
  font: inherit;
  margin: 0;
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"],
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button;
  cursor: pointer;
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box;
  padding: 0;
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield;
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0;
  padding: 0;
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
/*! Source: https://github.com/h5bp/html5-boilerplate/blob/master/src/css/main.css */
@media print {
  *,
  *:before,
  *:after {
    background: transparent !important;
    color: #000 !important;
    box-shadow: none !important;
    text-shadow: none !important;
  }
  a,
  a:visited {
    text-decoration: underline;
  }
  a[href]:after {
    content: " (" attr(href) ")";
  }
  abbr[title]:after {
    content: " (" attr(title) ")";
  }
  a[href^="#"]:after,
  a[href^="javascript:"]:after {
    content: "";
  }
  pre,
  blockquote {
    border: 1px solid #999;
    page-break-inside: avoid;
  }
  thead {
    display: table-header-group;
  }
  tr,
  img {
    page-break-inside: avoid;
  }
  img {
    max-width: 100% !important;
  }
  p,
  h2,
  h3 {
    orphans: 3;
    widows: 3;
  }
  h2,
  h3 {
    page-break-after: avoid;
  }
  .navbar {
    display: none;
  }
  .btn > .caret,
  .dropup > .btn > .caret {
    border-top-color: #000 !important;
  }
  .label {
    border: 1px solid #000;
  }
  .table {
    border-collapse: collapse !important;
  }
  .table td,
  .table th {
    background-color: #fff !important;
  }
  .table-bordered th,
  .table-bordered td {
    border: 1px solid #ddd !important;
  }
}
@font-face {
  font-family: 'Glyphicons Halflings';
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot');
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot?#iefix') format('embedded-opentype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff2') format('woff2'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff') format('woff'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.ttf') format('truetype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.svg#glyphicons_halflingsregular') format('svg');
}
.glyphicon {
  position: relative;
  top: 1px;
  display: inline-block;
  font-family: 'Glyphicons Halflings';
  font-style: normal;
  font-weight: normal;
  line-height: 1;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.glyphicon-asterisk:before {
  content: "\002a";
}
.glyphicon-plus:before {
  content: "\002b";
}
.glyphicon-euro:before,
.glyphicon-eur:before {
  content: "\20ac";
}
.glyphicon-minus:before {
  content: "\2212";
}
.glyphicon-cloud:before {
  content: "\2601";
}
.glyphicon-envelope:before {
  content: "\2709";
}
.glyphicon-pencil:before {
  content: "\270f";
}
.glyphicon-glass:before {
  content: "\e001";
}
.glyphicon-music:before {
  content: "\e002";
}
.glyphicon-search:before {
  content: "\e003";
}
.glyphicon-heart:before {
  content: "\e005";
}
.glyphicon-star:before {
  content: "\e006";
}
.glyphicon-star-empty:before {
  content: "\e007";
}
.glyphicon-user:before {
  content: "\e008";
}
.glyphicon-film:before {
  content: "\e009";
}
.glyphicon-th-large:before {
  content: "\e010";
}
.glyphicon-th:before {
  content: "\e011";
}
.glyphicon-th-list:before {
  content: "\e012";
}
.glyphicon-ok:before {
  content: "\e013";
}
.glyphicon-remove:before {
  content: "\e014";
}
.glyphicon-zoom-in:before {
  content: "\e015";
}
.glyphicon-zoom-out:before {
  content: "\e016";
}
.glyphicon-off:before {
  content: "\e017";
}
.glyphicon-signal:before {
  content: "\e018";
}
.glyphicon-cog:before {
  content: "\e019";
}
.glyphicon-trash:before {
  content: "\e020";
}
.glyphicon-home:before {
  content: "\e021";
}
.glyphicon-file:before {
  content: "\e022";
}
.glyphicon-time:before {
  content: "\e023";
}
.glyphicon-road:before {
  content: "\e024";
}
.glyphicon-download-alt:before {
  content: "\e025";
}
.glyphicon-download:before {
  content: "\e026";
}
.glyphicon-upload:before {
  content: "\e027";
}
.glyphicon-inbox:before {
  content: "\e028";
}
.glyphicon-play-circle:before {
  content: "\e029";
}
.glyphicon-repeat:before {
  content: "\e030";
}
.glyphicon-refresh:before {
  content: "\e031";
}
.glyphicon-list-alt:before {
  content: "\e032";
}
.glyphicon-lock:before {
  content: "\e033";
}
.glyphicon-flag:before {
  content: "\e034";
}
.glyphicon-headphones:before {
  content: "\e035";
}
.glyphicon-volume-off:before {
  content: "\e036";
}
.glyphicon-volume-down:before {
  content: "\e037";
}
.glyphicon-volume-up:before {
  content: "\e038";
}
.glyphicon-qrcode:before {
  content: "\e039";
}
.glyphicon-barcode:before {
  content: "\e040";
}
.glyphicon-tag:before {
  content: "\e041";
}
.glyphicon-tags:before {
  content: "\e042";
}
.glyphicon-book:before {
  content: "\e043";
}
.glyphicon-bookmark:before {
  content: "\e044";
}
.glyphicon-print:before {
  content: "\e045";
}
.glyphicon-camera:before {
  content: "\e046";
}
.glyphicon-font:before {
  content: "\e047";
}
.glyphicon-bold:before {
  content: "\e048";
}
.glyphicon-italic:before {
  content: "\e049";
}
.glyphicon-text-height:before {
  content: "\e050";
}
.glyphicon-text-width:before {
  content: "\e051";
}
.glyphicon-align-left:before {
  content: "\e052";
}
.glyphicon-align-center:before {
  content: "\e053";
}
.glyphicon-align-right:before {
  content: "\e054";
}
.glyphicon-align-justify:before {
  content: "\e055";
}
.glyphicon-list:before {
  content: "\e056";
}
.glyphicon-indent-left:before {
  content: "\e057";
}
.glyphicon-indent-right:before {
  content: "\e058";
}
.glyphicon-facetime-video:before {
  content: "\e059";
}
.glyphicon-picture:before {
  content: "\e060";
}
.glyphicon-map-marker:before {
  content: "\e062";
}
.glyphicon-adjust:before {
  content: "\e063";
}
.glyphicon-tint:before {
  content: "\e064";
}
.glyphicon-edit:before {
  content: "\e065";
}
.glyphicon-share:before {
  content: "\e066";
}
.glyphicon-check:before {
  content: "\e067";
}
.glyphicon-move:before {
  content: "\e068";
}
.glyphicon-step-backward:before {
  content: "\e069";
}
.glyphicon-fast-backward:before {
  content: "\e070";
}
.glyphicon-backward:before {
  content: "\e071";
}
.glyphicon-play:before {
  content: "\e072";
}
.glyphicon-pause:before {
  content: "\e073";
}
.glyphicon-stop:before {
  content: "\e074";
}
.glyphicon-forward:before {
  content: "\e075";
}
.glyphicon-fast-forward:before {
  content: "\e076";
}
.glyphicon-step-forward:before {
  content: "\e077";
}
.glyphicon-eject:before {
  content: "\e078";
}
.glyphicon-chevron-left:before {
  content: "\e079";
}
.glyphicon-chevron-right:before {
  content: "\e080";
}
.glyphicon-plus-sign:before {
  content: "\e081";
}
.glyphicon-minus-sign:before {
  content: "\e082";
}
.glyphicon-remove-sign:before {
  content: "\e083";
}
.glyphicon-ok-sign:before {
  content: "\e084";
}
.glyphicon-question-sign:before {
  content: "\e085";
}
.glyphicon-info-sign:before {
  content: "\e086";
}
.glyphicon-screenshot:before {
  content: "\e087";
}
.glyphicon-remove-circle:before {
  content: "\e088";
}
.glyphicon-ok-circle:before {
  content: "\e089";
}
.glyphicon-ban-circle:before {
  content: "\e090";
}
.glyphicon-arrow-left:before {
  content: "\e091";
}
.glyphicon-arrow-right:before {
  content: "\e092";
}
.glyphicon-arrow-up:before {
  content: "\e093";
}
.glyphicon-arrow-down:before {
  content: "\e094";
}
.glyphicon-share-alt:before {
  content: "\e095";
}
.glyphicon-resize-full:before {
  content: "\e096";
}
.glyphicon-resize-small:before {
  content: "\e097";
}
.glyphicon-exclamation-sign:before {
  content: "\e101";
}
.glyphicon-gift:before {
  content: "\e102";
}
.glyphicon-leaf:before {
  content: "\e103";
}
.glyphicon-fire:before {
  content: "\e104";
}
.glyphicon-eye-open:before {
  content: "\e105";
}
.glyphicon-eye-close:before {
  content: "\e106";
}
.glyphicon-warning-sign:before {
  content: "\e107";
}
.glyphicon-plane:before {
  content: "\e108";
}
.glyphicon-calendar:before {
  content: "\e109";
}
.glyphicon-random:before {
  content: "\e110";
}
.glyphicon-comment:before {
  content: "\e111";
}
.glyphicon-magnet:before {
  content: "\e112";
}
.glyphicon-chevron-up:before {
  content: "\e113";
}
.glyphicon-chevron-down:before {
  content: "\e114";
}
.glyphicon-retweet:before {
  content: "\e115";
}
.glyphicon-shopping-cart:before {
  content: "\e116";
}
.glyphicon-folder-close:before {
  content: "\e117";
}
.glyphicon-folder-open:before {
  content: "\e118";
}
.glyphicon-resize-vertical:before {
  content: "\e119";
}
.glyphicon-resize-horizontal:before {
  content: "\e120";
}
.glyphicon-hdd:before {
  content: "\e121";
}
.glyphicon-bullhorn:before {
  content: "\e122";
}
.glyphicon-bell:before {
  content: "\e123";
}
.glyphicon-certificate:before {
  content: "\e124";
}
.glyphicon-thumbs-up:before {
  content: "\e125";
}
.glyphicon-thumbs-down:before {
  content: "\e126";
}
.glyphicon-hand-right:before {
  content: "\e127";
}
.glyphicon-hand-left:before {
  content: "\e128";
}
.glyphicon-hand-up:before {
  content: "\e129";
}
.glyphicon-hand-down:before {
  content: "\e130";
}
.glyphicon-circle-arrow-right:before {
  content: "\e131";
}
.glyphicon-circle-arrow-left:before {
  content: "\e132";
}
.glyphicon-circle-arrow-up:before {
  content: "\e133";
}
.glyphicon-circle-arrow-down:before {
  content: "\e134";
}
.glyphicon-globe:before {
  content: "\e135";
}
.glyphicon-wrench:before {
  content: "\e136";
}
.glyphicon-tasks:before {
  content: "\e137";
}
.glyphicon-filter:before {
  content: "\e138";
}
.glyphicon-briefcase:before {
  content: "\e139";
}
.glyphicon-fullscreen:before {
  content: "\e140";
}
.glyphicon-dashboard:before {
  content: "\e141";
}
.glyphicon-paperclip:before {
  content: "\e142";
}
.glyphicon-heart-empty:before {
  content: "\e143";
}
.glyphicon-link:before {
  content: "\e144";
}
.glyphicon-phone:before {
  content: "\e145";
}
.glyphicon-pushpin:before {
  content: "\e146";
}
.glyphicon-usd:before {
  content: "\e148";
}
.glyphicon-gbp:before {
  content: "\e149";
}
.glyphicon-sort:before {
  content: "\e150";
}
.glyphicon-sort-by-alphabet:before {
  content: "\e151";
}
.glyphicon-sort-by-alphabet-alt:before {
  content: "\e152";
}
.glyphicon-sort-by-order:before {
  content: "\e153";
}
.glyphicon-sort-by-order-alt:before {
  content: "\e154";
}
.glyphicon-sort-by-attributes:before {
  content: "\e155";
}
.glyphicon-sort-by-attributes-alt:before {
  content: "\e156";
}
.glyphicon-unchecked:before {
  content: "\e157";
}
.glyphicon-expand:before {
  content: "\e158";
}
.glyphicon-collapse-down:before {
  content: "\e159";
}
.glyphicon-collapse-up:before {
  content: "\e160";
}
.glyphicon-log-in:before {
  content: "\e161";
}
.glyphicon-flash:before {
  content: "\e162";
}
.glyphicon-log-out:before {
  content: "\e163";
}
.glyphicon-new-window:before {
  content: "\e164";
}
.glyphicon-record:before {
  content: "\e165";
}
.glyphicon-save:before {
  content: "\e166";
}
.glyphicon-open:before {
  content: "\e167";
}
.glyphicon-saved:before {
  content: "\e168";
}
.glyphicon-import:before {
  content: "\e169";
}
.glyphicon-export:before {
  content: "\e170";
}
.glyphicon-send:before {
  content: "\e171";
}
.glyphicon-floppy-disk:before {
  content: "\e172";
}
.glyphicon-floppy-saved:before {
  content: "\e173";
}
.glyphicon-floppy-remove:before {
  content: "\e174";
}
.glyphicon-floppy-save:before {
  content: "\e175";
}
.glyphicon-floppy-open:before {
  content: "\e176";
}
.glyphicon-credit-card:before {
  content: "\e177";
}
.glyphicon-transfer:before {
  content: "\e178";
}
.glyphicon-cutlery:before {
  content: "\e179";
}
.glyphicon-header:before {
  content: "\e180";
}
.glyphicon-compressed:before {
  content: "\e181";
}
.glyphicon-earphone:before {
  content: "\e182";
}
.glyphicon-phone-alt:before {
  content: "\e183";
}
.glyphicon-tower:before {
  content: "\e184";
}
.glyphicon-stats:before {
  content: "\e185";
}
.glyphicon-sd-video:before {
  content: "\e186";
}
.glyphicon-hd-video:before {
  content: "\e187";
}
.glyphicon-subtitles:before {
  content: "\e188";
}
.glyphicon-sound-stereo:before {
  content: "\e189";
}
.glyphicon-sound-dolby:before {
  content: "\e190";
}
.glyphicon-sound-5-1:before {
  content: "\e191";
}
.glyphicon-sound-6-1:before {
  content: "\e192";
}
.glyphicon-sound-7-1:before {
  content: "\e193";
}
.glyphicon-copyright-mark:before {
  content: "\e194";
}
.glyphicon-registration-mark:before {
  content: "\e195";
}
.glyphicon-cloud-download:before {
  content: "\e197";
}
.glyphicon-cloud-upload:before {
  content: "\e198";
}
.glyphicon-tree-conifer:before {
  content: "\e199";
}
.glyphicon-tree-deciduous:before {
  content: "\e200";
}
.glyphicon-cd:before {
  content: "\e201";
}
.glyphicon-save-file:before {
  content: "\e202";
}
.glyphicon-open-file:before {
  content: "\e203";
}
.glyphicon-level-up:before {
  content: "\e204";
}
.glyphicon-copy:before {
  content: "\e205";
}
.glyphicon-paste:before {
  content: "\e206";
}
.glyphicon-alert:before {
  content: "\e209";
}
.glyphicon-equalizer:before {
  content: "\e210";
}
.glyphicon-king:before {
  content: "\e211";
}
.glyphicon-queen:before {
  content: "\e212";
}
.glyphicon-pawn:before {
  content: "\e213";
}
.glyphicon-bishop:before {
  content: "\e214";
}
.glyphicon-knight:before {
  content: "\e215";
}
.glyphicon-baby-formula:before {
  content: "\e216";
}
.glyphicon-tent:before {
  content: "\26fa";
}
.glyphicon-blackboard:before {
  content: "\e218";
}
.glyphicon-bed:before {
  content: "\e219";
}
.glyphicon-apple:before {
  content: "\f8ff";
}
.glyphicon-erase:before {
  content: "\e221";
}
.glyphicon-hourglass:before {
  content: "\231b";
}
.glyphicon-lamp:before {
  content: "\e223";
}
.glyphicon-duplicate:before {
  content: "\e224";
}
.glyphicon-piggy-bank:before {
  content: "\e225";
}
.glyphicon-scissors:before {
  content: "\e226";
}
.glyphicon-bitcoin:before {
  content: "\e227";
}
.glyphicon-btc:before {
  content: "\e227";
}
.glyphicon-xbt:before {
  content: "\e227";
}
.glyphicon-yen:before {
  content: "\00a5";
}
.glyphicon-jpy:before {
  content: "\00a5";
}
.glyphicon-ruble:before {
  content: "\20bd";
}
.glyphicon-rub:before {
  content: "\20bd";
}
.glyphicon-scale:before {
  content: "\e230";
}
.glyphicon-ice-lolly:before {
  content: "\e231";
}
.glyphicon-ice-lolly-tasted:before {
  content: "\e232";
}
.glyphicon-education:before {
  content: "\e233";
}
.glyphicon-option-horizontal:before {
  content: "\e234";
}
.glyphicon-option-vertical:before {
  content: "\e235";
}
.glyphicon-menu-hamburger:before {
  content: "\e236";
}
.glyphicon-modal-window:before {
  content: "\e237";
}
.glyphicon-oil:before {
  content: "\e238";
}
.glyphicon-grain:before {
  content: "\e239";
}
.glyphicon-sunglasses:before {
  content: "\e240";
}
.glyphicon-text-size:before {
  content: "\e241";
}
.glyphicon-text-color:before {
  content: "\e242";
}
.glyphicon-text-background:before {
  content: "\e243";
}
.glyphicon-object-align-top:before {
  content: "\e244";
}
.glyphicon-object-align-bottom:before {
  content: "\e245";
}
.glyphicon-object-align-horizontal:before {
  content: "\e246";
}
.glyphicon-object-align-left:before {
  content: "\e247";
}
.glyphicon-object-align-vertical:before {
  content: "\e248";
}
.glyphicon-object-align-right:before {
  content: "\e249";
}
.glyphicon-triangle-right:before {
  content: "\e250";
}
.glyphicon-triangle-left:before {
  content: "\e251";
}
.glyphicon-triangle-bottom:before {
  content: "\e252";
}
.glyphicon-triangle-top:before {
  content: "\e253";
}
.glyphicon-console:before {
  content: "\e254";
}
.glyphicon-superscript:before {
  content: "\e255";
}
.glyphicon-subscript:before {
  content: "\e256";
}
.glyphicon-menu-left:before {
  content: "\e257";
}
.glyphicon-menu-right:before {
  content: "\e258";
}
.glyphicon-menu-down:before {
  content: "\e259";
}
.glyphicon-menu-up:before {
  content: "\e260";
}
* {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
*:before,
*:after {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
html {
  font-size: 10px;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
}
body {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  line-height: 1.42857143;
  color: #000;
  background-color: #fff;
}
input,
button,
select,
textarea {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}
a {
  color: #337ab7;
  text-decoration: none;
}
a:hover,
a:focus {
  color: #23527c;
  text-decoration: underline;
}
a:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
figure {
  margin: 0;
}
img {
  vertical-align: middle;
}
.img-responsive,
.thumbnail > img,
.thumbnail a > img,
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  display: block;
  max-width: 100%;
  height: auto;
}
.img-rounded {
  border-radius: 3px;
}
.img-thumbnail {
  padding: 4px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: all 0.2s ease-in-out;
  -o-transition: all 0.2s ease-in-out;
  transition: all 0.2s ease-in-out;
  display: inline-block;
  max-width: 100%;
  height: auto;
}
.img-circle {
  border-radius: 50%;
}
hr {
  margin-top: 18px;
  margin-bottom: 18px;
  border: 0;
  border-top: 1px solid #eeeeee;
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  margin: -1px;
  padding: 0;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
[role="button"] {
  cursor: pointer;
}
h1,
h2,
h3,
h4,
h5,
h6,
.h1,
.h2,
.h3,
.h4,
.h5,
.h6 {
  font-family: inherit;
  font-weight: 500;
  line-height: 1.1;
  color: inherit;
}
h1 small,
h2 small,
h3 small,
h4 small,
h5 small,
h6 small,
.h1 small,
.h2 small,
.h3 small,
.h4 small,
.h5 small,
.h6 small,
h1 .small,
h2 .small,
h3 .small,
h4 .small,
h5 .small,
h6 .small,
.h1 .small,
.h2 .small,
.h3 .small,
.h4 .small,
.h5 .small,
.h6 .small {
  font-weight: normal;
  line-height: 1;
  color: #777777;
}
h1,
.h1,
h2,
.h2,
h3,
.h3 {
  margin-top: 18px;
  margin-bottom: 9px;
}
h1 small,
.h1 small,
h2 small,
.h2 small,
h3 small,
.h3 small,
h1 .small,
.h1 .small,
h2 .small,
.h2 .small,
h3 .small,
.h3 .small {
  font-size: 65%;
}
h4,
.h4,
h5,
.h5,
h6,
.h6 {
  margin-top: 9px;
  margin-bottom: 9px;
}
h4 small,
.h4 small,
h5 small,
.h5 small,
h6 small,
.h6 small,
h4 .small,
.h4 .small,
h5 .small,
.h5 .small,
h6 .small,
.h6 .small {
  font-size: 75%;
}
h1,
.h1 {
  font-size: 33px;
}
h2,
.h2 {
  font-size: 27px;
}
h3,
.h3 {
  font-size: 23px;
}
h4,
.h4 {
  font-size: 17px;
}
h5,
.h5 {
  font-size: 13px;
}
h6,
.h6 {
  font-size: 12px;
}
p {
  margin: 0 0 9px;
}
.lead {
  margin-bottom: 18px;
  font-size: 14px;
  font-weight: 300;
  line-height: 1.4;
}
@media (min-width: 768px) {
  .lead {
    font-size: 19.5px;
  }
}
small,
.small {
  font-size: 92%;
}
mark,
.mark {
  background-color: #fcf8e3;
  padding: .2em;
}
.text-left {
  text-align: left;
}
.text-right {
  text-align: right;
}
.text-center {
  text-align: center;
}
.text-justify {
  text-align: justify;
}
.text-nowrap {
  white-space: nowrap;
}
.text-lowercase {
  text-transform: lowercase;
}
.text-uppercase {
  text-transform: uppercase;
}
.text-capitalize {
  text-transform: capitalize;
}
.text-muted {
  color: #777777;
}
.text-primary {
  color: #337ab7;
}
a.text-primary:hover,
a.text-primary:focus {
  color: #286090;
}
.text-success {
  color: #3c763d;
}
a.text-success:hover,
a.text-success:focus {
  color: #2b542c;
}
.text-info {
  color: #31708f;
}
a.text-info:hover,
a.text-info:focus {
  color: #245269;
}
.text-warning {
  color: #8a6d3b;
}
a.text-warning:hover,
a.text-warning:focus {
  color: #66512c;
}
.text-danger {
  color: #a94442;
}
a.text-danger:hover,
a.text-danger:focus {
  color: #843534;
}
.bg-primary {
  color: #fff;
  background-color: #337ab7;
}
a.bg-primary:hover,
a.bg-primary:focus {
  background-color: #286090;
}
.bg-success {
  background-color: #dff0d8;
}
a.bg-success:hover,
a.bg-success:focus {
  background-color: #c1e2b3;
}
.bg-info {
  background-color: #d9edf7;
}
a.bg-info:hover,
a.bg-info:focus {
  background-color: #afd9ee;
}
.bg-warning {
  background-color: #fcf8e3;
}
a.bg-warning:hover,
a.bg-warning:focus {
  background-color: #f7ecb5;
}
.bg-danger {
  background-color: #f2dede;
}
a.bg-danger:hover,
a.bg-danger:focus {
  background-color: #e4b9b9;
}
.page-header {
  padding-bottom: 8px;
  margin: 36px 0 18px;
  border-bottom: 1px solid #eeeeee;
}
ul,
ol {
  margin-top: 0;
  margin-bottom: 9px;
}
ul ul,
ol ul,
ul ol,
ol ol {
  margin-bottom: 0;
}
.list-unstyled {
  padding-left: 0;
  list-style: none;
}
.list-inline {
  padding-left: 0;
  list-style: none;
  margin-left: -5px;
}
.list-inline > li {
  display: inline-block;
  padding-left: 5px;
  padding-right: 5px;
}
dl {
  margin-top: 0;
  margin-bottom: 18px;
}
dt,
dd {
  line-height: 1.42857143;
}
dt {
  font-weight: bold;
}
dd {
  margin-left: 0;
}
@media (min-width: 541px) {
  .dl-horizontal dt {
    float: left;
    width: 160px;
    clear: left;
    text-align: right;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
  }
  .dl-horizontal dd {
    margin-left: 180px;
  }
}
abbr[title],
abbr[data-original-title] {
  cursor: help;
  border-bottom: 1px dotted #777777;
}
.initialism {
  font-size: 90%;
  text-transform: uppercase;
}
blockquote {
  padding: 9px 18px;
  margin: 0 0 18px;
  font-size: inherit;
  border-left: 5px solid #eeeeee;
}
blockquote p:last-child,
blockquote ul:last-child,
blockquote ol:last-child {
  margin-bottom: 0;
}
blockquote footer,
blockquote small,
blockquote .small {
  display: block;
  font-size: 80%;
  line-height: 1.42857143;
  color: #777777;
}
blockquote footer:before,
blockquote small:before,
blockquote .small:before {
  content: '\2014 \00A0';
}
.blockquote-reverse,
blockquote.pull-right {
  padding-right: 15px;
  padding-left: 0;
  border-right: 5px solid #eeeeee;
  border-left: 0;
  text-align: right;
}
.blockquote-reverse footer:before,
blockquote.pull-right footer:before,
.blockquote-reverse small:before,
blockquote.pull-right small:before,
.blockquote-reverse .small:before,
blockquote.pull-right .small:before {
  content: '';
}
.blockquote-reverse footer:after,
blockquote.pull-right footer:after,
.blockquote-reverse small:after,
blockquote.pull-right small:after,
.blockquote-reverse .small:after,
blockquote.pull-right .small:after {
  content: '\00A0 \2014';
}
address {
  margin-bottom: 18px;
  font-style: normal;
  line-height: 1.42857143;
}
code,
kbd,
pre,
samp {
  font-family: monospace;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 2px;
}
kbd {
  padding: 2px 4px;
  font-size: 90%;
  color: #888;
  background-color: transparent;
  border-radius: 1px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
}
kbd kbd {
  padding: 0;
  font-size: 100%;
  font-weight: bold;
  box-shadow: none;
}
pre {
  display: block;
  padding: 8.5px;
  margin: 0 0 9px;
  font-size: 12px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  color: #333333;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 2px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  white-space: pre-wrap;
  background-color: transparent;
  border-radius: 0;
}
.pre-scrollable {
  max-height: 340px;
  overflow-y: scroll;
}
.container {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
@media (min-width: 768px) {
  .container {
    width: 768px;
  }
}
@media (min-width: 992px) {
  .container {
    width: 940px;
  }
}
@media (min-width: 1200px) {
  .container {
    width: 1140px;
  }
}
.container-fluid {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
.row {
  margin-left: 0px;
  margin-right: 0px;
}
.col-xs-1, .col-sm-1, .col-md-1, .col-lg-1, .col-xs-2, .col-sm-2, .col-md-2, .col-lg-2, .col-xs-3, .col-sm-3, .col-md-3, .col-lg-3, .col-xs-4, .col-sm-4, .col-md-4, .col-lg-4, .col-xs-5, .col-sm-5, .col-md-5, .col-lg-5, .col-xs-6, .col-sm-6, .col-md-6, .col-lg-6, .col-xs-7, .col-sm-7, .col-md-7, .col-lg-7, .col-xs-8, .col-sm-8, .col-md-8, .col-lg-8, .col-xs-9, .col-sm-9, .col-md-9, .col-lg-9, .col-xs-10, .col-sm-10, .col-md-10, .col-lg-10, .col-xs-11, .col-sm-11, .col-md-11, .col-lg-11, .col-xs-12, .col-sm-12, .col-md-12, .col-lg-12 {
  position: relative;
  min-height: 1px;
  padding-left: 0px;
  padding-right: 0px;
}
.col-xs-1, .col-xs-2, .col-xs-3, .col-xs-4, .col-xs-5, .col-xs-6, .col-xs-7, .col-xs-8, .col-xs-9, .col-xs-10, .col-xs-11, .col-xs-12 {
  float: left;
}
.col-xs-12 {
  width: 100%;
}
.col-xs-11 {
  width: 91.66666667%;
}
.col-xs-10 {
  width: 83.33333333%;
}
.col-xs-9 {
  width: 75%;
}
.col-xs-8 {
  width: 66.66666667%;
}
.col-xs-7 {
  width: 58.33333333%;
}
.col-xs-6 {
  width: 50%;
}
.col-xs-5 {
  width: 41.66666667%;
}
.col-xs-4 {
  width: 33.33333333%;
}
.col-xs-3 {
  width: 25%;
}
.col-xs-2 {
  width: 16.66666667%;
}
.col-xs-1 {
  width: 8.33333333%;
}
.col-xs-pull-12 {
  right: 100%;
}
.col-xs-pull-11 {
  right: 91.66666667%;
}
.col-xs-pull-10 {
  right: 83.33333333%;
}
.col-xs-pull-9 {
  right: 75%;
}
.col-xs-pull-8 {
  right: 66.66666667%;
}
.col-xs-pull-7 {
  right: 58.33333333%;
}
.col-xs-pull-6 {
  right: 50%;
}
.col-xs-pull-5 {
  right: 41.66666667%;
}
.col-xs-pull-4 {
  right: 33.33333333%;
}
.col-xs-pull-3 {
  right: 25%;
}
.col-xs-pull-2 {
  right: 16.66666667%;
}
.col-xs-pull-1 {
  right: 8.33333333%;
}
.col-xs-pull-0 {
  right: auto;
}
.col-xs-push-12 {
  left: 100%;
}
.col-xs-push-11 {
  left: 91.66666667%;
}
.col-xs-push-10 {
  left: 83.33333333%;
}
.col-xs-push-9 {
  left: 75%;
}
.col-xs-push-8 {
  left: 66.66666667%;
}
.col-xs-push-7 {
  left: 58.33333333%;
}
.col-xs-push-6 {
  left: 50%;
}
.col-xs-push-5 {
  left: 41.66666667%;
}
.col-xs-push-4 {
  left: 33.33333333%;
}
.col-xs-push-3 {
  left: 25%;
}
.col-xs-push-2 {
  left: 16.66666667%;
}
.col-xs-push-1 {
  left: 8.33333333%;
}
.col-xs-push-0 {
  left: auto;
}
.col-xs-offset-12 {
  margin-left: 100%;
}
.col-xs-offset-11 {
  margin-left: 91.66666667%;
}
.col-xs-offset-10 {
  margin-left: 83.33333333%;
}
.col-xs-offset-9 {
  margin-left: 75%;
}
.col-xs-offset-8 {
  margin-left: 66.66666667%;
}
.col-xs-offset-7 {
  margin-left: 58.33333333%;
}
.col-xs-offset-6 {
  margin-left: 50%;
}
.col-xs-offset-5 {
  margin-left: 41.66666667%;
}
.col-xs-offset-4 {
  margin-left: 33.33333333%;
}
.col-xs-offset-3 {
  margin-left: 25%;
}
.col-xs-offset-2 {
  margin-left: 16.66666667%;
}
.col-xs-offset-1 {
  margin-left: 8.33333333%;
}
.col-xs-offset-0 {
  margin-left: 0%;
}
@media (min-width: 768px) {
  .col-sm-1, .col-sm-2, .col-sm-3, .col-sm-4, .col-sm-5, .col-sm-6, .col-sm-7, .col-sm-8, .col-sm-9, .col-sm-10, .col-sm-11, .col-sm-12 {
    float: left;
  }
  .col-sm-12 {
    width: 100%;
  }
  .col-sm-11 {
    width: 91.66666667%;
  }
  .col-sm-10 {
    width: 83.33333333%;
  }
  .col-sm-9 {
    width: 75%;
  }
  .col-sm-8 {
    width: 66.66666667%;
  }
  .col-sm-7 {
    width: 58.33333333%;
  }
  .col-sm-6 {
    width: 50%;
  }
  .col-sm-5 {
    width: 41.66666667%;
  }
  .col-sm-4 {
    width: 33.33333333%;
  }
  .col-sm-3 {
    width: 25%;
  }
  .col-sm-2 {
    width: 16.66666667%;
  }
  .col-sm-1 {
    width: 8.33333333%;
  }
  .col-sm-pull-12 {
    right: 100%;
  }
  .col-sm-pull-11 {
    right: 91.66666667%;
  }
  .col-sm-pull-10 {
    right: 83.33333333%;
  }
  .col-sm-pull-9 {
    right: 75%;
  }
  .col-sm-pull-8 {
    right: 66.66666667%;
  }
  .col-sm-pull-7 {
    right: 58.33333333%;
  }
  .col-sm-pull-6 {
    right: 50%;
  }
  .col-sm-pull-5 {
    right: 41.66666667%;
  }
  .col-sm-pull-4 {
    right: 33.33333333%;
  }
  .col-sm-pull-3 {
    right: 25%;
  }
  .col-sm-pull-2 {
    right: 16.66666667%;
  }
  .col-sm-pull-1 {
    right: 8.33333333%;
  }
  .col-sm-pull-0 {
    right: auto;
  }
  .col-sm-push-12 {
    left: 100%;
  }
  .col-sm-push-11 {
    left: 91.66666667%;
  }
  .col-sm-push-10 {
    left: 83.33333333%;
  }
  .col-sm-push-9 {
    left: 75%;
  }
  .col-sm-push-8 {
    left: 66.66666667%;
  }
  .col-sm-push-7 {
    left: 58.33333333%;
  }
  .col-sm-push-6 {
    left: 50%;
  }
  .col-sm-push-5 {
    left: 41.66666667%;
  }
  .col-sm-push-4 {
    left: 33.33333333%;
  }
  .col-sm-push-3 {
    left: 25%;
  }
  .col-sm-push-2 {
    left: 16.66666667%;
  }
  .col-sm-push-1 {
    left: 8.33333333%;
  }
  .col-sm-push-0 {
    left: auto;
  }
  .col-sm-offset-12 {
    margin-left: 100%;
  }
  .col-sm-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-sm-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-sm-offset-9 {
    margin-left: 75%;
  }
  .col-sm-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-sm-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-sm-offset-6 {
    margin-left: 50%;
  }
  .col-sm-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-sm-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-sm-offset-3 {
    margin-left: 25%;
  }
  .col-sm-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-sm-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-sm-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 992px) {
  .col-md-1, .col-md-2, .col-md-3, .col-md-4, .col-md-5, .col-md-6, .col-md-7, .col-md-8, .col-md-9, .col-md-10, .col-md-11, .col-md-12 {
    float: left;
  }
  .col-md-12 {
    width: 100%;
  }
  .col-md-11 {
    width: 91.66666667%;
  }
  .col-md-10 {
    width: 83.33333333%;
  }
  .col-md-9 {
    width: 75%;
  }
  .col-md-8 {
    width: 66.66666667%;
  }
  .col-md-7 {
    width: 58.33333333%;
  }
  .col-md-6 {
    width: 50%;
  }
  .col-md-5 {
    width: 41.66666667%;
  }
  .col-md-4 {
    width: 33.33333333%;
  }
  .col-md-3 {
    width: 25%;
  }
  .col-md-2 {
    width: 16.66666667%;
  }
  .col-md-1 {
    width: 8.33333333%;
  }
  .col-md-pull-12 {
    right: 100%;
  }
  .col-md-pull-11 {
    right: 91.66666667%;
  }
  .col-md-pull-10 {
    right: 83.33333333%;
  }
  .col-md-pull-9 {
    right: 75%;
  }
  .col-md-pull-8 {
    right: 66.66666667%;
  }
  .col-md-pull-7 {
    right: 58.33333333%;
  }
  .col-md-pull-6 {
    right: 50%;
  }
  .col-md-pull-5 {
    right: 41.66666667%;
  }
  .col-md-pull-4 {
    right: 33.33333333%;
  }
  .col-md-pull-3 {
    right: 25%;
  }
  .col-md-pull-2 {
    right: 16.66666667%;
  }
  .col-md-pull-1 {
    right: 8.33333333%;
  }
  .col-md-pull-0 {
    right: auto;
  }
  .col-md-push-12 {
    left: 100%;
  }
  .col-md-push-11 {
    left: 91.66666667%;
  }
  .col-md-push-10 {
    left: 83.33333333%;
  }
  .col-md-push-9 {
    left: 75%;
  }
  .col-md-push-8 {
    left: 66.66666667%;
  }
  .col-md-push-7 {
    left: 58.33333333%;
  }
  .col-md-push-6 {
    left: 50%;
  }
  .col-md-push-5 {
    left: 41.66666667%;
  }
  .col-md-push-4 {
    left: 33.33333333%;
  }
  .col-md-push-3 {
    left: 25%;
  }
  .col-md-push-2 {
    left: 16.66666667%;
  }
  .col-md-push-1 {
    left: 8.33333333%;
  }
  .col-md-push-0 {
    left: auto;
  }
  .col-md-offset-12 {
    margin-left: 100%;
  }
  .col-md-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-md-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-md-offset-9 {
    margin-left: 75%;
  }
  .col-md-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-md-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-md-offset-6 {
    margin-left: 50%;
  }
  .col-md-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-md-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-md-offset-3 {
    margin-left: 25%;
  }
  .col-md-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-md-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-md-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 1200px) {
  .col-lg-1, .col-lg-2, .col-lg-3, .col-lg-4, .col-lg-5, .col-lg-6, .col-lg-7, .col-lg-8, .col-lg-9, .col-lg-10, .col-lg-11, .col-lg-12 {
    float: left;
  }
  .col-lg-12 {
    width: 100%;
  }
  .col-lg-11 {
    width: 91.66666667%;
  }
  .col-lg-10 {
    width: 83.33333333%;
  }
  .col-lg-9 {
    width: 75%;
  }
  .col-lg-8 {
    width: 66.66666667%;
  }
  .col-lg-7 {
    width: 58.33333333%;
  }
  .col-lg-6 {
    width: 50%;
  }
  .col-lg-5 {
    width: 41.66666667%;
  }
  .col-lg-4 {
    width: 33.33333333%;
  }
  .col-lg-3 {
    width: 25%;
  }
  .col-lg-2 {
    width: 16.66666667%;
  }
  .col-lg-1 {
    width: 8.33333333%;
  }
  .col-lg-pull-12 {
    right: 100%;
  }
  .col-lg-pull-11 {
    right: 91.66666667%;
  }
  .col-lg-pull-10 {
    right: 83.33333333%;
  }
  .col-lg-pull-9 {
    right: 75%;
  }
  .col-lg-pull-8 {
    right: 66.66666667%;
  }
  .col-lg-pull-7 {
    right: 58.33333333%;
  }
  .col-lg-pull-6 {
    right: 50%;
  }
  .col-lg-pull-5 {
    right: 41.66666667%;
  }
  .col-lg-pull-4 {
    right: 33.33333333%;
  }
  .col-lg-pull-3 {
    right: 25%;
  }
  .col-lg-pull-2 {
    right: 16.66666667%;
  }
  .col-lg-pull-1 {
    right: 8.33333333%;
  }
  .col-lg-pull-0 {
    right: auto;
  }
  .col-lg-push-12 {
    left: 100%;
  }
  .col-lg-push-11 {
    left: 91.66666667%;
  }
  .col-lg-push-10 {
    left: 83.33333333%;
  }
  .col-lg-push-9 {
    left: 75%;
  }
  .col-lg-push-8 {
    left: 66.66666667%;
  }
  .col-lg-push-7 {
    left: 58.33333333%;
  }
  .col-lg-push-6 {
    left: 50%;
  }
  .col-lg-push-5 {
    left: 41.66666667%;
  }
  .col-lg-push-4 {
    left: 33.33333333%;
  }
  .col-lg-push-3 {
    left: 25%;
  }
  .col-lg-push-2 {
    left: 16.66666667%;
  }
  .col-lg-push-1 {
    left: 8.33333333%;
  }
  .col-lg-push-0 {
    left: auto;
  }
  .col-lg-offset-12 {
    margin-left: 100%;
  }
  .col-lg-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-lg-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-lg-offset-9 {
    margin-left: 75%;
  }
  .col-lg-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-lg-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-lg-offset-6 {
    margin-left: 50%;
  }
  .col-lg-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-lg-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-lg-offset-3 {
    margin-left: 25%;
  }
  .col-lg-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-lg-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-lg-offset-0 {
    margin-left: 0%;
  }
}
table {
  background-color: transparent;
}
caption {
  padding-top: 8px;
  padding-bottom: 8px;
  color: #777777;
  text-align: left;
}
th {
  text-align: left;
}
.table {
  width: 100%;
  max-width: 100%;
  margin-bottom: 18px;
}
.table > thead > tr > th,
.table > tbody > tr > th,
.table > tfoot > tr > th,
.table > thead > tr > td,
.table > tbody > tr > td,
.table > tfoot > tr > td {
  padding: 8px;
  line-height: 1.42857143;
  vertical-align: top;
  border-top: 1px solid #ddd;
}
.table > thead > tr > th {
  vertical-align: bottom;
  border-bottom: 2px solid #ddd;
}
.table > caption + thead > tr:first-child > th,
.table > colgroup + thead > tr:first-child > th,
.table > thead:first-child > tr:first-child > th,
.table > caption + thead > tr:first-child > td,
.table > colgroup + thead > tr:first-child > td,
.table > thead:first-child > tr:first-child > td {
  border-top: 0;
}
.table > tbody + tbody {
  border-top: 2px solid #ddd;
}
.table .table {
  background-color: #fff;
}
.table-condensed > thead > tr > th,
.table-condensed > tbody > tr > th,
.table-condensed > tfoot > tr > th,
.table-condensed > thead > tr > td,
.table-condensed > tbody > tr > td,
.table-condensed > tfoot > tr > td {
  padding: 5px;
}
.table-bordered {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > tbody > tr > th,
.table-bordered > tfoot > tr > th,
.table-bordered > thead > tr > td,
.table-bordered > tbody > tr > td,
.table-bordered > tfoot > tr > td {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > thead > tr > td {
  border-bottom-width: 2px;
}
.table-striped > tbody > tr:nth-of-type(odd) {
  background-color: #f9f9f9;
}
.table-hover > tbody > tr:hover {
  background-color: #f5f5f5;
}
table col[class*="col-"] {
  position: static;
  float: none;
  display: table-column;
}
table td[class*="col-"],
table th[class*="col-"] {
  position: static;
  float: none;
  display: table-cell;
}
.table > thead > tr > td.active,
.table > tbody > tr > td.active,
.table > tfoot > tr > td.active,
.table > thead > tr > th.active,
.table > tbody > tr > th.active,
.table > tfoot > tr > th.active,
.table > thead > tr.active > td,
.table > tbody > tr.active > td,
.table > tfoot > tr.active > td,
.table > thead > tr.active > th,
.table > tbody > tr.active > th,
.table > tfoot > tr.active > th {
  background-color: #f5f5f5;
}
.table-hover > tbody > tr > td.active:hover,
.table-hover > tbody > tr > th.active:hover,
.table-hover > tbody > tr.active:hover > td,
.table-hover > tbody > tr:hover > .active,
.table-hover > tbody > tr.active:hover > th {
  background-color: #e8e8e8;
}
.table > thead > tr > td.success,
.table > tbody > tr > td.success,
.table > tfoot > tr > td.success,
.table > thead > tr > th.success,
.table > tbody > tr > th.success,
.table > tfoot > tr > th.success,
.table > thead > tr.success > td,
.table > tbody > tr.success > td,
.table > tfoot > tr.success > td,
.table > thead > tr.success > th,
.table > tbody > tr.success > th,
.table > tfoot > tr.success > th {
  background-color: #dff0d8;
}
.table-hover > tbody > tr > td.success:hover,
.table-hover > tbody > tr > th.success:hover,
.table-hover > tbody > tr.success:hover > td,
.table-hover > tbody > tr:hover > .success,
.table-hover > tbody > tr.success:hover > th {
  background-color: #d0e9c6;
}
.table > thead > tr > td.info,
.table > tbody > tr > td.info,
.table > tfoot > tr > td.info,
.table > thead > tr > th.info,
.table > tbody > tr > th.info,
.table > tfoot > tr > th.info,
.table > thead > tr.info > td,
.table > tbody > tr.info > td,
.table > tfoot > tr.info > td,
.table > thead > tr.info > th,
.table > tbody > tr.info > th,
.table > tfoot > tr.info > th {
  background-color: #d9edf7;
}
.table-hover > tbody > tr > td.info:hover,
.table-hover > tbody > tr > th.info:hover,
.table-hover > tbody > tr.info:hover > td,
.table-hover > tbody > tr:hover > .info,
.table-hover > tbody > tr.info:hover > th {
  background-color: #c4e3f3;
}
.table > thead > tr > td.warning,
.table > tbody > tr > td.warning,
.table > tfoot > tr > td.warning,
.table > thead > tr > th.warning,
.table > tbody > tr > th.warning,
.table > tfoot > tr > th.warning,
.table > thead > tr.warning > td,
.table > tbody > tr.warning > td,
.table > tfoot > tr.warning > td,
.table > thead > tr.warning > th,
.table > tbody > tr.warning > th,
.table > tfoot > tr.warning > th {
  background-color: #fcf8e3;
}
.table-hover > tbody > tr > td.warning:hover,
.table-hover > tbody > tr > th.warning:hover,
.table-hover > tbody > tr.warning:hover > td,
.table-hover > tbody > tr:hover > .warning,
.table-hover > tbody > tr.warning:hover > th {
  background-color: #faf2cc;
}
.table > thead > tr > td.danger,
.table > tbody > tr > td.danger,
.table > tfoot > tr > td.danger,
.table > thead > tr > th.danger,
.table > tbody > tr > th.danger,
.table > tfoot > tr > th.danger,
.table > thead > tr.danger > td,
.table > tbody > tr.danger > td,
.table > tfoot > tr.danger > td,
.table > thead > tr.danger > th,
.table > tbody > tr.danger > th,
.table > tfoot > tr.danger > th {
  background-color: #f2dede;
}
.table-hover > tbody > tr > td.danger:hover,
.table-hover > tbody > tr > th.danger:hover,
.table-hover > tbody > tr.danger:hover > td,
.table-hover > tbody > tr:hover > .danger,
.table-hover > tbody > tr.danger:hover > th {
  background-color: #ebcccc;
}
.table-responsive {
  overflow-x: auto;
  min-height: 0.01%;
}
@media screen and (max-width: 767px) {
  .table-responsive {
    width: 100%;
    margin-bottom: 13.5px;
    overflow-y: hidden;
    -ms-overflow-style: -ms-autohiding-scrollbar;
    border: 1px solid #ddd;
  }
  .table-responsive > .table {
    margin-bottom: 0;
  }
  .table-responsive > .table > thead > tr > th,
  .table-responsive > .table > tbody > tr > th,
  .table-responsive > .table > tfoot > tr > th,
  .table-responsive > .table > thead > tr > td,
  .table-responsive > .table > tbody > tr > td,
  .table-responsive > .table > tfoot > tr > td {
    white-space: nowrap;
  }
  .table-responsive > .table-bordered {
    border: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:first-child,
  .table-responsive > .table-bordered > tbody > tr > th:first-child,
  .table-responsive > .table-bordered > tfoot > tr > th:first-child,
  .table-responsive > .table-bordered > thead > tr > td:first-child,
  .table-responsive > .table-bordered > tbody > tr > td:first-child,
  .table-responsive > .table-bordered > tfoot > tr > td:first-child {
    border-left: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:last-child,
  .table-responsive > .table-bordered > tbody > tr > th:last-child,
  .table-responsive > .table-bordered > tfoot > tr > th:last-child,
  .table-responsive > .table-bordered > thead > tr > td:last-child,
  .table-responsive > .table-bordered > tbody > tr > td:last-child,
  .table-responsive > .table-bordered > tfoot > tr > td:last-child {
    border-right: 0;
  }
  .table-responsive > .table-bordered > tbody > tr:last-child > th,
  .table-responsive > .table-bordered > tfoot > tr:last-child > th,
  .table-responsive > .table-bordered > tbody > tr:last-child > td,
  .table-responsive > .table-bordered > tfoot > tr:last-child > td {
    border-bottom: 0;
  }
}
fieldset {
  padding: 0;
  margin: 0;
  border: 0;
  min-width: 0;
}
legend {
  display: block;
  width: 100%;
  padding: 0;
  margin-bottom: 18px;
  font-size: 19.5px;
  line-height: inherit;
  color: #333333;
  border: 0;
  border-bottom: 1px solid #e5e5e5;
}
label {
  display: inline-block;
  max-width: 100%;
  margin-bottom: 5px;
  font-weight: bold;
}
input[type="search"] {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
input[type="radio"],
input[type="checkbox"] {
  margin: 4px 0 0;
  margin-top: 1px \9;
  line-height: normal;
}
input[type="file"] {
  display: block;
}
input[type="range"] {
  display: block;
  width: 100%;
}
select[multiple],
select[size] {
  height: auto;
}
input[type="file"]:focus,
input[type="radio"]:focus,
input[type="checkbox"]:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
output {
  display: block;
  padding-top: 7px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
}
.form-control {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
}
.form-control:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.form-control::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.form-control:-ms-input-placeholder {
  color: #999;
}
.form-control::-webkit-input-placeholder {
  color: #999;
}
.form-control::-ms-expand {
  border: 0;
  background-color: transparent;
}
.form-control[disabled],
.form-control[readonly],
fieldset[disabled] .form-control {
  background-color: #eeeeee;
  opacity: 1;
}
.form-control[disabled],
fieldset[disabled] .form-control {
  cursor: not-allowed;
}
textarea.form-control {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: none;
}
@media screen and (-webkit-min-device-pixel-ratio: 0) {
  input[type="date"].form-control,
  input[type="time"].form-control,
  input[type="datetime-local"].form-control,
  input[type="month"].form-control {
    line-height: 32px;
  }
  input[type="date"].input-sm,
  input[type="time"].input-sm,
  input[type="datetime-local"].input-sm,
  input[type="month"].input-sm,
  .input-group-sm input[type="date"],
  .input-group-sm input[type="time"],
  .input-group-sm input[type="datetime-local"],
  .input-group-sm input[type="month"] {
    line-height: 30px;
  }
  input[type="date"].input-lg,
  input[type="time"].input-lg,
  input[type="datetime-local"].input-lg,
  input[type="month"].input-lg,
  .input-group-lg input[type="date"],
  .input-group-lg input[type="time"],
  .input-group-lg input[type="datetime-local"],
  .input-group-lg input[type="month"] {
    line-height: 45px;
  }
}
.form-group {
  margin-bottom: 15px;
}
.radio,
.checkbox {
  position: relative;
  display: block;
  margin-top: 10px;
  margin-bottom: 10px;
}
.radio label,
.checkbox label {
  min-height: 18px;
  padding-left: 20px;
  margin-bottom: 0;
  font-weight: normal;
  cursor: pointer;
}
.radio input[type="radio"],
.radio-inline input[type="radio"],
.checkbox input[type="checkbox"],
.checkbox-inline input[type="checkbox"] {
  position: absolute;
  margin-left: -20px;
  margin-top: 4px \9;
}
.radio + .radio,
.checkbox + .checkbox {
  margin-top: -5px;
}
.radio-inline,
.checkbox-inline {
  position: relative;
  display: inline-block;
  padding-left: 20px;
  margin-bottom: 0;
  vertical-align: middle;
  font-weight: normal;
  cursor: pointer;
}
.radio-inline + .radio-inline,
.checkbox-inline + .checkbox-inline {
  margin-top: 0;
  margin-left: 10px;
}
input[type="radio"][disabled],
input[type="checkbox"][disabled],
input[type="radio"].disabled,
input[type="checkbox"].disabled,
fieldset[disabled] input[type="radio"],
fieldset[disabled] input[type="checkbox"] {
  cursor: not-allowed;
}
.radio-inline.disabled,
.checkbox-inline.disabled,
fieldset[disabled] .radio-inline,
fieldset[disabled] .checkbox-inline {
  cursor: not-allowed;
}
.radio.disabled label,
.checkbox.disabled label,
fieldset[disabled] .radio label,
fieldset[disabled] .checkbox label {
  cursor: not-allowed;
}
.form-control-static {
  padding-top: 7px;
  padding-bottom: 7px;
  margin-bottom: 0;
  min-height: 31px;
}
.form-control-static.input-lg,
.form-control-static.input-sm {
  padding-left: 0;
  padding-right: 0;
}
.input-sm {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-sm {
  height: 30px;
  line-height: 30px;
}
textarea.input-sm,
select[multiple].input-sm {
  height: auto;
}
.form-group-sm .form-control {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.form-group-sm select.form-control {
  height: 30px;
  line-height: 30px;
}
.form-group-sm textarea.form-control,
.form-group-sm select[multiple].form-control {
  height: auto;
}
.form-group-sm .form-control-static {
  height: 30px;
  min-height: 30px;
  padding: 6px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.input-lg {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-lg {
  height: 45px;
  line-height: 45px;
}
textarea.input-lg,
select[multiple].input-lg {
  height: auto;
}
.form-group-lg .form-control {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.form-group-lg select.form-control {
  height: 45px;
  line-height: 45px;
}
.form-group-lg textarea.form-control,
.form-group-lg select[multiple].form-control {
  height: auto;
}
.form-group-lg .form-control-static {
  height: 45px;
  min-height: 35px;
  padding: 11px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.has-feedback {
  position: relative;
}
.has-feedback .form-control {
  padding-right: 40px;
}
.form-control-feedback {
  position: absolute;
  top: 0;
  right: 0;
  z-index: 2;
  display: block;
  width: 32px;
  height: 32px;
  line-height: 32px;
  text-align: center;
  pointer-events: none;
}
.input-lg + .form-control-feedback,
.input-group-lg + .form-control-feedback,
.form-group-lg .form-control + .form-control-feedback {
  width: 45px;
  height: 45px;
  line-height: 45px;
}
.input-sm + .form-control-feedback,
.input-group-sm + .form-control-feedback,
.form-group-sm .form-control + .form-control-feedback {
  width: 30px;
  height: 30px;
  line-height: 30px;
}
.has-success .help-block,
.has-success .control-label,
.has-success .radio,
.has-success .checkbox,
.has-success .radio-inline,
.has-success .checkbox-inline,
.has-success.radio label,
.has-success.checkbox label,
.has-success.radio-inline label,
.has-success.checkbox-inline label {
  color: #3c763d;
}
.has-success .form-control {
  border-color: #3c763d;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-success .form-control:focus {
  border-color: #2b542c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
}
.has-success .input-group-addon {
  color: #3c763d;
  border-color: #3c763d;
  background-color: #dff0d8;
}
.has-success .form-control-feedback {
  color: #3c763d;
}
.has-warning .help-block,
.has-warning .control-label,
.has-warning .radio,
.has-warning .checkbox,
.has-warning .radio-inline,
.has-warning .checkbox-inline,
.has-warning.radio label,
.has-warning.checkbox label,
.has-warning.radio-inline label,
.has-warning.checkbox-inline label {
  color: #8a6d3b;
}
.has-warning .form-control {
  border-color: #8a6d3b;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-warning .form-control:focus {
  border-color: #66512c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
}
.has-warning .input-group-addon {
  color: #8a6d3b;
  border-color: #8a6d3b;
  background-color: #fcf8e3;
}
.has-warning .form-control-feedback {
  color: #8a6d3b;
}
.has-error .help-block,
.has-error .control-label,
.has-error .radio,
.has-error .checkbox,
.has-error .radio-inline,
.has-error .checkbox-inline,
.has-error.radio label,
.has-error.checkbox label,
.has-error.radio-inline label,
.has-error.checkbox-inline label {
  color: #a94442;
}
.has-error .form-control {
  border-color: #a94442;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-error .form-control:focus {
  border-color: #843534;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
}
.has-error .input-group-addon {
  color: #a94442;
  border-color: #a94442;
  background-color: #f2dede;
}
.has-error .form-control-feedback {
  color: #a94442;
}
.has-feedback label ~ .form-control-feedback {
  top: 23px;
}
.has-feedback label.sr-only ~ .form-control-feedback {
  top: 0;
}
.help-block {
  display: block;
  margin-top: 5px;
  margin-bottom: 10px;
  color: #404040;
}
@media (min-width: 768px) {
  .form-inline .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .form-inline .form-control-static {
    display: inline-block;
  }
  .form-inline .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .form-inline .input-group .input-group-addon,
  .form-inline .input-group .input-group-btn,
  .form-inline .input-group .form-control {
    width: auto;
  }
  .form-inline .input-group > .form-control {
    width: 100%;
  }
  .form-inline .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio,
  .form-inline .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio label,
  .form-inline .checkbox label {
    padding-left: 0;
  }
  .form-inline .radio input[type="radio"],
  .form-inline .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .form-inline .has-feedback .form-control-feedback {
    top: 0;
  }
}
.form-horizontal .radio,
.form-horizontal .checkbox,
.form-horizontal .radio-inline,
.form-horizontal .checkbox-inline {
  margin-top: 0;
  margin-bottom: 0;
  padding-top: 7px;
}
.form-horizontal .radio,
.form-horizontal .checkbox {
  min-height: 25px;
}
.form-horizontal .form-group {
  margin-left: 0px;
  margin-right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .control-label {
    text-align: right;
    margin-bottom: 0;
    padding-top: 7px;
  }
}
.form-horizontal .has-feedback .form-control-feedback {
  right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .form-group-lg .control-label {
    padding-top: 11px;
    font-size: 17px;
  }
}
@media (min-width: 768px) {
  .form-horizontal .form-group-sm .control-label {
    padding-top: 6px;
    font-size: 12px;
  }
}
.btn {
  display: inline-block;
  margin-bottom: 0;
  font-weight: normal;
  text-align: center;
  vertical-align: middle;
  touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  white-space: nowrap;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  border-radius: 2px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #333;
  text-decoration: none;
}
.btn:active,
.btn.active {
  outline: 0;
  background-image: none;
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  opacity: 0.65;
  filter: alpha(opacity=65);
  -webkit-box-shadow: none;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.btn-default:focus,
.btn-default.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.btn-default:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active:hover,
.btn-default.active:hover,
.open > .dropdown-toggle.btn-default:hover,
.btn-default:active:focus,
.btn-default.active:focus,
.open > .dropdown-toggle.btn-default:focus,
.btn-default:active.focus,
.btn-default.active.focus,
.open > .dropdown-toggle.btn-default.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  background-image: none;
}
.btn-default.disabled:hover,
.btn-default[disabled]:hover,
fieldset[disabled] .btn-default:hover,
.btn-default.disabled:focus,
.btn-default[disabled]:focus,
fieldset[disabled] .btn-default:focus,
.btn-default.disabled.focus,
.btn-default[disabled].focus,
fieldset[disabled] .btn-default.focus {
  background-color: #fff;
  border-color: #ccc;
}
.btn-default .badge {
  color: #fff;
  background-color: #333;
}
.btn-primary {
  color: #fff;
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary:focus,
.btn-primary.focus {
  color: #fff;
  background-color: #286090;
  border-color: #122b40;
}
.btn-primary:hover {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active:hover,
.btn-primary.active:hover,
.open > .dropdown-toggle.btn-primary:hover,
.btn-primary:active:focus,
.btn-primary.active:focus,
.open > .dropdown-toggle.btn-primary:focus,
.btn-primary:active.focus,
.btn-primary.active.focus,
.open > .dropdown-toggle.btn-primary.focus {
  color: #fff;
  background-color: #204d74;
  border-color: #122b40;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  background-image: none;
}
.btn-primary.disabled:hover,
.btn-primary[disabled]:hover,
fieldset[disabled] .btn-primary:hover,
.btn-primary.disabled:focus,
.btn-primary[disabled]:focus,
fieldset[disabled] .btn-primary:focus,
.btn-primary.disabled.focus,
.btn-primary[disabled].focus,
fieldset[disabled] .btn-primary.focus {
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary .badge {
  color: #337ab7;
  background-color: #fff;
}
.btn-success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success:focus,
.btn-success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.btn-success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active:hover,
.btn-success.active:hover,
.open > .dropdown-toggle.btn-success:hover,
.btn-success:active:focus,
.btn-success.active:focus,
.open > .dropdown-toggle.btn-success:focus,
.btn-success:active.focus,
.btn-success.active.focus,
.open > .dropdown-toggle.btn-success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  background-image: none;
}
.btn-success.disabled:hover,
.btn-success[disabled]:hover,
fieldset[disabled] .btn-success:hover,
.btn-success.disabled:focus,
.btn-success[disabled]:focus,
fieldset[disabled] .btn-success:focus,
.btn-success.disabled.focus,
.btn-success[disabled].focus,
fieldset[disabled] .btn-success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.btn-info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info:focus,
.btn-info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.btn-info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active:hover,
.btn-info.active:hover,
.open > .dropdown-toggle.btn-info:hover,
.btn-info:active:focus,
.btn-info.active:focus,
.open > .dropdown-toggle.btn-info:focus,
.btn-info:active.focus,
.btn-info.active.focus,
.open > .dropdown-toggle.btn-info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  background-image: none;
}
.btn-info.disabled:hover,
.btn-info[disabled]:hover,
fieldset[disabled] .btn-info:hover,
.btn-info.disabled:focus,
.btn-info[disabled]:focus,
fieldset[disabled] .btn-info:focus,
.btn-info.disabled.focus,
.btn-info[disabled].focus,
fieldset[disabled] .btn-info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.btn-warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning:focus,
.btn-warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.btn-warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active:hover,
.btn-warning.active:hover,
.open > .dropdown-toggle.btn-warning:hover,
.btn-warning:active:focus,
.btn-warning.active:focus,
.open > .dropdown-toggle.btn-warning:focus,
.btn-warning:active.focus,
.btn-warning.active.focus,
.open > .dropdown-toggle.btn-warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  background-image: none;
}
.btn-warning.disabled:hover,
.btn-warning[disabled]:hover,
fieldset[disabled] .btn-warning:hover,
.btn-warning.disabled:focus,
.btn-warning[disabled]:focus,
fieldset[disabled] .btn-warning:focus,
.btn-warning.disabled.focus,
.btn-warning[disabled].focus,
fieldset[disabled] .btn-warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.btn-danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger:focus,
.btn-danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.btn-danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active:hover,
.btn-danger.active:hover,
.open > .dropdown-toggle.btn-danger:hover,
.btn-danger:active:focus,
.btn-danger.active:focus,
.open > .dropdown-toggle.btn-danger:focus,
.btn-danger:active.focus,
.btn-danger.active.focus,
.open > .dropdown-toggle.btn-danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  background-image: none;
}
.btn-danger.disabled:hover,
.btn-danger[disabled]:hover,
fieldset[disabled] .btn-danger:hover,
.btn-danger.disabled:focus,
.btn-danger[disabled]:focus,
fieldset[disabled] .btn-danger:focus,
.btn-danger.disabled.focus,
.btn-danger[disabled].focus,
fieldset[disabled] .btn-danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger .badge {
  color: #d9534f;
  background-color: #fff;
}
.btn-link {
  color: #337ab7;
  font-weight: normal;
  border-radius: 0;
}
.btn-link,
.btn-link:active,
.btn-link.active,
.btn-link[disabled],
fieldset[disabled] .btn-link {
  background-color: transparent;
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn-link,
.btn-link:hover,
.btn-link:focus,
.btn-link:active {
  border-color: transparent;
}
.btn-link:hover,
.btn-link:focus {
  color: #23527c;
  text-decoration: underline;
  background-color: transparent;
}
.btn-link[disabled]:hover,
fieldset[disabled] .btn-link:hover,
.btn-link[disabled]:focus,
fieldset[disabled] .btn-link:focus {
  color: #777777;
  text-decoration: none;
}
.btn-lg,
.btn-group-lg > .btn {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.btn-sm,
.btn-group-sm > .btn {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-xs,
.btn-group-xs > .btn {
  padding: 1px 5px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-block {
  display: block;
  width: 100%;
}
.btn-block + .btn-block {
  margin-top: 5px;
}
input[type="submit"].btn-block,
input[type="reset"].btn-block,
input[type="button"].btn-block {
  width: 100%;
}
.fade {
  opacity: 0;
  -webkit-transition: opacity 0.15s linear;
  -o-transition: opacity 0.15s linear;
  transition: opacity 0.15s linear;
}
.fade.in {
  opacity: 1;
}
.collapse {
  display: none;
}
.collapse.in {
  display: block;
}
tr.collapse.in {
  display: table-row;
}
tbody.collapse.in {
  display: table-row-group;
}
.collapsing {
  position: relative;
  height: 0;
  overflow: hidden;
  -webkit-transition-property: height, visibility;
  transition-property: height, visibility;
  -webkit-transition-duration: 0.35s;
  transition-duration: 0.35s;
  -webkit-transition-timing-function: ease;
  transition-timing-function: ease;
}
.caret {
  display: inline-block;
  width: 0;
  height: 0;
  margin-left: 2px;
  vertical-align: middle;
  border-top: 4px dashed;
  border-top: 4px solid \9;
  border-right: 4px solid transparent;
  border-left: 4px solid transparent;
}
.dropup,
.dropdown {
  position: relative;
}
.dropdown-toggle:focus {
  outline: 0;
}
.dropdown-menu {
  position: absolute;
  top: 100%;
  left: 0;
  z-index: 1000;
  display: none;
  float: left;
  min-width: 160px;
  padding: 5px 0;
  margin: 2px 0 0;
  list-style: none;
  font-size: 13px;
  text-align: left;
  background-color: #fff;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.15);
  border-radius: 2px;
  -webkit-box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  background-clip: padding-box;
}
.dropdown-menu.pull-right {
  right: 0;
  left: auto;
}
.dropdown-menu .divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: normal;
  line-height: 1.42857143;
  color: #333333;
  white-space: nowrap;
}
.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  text-decoration: none;
  color: #262626;
  background-color: #f5f5f5;
}
.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #fff;
  text-decoration: none;
  outline: 0;
  background-color: #337ab7;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #777777;
}
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
  cursor: not-allowed;
}
.open > .dropdown-menu {
  display: block;
}
.open > a {
  outline: 0;
}
.dropdown-menu-right {
  left: auto;
  right: 0;
}
.dropdown-menu-left {
  left: 0;
  right: auto;
}
.dropdown-header {
  display: block;
  padding: 3px 20px;
  font-size: 12px;
  line-height: 1.42857143;
  color: #777777;
  white-space: nowrap;
}
.dropdown-backdrop {
  position: fixed;
  left: 0;
  right: 0;
  bottom: 0;
  top: 0;
  z-index: 990;
}
.pull-right > .dropdown-menu {
  right: 0;
  left: auto;
}
.dropup .caret,
.navbar-fixed-bottom .dropdown .caret {
  border-top: 0;
  border-bottom: 4px dashed;
  border-bottom: 4px solid \9;
  content: "";
}
.dropup .dropdown-menu,
.navbar-fixed-bottom .dropdown .dropdown-menu {
  top: auto;
  bottom: 100%;
  margin-bottom: 2px;
}
@media (min-width: 541px) {
  .navbar-right .dropdown-menu {
    left: auto;
    right: 0;
  }
  .navbar-right .dropdown-menu-left {
    left: 0;
    right: auto;
  }
}
.btn-group,
.btn-group-vertical {
  position: relative;
  display: inline-block;
  vertical-align: middle;
}
.btn-group > .btn,
.btn-group-vertical > .btn {
  position: relative;
  float: left;
}
.btn-group > .btn:hover,
.btn-group-vertical > .btn:hover,
.btn-group > .btn:focus,
.btn-group-vertical > .btn:focus,
.btn-group > .btn:active,
.btn-group-vertical > .btn:active,
.btn-group > .btn.active,
.btn-group-vertical > .btn.active {
  z-index: 2;
}
.btn-group .btn + .btn,
.btn-group .btn + .btn-group,
.btn-group .btn-group + .btn,
.btn-group .btn-group + .btn-group {
  margin-left: -1px;
}
.btn-toolbar {
  margin-left: -5px;
}
.btn-toolbar .btn,
.btn-toolbar .btn-group,
.btn-toolbar .input-group {
  float: left;
}
.btn-toolbar > .btn,
.btn-toolbar > .btn-group,
.btn-toolbar > .input-group {
  margin-left: 5px;
}
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-left: 8px;
  padding-right: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-left: 12px;
  padding-right: 12px;
}
.btn-group.open .dropdown-toggle {
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn .caret {
  margin-left: 0;
}
.btn-lg .caret {
  border-width: 5px 5px 0;
  border-bottom-width: 0;
}
.dropup .btn-lg .caret {
  border-width: 0 5px 5px;
}
.btn-group-vertical > .btn,
.btn-group-vertical > .btn-group,
.btn-group-vertical > .btn-group > .btn {
  display: block;
  float: none;
  width: 100%;
  max-width: 100%;
}
.btn-group-vertical > .btn-group > .btn {
  float: none;
}
.btn-group-vertical > .btn + .btn,
.btn-group-vertical > .btn + .btn-group,
.btn-group-vertical > .btn-group + .btn,
.btn-group-vertical > .btn-group + .btn-group {
  margin-top: -1px;
  margin-left: 0;
}
.btn-group-vertical > .btn:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.btn-group-vertical > .btn:first-child:not(:last-child) {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn:last-child:not(:first-child) {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
.btn-group-vertical > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.btn-group-justified {
  display: table;
  width: 100%;
  table-layout: fixed;
  border-collapse: separate;
}
.btn-group-justified > .btn,
.btn-group-justified > .btn-group {
  float: none;
  display: table-cell;
  width: 1%;
}
.btn-group-justified > .btn-group .btn {
  width: 100%;
}
.btn-group-justified > .btn-group .dropdown-menu {
  left: auto;
}
[data-toggle="buttons"] > .btn input[type="radio"],
[data-toggle="buttons"] > .btn-group > .btn input[type="radio"],
[data-toggle="buttons"] > .btn input[type="checkbox"],
[data-toggle="buttons"] > .btn-group > .btn input[type="checkbox"] {
  position: absolute;
  clip: rect(0, 0, 0, 0);
  pointer-events: none;
}
.input-group {
  position: relative;
  display: table;
  border-collapse: separate;
}
.input-group[class*="col-"] {
  float: none;
  padding-left: 0;
  padding-right: 0;
}
.input-group .form-control {
  position: relative;
  z-index: 2;
  float: left;
  width: 100%;
  margin-bottom: 0;
}
.input-group .form-control:focus {
  z-index: 3;
}
.input-group-lg > .form-control,
.input-group-lg > .input-group-addon,
.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-group-lg > .form-control,
select.input-group-lg > .input-group-addon,
select.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  line-height: 45px;
}
textarea.input-group-lg > .form-control,
textarea.input-group-lg > .input-group-addon,
textarea.input-group-lg > .input-group-btn > .btn,
select[multiple].input-group-lg > .form-control,
select[multiple].input-group-lg > .input-group-addon,
select[multiple].input-group-lg > .input-group-btn > .btn {
  height: auto;
}
.input-group-sm > .form-control,
.input-group-sm > .input-group-addon,
.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-group-sm > .form-control,
select.input-group-sm > .input-group-addon,
select.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  line-height: 30px;
}
textarea.input-group-sm > .form-control,
textarea.input-group-sm > .input-group-addon,
textarea.input-group-sm > .input-group-btn > .btn,
select[multiple].input-group-sm > .form-control,
select[multiple].input-group-sm > .input-group-addon,
select[multiple].input-group-sm > .input-group-btn > .btn {
  height: auto;
}
.input-group-addon,
.input-group-btn,
.input-group .form-control {
  display: table-cell;
}
.input-group-addon:not(:first-child):not(:last-child),
.input-group-btn:not(:first-child):not(:last-child),
.input-group .form-control:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.input-group-addon,
.input-group-btn {
  width: 1%;
  white-space: nowrap;
  vertical-align: middle;
}
.input-group-addon {
  padding: 6px 12px;
  font-size: 13px;
  font-weight: normal;
  line-height: 1;
  color: #555555;
  text-align: center;
  background-color: #eeeeee;
  border: 1px solid #ccc;
  border-radius: 2px;
}
.input-group-addon.input-sm {
  padding: 5px 10px;
  font-size: 12px;
  border-radius: 1px;
}
.input-group-addon.input-lg {
  padding: 10px 16px;
  font-size: 17px;
  border-radius: 3px;
}
.input-group-addon input[type="radio"],
.input-group-addon input[type="checkbox"] {
  margin-top: 0;
}
.input-group .form-control:first-child,
.input-group-addon:first-child,
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group > .btn,
.input-group-btn:first-child > .dropdown-toggle,
.input-group-btn:last-child > .btn:not(:last-child):not(.dropdown-toggle),
.input-group-btn:last-child > .btn-group:not(:last-child) > .btn {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.input-group-addon:first-child {
  border-right: 0;
}
.input-group .form-control:last-child,
.input-group-addon:last-child,
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group > .btn,
.input-group-btn:last-child > .dropdown-toggle,
.input-group-btn:first-child > .btn:not(:first-child),
.input-group-btn:first-child > .btn-group:not(:first-child) > .btn {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.input-group-addon:last-child {
  border-left: 0;
}
.input-group-btn {
  position: relative;
  font-size: 0;
  white-space: nowrap;
}
.input-group-btn > .btn {
  position: relative;
}
.input-group-btn > .btn + .btn {
  margin-left: -1px;
}
.input-group-btn > .btn:hover,
.input-group-btn > .btn:focus,
.input-group-btn > .btn:active {
  z-index: 2;
}
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group {
  margin-right: -1px;
}
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group {
  z-index: 2;
  margin-left: -1px;
}
.nav {
  margin-bottom: 0;
  padding-left: 0;
  list-style: none;
}
.nav > li {
  position: relative;
  display: block;
}
.nav > li > a {
  position: relative;
  display: block;
  padding: 10px 15px;
}
.nav > li > a:hover,
.nav > li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.nav > li.disabled > a {
  color: #777777;
}
.nav > li.disabled > a:hover,
.nav > li.disabled > a:focus {
  color: #777777;
  text-decoration: none;
  background-color: transparent;
  cursor: not-allowed;
}
.nav .open > a,
.nav .open > a:hover,
.nav .open > a:focus {
  background-color: #eeeeee;
  border-color: #337ab7;
}
.nav .nav-divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.nav > li > a > img {
  max-width: none;
}
.nav-tabs {
  border-bottom: 1px solid #ddd;
}
.nav-tabs > li {
  float: left;
  margin-bottom: -1px;
}
.nav-tabs > li > a {
  margin-right: 2px;
  line-height: 1.42857143;
  border: 1px solid transparent;
  border-radius: 2px 2px 0 0;
}
.nav-tabs > li > a:hover {
  border-color: #eeeeee #eeeeee #ddd;
}
.nav-tabs > li.active > a,
.nav-tabs > li.active > a:hover,
.nav-tabs > li.active > a:focus {
  color: #555555;
  background-color: #fff;
  border: 1px solid #ddd;
  border-bottom-color: transparent;
  cursor: default;
}
.nav-tabs.nav-justified {
  width: 100%;
  border-bottom: 0;
}
.nav-tabs.nav-justified > li {
  float: none;
}
.nav-tabs.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-tabs.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-tabs.nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs.nav-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs.nav-justified > .active > a,
.nav-tabs.nav-justified > .active > a:hover,
.nav-tabs.nav-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs.nav-justified > .active > a,
  .nav-tabs.nav-justified > .active > a:hover,
  .nav-tabs.nav-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.nav-pills > li {
  float: left;
}
.nav-pills > li > a {
  border-radius: 2px;
}
.nav-pills > li + li {
  margin-left: 2px;
}
.nav-pills > li.active > a,
.nav-pills > li.active > a:hover,
.nav-pills > li.active > a:focus {
  color: #fff;
  background-color: #337ab7;
}
.nav-stacked > li {
  float: none;
}
.nav-stacked > li + li {
  margin-top: 2px;
  margin-left: 0;
}
.nav-justified {
  width: 100%;
}
.nav-justified > li {
  float: none;
}
.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs-justified {
  border-bottom: 0;
}
.nav-tabs-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs-justified > .active > a,
.nav-tabs-justified > .active > a:hover,
.nav-tabs-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs-justified > .active > a,
  .nav-tabs-justified > .active > a:hover,
  .nav-tabs-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.tab-content > .tab-pane {
  display: none;
}
.tab-content > .active {
  display: block;
}
.nav-tabs .dropdown-menu {
  margin-top: -1px;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar {
  position: relative;
  min-height: 30px;
  margin-bottom: 18px;
  border: 1px solid transparent;
}
@media (min-width: 541px) {
  .navbar {
    border-radius: 2px;
  }
}
@media (min-width: 541px) {
  .navbar-header {
    float: left;
  }
}
.navbar-collapse {
  overflow-x: visible;
  padding-right: 0px;
  padding-left: 0px;
  border-top: 1px solid transparent;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1);
  -webkit-overflow-scrolling: touch;
}
.navbar-collapse.in {
  overflow-y: auto;
}
@media (min-width: 541px) {
  .navbar-collapse {
    width: auto;
    border-top: 0;
    box-shadow: none;
  }
  .navbar-collapse.collapse {
    display: block !important;
    height: auto !important;
    padding-bottom: 0;
    overflow: visible !important;
  }
  .navbar-collapse.in {
    overflow-y: visible;
  }
  .navbar-fixed-top .navbar-collapse,
  .navbar-static-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    padding-left: 0;
    padding-right: 0;
  }
}
.navbar-fixed-top .navbar-collapse,
.navbar-fixed-bottom .navbar-collapse {
  max-height: 340px;
}
@media (max-device-width: 540px) and (orientation: landscape) {
  .navbar-fixed-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    max-height: 200px;
  }
}
.container > .navbar-header,
.container-fluid > .navbar-header,
.container > .navbar-collapse,
.container-fluid > .navbar-collapse {
  margin-right: 0px;
  margin-left: 0px;
}
@media (min-width: 541px) {
  .container > .navbar-header,
  .container-fluid > .navbar-header,
  .container > .navbar-collapse,
  .container-fluid > .navbar-collapse {
    margin-right: 0;
    margin-left: 0;
  }
}
.navbar-static-top {
  z-index: 1000;
  border-width: 0 0 1px;
}
@media (min-width: 541px) {
  .navbar-static-top {
    border-radius: 0;
  }
}
.navbar-fixed-top,
.navbar-fixed-bottom {
  position: fixed;
  right: 0;
  left: 0;
  z-index: 1030;
}
@media (min-width: 541px) {
  .navbar-fixed-top,
  .navbar-fixed-bottom {
    border-radius: 0;
  }
}
.navbar-fixed-top {
  top: 0;
  border-width: 0 0 1px;
}
.navbar-fixed-bottom {
  bottom: 0;
  margin-bottom: 0;
  border-width: 1px 0 0;
}
.navbar-brand {
  float: left;
  padding: 6px 0px;
  font-size: 17px;
  line-height: 18px;
  height: 30px;
}
.navbar-brand:hover,
.navbar-brand:focus {
  text-decoration: none;
}
.navbar-brand > img {
  display: block;
}
@media (min-width: 541px) {
  .navbar > .container .navbar-brand,
  .navbar > .container-fluid .navbar-brand {
    margin-left: 0px;
  }
}
.navbar-toggle {
  position: relative;
  float: right;
  margin-right: 0px;
  padding: 9px 10px;
  margin-top: -2px;
  margin-bottom: -2px;
  background-color: transparent;
  background-image: none;
  border: 1px solid transparent;
  border-radius: 2px;
}
.navbar-toggle:focus {
  outline: 0;
}
.navbar-toggle .icon-bar {
  display: block;
  width: 22px;
  height: 2px;
  border-radius: 1px;
}
.navbar-toggle .icon-bar + .icon-bar {
  margin-top: 4px;
}
@media (min-width: 541px) {
  .navbar-toggle {
    display: none;
  }
}
.navbar-nav {
  margin: 3px 0px;
}
.navbar-nav > li > a {
  padding-top: 10px;
  padding-bottom: 10px;
  line-height: 18px;
}
@media (max-width: 540px) {
  .navbar-nav .open .dropdown-menu {
    position: static;
    float: none;
    width: auto;
    margin-top: 0;
    background-color: transparent;
    border: 0;
    box-shadow: none;
  }
  .navbar-nav .open .dropdown-menu > li > a,
  .navbar-nav .open .dropdown-menu .dropdown-header {
    padding: 5px 15px 5px 25px;
  }
  .navbar-nav .open .dropdown-menu > li > a {
    line-height: 18px;
  }
  .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-nav .open .dropdown-menu > li > a:focus {
    background-image: none;
  }
}
@media (min-width: 541px) {
  .navbar-nav {
    float: left;
    margin: 0;
  }
  .navbar-nav > li {
    float: left;
  }
  .navbar-nav > li > a {
    padding-top: 6px;
    padding-bottom: 6px;
  }
}
.navbar-form {
  margin-left: 0px;
  margin-right: 0px;
  padding: 10px 0px;
  border-top: 1px solid transparent;
  border-bottom: 1px solid transparent;
  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  margin-top: -1px;
  margin-bottom: -1px;
}
@media (min-width: 768px) {
  .navbar-form .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .navbar-form .form-control-static {
    display: inline-block;
  }
  .navbar-form .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .navbar-form .input-group .input-group-addon,
  .navbar-form .input-group .input-group-btn,
  .navbar-form .input-group .form-control {
    width: auto;
  }
  .navbar-form .input-group > .form-control {
    width: 100%;
  }
  .navbar-form .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio,
  .navbar-form .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio label,
  .navbar-form .checkbox label {
    padding-left: 0;
  }
  .navbar-form .radio input[type="radio"],
  .navbar-form .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .navbar-form .has-feedback .form-control-feedback {
    top: 0;
  }
}
@media (max-width: 540px) {
  .navbar-form .form-group {
    margin-bottom: 5px;
  }
  .navbar-form .form-group:last-child {
    margin-bottom: 0;
  }
}
@media (min-width: 541px) {
  .navbar-form {
    width: auto;
    border: 0;
    margin-left: 0;
    margin-right: 0;
    padding-top: 0;
    padding-bottom: 0;
    -webkit-box-shadow: none;
    box-shadow: none;
  }
}
.navbar-nav > li > .dropdown-menu {
  margin-top: 0;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar-fixed-bottom .navbar-nav > li > .dropdown-menu {
  margin-bottom: 0;
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.navbar-btn {
  margin-top: -1px;
  margin-bottom: -1px;
}
.navbar-btn.btn-sm {
  margin-top: 0px;
  margin-bottom: 0px;
}
.navbar-btn.btn-xs {
  margin-top: 4px;
  margin-bottom: 4px;
}
.navbar-text {
  margin-top: 6px;
  margin-bottom: 6px;
}
@media (min-width: 541px) {
  .navbar-text {
    float: left;
    margin-left: 0px;
    margin-right: 0px;
  }
}
@media (min-width: 541px) {
  .navbar-left {
    float: left !important;
    float: left;
  }
  .navbar-right {
    float: right !important;
    float: right;
    margin-right: 0px;
  }
  .navbar-right ~ .navbar-right {
    margin-right: 0;
  }
}
.navbar-default {
  background-color: #f8f8f8;
  border-color: #e7e7e7;
}
.navbar-default .navbar-brand {
  color: #777;
}
.navbar-default .navbar-brand:hover,
.navbar-default .navbar-brand:focus {
  color: #5e5e5e;
  background-color: transparent;
}
.navbar-default .navbar-text {
  color: #777;
}
.navbar-default .navbar-nav > li > a {
  color: #777;
}
.navbar-default .navbar-nav > li > a:hover,
.navbar-default .navbar-nav > li > a:focus {
  color: #333;
  background-color: transparent;
}
.navbar-default .navbar-nav > .active > a,
.navbar-default .navbar-nav > .active > a:hover,
.navbar-default .navbar-nav > .active > a:focus {
  color: #555;
  background-color: #e7e7e7;
}
.navbar-default .navbar-nav > .disabled > a,
.navbar-default .navbar-nav > .disabled > a:hover,
.navbar-default .navbar-nav > .disabled > a:focus {
  color: #ccc;
  background-color: transparent;
}
.navbar-default .navbar-toggle {
  border-color: #ddd;
}
.navbar-default .navbar-toggle:hover,
.navbar-default .navbar-toggle:focus {
  background-color: #ddd;
}
.navbar-default .navbar-toggle .icon-bar {
  background-color: #888;
}
.navbar-default .navbar-collapse,
.navbar-default .navbar-form {
  border-color: #e7e7e7;
}
.navbar-default .navbar-nav > .open > a,
.navbar-default .navbar-nav > .open > a:hover,
.navbar-default .navbar-nav > .open > a:focus {
  background-color: #e7e7e7;
  color: #555;
}
@media (max-width: 540px) {
  .navbar-default .navbar-nav .open .dropdown-menu > li > a {
    color: #777;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #333;
    background-color: transparent;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #555;
    background-color: #e7e7e7;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #ccc;
    background-color: transparent;
  }
}
.navbar-default .navbar-link {
  color: #777;
}
.navbar-default .navbar-link:hover {
  color: #333;
}
.navbar-default .btn-link {
  color: #777;
}
.navbar-default .btn-link:hover,
.navbar-default .btn-link:focus {
  color: #333;
}
.navbar-default .btn-link[disabled]:hover,
fieldset[disabled] .navbar-default .btn-link:hover,
.navbar-default .btn-link[disabled]:focus,
fieldset[disabled] .navbar-default .btn-link:focus {
  color: #ccc;
}
.navbar-inverse {
  background-color: #222;
  border-color: #080808;
}
.navbar-inverse .navbar-brand {
  color: #9d9d9d;
}
.navbar-inverse .navbar-brand:hover,
.navbar-inverse .navbar-brand:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-text {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a:hover,
.navbar-inverse .navbar-nav > li > a:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-nav > .active > a,
.navbar-inverse .navbar-nav > .active > a:hover,
.navbar-inverse .navbar-nav > .active > a:focus {
  color: #fff;
  background-color: #080808;
}
.navbar-inverse .navbar-nav > .disabled > a,
.navbar-inverse .navbar-nav > .disabled > a:hover,
.navbar-inverse .navbar-nav > .disabled > a:focus {
  color: #444;
  background-color: transparent;
}
.navbar-inverse .navbar-toggle {
  border-color: #333;
}
.navbar-inverse .navbar-toggle:hover,
.navbar-inverse .navbar-toggle:focus {
  background-color: #333;
}
.navbar-inverse .navbar-toggle .icon-bar {
  background-color: #fff;
}
.navbar-inverse .navbar-collapse,
.navbar-inverse .navbar-form {
  border-color: #101010;
}
.navbar-inverse .navbar-nav > .open > a,
.navbar-inverse .navbar-nav > .open > a:hover,
.navbar-inverse .navbar-nav > .open > a:focus {
  background-color: #080808;
  color: #fff;
}
@media (max-width: 540px) {
  .navbar-inverse .navbar-nav .open .dropdown-menu > .dropdown-header {
    border-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu .divider {
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a {
    color: #9d9d9d;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #fff;
    background-color: transparent;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #fff;
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #444;
    background-color: transparent;
  }
}
.navbar-inverse .navbar-link {
  color: #9d9d9d;
}
.navbar-inverse .navbar-link:hover {
  color: #fff;
}
.navbar-inverse .btn-link {
  color: #9d9d9d;
}
.navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link:focus {
  color: #fff;
}
.navbar-inverse .btn-link[disabled]:hover,
fieldset[disabled] .navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link[disabled]:focus,
fieldset[disabled] .navbar-inverse .btn-link:focus {
  color: #444;
}
.breadcrumb {
  padding: 8px 15px;
  margin-bottom: 18px;
  list-style: none;
  background-color: #f5f5f5;
  border-radius: 2px;
}
.breadcrumb > li {
  display: inline-block;
}
.breadcrumb > li + li:before {
  content: "/\00a0";
  padding: 0 5px;
  color: #5e5e5e;
}
.breadcrumb > .active {
  color: #777777;
}
.pagination {
  display: inline-block;
  padding-left: 0;
  margin: 18px 0;
  border-radius: 2px;
}
.pagination > li {
  display: inline;
}
.pagination > li > a,
.pagination > li > span {
  position: relative;
  float: left;
  padding: 6px 12px;
  line-height: 1.42857143;
  text-decoration: none;
  color: #337ab7;
  background-color: #fff;
  border: 1px solid #ddd;
  margin-left: -1px;
}
.pagination > li:first-child > a,
.pagination > li:first-child > span {
  margin-left: 0;
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.pagination > li:last-child > a,
.pagination > li:last-child > span {
  border-bottom-right-radius: 2px;
  border-top-right-radius: 2px;
}
.pagination > li > a:hover,
.pagination > li > span:hover,
.pagination > li > a:focus,
.pagination > li > span:focus {
  z-index: 2;
  color: #23527c;
  background-color: #eeeeee;
  border-color: #ddd;
}
.pagination > .active > a,
.pagination > .active > span,
.pagination > .active > a:hover,
.pagination > .active > span:hover,
.pagination > .active > a:focus,
.pagination > .active > span:focus {
  z-index: 3;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
  cursor: default;
}
.pagination > .disabled > span,
.pagination > .disabled > span:hover,
.pagination > .disabled > span:focus,
.pagination > .disabled > a,
.pagination > .disabled > a:hover,
.pagination > .disabled > a:focus {
  color: #777777;
  background-color: #fff;
  border-color: #ddd;
  cursor: not-allowed;
}
.pagination-lg > li > a,
.pagination-lg > li > span {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.pagination-lg > li:first-child > a,
.pagination-lg > li:first-child > span {
  border-bottom-left-radius: 3px;
  border-top-left-radius: 3px;
}
.pagination-lg > li:last-child > a,
.pagination-lg > li:last-child > span {
  border-bottom-right-radius: 3px;
  border-top-right-radius: 3px;
}
.pagination-sm > li > a,
.pagination-sm > li > span {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.pagination-sm > li:first-child > a,
.pagination-sm > li:first-child > span {
  border-bottom-left-radius: 1px;
  border-top-left-radius: 1px;
}
.pagination-sm > li:last-child > a,
.pagination-sm > li:last-child > span {
  border-bottom-right-radius: 1px;
  border-top-right-radius: 1px;
}
.pager {
  padding-left: 0;
  margin: 18px 0;
  list-style: none;
  text-align: center;
}
.pager li {
  display: inline;
}
.pager li > a,
.pager li > span {
  display: inline-block;
  padding: 5px 14px;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 15px;
}
.pager li > a:hover,
.pager li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.pager .next > a,
.pager .next > span {
  float: right;
}
.pager .previous > a,
.pager .previous > span {
  float: left;
}
.pager .disabled > a,
.pager .disabled > a:hover,
.pager .disabled > a:focus,
.pager .disabled > span {
  color: #777777;
  background-color: #fff;
  cursor: not-allowed;
}
.label {
  display: inline;
  padding: .2em .6em .3em;
  font-size: 75%;
  font-weight: bold;
  line-height: 1;
  color: #fff;
  text-align: center;
  white-space: nowrap;
  vertical-align: baseline;
  border-radius: .25em;
}
a.label:hover,
a.label:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.label:empty {
  display: none;
}
.btn .label {
  position: relative;
  top: -1px;
}
.label-default {
  background-color: #777777;
}
.label-default[href]:hover,
.label-default[href]:focus {
  background-color: #5e5e5e;
}
.label-primary {
  background-color: #337ab7;
}
.label-primary[href]:hover,
.label-primary[href]:focus {
  background-color: #286090;
}
.label-success {
  background-color: #5cb85c;
}
.label-success[href]:hover,
.label-success[href]:focus {
  background-color: #449d44;
}
.label-info {
  background-color: #5bc0de;
}
.label-info[href]:hover,
.label-info[href]:focus {
  background-color: #31b0d5;
}
.label-warning {
  background-color: #f0ad4e;
}
.label-warning[href]:hover,
.label-warning[href]:focus {
  background-color: #ec971f;
}
.label-danger {
  background-color: #d9534f;
}
.label-danger[href]:hover,
.label-danger[href]:focus {
  background-color: #c9302c;
}
.badge {
  display: inline-block;
  min-width: 10px;
  padding: 3px 7px;
  font-size: 12px;
  font-weight: bold;
  color: #fff;
  line-height: 1;
  vertical-align: middle;
  white-space: nowrap;
  text-align: center;
  background-color: #777777;
  border-radius: 10px;
}
.badge:empty {
  display: none;
}
.btn .badge {
  position: relative;
  top: -1px;
}
.btn-xs .badge,
.btn-group-xs > .btn .badge {
  top: 0;
  padding: 1px 5px;
}
a.badge:hover,
a.badge:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.list-group-item.active > .badge,
.nav-pills > .active > a > .badge {
  color: #337ab7;
  background-color: #fff;
}
.list-group-item > .badge {
  float: right;
}
.list-group-item > .badge + .badge {
  margin-right: 5px;
}
.nav-pills > li > a > .badge {
  margin-left: 3px;
}
.jumbotron {
  padding-top: 30px;
  padding-bottom: 30px;
  margin-bottom: 30px;
  color: inherit;
  background-color: #eeeeee;
}
.jumbotron h1,
.jumbotron .h1 {
  color: inherit;
}
.jumbotron p {
  margin-bottom: 15px;
  font-size: 20px;
  font-weight: 200;
}
.jumbotron > hr {
  border-top-color: #d5d5d5;
}
.container .jumbotron,
.container-fluid .jumbotron {
  border-radius: 3px;
  padding-left: 0px;
  padding-right: 0px;
}
.jumbotron .container {
  max-width: 100%;
}
@media screen and (min-width: 768px) {
  .jumbotron {
    padding-top: 48px;
    padding-bottom: 48px;
  }
  .container .jumbotron,
  .container-fluid .jumbotron {
    padding-left: 60px;
    padding-right: 60px;
  }
  .jumbotron h1,
  .jumbotron .h1 {
    font-size: 59px;
  }
}
.thumbnail {
  display: block;
  padding: 4px;
  margin-bottom: 18px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: border 0.2s ease-in-out;
  -o-transition: border 0.2s ease-in-out;
  transition: border 0.2s ease-in-out;
}
.thumbnail > img,
.thumbnail a > img {
  margin-left: auto;
  margin-right: auto;
}
a.thumbnail:hover,
a.thumbnail:focus,
a.thumbnail.active {
  border-color: #337ab7;
}
.thumbnail .caption {
  padding: 9px;
  color: #000;
}
.alert {
  padding: 15px;
  margin-bottom: 18px;
  border: 1px solid transparent;
  border-radius: 2px;
}
.alert h4 {
  margin-top: 0;
  color: inherit;
}
.alert .alert-link {
  font-weight: bold;
}
.alert > p,
.alert > ul {
  margin-bottom: 0;
}
.alert > p + p {
  margin-top: 5px;
}
.alert-dismissable,
.alert-dismissible {
  padding-right: 35px;
}
.alert-dismissable .close,
.alert-dismissible .close {
  position: relative;
  top: -2px;
  right: -21px;
  color: inherit;
}
.alert-success {
  background-color: #dff0d8;
  border-color: #d6e9c6;
  color: #3c763d;
}
.alert-success hr {
  border-top-color: #c9e2b3;
}
.alert-success .alert-link {
  color: #2b542c;
}
.alert-info {
  background-color: #d9edf7;
  border-color: #bce8f1;
  color: #31708f;
}
.alert-info hr {
  border-top-color: #a6e1ec;
}
.alert-info .alert-link {
  color: #245269;
}
.alert-warning {
  background-color: #fcf8e3;
  border-color: #faebcc;
  color: #8a6d3b;
}
.alert-warning hr {
  border-top-color: #f7e1b5;
}
.alert-warning .alert-link {
  color: #66512c;
}
.alert-danger {
  background-color: #f2dede;
  border-color: #ebccd1;
  color: #a94442;
}
.alert-danger hr {
  border-top-color: #e4b9c0;
}
.alert-danger .alert-link {
  color: #843534;
}
@-webkit-keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
@keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
.progress {
  overflow: hidden;
  height: 18px;
  margin-bottom: 18px;
  background-color: #f5f5f5;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
  box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
}
.progress-bar {
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 18px;
  color: #fff;
  text-align: center;
  background-color: #337ab7;
  -webkit-box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  -webkit-transition: width 0.6s ease;
  -o-transition: width 0.6s ease;
  transition: width 0.6s ease;
}
.progress-striped .progress-bar,
.progress-bar-striped {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-size: 40px 40px;
}
.progress.active .progress-bar,
.progress-bar.active {
  -webkit-animation: progress-bar-stripes 2s linear infinite;
  -o-animation: progress-bar-stripes 2s linear infinite;
  animation: progress-bar-stripes 2s linear infinite;
}
.progress-bar-success {
  background-color: #5cb85c;
}
.progress-striped .progress-bar-success {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-info {
  background-color: #5bc0de;
}
.progress-striped .progress-bar-info {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-warning {
  background-color: #f0ad4e;
}
.progress-striped .progress-bar-warning {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-danger {
  background-color: #d9534f;
}
.progress-striped .progress-bar-danger {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.media {
  margin-top: 15px;
}
.media:first-child {
  margin-top: 0;
}
.media,
.media-body {
  zoom: 1;
  overflow: hidden;
}
.media-body {
  width: 10000px;
}
.media-object {
  display: block;
}
.media-object.img-thumbnail {
  max-width: none;
}
.media-right,
.media > .pull-right {
  padding-left: 10px;
}
.media-left,
.media > .pull-left {
  padding-right: 10px;
}
.media-left,
.media-right,
.media-body {
  display: table-cell;
  vertical-align: top;
}
.media-middle {
  vertical-align: middle;
}
.media-bottom {
  vertical-align: bottom;
}
.media-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.media-list {
  padding-left: 0;
  list-style: none;
}
.list-group {
  margin-bottom: 20px;
  padding-left: 0;
}
.list-group-item {
  position: relative;
  display: block;
  padding: 10px 15px;
  margin-bottom: -1px;
  background-color: #fff;
  border: 1px solid #ddd;
}
.list-group-item:first-child {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
}
.list-group-item:last-child {
  margin-bottom: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
a.list-group-item,
button.list-group-item {
  color: #555;
}
a.list-group-item .list-group-item-heading,
button.list-group-item .list-group-item-heading {
  color: #333;
}
a.list-group-item:hover,
button.list-group-item:hover,
a.list-group-item:focus,
button.list-group-item:focus {
  text-decoration: none;
  color: #555;
  background-color: #f5f5f5;
}
button.list-group-item {
  width: 100%;
  text-align: left;
}
.list-group-item.disabled,
.list-group-item.disabled:hover,
.list-group-item.disabled:focus {
  background-color: #eeeeee;
  color: #777777;
  cursor: not-allowed;
}
.list-group-item.disabled .list-group-item-heading,
.list-group-item.disabled:hover .list-group-item-heading,
.list-group-item.disabled:focus .list-group-item-heading {
  color: inherit;
}
.list-group-item.disabled .list-group-item-text,
.list-group-item.disabled:hover .list-group-item-text,
.list-group-item.disabled:focus .list-group-item-text {
  color: #777777;
}
.list-group-item.active,
.list-group-item.active:hover,
.list-group-item.active:focus {
  z-index: 2;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.list-group-item.active .list-group-item-heading,
.list-group-item.active:hover .list-group-item-heading,
.list-group-item.active:focus .list-group-item-heading,
.list-group-item.active .list-group-item-heading > small,
.list-group-item.active:hover .list-group-item-heading > small,
.list-group-item.active:focus .list-group-item-heading > small,
.list-group-item.active .list-group-item-heading > .small,
.list-group-item.active:hover .list-group-item-heading > .small,
.list-group-item.active:focus .list-group-item-heading > .small {
  color: inherit;
}
.list-group-item.active .list-group-item-text,
.list-group-item.active:hover .list-group-item-text,
.list-group-item.active:focus .list-group-item-text {
  color: #c7ddef;
}
.list-group-item-success {
  color: #3c763d;
  background-color: #dff0d8;
}
a.list-group-item-success,
button.list-group-item-success {
  color: #3c763d;
}
a.list-group-item-success .list-group-item-heading,
button.list-group-item-success .list-group-item-heading {
  color: inherit;
}
a.list-group-item-success:hover,
button.list-group-item-success:hover,
a.list-group-item-success:focus,
button.list-group-item-success:focus {
  color: #3c763d;
  background-color: #d0e9c6;
}
a.list-group-item-success.active,
button.list-group-item-success.active,
a.list-group-item-success.active:hover,
button.list-group-item-success.active:hover,
a.list-group-item-success.active:focus,
button.list-group-item-success.active:focus {
  color: #fff;
  background-color: #3c763d;
  border-color: #3c763d;
}
.list-group-item-info {
  color: #31708f;
  background-color: #d9edf7;
}
a.list-group-item-info,
button.list-group-item-info {
  color: #31708f;
}
a.list-group-item-info .list-group-item-heading,
button.list-group-item-info .list-group-item-heading {
  color: inherit;
}
a.list-group-item-info:hover,
button.list-group-item-info:hover,
a.list-group-item-info:focus,
button.list-group-item-info:focus {
  color: #31708f;
  background-color: #c4e3f3;
}
a.list-group-item-info.active,
button.list-group-item-info.active,
a.list-group-item-info.active:hover,
button.list-group-item-info.active:hover,
a.list-group-item-info.active:focus,
button.list-group-item-info.active:focus {
  color: #fff;
  background-color: #31708f;
  border-color: #31708f;
}
.list-group-item-warning {
  color: #8a6d3b;
  background-color: #fcf8e3;
}
a.list-group-item-warning,
button.list-group-item-warning {
  color: #8a6d3b;
}
a.list-group-item-warning .list-group-item-heading,
button.list-group-item-warning .list-group-item-heading {
  color: inherit;
}
a.list-group-item-warning:hover,
button.list-group-item-warning:hover,
a.list-group-item-warning:focus,
button.list-group-item-warning:focus {
  color: #8a6d3b;
  background-color: #faf2cc;
}
a.list-group-item-warning.active,
button.list-group-item-warning.active,
a.list-group-item-warning.active:hover,
button.list-group-item-warning.active:hover,
a.list-group-item-warning.active:focus,
button.list-group-item-warning.active:focus {
  color: #fff;
  background-color: #8a6d3b;
  border-color: #8a6d3b;
}
.list-group-item-danger {
  color: #a94442;
  background-color: #f2dede;
}
a.list-group-item-danger,
button.list-group-item-danger {
  color: #a94442;
}
a.list-group-item-danger .list-group-item-heading,
button.list-group-item-danger .list-group-item-heading {
  color: inherit;
}
a.list-group-item-danger:hover,
button.list-group-item-danger:hover,
a.list-group-item-danger:focus,
button.list-group-item-danger:focus {
  color: #a94442;
  background-color: #ebcccc;
}
a.list-group-item-danger.active,
button.list-group-item-danger.active,
a.list-group-item-danger.active:hover,
button.list-group-item-danger.active:hover,
a.list-group-item-danger.active:focus,
button.list-group-item-danger.active:focus {
  color: #fff;
  background-color: #a94442;
  border-color: #a94442;
}
.list-group-item-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.list-group-item-text {
  margin-bottom: 0;
  line-height: 1.3;
}
.panel {
  margin-bottom: 18px;
  background-color: #fff;
  border: 1px solid transparent;
  border-radius: 2px;
  -webkit-box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
}
.panel-body {
  padding: 15px;
}
.panel-heading {
  padding: 10px 15px;
  border-bottom: 1px solid transparent;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel-heading > .dropdown .dropdown-toggle {
  color: inherit;
}
.panel-title {
  margin-top: 0;
  margin-bottom: 0;
  font-size: 15px;
  color: inherit;
}
.panel-title > a,
.panel-title > small,
.panel-title > .small,
.panel-title > small > a,
.panel-title > .small > a {
  color: inherit;
}
.panel-footer {
  padding: 10px 15px;
  background-color: #f5f5f5;
  border-top: 1px solid #ddd;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .list-group,
.panel > .panel-collapse > .list-group {
  margin-bottom: 0;
}
.panel > .list-group .list-group-item,
.panel > .panel-collapse > .list-group .list-group-item {
  border-width: 1px 0;
  border-radius: 0;
}
.panel > .list-group:first-child .list-group-item:first-child,
.panel > .panel-collapse > .list-group:first-child .list-group-item:first-child {
  border-top: 0;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .list-group:last-child .list-group-item:last-child,
.panel > .panel-collapse > .list-group:last-child .list-group-item:last-child {
  border-bottom: 0;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .panel-heading + .panel-collapse > .list-group .list-group-item:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.panel-heading + .list-group .list-group-item:first-child {
  border-top-width: 0;
}
.list-group + .panel-footer {
  border-top-width: 0;
}
.panel > .table,
.panel > .table-responsive > .table,
.panel > .panel-collapse > .table {
  margin-bottom: 0;
}
.panel > .table caption,
.panel > .table-responsive > .table caption,
.panel > .panel-collapse > .table caption {
  padding-left: 15px;
  padding-right: 15px;
}
.panel > .table:first-child,
.panel > .table-responsive:first-child > .table:first-child {
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child {
  border-top-left-radius: 1px;
  border-top-right-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:first-child {
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:last-child {
  border-top-right-radius: 1px;
}
.panel > .table:last-child,
.panel > .table-responsive:last-child > .table:last-child {
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child {
  border-bottom-left-radius: 1px;
  border-bottom-right-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:first-child {
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:last-child {
  border-bottom-right-radius: 1px;
}
.panel > .panel-body + .table,
.panel > .panel-body + .table-responsive,
.panel > .table + .panel-body,
.panel > .table-responsive + .panel-body {
  border-top: 1px solid #ddd;
}
.panel > .table > tbody:first-child > tr:first-child th,
.panel > .table > tbody:first-child > tr:first-child td {
  border-top: 0;
}
.panel > .table-bordered,
.panel > .table-responsive > .table-bordered {
  border: 0;
}
.panel > .table-bordered > thead > tr > th:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:first-child,
.panel > .table-bordered > tbody > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:first-child,
.panel > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-bordered > thead > tr > td:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:first-child,
.panel > .table-bordered > tbody > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:first-child,
.panel > .table-bordered > tfoot > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:first-child {
  border-left: 0;
}
.panel > .table-bordered > thead > tr > th:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:last-child,
.panel > .table-bordered > tbody > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:last-child,
.panel > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-bordered > thead > tr > td:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:last-child,
.panel > .table-bordered > tbody > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:last-child,
.panel > .table-bordered > tfoot > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:last-child {
  border-right: 0;
}
.panel > .table-bordered > thead > tr:first-child > td,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > td,
.panel > .table-bordered > tbody > tr:first-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > td,
.panel > .table-bordered > thead > tr:first-child > th,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > th,
.panel > .table-bordered > tbody > tr:first-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > th {
  border-bottom: 0;
}
.panel > .table-bordered > tbody > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > td,
.panel > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-bordered > tbody > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > th,
.panel > .table-bordered > tfoot > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > th {
  border-bottom: 0;
}
.panel > .table-responsive {
  border: 0;
  margin-bottom: 0;
}
.panel-group {
  margin-bottom: 18px;
}
.panel-group .panel {
  margin-bottom: 0;
  border-radius: 2px;
}
.panel-group .panel + .panel {
  margin-top: 5px;
}
.panel-group .panel-heading {
  border-bottom: 0;
}
.panel-group .panel-heading + .panel-collapse > .panel-body,
.panel-group .panel-heading + .panel-collapse > .list-group {
  border-top: 1px solid #ddd;
}
.panel-group .panel-footer {
  border-top: 0;
}
.panel-group .panel-footer + .panel-collapse .panel-body {
  border-bottom: 1px solid #ddd;
}
.panel-default {
  border-color: #ddd;
}
.panel-default > .panel-heading {
  color: #333333;
  background-color: #f5f5f5;
  border-color: #ddd;
}
.panel-default > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ddd;
}
.panel-default > .panel-heading .badge {
  color: #f5f5f5;
  background-color: #333333;
}
.panel-default > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ddd;
}
.panel-primary {
  border-color: #337ab7;
}
.panel-primary > .panel-heading {
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.panel-primary > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #337ab7;
}
.panel-primary > .panel-heading .badge {
  color: #337ab7;
  background-color: #fff;
}
.panel-primary > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #337ab7;
}
.panel-success {
  border-color: #d6e9c6;
}
.panel-success > .panel-heading {
  color: #3c763d;
  background-color: #dff0d8;
  border-color: #d6e9c6;
}
.panel-success > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #d6e9c6;
}
.panel-success > .panel-heading .badge {
  color: #dff0d8;
  background-color: #3c763d;
}
.panel-success > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #d6e9c6;
}
.panel-info {
  border-color: #bce8f1;
}
.panel-info > .panel-heading {
  color: #31708f;
  background-color: #d9edf7;
  border-color: #bce8f1;
}
.panel-info > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #bce8f1;
}
.panel-info > .panel-heading .badge {
  color: #d9edf7;
  background-color: #31708f;
}
.panel-info > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #bce8f1;
}
.panel-warning {
  border-color: #faebcc;
}
.panel-warning > .panel-heading {
  color: #8a6d3b;
  background-color: #fcf8e3;
  border-color: #faebcc;
}
.panel-warning > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #faebcc;
}
.panel-warning > .panel-heading .badge {
  color: #fcf8e3;
  background-color: #8a6d3b;
}
.panel-warning > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #faebcc;
}
.panel-danger {
  border-color: #ebccd1;
}
.panel-danger > .panel-heading {
  color: #a94442;
  background-color: #f2dede;
  border-color: #ebccd1;
}
.panel-danger > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ebccd1;
}
.panel-danger > .panel-heading .badge {
  color: #f2dede;
  background-color: #a94442;
}
.panel-danger > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ebccd1;
}
.embed-responsive {
  position: relative;
  display: block;
  height: 0;
  padding: 0;
  overflow: hidden;
}
.embed-responsive .embed-responsive-item,
.embed-responsive iframe,
.embed-responsive embed,
.embed-responsive object,
.embed-responsive video {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  height: 100%;
  width: 100%;
  border: 0;
}
.embed-responsive-16by9 {
  padding-bottom: 56.25%;
}
.embed-responsive-4by3 {
  padding-bottom: 75%;
}
.well {
  min-height: 20px;
  padding: 19px;
  margin-bottom: 20px;
  background-color: #f5f5f5;
  border: 1px solid #e3e3e3;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
}
.well blockquote {
  border-color: #ddd;
  border-color: rgba(0, 0, 0, 0.15);
}
.well-lg {
  padding: 24px;
  border-radius: 3px;
}
.well-sm {
  padding: 9px;
  border-radius: 1px;
}
.close {
  float: right;
  font-size: 19.5px;
  font-weight: bold;
  line-height: 1;
  color: #000;
  text-shadow: 0 1px 0 #fff;
  opacity: 0.2;
  filter: alpha(opacity=20);
}
.close:hover,
.close:focus {
  color: #000;
  text-decoration: none;
  cursor: pointer;
  opacity: 0.5;
  filter: alpha(opacity=50);
}
button.close {
  padding: 0;
  cursor: pointer;
  background: transparent;
  border: 0;
  -webkit-appearance: none;
}
.modal-open {
  overflow: hidden;
}
.modal {
  display: none;
  overflow: hidden;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1050;
  -webkit-overflow-scrolling: touch;
  outline: 0;
}
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, -25%);
  -ms-transform: translate(0, -25%);
  -o-transform: translate(0, -25%);
  transform: translate(0, -25%);
  -webkit-transition: -webkit-transform 0.3s ease-out;
  -moz-transition: -moz-transform 0.3s ease-out;
  -o-transition: -o-transform 0.3s ease-out;
  transition: transform 0.3s ease-out;
}
.modal.in .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
.modal-open .modal {
  overflow-x: hidden;
  overflow-y: auto;
}
.modal-dialog {
  position: relative;
  width: auto;
  margin: 10px;
}
.modal-content {
  position: relative;
  background-color: #fff;
  border: 1px solid #999;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  background-clip: padding-box;
  outline: 0;
}
.modal-backdrop {
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1040;
  background-color: #000;
}
.modal-backdrop.fade {
  opacity: 0;
  filter: alpha(opacity=0);
}
.modal-backdrop.in {
  opacity: 0.5;
  filter: alpha(opacity=50);
}
.modal-header {
  padding: 15px;
  border-bottom: 1px solid #e5e5e5;
}
.modal-header .close {
  margin-top: -2px;
}
.modal-title {
  margin: 0;
  line-height: 1.42857143;
}
.modal-body {
  position: relative;
  padding: 15px;
}
.modal-footer {
  padding: 15px;
  text-align: right;
  border-top: 1px solid #e5e5e5;
}
.modal-footer .btn + .btn {
  margin-left: 5px;
  margin-bottom: 0;
}
.modal-footer .btn-group .btn + .btn {
  margin-left: -1px;
}
.modal-footer .btn-block + .btn-block {
  margin-left: 0;
}
.modal-scrollbar-measure {
  position: absolute;
  top: -9999px;
  width: 50px;
  height: 50px;
  overflow: scroll;
}
@media (min-width: 768px) {
  .modal-dialog {
    width: 600px;
    margin: 30px auto;
  }
  .modal-content {
    -webkit-box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
  }
  .modal-sm {
    width: 300px;
  }
}
@media (min-width: 992px) {
  .modal-lg {
    width: 900px;
  }
}
.tooltip {
  position: absolute;
  z-index: 1070;
  display: block;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 12px;
  opacity: 0;
  filter: alpha(opacity=0);
}
.tooltip.in {
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.tooltip.top {
  margin-top: -3px;
  padding: 5px 0;
}
.tooltip.right {
  margin-left: 3px;
  padding: 0 5px;
}
.tooltip.bottom {
  margin-top: 3px;
  padding: 5px 0;
}
.tooltip.left {
  margin-left: -3px;
  padding: 0 5px;
}
.tooltip-inner {
  max-width: 200px;
  padding: 3px 8px;
  color: #fff;
  text-align: center;
  background-color: #000;
  border-radius: 2px;
}
.tooltip-arrow {
  position: absolute;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.tooltip.top .tooltip-arrow {
  bottom: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-left .tooltip-arrow {
  bottom: 0;
  right: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-right .tooltip-arrow {
  bottom: 0;
  left: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.right .tooltip-arrow {
  top: 50%;
  left: 0;
  margin-top: -5px;
  border-width: 5px 5px 5px 0;
  border-right-color: #000;
}
.tooltip.left .tooltip-arrow {
  top: 50%;
  right: 0;
  margin-top: -5px;
  border-width: 5px 0 5px 5px;
  border-left-color: #000;
}
.tooltip.bottom .tooltip-arrow {
  top: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-left .tooltip-arrow {
  top: 0;
  right: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-right .tooltip-arrow {
  top: 0;
  left: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.popover {
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1060;
  display: none;
  max-width: 276px;
  padding: 1px;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 13px;
  background-color: #fff;
  background-clip: padding-box;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
  box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
}
.popover.top {
  margin-top: -10px;
}
.popover.right {
  margin-left: 10px;
}
.popover.bottom {
  margin-top: 10px;
}
.popover.left {
  margin-left: -10px;
}
.popover-title {
  margin: 0;
  padding: 8px 14px;
  font-size: 13px;
  background-color: #f7f7f7;
  border-bottom: 1px solid #ebebeb;
  border-radius: 2px 2px 0 0;
}
.popover-content {
  padding: 9px 14px;
}
.popover > .arrow,
.popover > .arrow:after {
  position: absolute;
  display: block;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.popover > .arrow {
  border-width: 11px;
}
.popover > .arrow:after {
  border-width: 10px;
  content: "";
}
.popover.top > .arrow {
  left: 50%;
  margin-left: -11px;
  border-bottom-width: 0;
  border-top-color: #999999;
  border-top-color: rgba(0, 0, 0, 0.25);
  bottom: -11px;
}
.popover.top > .arrow:after {
  content: " ";
  bottom: 1px;
  margin-left: -10px;
  border-bottom-width: 0;
  border-top-color: #fff;
}
.popover.right > .arrow {
  top: 50%;
  left: -11px;
  margin-top: -11px;
  border-left-width: 0;
  border-right-color: #999999;
  border-right-color: rgba(0, 0, 0, 0.25);
}
.popover.right > .arrow:after {
  content: " ";
  left: 1px;
  bottom: -10px;
  border-left-width: 0;
  border-right-color: #fff;
}
.popover.bottom > .arrow {
  left: 50%;
  margin-left: -11px;
  border-top-width: 0;
  border-bottom-color: #999999;
  border-bottom-color: rgba(0, 0, 0, 0.25);
  top: -11px;
}
.popover.bottom > .arrow:after {
  content: " ";
  top: 1px;
  margin-left: -10px;
  border-top-width: 0;
  border-bottom-color: #fff;
}
.popover.left > .arrow {
  top: 50%;
  right: -11px;
  margin-top: -11px;
  border-right-width: 0;
  border-left-color: #999999;
  border-left-color: rgba(0, 0, 0, 0.25);
}
.popover.left > .arrow:after {
  content: " ";
  right: 1px;
  border-right-width: 0;
  border-left-color: #fff;
  bottom: -10px;
}
.carousel {
  position: relative;
}
.carousel-inner {
  position: relative;
  overflow: hidden;
  width: 100%;
}
.carousel-inner > .item {
  display: none;
  position: relative;
  -webkit-transition: 0.6s ease-in-out left;
  -o-transition: 0.6s ease-in-out left;
  transition: 0.6s ease-in-out left;
}
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  line-height: 1;
}
@media all and (transform-3d), (-webkit-transform-3d) {
  .carousel-inner > .item {
    -webkit-transition: -webkit-transform 0.6s ease-in-out;
    -moz-transition: -moz-transform 0.6s ease-in-out;
    -o-transition: -o-transform 0.6s ease-in-out;
    transition: transform 0.6s ease-in-out;
    -webkit-backface-visibility: hidden;
    -moz-backface-visibility: hidden;
    backface-visibility: hidden;
    -webkit-perspective: 1000px;
    -moz-perspective: 1000px;
    perspective: 1000px;
  }
  .carousel-inner > .item.next,
  .carousel-inner > .item.active.right {
    -webkit-transform: translate3d(100%, 0, 0);
    transform: translate3d(100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.prev,
  .carousel-inner > .item.active.left {
    -webkit-transform: translate3d(-100%, 0, 0);
    transform: translate3d(-100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.next.left,
  .carousel-inner > .item.prev.right,
  .carousel-inner > .item.active {
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    left: 0;
  }
}
.carousel-inner > .active,
.carousel-inner > .next,
.carousel-inner > .prev {
  display: block;
}
.carousel-inner > .active {
  left: 0;
}
.carousel-inner > .next,
.carousel-inner > .prev {
  position: absolute;
  top: 0;
  width: 100%;
}
.carousel-inner > .next {
  left: 100%;
}
.carousel-inner > .prev {
  left: -100%;
}
.carousel-inner > .next.left,
.carousel-inner > .prev.right {
  left: 0;
}
.carousel-inner > .active.left {
  left: -100%;
}
.carousel-inner > .active.right {
  left: 100%;
}
.carousel-control {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  width: 15%;
  opacity: 0.5;
  filter: alpha(opacity=50);
  font-size: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
  background-color: rgba(0, 0, 0, 0);
}
.carousel-control.left {
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#80000000', endColorstr='#00000000', GradientType=1);
}
.carousel-control.right {
  left: auto;
  right: 0;
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#00000000', endColorstr='#80000000', GradientType=1);
}
.carousel-control:hover,
.carousel-control:focus {
  outline: 0;
  color: #fff;
  text-decoration: none;
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.carousel-control .icon-prev,
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-left,
.carousel-control .glyphicon-chevron-right {
  position: absolute;
  top: 50%;
  margin-top: -10px;
  z-index: 5;
  display: inline-block;
}
.carousel-control .icon-prev,
.carousel-control .glyphicon-chevron-left {
  left: 50%;
  margin-left: -10px;
}
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-right {
  right: 50%;
  margin-right: -10px;
}
.carousel-control .icon-prev,
.carousel-control .icon-next {
  width: 20px;
  height: 20px;
  line-height: 1;
  font-family: serif;
}
.carousel-control .icon-prev:before {
  content: '\2039';
}
.carousel-control .icon-next:before {
  content: '\203a';
}
.carousel-indicators {
  position: absolute;
  bottom: 10px;
  left: 50%;
  z-index: 15;
  width: 60%;
  margin-left: -30%;
  padding-left: 0;
  list-style: none;
  text-align: center;
}
.carousel-indicators li {
  display: inline-block;
  width: 10px;
  height: 10px;
  margin: 1px;
  text-indent: -999px;
  border: 1px solid #fff;
  border-radius: 10px;
  cursor: pointer;
  background-color: #000 \9;
  background-color: rgba(0, 0, 0, 0);
}
.carousel-indicators .active {
  margin: 0;
  width: 12px;
  height: 12px;
  background-color: #fff;
}
.carousel-caption {
  position: absolute;
  left: 15%;
  right: 15%;
  bottom: 20px;
  z-index: 10;
  padding-top: 20px;
  padding-bottom: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
}
.carousel-caption .btn {
  text-shadow: none;
}
@media screen and (min-width: 768px) {
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-prev,
  .carousel-control .icon-next {
    width: 30px;
    height: 30px;
    margin-top: -10px;
    font-size: 30px;
  }
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .icon-prev {
    margin-left: -10px;
  }
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-next {
    margin-right: -10px;
  }
  .carousel-caption {
    left: 20%;
    right: 20%;
    padding-bottom: 30px;
  }
  .carousel-indicators {
    bottom: 20px;
  }
}
.clearfix:before,
.clearfix:after,
.dl-horizontal dd:before,
.dl-horizontal dd:after,
.container:before,
.container:after,
.container-fluid:before,
.container-fluid:after,
.row:before,
.row:after,
.form-horizontal .form-group:before,
.form-horizontal .form-group:after,
.btn-toolbar:before,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:before,
.btn-group-vertical > .btn-group:after,
.nav:before,
.nav:after,
.navbar:before,
.navbar:after,
.navbar-header:before,
.navbar-header:after,
.navbar-collapse:before,
.navbar-collapse:after,
.pager:before,
.pager:after,
.panel-body:before,
.panel-body:after,
.modal-header:before,
.modal-header:after,
.modal-footer:before,
.modal-footer:after,
.item_buttons:before,
.item_buttons:after {
  content: " ";
  display: table;
}
.clearfix:after,
.dl-horizontal dd:after,
.container:after,
.container-fluid:after,
.row:after,
.form-horizontal .form-group:after,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:after,
.nav:after,
.navbar:after,
.navbar-header:after,
.navbar-collapse:after,
.pager:after,
.panel-body:after,
.modal-header:after,
.modal-footer:after,
.item_buttons:after {
  clear: both;
}
.center-block {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.pull-right {
  float: right !important;
}
.pull-left {
  float: left !important;
}
.hide {
  display: none !important;
}
.show {
  display: block !important;
}
.invisible {
  visibility: hidden;
}
.text-hide {
  font: 0/0 a;
  color: transparent;
  text-shadow: none;
  background-color: transparent;
  border: 0;
}
.hidden {
  display: none !important;
}
.affix {
  position: fixed;
}
@-ms-viewport {
  width: device-width;
}
.visible-xs,
.visible-sm,
.visible-md,
.visible-lg {
  display: none !important;
}
.visible-xs-block,
.visible-xs-inline,
.visible-xs-inline-block,
.visible-sm-block,
.visible-sm-inline,
.visible-sm-inline-block,
.visible-md-block,
.visible-md-inline,
.visible-md-inline-block,
.visible-lg-block,
.visible-lg-inline,
.visible-lg-inline-block {
  display: none !important;
}
@media (max-width: 767px) {
  .visible-xs {
    display: block !important;
  }
  table.visible-xs {
    display: table !important;
  }
  tr.visible-xs {
    display: table-row !important;
  }
  th.visible-xs,
  td.visible-xs {
    display: table-cell !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-block {
    display: block !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline {
    display: inline !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm {
    display: block !important;
  }
  table.visible-sm {
    display: table !important;
  }
  tr.visible-sm {
    display: table-row !important;
  }
  th.visible-sm,
  td.visible-sm {
    display: table-cell !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-block {
    display: block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline {
    display: inline !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md {
    display: block !important;
  }
  table.visible-md {
    display: table !important;
  }
  tr.visible-md {
    display: table-row !important;
  }
  th.visible-md,
  td.visible-md {
    display: table-cell !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-block {
    display: block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline {
    display: inline !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg {
    display: block !important;
  }
  table.visible-lg {
    display: table !important;
  }
  tr.visible-lg {
    display: table-row !important;
  }
  th.visible-lg,
  td.visible-lg {
    display: table-cell !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-block {
    display: block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline {
    display: inline !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline-block {
    display: inline-block !important;
  }
}
@media (max-width: 767px) {
  .hidden-xs {
    display: none !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .hidden-sm {
    display: none !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .hidden-md {
    display: none !important;
  }
}
@media (min-width: 1200px) {
  .hidden-lg {
    display: none !important;
  }
}
.visible-print {
  display: none !important;
}
@media print {
  .visible-print {
    display: block !important;
  }
  table.visible-print {
    display: table !important;
  }
  tr.visible-print {
    display: table-row !important;
  }
  th.visible-print,
  td.visible-print {
    display: table-cell !important;
  }
}
.visible-print-block {
  display: none !important;
}
@media print {
  .visible-print-block {
    display: block !important;
  }
}
.visible-print-inline {
  display: none !important;
}
@media print {
  .visible-print-inline {
    display: inline !important;
  }
}
.visible-print-inline-block {
  display: none !important;
}
@media print {
  .visible-print-inline-block {
    display: inline-block !important;
  }
}
@media print {
  .hidden-print {
    display: none !important;
  }
}
/*!
*
* Font Awesome
*
*/
/*!
 *  Font Awesome 4.2.0 by @davegandy - http://fontawesome.io - @fontawesome
 *  License - http://fontawesome.io/license (Font: SIL OFL 1.1, CSS: MIT License)
 */
/* FONT PATH
 * -------------------------- */
@font-face {
  font-family: 'FontAwesome';
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?v=4.2.0');
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?#iefix&v=4.2.0') format('embedded-opentype'), url('../components/font-awesome/fonts/fontawesome-webfont.woff?v=4.2.0') format('woff'), url('../components/font-awesome/fonts/fontawesome-webfont.ttf?v=4.2.0') format('truetype'), url('../components/font-awesome/fonts/fontawesome-webfont.svg?v=4.2.0#fontawesomeregular') format('svg');
  font-weight: normal;
  font-style: normal;
}
.fa {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
/* makes the font 33% larger relative to the icon container */
.fa-lg {
  font-size: 1.33333333em;
  line-height: 0.75em;
  vertical-align: -15%;
}
.fa-2x {
  font-size: 2em;
}
.fa-3x {
  font-size: 3em;
}
.fa-4x {
  font-size: 4em;
}
.fa-5x {
  font-size: 5em;
}
.fa-fw {
  width: 1.28571429em;
  text-align: center;
}
.fa-ul {
  padding-left: 0;
  margin-left: 2.14285714em;
  list-style-type: none;
}
.fa-ul > li {
  position: relative;
}
.fa-li {
  position: absolute;
  left: -2.14285714em;
  width: 2.14285714em;
  top: 0.14285714em;
  text-align: center;
}
.fa-li.fa-lg {
  left: -1.85714286em;
}
.fa-border {
  padding: .2em .25em .15em;
  border: solid 0.08em #eee;
  border-radius: .1em;
}
.pull-right {
  float: right;
}
.pull-left {
  float: left;
}
.fa.pull-left {
  margin-right: .3em;
}
.fa.pull-right {
  margin-left: .3em;
}
.fa-spin {
  -webkit-animation: fa-spin 2s infinite linear;
  animation: fa-spin 2s infinite linear;
}
@-webkit-keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
@keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
.fa-rotate-90 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=1);
  -webkit-transform: rotate(90deg);
  -ms-transform: rotate(90deg);
  transform: rotate(90deg);
}
.fa-rotate-180 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=2);
  -webkit-transform: rotate(180deg);
  -ms-transform: rotate(180deg);
  transform: rotate(180deg);
}
.fa-rotate-270 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=3);
  -webkit-transform: rotate(270deg);
  -ms-transform: rotate(270deg);
  transform: rotate(270deg);
}
.fa-flip-horizontal {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1);
  -webkit-transform: scale(-1, 1);
  -ms-transform: scale(-1, 1);
  transform: scale(-1, 1);
}
.fa-flip-vertical {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1);
  -webkit-transform: scale(1, -1);
  -ms-transform: scale(1, -1);
  transform: scale(1, -1);
}
:root .fa-rotate-90,
:root .fa-rotate-180,
:root .fa-rotate-270,
:root .fa-flip-horizontal,
:root .fa-flip-vertical {
  filter: none;
}
.fa-stack {
  position: relative;
  display: inline-block;
  width: 2em;
  height: 2em;
  line-height: 2em;
  vertical-align: middle;
}
.fa-stack-1x,
.fa-stack-2x {
  position: absolute;
  left: 0;
  width: 100%;
  text-align: center;
}
.fa-stack-1x {
  line-height: inherit;
}
.fa-stack-2x {
  font-size: 2em;
}
.fa-inverse {
  color: #fff;
}
/* Font Awesome uses the Unicode Private Use Area (PUA) to ensure screen
   readers do not read off random characters that represent icons */
.fa-glass:before {
  content: "\f000";
}
.fa-music:before {
  content: "\f001";
}
.fa-search:before {
  content: "\f002";
}
.fa-envelope-o:before {
  content: "\f003";
}
.fa-heart:before {
  content: "\f004";
}
.fa-star:before {
  content: "\f005";
}
.fa-star-o:before {
  content: "\f006";
}
.fa-user:before {
  content: "\f007";
}
.fa-film:before {
  content: "\f008";
}
.fa-th-large:before {
  content: "\f009";
}
.fa-th:before {
  content: "\f00a";
}
.fa-th-list:before {
  content: "\f00b";
}
.fa-check:before {
  content: "\f00c";
}
.fa-remove:before,
.fa-close:before,
.fa-times:before {
  content: "\f00d";
}
.fa-search-plus:before {
  content: "\f00e";
}
.fa-search-minus:before {
  content: "\f010";
}
.fa-power-off:before {
  content: "\f011";
}
.fa-signal:before {
  content: "\f012";
}
.fa-gear:before,
.fa-cog:before {
  content: "\f013";
}
.fa-trash-o:before {
  content: "\f014";
}
.fa-home:before {
  content: "\f015";
}
.fa-file-o:before {
  content: "\f016";
}
.fa-clock-o:before {
  content: "\f017";
}
.fa-road:before {
  content: "\f018";
}
.fa-download:before {
  content: "\f019";
}
.fa-arrow-circle-o-down:before {
  content: "\f01a";
}
.fa-arrow-circle-o-up:before {
  content: "\f01b";
}
.fa-inbox:before {
  content: "\f01c";
}
.fa-play-circle-o:before {
  content: "\f01d";
}
.fa-rotate-right:before,
.fa-repeat:before {
  content: "\f01e";
}
.fa-refresh:before {
  content: "\f021";
}
.fa-list-alt:before {
  content: "\f022";
}
.fa-lock:before {
  content: "\f023";
}
.fa-flag:before {
  content: "\f024";
}
.fa-headphones:before {
  content: "\f025";
}
.fa-volume-off:before {
  content: "\f026";
}
.fa-volume-down:before {
  content: "\f027";
}
.fa-volume-up:before {
  content: "\f028";
}
.fa-qrcode:before {
  content: "\f029";
}
.fa-barcode:before {
  content: "\f02a";
}
.fa-tag:before {
  content: "\f02b";
}
.fa-tags:before {
  content: "\f02c";
}
.fa-book:before {
  content: "\f02d";
}
.fa-bookmark:before {
  content: "\f02e";
}
.fa-print:before {
  content: "\f02f";
}
.fa-camera:before {
  content: "\f030";
}
.fa-font:before {
  content: "\f031";
}
.fa-bold:before {
  content: "\f032";
}
.fa-italic:before {
  content: "\f033";
}
.fa-text-height:before {
  content: "\f034";
}
.fa-text-width:before {
  content: "\f035";
}
.fa-align-left:before {
  content: "\f036";
}
.fa-align-center:before {
  content: "\f037";
}
.fa-align-right:before {
  content: "\f038";
}
.fa-align-justify:before {
  content: "\f039";
}
.fa-list:before {
  content: "\f03a";
}
.fa-dedent:before,
.fa-outdent:before {
  content: "\f03b";
}
.fa-indent:before {
  content: "\f03c";
}
.fa-video-camera:before {
  content: "\f03d";
}
.fa-photo:before,
.fa-image:before,
.fa-picture-o:before {
  content: "\f03e";
}
.fa-pencil:before {
  content: "\f040";
}
.fa-map-marker:before {
  content: "\f041";
}
.fa-adjust:before {
  content: "\f042";
}
.fa-tint:before {
  content: "\f043";
}
.fa-edit:before,
.fa-pencil-square-o:before {
  content: "\f044";
}
.fa-share-square-o:before {
  content: "\f045";
}
.fa-check-square-o:before {
  content: "\f046";
}
.fa-arrows:before {
  content: "\f047";
}
.fa-step-backward:before {
  content: "\f048";
}
.fa-fast-backward:before {
  content: "\f049";
}
.fa-backward:before {
  content: "\f04a";
}
.fa-play:before {
  content: "\f04b";
}
.fa-pause:before {
  content: "\f04c";
}
.fa-stop:before {
  content: "\f04d";
}
.fa-forward:before {
  content: "\f04e";
}
.fa-fast-forward:before {
  content: "\f050";
}
.fa-step-forward:before {
  content: "\f051";
}
.fa-eject:before {
  content: "\f052";
}
.fa-chevron-left:before {
  content: "\f053";
}
.fa-chevron-right:before {
  content: "\f054";
}
.fa-plus-circle:before {
  content: "\f055";
}
.fa-minus-circle:before {
  content: "\f056";
}
.fa-times-circle:before {
  content: "\f057";
}
.fa-check-circle:before {
  content: "\f058";
}
.fa-question-circle:before {
  content: "\f059";
}
.fa-info-circle:before {
  content: "\f05a";
}
.fa-crosshairs:before {
  content: "\f05b";
}
.fa-times-circle-o:before {
  content: "\f05c";
}
.fa-check-circle-o:before {
  content: "\f05d";
}
.fa-ban:before {
  content: "\f05e";
}
.fa-arrow-left:before {
  content: "\f060";
}
.fa-arrow-right:before {
  content: "\f061";
}
.fa-arrow-up:before {
  content: "\f062";
}
.fa-arrow-down:before {
  content: "\f063";
}
.fa-mail-forward:before,
.fa-share:before {
  content: "\f064";
}
.fa-expand:before {
  content: "\f065";
}
.fa-compress:before {
  content: "\f066";
}
.fa-plus:before {
  content: "\f067";
}
.fa-minus:before {
  content: "\f068";
}
.fa-asterisk:before {
  content: "\f069";
}
.fa-exclamation-circle:before {
  content: "\f06a";
}
.fa-gift:before {
  content: "\f06b";
}
.fa-leaf:before {
  content: "\f06c";
}
.fa-fire:before {
  content: "\f06d";
}
.fa-eye:before {
  content: "\f06e";
}
.fa-eye-slash:before {
  content: "\f070";
}
.fa-warning:before,
.fa-exclamation-triangle:before {
  content: "\f071";
}
.fa-plane:before {
  content: "\f072";
}
.fa-calendar:before {
  content: "\f073";
}
.fa-random:before {
  content: "\f074";
}
.fa-comment:before {
  content: "\f075";
}
.fa-magnet:before {
  content: "\f076";
}
.fa-chevron-up:before {
  content: "\f077";
}
.fa-chevron-down:before {
  content: "\f078";
}
.fa-retweet:before {
  content: "\f079";
}
.fa-shopping-cart:before {
  content: "\f07a";
}
.fa-folder:before {
  content: "\f07b";
}
.fa-folder-open:before {
  content: "\f07c";
}
.fa-arrows-v:before {
  content: "\f07d";
}
.fa-arrows-h:before {
  content: "\f07e";
}
.fa-bar-chart-o:before,
.fa-bar-chart:before {
  content: "\f080";
}
.fa-twitter-square:before {
  content: "\f081";
}
.fa-facebook-square:before {
  content: "\f082";
}
.fa-camera-retro:before {
  content: "\f083";
}
.fa-key:before {
  content: "\f084";
}
.fa-gears:before,
.fa-cogs:before {
  content: "\f085";
}
.fa-comments:before {
  content: "\f086";
}
.fa-thumbs-o-up:before {
  content: "\f087";
}
.fa-thumbs-o-down:before {
  content: "\f088";
}
.fa-star-half:before {
  content: "\f089";
}
.fa-heart-o:before {
  content: "\f08a";
}
.fa-sign-out:before {
  content: "\f08b";
}
.fa-linkedin-square:before {
  content: "\f08c";
}
.fa-thumb-tack:before {
  content: "\f08d";
}
.fa-external-link:before {
  content: "\f08e";
}
.fa-sign-in:before {
  content: "\f090";
}
.fa-trophy:before {
  content: "\f091";
}
.fa-github-square:before {
  content: "\f092";
}
.fa-upload:before {
  content: "\f093";
}
.fa-lemon-o:before {
  content: "\f094";
}
.fa-phone:before {
  content: "\f095";
}
.fa-square-o:before {
  content: "\f096";
}
.fa-bookmark-o:before {
  content: "\f097";
}
.fa-phone-square:before {
  content: "\f098";
}
.fa-twitter:before {
  content: "\f099";
}
.fa-facebook:before {
  content: "\f09a";
}
.fa-github:before {
  content: "\f09b";
}
.fa-unlock:before {
  content: "\f09c";
}
.fa-credit-card:before {
  content: "\f09d";
}
.fa-rss:before {
  content: "\f09e";
}
.fa-hdd-o:before {
  content: "\f0a0";
}
.fa-bullhorn:before {
  content: "\f0a1";
}
.fa-bell:before {
  content: "\f0f3";
}
.fa-certificate:before {
  content: "\f0a3";
}
.fa-hand-o-right:before {
  content: "\f0a4";
}
.fa-hand-o-left:before {
  content: "\f0a5";
}
.fa-hand-o-up:before {
  content: "\f0a6";
}
.fa-hand-o-down:before {
  content: "\f0a7";
}
.fa-arrow-circle-left:before {
  content: "\f0a8";
}
.fa-arrow-circle-right:before {
  content: "\f0a9";
}
.fa-arrow-circle-up:before {
  content: "\f0aa";
}
.fa-arrow-circle-down:before {
  content: "\f0ab";
}
.fa-globe:before {
  content: "\f0ac";
}
.fa-wrench:before {
  content: "\f0ad";
}
.fa-tasks:before {
  content: "\f0ae";
}
.fa-filter:before {
  content: "\f0b0";
}
.fa-briefcase:before {
  content: "\f0b1";
}
.fa-arrows-alt:before {
  content: "\f0b2";
}
.fa-group:before,
.fa-users:before {
  content: "\f0c0";
}
.fa-chain:before,
.fa-link:before {
  content: "\f0c1";
}
.fa-cloud:before {
  content: "\f0c2";
}
.fa-flask:before {
  content: "\f0c3";
}
.fa-cut:before,
.fa-scissors:before {
  content: "\f0c4";
}
.fa-copy:before,
.fa-files-o:before {
  content: "\f0c5";
}
.fa-paperclip:before {
  content: "\f0c6";
}
.fa-save:before,
.fa-floppy-o:before {
  content: "\f0c7";
}
.fa-square:before {
  content: "\f0c8";
}
.fa-navicon:before,
.fa-reorder:before,
.fa-bars:before {
  content: "\f0c9";
}
.fa-list-ul:before {
  content: "\f0ca";
}
.fa-list-ol:before {
  content: "\f0cb";
}
.fa-strikethrough:before {
  content: "\f0cc";
}
.fa-underline:before {
  content: "\f0cd";
}
.fa-table:before {
  content: "\f0ce";
}
.fa-magic:before {
  content: "\f0d0";
}
.fa-truck:before {
  content: "\f0d1";
}
.fa-pinterest:before {
  content: "\f0d2";
}
.fa-pinterest-square:before {
  content: "\f0d3";
}
.fa-google-plus-square:before {
  content: "\f0d4";
}
.fa-google-plus:before {
  content: "\f0d5";
}
.fa-money:before {
  content: "\f0d6";
}
.fa-caret-down:before {
  content: "\f0d7";
}
.fa-caret-up:before {
  content: "\f0d8";
}
.fa-caret-left:before {
  content: "\f0d9";
}
.fa-caret-right:before {
  content: "\f0da";
}
.fa-columns:before {
  content: "\f0db";
}
.fa-unsorted:before,
.fa-sort:before {
  content: "\f0dc";
}
.fa-sort-down:before,
.fa-sort-desc:before {
  content: "\f0dd";
}
.fa-sort-up:before,
.fa-sort-asc:before {
  content: "\f0de";
}
.fa-envelope:before {
  content: "\f0e0";
}
.fa-linkedin:before {
  content: "\f0e1";
}
.fa-rotate-left:before,
.fa-undo:before {
  content: "\f0e2";
}
.fa-legal:before,
.fa-gavel:before {
  content: "\f0e3";
}
.fa-dashboard:before,
.fa-tachometer:before {
  content: "\f0e4";
}
.fa-comment-o:before {
  content: "\f0e5";
}
.fa-comments-o:before {
  content: "\f0e6";
}
.fa-flash:before,
.fa-bolt:before {
  content: "\f0e7";
}
.fa-sitemap:before {
  content: "\f0e8";
}
.fa-umbrella:before {
  content: "\f0e9";
}
.fa-paste:before,
.fa-clipboard:before {
  content: "\f0ea";
}
.fa-lightbulb-o:before {
  content: "\f0eb";
}
.fa-exchange:before {
  content: "\f0ec";
}
.fa-cloud-download:before {
  content: "\f0ed";
}
.fa-cloud-upload:before {
  content: "\f0ee";
}
.fa-user-md:before {
  content: "\f0f0";
}
.fa-stethoscope:before {
  content: "\f0f1";
}
.fa-suitcase:before {
  content: "\f0f2";
}
.fa-bell-o:before {
  content: "\f0a2";
}
.fa-coffee:before {
  content: "\f0f4";
}
.fa-cutlery:before {
  content: "\f0f5";
}
.fa-file-text-o:before {
  content: "\f0f6";
}
.fa-building-o:before {
  content: "\f0f7";
}
.fa-hospital-o:before {
  content: "\f0f8";
}
.fa-ambulance:before {
  content: "\f0f9";
}
.fa-medkit:before {
  content: "\f0fa";
}
.fa-fighter-jet:before {
  content: "\f0fb";
}
.fa-beer:before {
  content: "\f0fc";
}
.fa-h-square:before {
  content: "\f0fd";
}
.fa-plus-square:before {
  content: "\f0fe";
}
.fa-angle-double-left:before {
  content: "\f100";
}
.fa-angle-double-right:before {
  content: "\f101";
}
.fa-angle-double-up:before {
  content: "\f102";
}
.fa-angle-double-down:before {
  content: "\f103";
}
.fa-angle-left:before {
  content: "\f104";
}
.fa-angle-right:before {
  content: "\f105";
}
.fa-angle-up:before {
  content: "\f106";
}
.fa-angle-down:before {
  content: "\f107";
}
.fa-desktop:before {
  content: "\f108";
}
.fa-laptop:before {
  content: "\f109";
}
.fa-tablet:before {
  content: "\f10a";
}
.fa-mobile-phone:before,
.fa-mobile:before {
  content: "\f10b";
}
.fa-circle-o:before {
  content: "\f10c";
}
.fa-quote-left:before {
  content: "\f10d";
}
.fa-quote-right:before {
  content: "\f10e";
}
.fa-spinner:before {
  content: "\f110";
}
.fa-circle:before {
  content: "\f111";
}
.fa-mail-reply:before,
.fa-reply:before {
  content: "\f112";
}
.fa-github-alt:before {
  content: "\f113";
}
.fa-folder-o:before {
  content: "\f114";
}
.fa-folder-open-o:before {
  content: "\f115";
}
.fa-smile-o:before {
  content: "\f118";
}
.fa-frown-o:before {
  content: "\f119";
}
.fa-meh-o:before {
  content: "\f11a";
}
.fa-gamepad:before {
  content: "\f11b";
}
.fa-keyboard-o:before {
  content: "\f11c";
}
.fa-flag-o:before {
  content: "\f11d";
}
.fa-flag-checkered:before {
  content: "\f11e";
}
.fa-terminal:before {
  content: "\f120";
}
.fa-code:before {
  content: "\f121";
}
.fa-mail-reply-all:before,
.fa-reply-all:before {
  content: "\f122";
}
.fa-star-half-empty:before,
.fa-star-half-full:before,
.fa-star-half-o:before {
  content: "\f123";
}
.fa-location-arrow:before {
  content: "\f124";
}
.fa-crop:before {
  content: "\f125";
}
.fa-code-fork:before {
  content: "\f126";
}
.fa-unlink:before,
.fa-chain-broken:before {
  content: "\f127";
}
.fa-question:before {
  content: "\f128";
}
.fa-info:before {
  content: "\f129";
}
.fa-exclamation:before {
  content: "\f12a";
}
.fa-superscript:before {
  content: "\f12b";
}
.fa-subscript:before {
  content: "\f12c";
}
.fa-eraser:before {
  content: "\f12d";
}
.fa-puzzle-piece:before {
  content: "\f12e";
}
.fa-microphone:before {
  content: "\f130";
}
.fa-microphone-slash:before {
  content: "\f131";
}
.fa-shield:before {
  content: "\f132";
}
.fa-calendar-o:before {
  content: "\f133";
}
.fa-fire-extinguisher:before {
  content: "\f134";
}
.fa-rocket:before {
  content: "\f135";
}
.fa-maxcdn:before {
  content: "\f136";
}
.fa-chevron-circle-left:before {
  content: "\f137";
}
.fa-chevron-circle-right:before {
  content: "\f138";
}
.fa-chevron-circle-up:before {
  content: "\f139";
}
.fa-chevron-circle-down:before {
  content: "\f13a";
}
.fa-html5:before {
  content: "\f13b";
}
.fa-css3:before {
  content: "\f13c";
}
.fa-anchor:before {
  content: "\f13d";
}
.fa-unlock-alt:before {
  content: "\f13e";
}
.fa-bullseye:before {
  content: "\f140";
}
.fa-ellipsis-h:before {
  content: "\f141";
}
.fa-ellipsis-v:before {
  content: "\f142";
}
.fa-rss-square:before {
  content: "\f143";
}
.fa-play-circle:before {
  content: "\f144";
}
.fa-ticket:before {
  content: "\f145";
}
.fa-minus-square:before {
  content: "\f146";
}
.fa-minus-square-o:before {
  content: "\f147";
}
.fa-level-up:before {
  content: "\f148";
}
.fa-level-down:before {
  content: "\f149";
}
.fa-check-square:before {
  content: "\f14a";
}
.fa-pencil-square:before {
  content: "\f14b";
}
.fa-external-link-square:before {
  content: "\f14c";
}
.fa-share-square:before {
  content: "\f14d";
}
.fa-compass:before {
  content: "\f14e";
}
.fa-toggle-down:before,
.fa-caret-square-o-down:before {
  content: "\f150";
}
.fa-toggle-up:before,
.fa-caret-square-o-up:before {
  content: "\f151";
}
.fa-toggle-right:before,
.fa-caret-square-o-right:before {
  content: "\f152";
}
.fa-euro:before,
.fa-eur:before {
  content: "\f153";
}
.fa-gbp:before {
  content: "\f154";
}
.fa-dollar:before,
.fa-usd:before {
  content: "\f155";
}
.fa-rupee:before,
.fa-inr:before {
  content: "\f156";
}
.fa-cny:before,
.fa-rmb:before,
.fa-yen:before,
.fa-jpy:before {
  content: "\f157";
}
.fa-ruble:before,
.fa-rouble:before,
.fa-rub:before {
  content: "\f158";
}
.fa-won:before,
.fa-krw:before {
  content: "\f159";
}
.fa-bitcoin:before,
.fa-btc:before {
  content: "\f15a";
}
.fa-file:before {
  content: "\f15b";
}
.fa-file-text:before {
  content: "\f15c";
}
.fa-sort-alpha-asc:before {
  content: "\f15d";
}
.fa-sort-alpha-desc:before {
  content: "\f15e";
}
.fa-sort-amount-asc:before {
  content: "\f160";
}
.fa-sort-amount-desc:before {
  content: "\f161";
}
.fa-sort-numeric-asc:before {
  content: "\f162";
}
.fa-sort-numeric-desc:before {
  content: "\f163";
}
.fa-thumbs-up:before {
  content: "\f164";
}
.fa-thumbs-down:before {
  content: "\f165";
}
.fa-youtube-square:before {
  content: "\f166";
}
.fa-youtube:before {
  content: "\f167";
}
.fa-xing:before {
  content: "\f168";
}
.fa-xing-square:before {
  content: "\f169";
}
.fa-youtube-play:before {
  content: "\f16a";
}
.fa-dropbox:before {
  content: "\f16b";
}
.fa-stack-overflow:before {
  content: "\f16c";
}
.fa-instagram:before {
  content: "\f16d";
}
.fa-flickr:before {
  content: "\f16e";
}
.fa-adn:before {
  content: "\f170";
}
.fa-bitbucket:before {
  content: "\f171";
}
.fa-bitbucket-square:before {
  content: "\f172";
}
.fa-tumblr:before {
  content: "\f173";
}
.fa-tumblr-square:before {
  content: "\f174";
}
.fa-long-arrow-down:before {
  content: "\f175";
}
.fa-long-arrow-up:before {
  content: "\f176";
}
.fa-long-arrow-left:before {
  content: "\f177";
}
.fa-long-arrow-right:before {
  content: "\f178";
}
.fa-apple:before {
  content: "\f179";
}
.fa-windows:before {
  content: "\f17a";
}
.fa-android:before {
  content: "\f17b";
}
.fa-linux:before {
  content: "\f17c";
}
.fa-dribbble:before {
  content: "\f17d";
}
.fa-skype:before {
  content: "\f17e";
}
.fa-foursquare:before {
  content: "\f180";
}
.fa-trello:before {
  content: "\f181";
}
.fa-female:before {
  content: "\f182";
}
.fa-male:before {
  content: "\f183";
}
.fa-gittip:before {
  content: "\f184";
}
.fa-sun-o:before {
  content: "\f185";
}
.fa-moon-o:before {
  content: "\f186";
}
.fa-archive:before {
  content: "\f187";
}
.fa-bug:before {
  content: "\f188";
}
.fa-vk:before {
  content: "\f189";
}
.fa-weibo:before {
  content: "\f18a";
}
.fa-renren:before {
  content: "\f18b";
}
.fa-pagelines:before {
  content: "\f18c";
}
.fa-stack-exchange:before {
  content: "\f18d";
}
.fa-arrow-circle-o-right:before {
  content: "\f18e";
}
.fa-arrow-circle-o-left:before {
  content: "\f190";
}
.fa-toggle-left:before,
.fa-caret-square-o-left:before {
  content: "\f191";
}
.fa-dot-circle-o:before {
  content: "\f192";
}
.fa-wheelchair:before {
  content: "\f193";
}
.fa-vimeo-square:before {
  content: "\f194";
}
.fa-turkish-lira:before,
.fa-try:before {
  content: "\f195";
}
.fa-plus-square-o:before {
  content: "\f196";
}
.fa-space-shuttle:before {
  content: "\f197";
}
.fa-slack:before {
  content: "\f198";
}
.fa-envelope-square:before {
  content: "\f199";
}
.fa-wordpress:before {
  content: "\f19a";
}
.fa-openid:before {
  content: "\f19b";
}
.fa-institution:before,
.fa-bank:before,
.fa-university:before {
  content: "\f19c";
}
.fa-mortar-board:before,
.fa-graduation-cap:before {
  content: "\f19d";
}
.fa-yahoo:before {
  content: "\f19e";
}
.fa-google:before {
  content: "\f1a0";
}
.fa-reddit:before {
  content: "\f1a1";
}
.fa-reddit-square:before {
  content: "\f1a2";
}
.fa-stumbleupon-circle:before {
  content: "\f1a3";
}
.fa-stumbleupon:before {
  content: "\f1a4";
}
.fa-delicious:before {
  content: "\f1a5";
}
.fa-digg:before {
  content: "\f1a6";
}
.fa-pied-piper:before {
  content: "\f1a7";
}
.fa-pied-piper-alt:before {
  content: "\f1a8";
}
.fa-drupal:before {
  content: "\f1a9";
}
.fa-joomla:before {
  content: "\f1aa";
}
.fa-language:before {
  content: "\f1ab";
}
.fa-fax:before {
  content: "\f1ac";
}
.fa-building:before {
  content: "\f1ad";
}
.fa-child:before {
  content: "\f1ae";
}
.fa-paw:before {
  content: "\f1b0";
}
.fa-spoon:before {
  content: "\f1b1";
}
.fa-cube:before {
  content: "\f1b2";
}
.fa-cubes:before {
  content: "\f1b3";
}
.fa-behance:before {
  content: "\f1b4";
}
.fa-behance-square:before {
  content: "\f1b5";
}
.fa-steam:before {
  content: "\f1b6";
}
.fa-steam-square:before {
  content: "\f1b7";
}
.fa-recycle:before {
  content: "\f1b8";
}
.fa-automobile:before,
.fa-car:before {
  content: "\f1b9";
}
.fa-cab:before,
.fa-taxi:before {
  content: "\f1ba";
}
.fa-tree:before {
  content: "\f1bb";
}
.fa-spotify:before {
  content: "\f1bc";
}
.fa-deviantart:before {
  content: "\f1bd";
}
.fa-soundcloud:before {
  content: "\f1be";
}
.fa-database:before {
  content: "\f1c0";
}
.fa-file-pdf-o:before {
  content: "\f1c1";
}
.fa-file-word-o:before {
  content: "\f1c2";
}
.fa-file-excel-o:before {
  content: "\f1c3";
}
.fa-file-powerpoint-o:before {
  content: "\f1c4";
}
.fa-file-photo-o:before,
.fa-file-picture-o:before,
.fa-file-image-o:before {
  content: "\f1c5";
}
.fa-file-zip-o:before,
.fa-file-archive-o:before {
  content: "\f1c6";
}
.fa-file-sound-o:before,
.fa-file-audio-o:before {
  content: "\f1c7";
}
.fa-file-movie-o:before,
.fa-file-video-o:before {
  content: "\f1c8";
}
.fa-file-code-o:before {
  content: "\f1c9";
}
.fa-vine:before {
  content: "\f1ca";
}
.fa-codepen:before {
  content: "\f1cb";
}
.fa-jsfiddle:before {
  content: "\f1cc";
}
.fa-life-bouy:before,
.fa-life-buoy:before,
.fa-life-saver:before,
.fa-support:before,
.fa-life-ring:before {
  content: "\f1cd";
}
.fa-circle-o-notch:before {
  content: "\f1ce";
}
.fa-ra:before,
.fa-rebel:before {
  content: "\f1d0";
}
.fa-ge:before,
.fa-empire:before {
  content: "\f1d1";
}
.fa-git-square:before {
  content: "\f1d2";
}
.fa-git:before {
  content: "\f1d3";
}
.fa-hacker-news:before {
  content: "\f1d4";
}
.fa-tencent-weibo:before {
  content: "\f1d5";
}
.fa-qq:before {
  content: "\f1d6";
}
.fa-wechat:before,
.fa-weixin:before {
  content: "\f1d7";
}
.fa-send:before,
.fa-paper-plane:before {
  content: "\f1d8";
}
.fa-send-o:before,
.fa-paper-plane-o:before {
  content: "\f1d9";
}
.fa-history:before {
  content: "\f1da";
}
.fa-circle-thin:before {
  content: "\f1db";
}
.fa-header:before {
  content: "\f1dc";
}
.fa-paragraph:before {
  content: "\f1dd";
}
.fa-sliders:before {
  content: "\f1de";
}
.fa-share-alt:before {
  content: "\f1e0";
}
.fa-share-alt-square:before {
  content: "\f1e1";
}
.fa-bomb:before {
  content: "\f1e2";
}
.fa-soccer-ball-o:before,
.fa-futbol-o:before {
  content: "\f1e3";
}
.fa-tty:before {
  content: "\f1e4";
}
.fa-binoculars:before {
  content: "\f1e5";
}
.fa-plug:before {
  content: "\f1e6";
}
.fa-slideshare:before {
  content: "\f1e7";
}
.fa-twitch:before {
  content: "\f1e8";
}
.fa-yelp:before {
  content: "\f1e9";
}
.fa-newspaper-o:before {
  content: "\f1ea";
}
.fa-wifi:before {
  content: "\f1eb";
}
.fa-calculator:before {
  content: "\f1ec";
}
.fa-paypal:before {
  content: "\f1ed";
}
.fa-google-wallet:before {
  content: "\f1ee";
}
.fa-cc-visa:before {
  content: "\f1f0";
}
.fa-cc-mastercard:before {
  content: "\f1f1";
}
.fa-cc-discover:before {
  content: "\f1f2";
}
.fa-cc-amex:before {
  content: "\f1f3";
}
.fa-cc-paypal:before {
  content: "\f1f4";
}
.fa-cc-stripe:before {
  content: "\f1f5";
}
.fa-bell-slash:before {
  content: "\f1f6";
}
.fa-bell-slash-o:before {
  content: "\f1f7";
}
.fa-trash:before {
  content: "\f1f8";
}
.fa-copyright:before {
  content: "\f1f9";
}
.fa-at:before {
  content: "\f1fa";
}
.fa-eyedropper:before {
  content: "\f1fb";
}
.fa-paint-brush:before {
  content: "\f1fc";
}
.fa-birthday-cake:before {
  content: "\f1fd";
}
.fa-area-chart:before {
  content: "\f1fe";
}
.fa-pie-chart:before {
  content: "\f200";
}
.fa-line-chart:before {
  content: "\f201";
}
.fa-lastfm:before {
  content: "\f202";
}
.fa-lastfm-square:before {
  content: "\f203";
}
.fa-toggle-off:before {
  content: "\f204";
}
.fa-toggle-on:before {
  content: "\f205";
}
.fa-bicycle:before {
  content: "\f206";
}
.fa-bus:before {
  content: "\f207";
}
.fa-ioxhost:before {
  content: "\f208";
}
.fa-angellist:before {
  content: "\f209";
}
.fa-cc:before {
  content: "\f20a";
}
.fa-shekel:before,
.fa-sheqel:before,
.fa-ils:before {
  content: "\f20b";
}
.fa-meanpath:before {
  content: "\f20c";
}
/*!
*
* IPython base
*
*/
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
code {
  color: #000;
}
pre {
  font-size: inherit;
  line-height: inherit;
}
label {
  font-weight: normal;
}
/* Make the page background atleast 100% the height of the view port */
/* Make the page itself atleast 70% the height of the view port */
.border-box-sizing {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.corner-all {
  border-radius: 2px;
}
.no-padding {
  padding: 0px;
}
/* Flexible box model classes */
/* Taken from Alex Russell http://infrequently.org/2009/08/css-3-progress/ */
/* This file is a compatability layer.  It allows the usage of flexible box 
model layouts accross multiple browsers, including older browsers.  The newest,
universal implementation of the flexible box model is used when available (see
`Modern browsers` comments below).  Browsers that are known to implement this 
new spec completely include:

    Firefox 28.0+
    Chrome 29.0+
    Internet Explorer 11+ 
    Opera 17.0+

Browsers not listed, including Safari, are supported via the styling under the
`Old browsers` comments below.
*/
.hbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
.hbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.vbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
.vbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.hbox.reverse,
.vbox.reverse,
.reverse {
  /* Old browsers */
  -webkit-box-direction: reverse;
  -moz-box-direction: reverse;
  box-direction: reverse;
  /* Modern browsers */
  flex-direction: row-reverse;
}
.hbox.box-flex0,
.vbox.box-flex0,
.box-flex0 {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
  width: auto;
}
.hbox.box-flex1,
.vbox.box-flex1,
.box-flex1 {
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex,
.vbox.box-flex,
.box-flex {
  /* Old browsers */
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex2,
.vbox.box-flex2,
.box-flex2 {
  /* Old browsers */
  -webkit-box-flex: 2;
  -moz-box-flex: 2;
  box-flex: 2;
  /* Modern browsers */
  flex: 2;
}
.box-group1 {
  /*  Deprecated */
  -webkit-box-flex-group: 1;
  -moz-box-flex-group: 1;
  box-flex-group: 1;
}
.box-group2 {
  /* Deprecated */
  -webkit-box-flex-group: 2;
  -moz-box-flex-group: 2;
  box-flex-group: 2;
}
.hbox.start,
.vbox.start,
.start {
  /* Old browsers */
  -webkit-box-pack: start;
  -moz-box-pack: start;
  box-pack: start;
  /* Modern browsers */
  justify-content: flex-start;
}
.hbox.end,
.vbox.end,
.end {
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
}
.hbox.center,
.vbox.center,
.center {
  /* Old browsers */
  -webkit-box-pack: center;
  -moz-box-pack: center;
  box-pack: center;
  /* Modern browsers */
  justify-content: center;
}
.hbox.baseline,
.vbox.baseline,
.baseline {
  /* Old browsers */
  -webkit-box-pack: baseline;
  -moz-box-pack: baseline;
  box-pack: baseline;
  /* Modern browsers */
  justify-content: baseline;
}
.hbox.stretch,
.vbox.stretch,
.stretch {
  /* Old browsers */
  -webkit-box-pack: stretch;
  -moz-box-pack: stretch;
  box-pack: stretch;
  /* Modern browsers */
  justify-content: stretch;
}
.hbox.align-start,
.vbox.align-start,
.align-start {
  /* Old browsers */
  -webkit-box-align: start;
  -moz-box-align: start;
  box-align: start;
  /* Modern browsers */
  align-items: flex-start;
}
.hbox.align-end,
.vbox.align-end,
.align-end {
  /* Old browsers */
  -webkit-box-align: end;
  -moz-box-align: end;
  box-align: end;
  /* Modern browsers */
  align-items: flex-end;
}
.hbox.align-center,
.vbox.align-center,
.align-center {
  /* Old browsers */
  -webkit-box-align: center;
  -moz-box-align: center;
  box-align: center;
  /* Modern browsers */
  align-items: center;
}
.hbox.align-baseline,
.vbox.align-baseline,
.align-baseline {
  /* Old browsers */
  -webkit-box-align: baseline;
  -moz-box-align: baseline;
  box-align: baseline;
  /* Modern browsers */
  align-items: baseline;
}
.hbox.align-stretch,
.vbox.align-stretch,
.align-stretch {
  /* Old browsers */
  -webkit-box-align: stretch;
  -moz-box-align: stretch;
  box-align: stretch;
  /* Modern browsers */
  align-items: stretch;
}
div.error {
  margin: 2em;
  text-align: center;
}
div.error > h1 {
  font-size: 500%;
  line-height: normal;
}
div.error > p {
  font-size: 200%;
  line-height: normal;
}
div.traceback-wrapper {
  text-align: left;
  max-width: 800px;
  margin: auto;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
body {
  background-color: #fff;
  /* This makes sure that the body covers the entire window and needs to
       be in a different element than the display: box in wrapper below */
  position: absolute;
  left: 0px;
  right: 0px;
  top: 0px;
  bottom: 0px;
  overflow: visible;
}
body > #header {
  /* Initially hidden to prevent FLOUC */
  display: none;
  background-color: #fff;
  /* Display over codemirror */
  position: relative;
  z-index: 100;
}
body > #header #header-container {
  padding-bottom: 5px;
  padding-top: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
body > #header .header-bar {
  width: 100%;
  height: 1px;
  background: #e7e7e7;
  margin-bottom: -1px;
}
@media print {
  body > #header {
    display: none !important;
  }
}
#header-spacer {
  width: 100%;
  visibility: hidden;
}
@media print {
  #header-spacer {
    display: none;
  }
}
#ipython_notebook {
  padding-left: 0px;
  padding-top: 1px;
  padding-bottom: 1px;
}
@media (max-width: 991px) {
  #ipython_notebook {
    margin-left: 10px;
  }
}
[dir="rtl"] #ipython_notebook {
  float: right !important;
}
#noscript {
  width: auto;
  padding-top: 16px;
  padding-bottom: 16px;
  text-align: center;
  font-size: 22px;
  color: red;
  font-weight: bold;
}
#ipython_notebook img {
  height: 28px;
}
#site {
  width: 100%;
  display: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  overflow: auto;
}
@media print {
  #site {
    height: auto !important;
  }
}
/* Smaller buttons */
.ui-button .ui-button-text {
  padding: 0.2em 0.8em;
  font-size: 77%;
}
input.ui-button {
  padding: 0.3em 0.9em;
}
span#login_widget {
  float: right;
}
span#login_widget > .button,
#logout {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button:focus,
#logout:focus,
span#login_widget > .button.focus,
#logout.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
span#login_widget > .button:hover,
#logout:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active:hover,
#logout:active:hover,
span#login_widget > .button.active:hover,
#logout.active:hover,
.open > .dropdown-togglespan#login_widget > .button:hover,
.open > .dropdown-toggle#logout:hover,
span#login_widget > .button:active:focus,
#logout:active:focus,
span#login_widget > .button.active:focus,
#logout.active:focus,
.open > .dropdown-togglespan#login_widget > .button:focus,
.open > .dropdown-toggle#logout:focus,
span#login_widget > .button:active.focus,
#logout:active.focus,
span#login_widget > .button.active.focus,
#logout.active.focus,
.open > .dropdown-togglespan#login_widget > .button.focus,
.open > .dropdown-toggle#logout.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  background-image: none;
}
span#login_widget > .button.disabled:hover,
#logout.disabled:hover,
span#login_widget > .button[disabled]:hover,
#logout[disabled]:hover,
fieldset[disabled] span#login_widget > .button:hover,
fieldset[disabled] #logout:hover,
span#login_widget > .button.disabled:focus,
#logout.disabled:focus,
span#login_widget > .button[disabled]:focus,
#logout[disabled]:focus,
fieldset[disabled] span#login_widget > .button:focus,
fieldset[disabled] #logout:focus,
span#login_widget > .button.disabled.focus,
#logout.disabled.focus,
span#login_widget > .button[disabled].focus,
#logout[disabled].focus,
fieldset[disabled] span#login_widget > .button.focus,
fieldset[disabled] #logout.focus {
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button .badge,
#logout .badge {
  color: #fff;
  background-color: #333;
}
.nav-header {
  text-transform: none;
}
#header > span {
  margin-top: 10px;
}
.modal_stretch .modal-dialog {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  min-height: 80vh;
}
.modal_stretch .modal-dialog .modal-body {
  max-height: calc(100vh - 200px);
  overflow: auto;
  flex: 1;
}
@media (min-width: 768px) {
  .modal .modal-dialog {
    width: 700px;
  }
}
@media (min-width: 768px) {
  select.form-control {
    margin-left: 12px;
    margin-right: 12px;
  }
}
/*!
*
* IPython auth
*
*/
.center-nav {
  display: inline-block;
  margin-bottom: -4px;
}
/*!
*
* IPython tree view
*
*/
/* We need an invisible input field on top of the sentense*/
/* "Drag file onto the list ..." */
.alternate_upload {
  background-color: none;
  display: inline;
}
.alternate_upload.form {
  padding: 0;
  margin: 0;
}
.alternate_upload input.fileinput {
  text-align: center;
  vertical-align: middle;
  display: inline;
  opacity: 0;
  z-index: 2;
  width: 12ex;
  margin-right: -12ex;
}
.alternate_upload .btn-upload {
  height: 22px;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
[dir="rtl"] #tabs li {
  float: right;
}
ul#tabs {
  margin-bottom: 4px;
}
[dir="rtl"] ul#tabs {
  margin-right: 0px;
}
ul#tabs a {
  padding-top: 6px;
  padding-bottom: 4px;
}
ul.breadcrumb a:focus,
ul.breadcrumb a:hover {
  text-decoration: none;
}
ul.breadcrumb i.icon-home {
  font-size: 16px;
  margin-right: 4px;
}
ul.breadcrumb span {
  color: #5e5e5e;
}
.list_toolbar {
  padding: 4px 0 4px 0;
  vertical-align: middle;
}
.list_toolbar .tree-buttons {
  padding-top: 1px;
}
[dir="rtl"] .list_toolbar .tree-buttons {
  float: left !important;
}
[dir="rtl"] .list_toolbar .pull-right {
  padding-top: 1px;
  float: left !important;
}
[dir="rtl"] .list_toolbar .pull-left {
  float: right !important;
}
.dynamic-buttons {
  padding-top: 3px;
  display: inline-block;
}
.list_toolbar [class*="span"] {
  min-height: 24px;
}
.list_header {
  font-weight: bold;
  background-color: #EEE;
}
.list_placeholder {
  font-weight: bold;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
}
.list_container {
  margin-top: 4px;
  margin-bottom: 20px;
  border: 1px solid #ddd;
  border-radius: 2px;
}
.list_container > div {
  border-bottom: 1px solid #ddd;
}
.list_container > div:hover .list-item {
  background-color: red;
}
.list_container > div:last-child {
  border: none;
}
.list_item:hover .list_item {
  background-color: #ddd;
}
.list_item a {
  text-decoration: none;
}
.list_item:hover {
  background-color: #fafafa;
}
.list_header > div,
.list_item > div {
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
.list_header > div input,
.list_item > div input {
  margin-right: 7px;
  margin-left: 14px;
  vertical-align: baseline;
  line-height: 22px;
  position: relative;
  top: -1px;
}
.list_header > div .item_link,
.list_item > div .item_link {
  margin-left: -1px;
  vertical-align: baseline;
  line-height: 22px;
}
.new-file input[type=checkbox] {
  visibility: hidden;
}
.item_name {
  line-height: 22px;
  height: 24px;
}
.item_icon {
  font-size: 14px;
  color: #5e5e5e;
  margin-right: 7px;
  margin-left: 7px;
  line-height: 22px;
  vertical-align: baseline;
}
.item_buttons {
  line-height: 1em;
  margin-left: -5px;
}
.item_buttons .btn,
.item_buttons .btn-group,
.item_buttons .input-group {
  float: left;
}
.item_buttons > .btn,
.item_buttons > .btn-group,
.item_buttons > .input-group {
  margin-left: 5px;
}
.item_buttons .btn {
  min-width: 13ex;
}
.item_buttons .running-indicator {
  padding-top: 4px;
  color: #5cb85c;
}
.item_buttons .kernel-name {
  padding-top: 4px;
  color: #5bc0de;
  margin-right: 7px;
  float: left;
}
.toolbar_info {
  height: 24px;
  line-height: 24px;
}
.list_item input:not([type=checkbox]) {
  padding-top: 3px;
  padding-bottom: 3px;
  height: 22px;
  line-height: 14px;
  margin: 0px;
}
.highlight_text {
  color: blue;
}
#project_name {
  display: inline-block;
  padding-left: 7px;
  margin-left: -2px;
}
#project_name > .breadcrumb {
  padding: 0px;
  margin-bottom: 0px;
  background-color: transparent;
  font-weight: bold;
}
#tree-selector {
  padding-right: 0px;
}
[dir="rtl"] #tree-selector a {
  float: right;
}
#button-select-all {
  min-width: 50px;
}
#select-all {
  margin-left: 7px;
  margin-right: 2px;
}
.menu_icon {
  margin-right: 2px;
}
.tab-content .row {
  margin-left: 0px;
  margin-right: 0px;
}
.folder_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f114";
}
.folder_icon:before.pull-left {
  margin-right: .3em;
}
.folder_icon:before.pull-right {
  margin-left: .3em;
}
.notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
}
.notebook_icon:before.pull-left {
  margin-right: .3em;
}
.notebook_icon:before.pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
  color: #5cb85c;
}
.running_notebook_icon:before.pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.pull-right {
  margin-left: .3em;
}
.file_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f016";
  position: relative;
  top: -2px;
}
.file_icon:before.pull-left {
  margin-right: .3em;
}
.file_icon:before.pull-right {
  margin-left: .3em;
}
#notebook_toolbar .pull-right {
  padding-top: 0px;
  margin-right: -1px;
}
ul#new-menu {
  left: auto;
  right: 0;
}
[dir="rtl"] #new-menu {
  text-align: right;
}
.kernel-menu-icon {
  padding-right: 12px;
  width: 24px;
  content: "\f096";
}
.kernel-menu-icon:before {
  content: "\f096";
}
.kernel-menu-icon-current:before {
  content: "\f00c";
}
#tab_content {
  padding-top: 20px;
}
#running .panel-group .panel {
  margin-top: 3px;
  margin-bottom: 1em;
}
#running .panel-group .panel .panel-heading {
  background-color: #EEE;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
#running .panel-group .panel .panel-heading a:focus,
#running .panel-group .panel .panel-heading a:hover {
  text-decoration: none;
}
#running .panel-group .panel .panel-body {
  padding: 0px;
}
#running .panel-group .panel .panel-body .list_container {
  margin-top: 0px;
  margin-bottom: 0px;
  border: 0px;
  border-radius: 0px;
}
#running .panel-group .panel .panel-body .list_container .list_item {
  border-bottom: 1px solid #ddd;
}
#running .panel-group .panel .panel-body .list_container .list_item:last-child {
  border-bottom: 0px;
}
[dir="rtl"] #running .col-sm-8 {
  float: right !important;
}
.delete-button {
  display: none;
}
.duplicate-button {
  display: none;
}
.rename-button {
  display: none;
}
.shutdown-button {
  display: none;
}
.dynamic-instructions {
  display: inline-block;
  padding-top: 4px;
}
/*!
*
* IPython text editor webapp
*
*/
.selected-keymap i.fa {
  padding: 0px 5px;
}
.selected-keymap i.fa:before {
  content: "\f00c";
}
#mode-menu {
  overflow: auto;
  max-height: 20em;
}
.edit_app #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.edit_app #menubar .navbar {
  /* Use a negative 1 bottom margin, so the border overlaps the border of the
    header */
  margin-bottom: -1px;
}
.dirty-indicator {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator.pull-left {
  margin-right: .3em;
}
.dirty-indicator.pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-dirty.pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-clean.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f00c";
}
.dirty-indicator-clean:before.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.pull-right {
  margin-left: .3em;
}
#filename {
  font-size: 16pt;
  display: table;
  padding: 0px 5px;
}
#current-mode {
  padding-left: 5px;
  padding-right: 5px;
}
#texteditor-backdrop {
  padding-top: 20px;
  padding-bottom: 20px;
}
@media not print {
  #texteditor-backdrop {
    background-color: #EEE;
  }
}
@media print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container {
    padding: 0px;
    background-color: #fff;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI colors. */
.ansibold {
  font-weight: bold;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  border-left-width: 1px;
  padding-left: 5px;
  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
}
div.cell.jupyter-soft-selected {
  border-left-color: #90CAF9;
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected {
  border-color: #ababab;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
}
@media print {
  div.cell.selected {
    border-color: transparent;
  }
}
div.cell.selected.jupyter-soft-selected {
  border-left-width: 0;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  padding: 0.4em;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
  /* .CodeMirror-lines */
  padding: 0;
  border: 0;
  border-radius: 0;
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area .rendered_html table {
  margin-left: 0;
  margin-right: 0;
}
div.output_area .rendered_html img {
  margin-left: 0;
  margin-right: 0;
}
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}
.rendered_html em {
  font-style: italic;
}
.rendered_html strong {
  font-weight: bold;
}
.rendered_html u {
  text-decoration: underline;
}
.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}
.rendered_html h1 {
  font-size: 185.7%;
  margin: 1.08em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h2 {
  font-size: 157.1%;
  margin: 1.27em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h3 {
  font-size: 128.6%;
  margin: 1.55em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h4 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h5 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h6 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul {
  list-style: disc;
  margin: 0em 2em;
  padding-left: 0px;
}
.rendered_html ul ul {
  list-style: square;
  margin: 0em 2em;
}
.rendered_html ul ul ul {
  list-style: circle;
  margin: 0em 2em;
}
.rendered_html ol {
  list-style: decimal;
  margin: 0em 2em;
  padding-left: 0px;
}
.rendered_html ol ol {
  list-style: upper-alpha;
  margin: 0em 2em;
}
.rendered_html ol ol ol {
  list-style: lower-alpha;
  margin: 0em 2em;
}
.rendered_html ol ol ol ol {
  list-style: lower-roman;
  margin: 0em 2em;
}
.rendered_html ol ol ol ol ol {
  list-style: decimal;
  margin: 0em 2em;
}
.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}
.rendered_html hr {
  color: black;
  background-color: black;
}
.rendered_html pre {
  margin: 1em 2em;
}
.rendered_html pre,
.rendered_html code {
  border: 0;
  background-color: #fff;
  color: #000;
  font-size: 100%;
  padding: 0px;
}
.rendered_html blockquote {
  margin: 1em 2em;
}
.rendered_html table {
  margin-left: auto;
  margin-right: auto;
  border: 1px solid black;
  border-collapse: collapse;
}
.rendered_html tr,
.rendered_html th,
.rendered_html td {
  border: 1px solid black;
  border-collapse: collapse;
  margin: 1em 2em;
}
.rendered_html td,
.rendered_html th {
  text-align: left;
  vertical-align: middle;
  padding: 4px;
}
.rendered_html th {
  font-weight: bold;
}
.rendered_html * + table {
  margin-top: 1em;
}
.rendered_html p {
  text-align: left;
}
.rendered_html * + p {
  margin-top: 1em;
}
.rendered_html img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,
.rendered_html svg {
  max-width: 100%;
  height: auto;
}
.rendered_html img.unconfined,
.rendered_html svg.unconfined {
  max-width: none;
}
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered .rendered_html {
  overflow-x: auto;
  overflow-y: hidden;
}
.text_cell.unrendered .text_cell_render {
  display: none;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
/*!
*
* IPython notebook webapp
*
*/
@media (max-width: 767px) {
  .notebook_app {
    padding-left: 0px;
    padding-right: 0px;
  }
}
#ipython-main-app {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook_panel {
  margin: 0px;
  padding: 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook {
  font-size: 14px;
  line-height: 20px;
  overflow-y: hidden;
  overflow-x: auto;
  width: 100%;
  /* This spaces the page away from the edge of the notebook area */
  padding-top: 20px;
  margin: 0px;
  outline: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  min-height: 100%;
}
@media not print {
  #notebook-container {
    padding: 15px;
    background-color: #fff;
    min-height: 0;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
@media print {
  #notebook-container {
    width: 100%;
  }
}
div.ui-widget-content {
  border: 1px solid #ababab;
  outline: none;
}
pre.dialog {
  background-color: #f7f7f7;
  border: 1px solid #ddd;
  border-radius: 2px;
  padding: 0.4em;
  padding-left: 2em;
}
p.dialog {
  padding: 0.2em;
}
/* Word-wrap output correctly.  This is the CSS3 spelling, though Firefox seems
   to not honor it correctly.  Webkit browsers (Chrome, rekonq, Safari) do.
 */
pre,
code,
kbd,
samp {
  white-space: pre-wrap;
}
#fonttest {
  font-family: monospace;
}
p {
  margin-bottom: 0;
}
.end_space {
  min-height: 100px;
  transition: height .2s ease;
}
.notebook_app > #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
@media not print {
  .notebook_app {
    background-color: #EEE;
  }
}
kbd {
  border-style: solid;
  border-width: 1px;
  box-shadow: none;
  margin: 2px;
  padding-left: 2px;
  padding-right: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
/* CSS for the cell toolbar */
.celltoolbar {
  border: thin solid #CFCFCF;
  border-bottom: none;
  background: #EEE;
  border-radius: 2px 2px 0px 0px;
  width: 100%;
  height: 29px;
  padding-right: 4px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
  display: -webkit-flex;
}
@media print {
  .celltoolbar {
    display: none;
  }
}
.ctb_hideshow {
  display: none;
  vertical-align: bottom;
}
/* ctb_show is added to the ctb_hideshow div to show the cell toolbar.
   Cell toolbars are only shown when the ctb_global_show class is also set.
*/
.ctb_global_show .ctb_show.ctb_hideshow {
  display: block;
}
.ctb_global_show .ctb_show + .input_area,
.ctb_global_show .ctb_show + div.text_cell_input,
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border-top-right-radius: 0px;
  border-top-left-radius: 0px;
}
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border: 1px solid #cfcfcf;
}
.celltoolbar {
  font-size: 87%;
  padding-top: 3px;
}
.celltoolbar select {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  width: inherit;
  font-size: inherit;
  height: 22px;
  padding: 0px;
  display: inline-block;
}
.celltoolbar select:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.celltoolbar select::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.celltoolbar select:-ms-input-placeholder {
  color: #999;
}
.celltoolbar select::-webkit-input-placeholder {
  color: #999;
}
.celltoolbar select::-ms-expand {
  border: 0;
  background-color: transparent;
}
.celltoolbar select[disabled],
.celltoolbar select[readonly],
fieldset[disabled] .celltoolbar select {
  background-color: #eeeeee;
  opacity: 1;
}
.celltoolbar select[disabled],
fieldset[disabled] .celltoolbar select {
  cursor: not-allowed;
}
textarea.celltoolbar select {
  height: auto;
}
select.celltoolbar select {
  height: 30px;
  line-height: 30px;
}
textarea.celltoolbar select,
select[multiple].celltoolbar select {
  height: auto;
}
.celltoolbar label {
  margin-left: 5px;
  margin-right: 5px;
}
.completions {
  position: absolute;
  z-index: 110;
  overflow: hidden;
  border: 1px solid #ababab;
  border-radius: 2px;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  line-height: 1;
}
.completions select {
  background: white;
  outline: none;
  border: none;
  padding: 0px;
  margin: 0px;
  overflow: auto;
  font-family: monospace;
  font-size: 110%;
  color: #000;
  width: auto;
}
.completions select option.context {
  color: #286090;
}
#kernel_logo_widget {
  float: right !important;
  float: right;
}
#kernel_logo_widget .current_kernel_logo {
  display: none;
  margin-top: -1px;
  margin-bottom: -1px;
  width: 32px;
  height: 32px;
}
#menubar {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  margin-top: 1px;
}
#menubar .navbar {
  border-top: 1px;
  border-radius: 0px 0px 2px 2px;
  margin-bottom: 0px;
}
#menubar .navbar-toggle {
  float: left;
  padding-top: 7px;
  padding-bottom: 7px;
  border: none;
}
#menubar .navbar-collapse {
  clear: left;
}
.nav-wrapper {
  border-bottom: 1px solid #e7e7e7;
}
i.menu-icon {
  padding-top: 4px;
}
ul#help_menu li a {
  overflow: hidden;
  padding-right: 2.2em;
}
ul#help_menu li a i {
  margin-right: -1.2em;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu > .dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
}
.dropdown-submenu:hover > .dropdown-menu {
  display: block;
}
.dropdown-submenu > a:after {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  display: block;
  content: "\f0da";
  float: right;
  color: #333333;
  margin-top: 2px;
  margin-right: -10px;
}
.dropdown-submenu > a:after.pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.pull-right {
  margin-left: .3em;
}
.dropdown-submenu:hover > a:after {
  color: #262626;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left > .dropdown-menu {
  left: -100%;
  margin-left: 10px;
}
#notification_area {
  float: right !important;
  float: right;
  z-index: 10;
}
.indicator_area {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
#kernel_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  border-left: 1px solid;
}
#kernel_indicator .kernel_indicator_name {
  padding-left: 5px;
  padding-right: 5px;
}
#modal_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
#readonly-indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  margin-top: 2px;
  margin-bottom: 0px;
  margin-left: 0px;
  margin-right: 0px;
  display: none;
}
.modal_indicator:before {
  width: 1.28571429em;
  text-align: center;
}
.edit_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f040";
}
.edit_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: ' ';
}
.command_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f10c";
}
.kernel_idle_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f111";
}
.kernel_busy_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f1e2";
}
.kernel_dead_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f127";
}
.kernel_disconnected_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.pull-right {
  margin-left: .3em;
}
.notification_widget {
  color: #777;
  z-index: 10;
  background: rgba(240, 240, 240, 0.5);
  margin-right: 4px;
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget:focus,
.notification_widget.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.notification_widget:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active:hover,
.notification_widget.active:hover,
.open > .dropdown-toggle.notification_widget:hover,
.notification_widget:active:focus,
.notification_widget.active:focus,
.open > .dropdown-toggle.notification_widget:focus,
.notification_widget:active.focus,
.notification_widget.active.focus,
.open > .dropdown-toggle.notification_widget.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  background-image: none;
}
.notification_widget.disabled:hover,
.notification_widget[disabled]:hover,
fieldset[disabled] .notification_widget:hover,
.notification_widget.disabled:focus,
.notification_widget[disabled]:focus,
fieldset[disabled] .notification_widget:focus,
.notification_widget.disabled.focus,
.notification_widget[disabled].focus,
fieldset[disabled] .notification_widget.focus {
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget .badge {
  color: #fff;
  background-color: #333;
}
.notification_widget.warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning:focus,
.notification_widget.warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.notification_widget.warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active:hover,
.notification_widget.warning.active:hover,
.open > .dropdown-toggle.notification_widget.warning:hover,
.notification_widget.warning:active:focus,
.notification_widget.warning.active:focus,
.open > .dropdown-toggle.notification_widget.warning:focus,
.notification_widget.warning:active.focus,
.notification_widget.warning.active.focus,
.open > .dropdown-toggle.notification_widget.warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  background-image: none;
}
.notification_widget.warning.disabled:hover,
.notification_widget.warning[disabled]:hover,
fieldset[disabled] .notification_widget.warning:hover,
.notification_widget.warning.disabled:focus,
.notification_widget.warning[disabled]:focus,
fieldset[disabled] .notification_widget.warning:focus,
.notification_widget.warning.disabled.focus,
.notification_widget.warning[disabled].focus,
fieldset[disabled] .notification_widget.warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.notification_widget.success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success:focus,
.notification_widget.success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.notification_widget.success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active:hover,
.notification_widget.success.active:hover,
.open > .dropdown-toggle.notification_widget.success:hover,
.notification_widget.success:active:focus,
.notification_widget.success.active:focus,
.open > .dropdown-toggle.notification_widget.success:focus,
.notification_widget.success:active.focus,
.notification_widget.success.active.focus,
.open > .dropdown-toggle.notification_widget.success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  background-image: none;
}
.notification_widget.success.disabled:hover,
.notification_widget.success[disabled]:hover,
fieldset[disabled] .notification_widget.success:hover,
.notification_widget.success.disabled:focus,
.notification_widget.success[disabled]:focus,
fieldset[disabled] .notification_widget.success:focus,
.notification_widget.success.disabled.focus,
.notification_widget.success[disabled].focus,
fieldset[disabled] .notification_widget.success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.notification_widget.info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info:focus,
.notification_widget.info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.notification_widget.info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active:hover,
.notification_widget.info.active:hover,
.open > .dropdown-toggle.notification_widget.info:hover,
.notification_widget.info:active:focus,
.notification_widget.info.active:focus,
.open > .dropdown-toggle.notification_widget.info:focus,
.notification_widget.info:active.focus,
.notification_widget.info.active.focus,
.open > .dropdown-toggle.notification_widget.info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  background-image: none;
}
.notification_widget.info.disabled:hover,
.notification_widget.info[disabled]:hover,
fieldset[disabled] .notification_widget.info:hover,
.notification_widget.info.disabled:focus,
.notification_widget.info[disabled]:focus,
fieldset[disabled] .notification_widget.info:focus,
.notification_widget.info.disabled.focus,
.notification_widget.info[disabled].focus,
fieldset[disabled] .notification_widget.info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.notification_widget.danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger:focus,
.notification_widget.danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.notification_widget.danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active:hover,
.notification_widget.danger.active:hover,
.open > .dropdown-toggle.notification_widget.danger:hover,
.notification_widget.danger:active:focus,
.notification_widget.danger.active:focus,
.open > .dropdown-toggle.notification_widget.danger:focus,
.notification_widget.danger:active.focus,
.notification_widget.danger.active.focus,
.open > .dropdown-toggle.notification_widget.danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  background-image: none;
}
.notification_widget.danger.disabled:hover,
.notification_widget.danger[disabled]:hover,
fieldset[disabled] .notification_widget.danger:hover,
.notification_widget.danger.disabled:focus,
.notification_widget.danger[disabled]:focus,
fieldset[disabled] .notification_widget.danger:focus,
.notification_widget.danger.disabled.focus,
.notification_widget.danger[disabled].focus,
fieldset[disabled] .notification_widget.danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger .badge {
  color: #d9534f;
  background-color: #fff;
}
div#pager {
  background-color: #fff;
  font-size: 14px;
  line-height: 20px;
  overflow: hidden;
  display: none;
  position: fixed;
  bottom: 0px;
  width: 100%;
  max-height: 50%;
  padding-top: 8px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  /* Display over codemirror */
  z-index: 100;
  /* Hack which prevents jquery ui resizable from changing top. */
  top: auto !important;
}
div#pager pre {
  line-height: 1.21429em;
  color: #000;
  background-color: #f7f7f7;
  padding: 0.4em;
}
div#pager #pager-button-area {
  position: absolute;
  top: 8px;
  right: 20px;
}
div#pager #pager-contents {
  position: relative;
  overflow: auto;
  width: 100%;
  height: 100%;
}
div#pager #pager-contents #pager-container {
  position: relative;
  padding: 15px 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
div#pager .ui-resizable-handle {
  top: 0px;
  height: 8px;
  background: #f7f7f7;
  border-top: 1px solid #cfcfcf;
  border-bottom: 1px solid #cfcfcf;
  /* This injects handle bars (a short, wide = symbol) for 
        the resize handle. */
}
div#pager .ui-resizable-handle::after {
  content: '';
  top: 2px;
  left: 50%;
  height: 3px;
  width: 30px;
  margin-left: -15px;
  position: absolute;
  border-top: 1px solid #cfcfcf;
}
.quickhelp {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  line-height: 1.8em;
}
.shortcut_key {
  display: inline-block;
  width: 21ex;
  text-align: right;
  font-family: monospace;
}
.shortcut_descr {
  display: inline-block;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
span.save_widget {
  margin-top: 6px;
}
span.save_widget span.filename {
  height: 1em;
  line-height: 1em;
  padding: 3px;
  margin-left: 16px;
  border: none;
  font-size: 146.5%;
  border-radius: 2px;
}
span.save_widget span.filename:hover {
  background-color: #e6e6e6;
}
span.checkpoint_status,
span.autosave_status {
  font-size: small;
}
@media (max-width: 767px) {
  span.save_widget {
    font-size: small;
  }
  span.checkpoint_status,
  span.autosave_status {
    display: none;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  span.checkpoint_status {
    display: none;
  }
  span.autosave_status {
    font-size: x-small;
  }
}
.toolbar {
  padding: 0px;
  margin-left: -5px;
  margin-top: 2px;
  margin-bottom: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.toolbar select,
.toolbar label {
  width: auto;
  vertical-align: middle;
  margin-right: 2px;
  margin-bottom: 0px;
  display: inline;
  font-size: 92%;
  margin-left: 0.3em;
  margin-right: 0.3em;
  padding: 0px;
  padding-top: 3px;
}
.toolbar .btn {
  padding: 2px 8px;
}
.toolbar .btn-group {
  margin-top: 0px;
  margin-left: 5px;
}
#maintoolbar {
  margin-bottom: -3px;
  margin-top: -8px;
  border: 0px;
  min-height: 27px;
  margin-left: 0px;
  padding-top: 11px;
  padding-bottom: 3px;
}
#maintoolbar .navbar-text {
  float: none;
  vertical-align: middle;
  text-align: right;
  margin-left: 5px;
  margin-right: 0px;
  margin-top: 0px;
}
.select-xs {
  height: 24px;
}
.pulse,
.dropdown-menu > li > a.pulse,
li.pulse > a.dropdown-toggle,
li.pulse.open > a.dropdown-toggle {
  background-color: #F37626;
  color: white;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
/** WARNING IF YOU ARE EDITTING THIS FILE, if this is a .css file, It has a lot
 * of chance of beeing generated from the ../less/[samename].less file, you can
 * try to get back the less file by reverting somme commit in history
 **/
/*
 * We'll try to get something pretty, so we
 * have some strange css to have the scroll bar on
 * the left with fix button on the top right of the tooltip
 */
@-moz-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-webkit-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-moz-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
@-webkit-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
/*properties of tooltip after "expand"*/
.bigtooltip {
  overflow: auto;
  height: 200px;
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
}
/*properties of tooltip before "expand"*/
.smalltooltip {
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
  text-overflow: ellipsis;
  overflow: hidden;
  height: 80px;
}
.tooltipbuttons {
  position: absolute;
  padding-right: 15px;
  top: 0px;
  right: 0px;
}
.tooltiptext {
  /*avoid the button to overlap on some docstring*/
  padding-right: 30px;
}
.ipython_tooltip {
  max-width: 700px;
  /*fade-in animation when inserted*/
  -webkit-animation: fadeOut 400ms;
  -moz-animation: fadeOut 400ms;
  animation: fadeOut 400ms;
  -webkit-animation: fadeIn 400ms;
  -moz-animation: fadeIn 400ms;
  animation: fadeIn 400ms;
  vertical-align: middle;
  background-color: #f7f7f7;
  overflow: visible;
  border: #ababab 1px solid;
  outline: none;
  padding: 3px;
  margin: 0px;
  padding-left: 7px;
  font-family: monospace;
  min-height: 50px;
  -moz-box-shadow: 0px 6px 10px -1px #adadad;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  border-radius: 2px;
  position: absolute;
  z-index: 1000;
}
.ipython_tooltip a {
  float: right;
}
.ipython_tooltip .tooltiptext pre {
  border: 0;
  border-radius: 0;
  font-size: 100%;
  background-color: #f7f7f7;
}
.pretooltiparrow {
  left: 0px;
  margin: 0px;
  top: -16px;
  width: 40px;
  height: 16px;
  overflow: hidden;
  position: absolute;
}
.pretooltiparrow:before {
  background-color: #f7f7f7;
  border: 1px #ababab solid;
  z-index: 11;
  content: "";
  position: absolute;
  left: 15px;
  top: 10px;
  width: 25px;
  height: 25px;
  -webkit-transform: rotate(45deg);
  -moz-transform: rotate(45deg);
  -ms-transform: rotate(45deg);
  -o-transform: rotate(45deg);
}
ul.typeahead-list i {
  margin-left: -10px;
  width: 18px;
}
ul.typeahead-list {
  max-height: 80vh;
  overflow: auto;
}
ul.typeahead-list > li > a {
  /** Firefox bug **/
  /* see https://github.com/jupyter/notebook/issues/559 */
  white-space: normal;
}
.cmd-palette .modal-body {
  padding: 7px;
}
.cmd-palette form {
  background: white;
}
.cmd-palette input {
  outline: none;
}
.no-shortcut {
  display: none;
}
.command-shortcut:before {
  content: "(command)";
  padding-right: 3px;
  color: #777777;
}
.edit-shortcut:before {
  content: "(edit)";
  padding-right: 3px;
  color: #777777;
}
#find-and-replace #replace-preview .match,
#find-and-replace #replace-preview .insert {
  background-color: #BBDEFB;
  border-color: #90CAF9;
  border-style: solid;
  border-width: 1px;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .match {
  background-color: #FFCDD2;
  border-color: #EF9A9A;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .insert {
  background-color: #C8E6C9;
  border-color: #A5D6A7;
  border-radius: 0px;
}
#find-and-replace #replace-preview {
  max-height: 60vh;
  overflow: auto;
}
#find-and-replace #replace-preview pre {
  padding: 5px 10px;
}
.terminal-app {
  background: #EEE;
}
.terminal-app #header {
  background: #fff;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.terminal-app .terminal {
  width: 100%;
  float: left;
  font-family: monospace;
  color: white;
  background: black;
  padding: 0.4em;
  border-radius: 2px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
}
.terminal-app .terminal,
.terminal-app .terminal dummy-screen {
  line-height: 1em;
  font-size: 14px;
}
.terminal-app .terminal .xterm-rows {
  padding: 10px;
}
.terminal-app .terminal-cursor {
  color: black;
  background: white;
}
.terminal-app #terminado-container {
  margin-top: 20px;
}
/*# sourceMappingURL=style.min.css.map */
    </style>
<style type="text/css">
    .highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
    </style>
<style type="text/css">
    
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

    </style>


<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
body {
  overflow: visible;
  padding: 8px;
}

div#notebook {
  overflow: visible;
  border-top: none;
}

@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Lunar-Lander-using-DeepQLearning">Lunar Lander using DeepQLearning<a class="anchor-link" href="#Lunar-Lander-using-DeepQLearning">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Sitabja-Ukil-(17200734)-&amp;-Anandita-Pal-(17200262)">Sitabja Ukil (17200734) &amp; Anandita Pal (17200262)<a class="anchor-link" href="#Sitabja-Ukil-(17200734)-&amp;-Anandita-Pal-(17200262)">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Train a DeepQLearning Reinforcement Learning model to control the Lunar Lander craft based on state vectors.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Imports">Imports<a class="anchor-link" href="#Imports">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">gym</span>

<span class="kn">from</span> <span class="nn">keras.models</span> <span class="k">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="k">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Activation</span><span class="p">,</span> <span class="n">Flatten</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="k">import</span> <span class="n">Adam</span>

<span class="kn">from</span> <span class="nn">rl.agents.dqn</span> <span class="k">import</span> <span class="n">DQNAgent</span>
<span class="kn">from</span> <span class="nn">rl.policy</span> <span class="k">import</span> <span class="n">BoltzmannQPolicy</span><span class="p">,</span> <span class="n">EpsGreedyQPolicy</span>
<span class="kn">from</span> <span class="nn">rl.memory</span> <span class="k">import</span> <span class="n">SequentialMemory</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stderr output_text">
<pre>Using TensorFlow backend.
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Set-the-Environment-and-create-the-DQN">Set the Environment and create the DQN<a class="anchor-link" href="#Set-the-Environment-and-create-the-DQN">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ENV_NAME</span> <span class="o">=</span> <span class="s1">&#39;LunarLander-v2&#39;</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # Get the environment and extract the number of actions.</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">ENV_NAME</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">nb_actions</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre><span class="ansi-yellow-fg">WARN: gym.spaces.Box autodetected dtype as &lt;class &#39;numpy.float32&#39;&gt;. Please provide explicit dtype.</span>
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Next, we build the DeepQLearning model.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 8)                 0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               4608      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 256)               131328    
_________________________________________________________________
activation_2 (Activation)    (None, 256)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 128)               32896     
_________________________________________________________________
activation_3 (Activation)    (None, 128)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 4)                 516       
_________________________________________________________________
activation_4 (Activation)    (None, 4)                 0         
=================================================================
Total params: 169,348
Trainable params: 169,348
Non-trainable params: 0
_________________________________________________________________
None
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Configure-and-train-DQN-Agent">Configure and train DQN Agent<a class="anchor-link" href="#Configure-and-train-DQN-Agent">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Finally, we configure and compile our agent</span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">SequentialMemory</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">300000</span><span class="p">,</span> <span class="n">window_length</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">EpsGreedyQPolicy</span><span class="p">()</span>
<span class="n">dqn</span> <span class="o">=</span> <span class="n">DQNAgent</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">nb_actions</span><span class="o">=</span><span class="n">nb_actions</span><span class="p">,</span> <span class="n">memory</span><span class="o">=</span><span class="n">memory</span><span class="p">,</span> <span class="n">nb_steps_warmup</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span>
               <span class="n">target_model_update</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">)</span>
<span class="n">dqn</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">),</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;mae&#39;</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Train the model for 1.5b millions steps.</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">dqn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">nb_steps</span><span class="o">=</span><span class="mi">1500000</span><span class="p">,</span> <span class="n">visualize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Training for 1500000 steps ...
     263/1500000: episode: 1, duration: 0.455s, episode steps: 263, steps per second: 579, episode reward: -1589.269, mean reward: -6.043 [-100.000, 2.295], mean action: 1.947 [0.000, 3.000], mean observation: 0.772 [-4.101, 9.065], loss: --, mean_absolute_error: --, mean_q: --
     349/1500000: episode: 2, duration: 0.092s, episode steps: 86, steps per second: 938, episode reward: -527.654, mean reward: -6.136 [-100.000, 0.164], mean action: 1.919 [0.000, 2.000], mean observation: 0.168 [-2.203, 2.275], loss: --, mean_absolute_error: --, mean_q: --
     446/1500000: episode: 3, duration: 0.107s, episode steps: 97, steps per second: 904, episode reward: -417.161, mean reward: -4.301 [-100.000, 1.746], mean action: 1.990 [0.000, 3.000], mean observation: 0.332 [-0.419, 1.893], loss: --, mean_absolute_error: --, mean_q: --
     614/1500000: episode: 4, duration: 0.262s, episode steps: 168, steps per second: 641, episode reward: -832.047, mean reward: -4.953 [-100.000, 1.772], mean action: 2.000 [0.000, 3.000], mean observation: 0.373 [-2.081, 4.419], loss: --, mean_absolute_error: --, mean_q: --
     719/1500000: episode: 5, duration: 0.152s, episode steps: 105, steps per second: 693, episode reward: -563.327, mean reward: -5.365 [-100.000, 1.118], mean action: 1.981 [1.000, 3.000], mean observation: 0.123 [-2.527, 1.817], loss: --, mean_absolute_error: --, mean_q: --
     867/1500000: episode: 6, duration: 0.194s, episode steps: 148, steps per second: 763, episode reward: -775.499, mean reward: -5.240 [-100.000, 2.196], mean action: 1.973 [0.000, 3.000], mean observation: 0.295 [-2.170, 3.621], loss: --, mean_absolute_error: --, mean_q: --
     977/1500000: episode: 7, duration: 0.122s, episode steps: 110, steps per second: 902, episode reward: -534.558, mean reward: -4.860 [-100.000, 3.322], mean action: 1.936 [0.000, 3.000], mean observation: 0.121 [-2.297, 1.931], loss: --, mean_absolute_error: --, mean_q: --
    1085/1500000: episode: 8, duration: 0.142s, episode steps: 108, steps per second: 763, episode reward: -450.066, mean reward: -4.167 [-100.000, 3.293], mean action: 2.130 [0.000, 3.000], mean observation: 0.199 [-1.392, 1.913], loss: --, mean_absolute_error: --, mean_q: --
    1190/1500000: episode: 9, duration: 0.118s, episode steps: 105, steps per second: 886, episode reward: -375.961, mean reward: -3.581 [-100.000, 2.119], mean action: 1.876 [0.000, 3.000], mean observation: 0.019 [-1.879, 1.296], loss: --, mean_absolute_error: --, mean_q: --
    1287/1500000: episode: 10, duration: 0.136s, episode steps: 97, steps per second: 711, episode reward: -497.024, mean reward: -5.124 [-100.000, 0.615], mean action: 1.938 [0.000, 3.000], mean observation: 0.497 [-0.215, 2.559], loss: --, mean_absolute_error: --, mean_q: --
    1451/1500000: episode: 11, duration: 0.242s, episode steps: 164, steps per second: 677, episode reward: -802.895, mean reward: -4.896 [-100.000, 1.578], mean action: 1.970 [0.000, 3.000], mean observation: 0.397 [-1.388, 4.730], loss: --, mean_absolute_error: --, mean_q: --
    1527/1500000: episode: 12, duration: 0.180s, episode steps: 76, steps per second: 422, episode reward: -544.694, mean reward: -7.167 [-100.000, -0.872], mean action: 1.961 [1.000, 2.000], mean observation: 0.165 [-2.582, 2.077], loss: --, mean_absolute_error: --, mean_q: --
    1705/1500000: episode: 13, duration: 0.341s, episode steps: 178, steps per second: 522, episode reward: -831.567, mean reward: -4.672 [-100.000, 1.537], mean action: 1.966 [0.000, 3.000], mean observation: 0.330 [-1.019, 3.429], loss: --, mean_absolute_error: --, mean_q: --
    1884/1500000: episode: 14, duration: 0.231s, episode steps: 179, steps per second: 773, episode reward: -1049.201, mean reward: -5.861 [-100.000, 1.674], mean action: 1.950 [0.000, 3.000], mean observation: 0.604 [-0.995, 5.576], loss: --, mean_absolute_error: --, mean_q: --
    1959/1500000: episode: 15, duration: 0.087s, episode steps: 75, steps per second: 860, episode reward: -451.573, mean reward: -6.021 [-100.000, 0.339], mean action: 1.920 [0.000, 2.000], mean observation: 0.032 [-2.462, 1.396], loss: --, mean_absolute_error: --, mean_q: --
    2037/1500000: episode: 16, duration: 0.084s, episode steps: 78, steps per second: 930, episode reward: -476.638, mean reward: -6.111 [-100.000, 0.586], mean action: 2.013 [1.000, 3.000], mean observation: 0.062 [-2.539, 1.519], loss: --, mean_absolute_error: --, mean_q: --
    2188/1500000: episode: 17, duration: 0.188s, episode steps: 151, steps per second: 804, episode reward: -650.726, mean reward: -4.309 [-100.000, 2.452], mean action: 1.901 [0.000, 3.000], mean observation: 0.221 [-1.994, 3.005], loss: --, mean_absolute_error: --, mean_q: --
    2294/1500000: episode: 18, duration: 0.122s, episode steps: 106, steps per second: 868, episode reward: -535.837, mean reward: -5.055 [-100.000, 0.782], mean action: 1.991 [0.000, 3.000], mean observation: 0.217 [-1.487, 2.874], loss: --, mean_absolute_error: --, mean_q: --
    2418/1500000: episode: 19, duration: 0.151s, episode steps: 124, steps per second: 820, episode reward: -670.282, mean reward: -5.405 [-100.000, 1.717], mean action: 1.952 [0.000, 3.000], mean observation: 0.472 [-0.607, 3.157], loss: --, mean_absolute_error: --, mean_q: --
    2535/1500000: episode: 20, duration: 0.132s, episode steps: 117, steps per second: 889, episode reward: -624.941, mean reward: -5.341 [-100.000, 2.028], mean action: 1.915 [0.000, 3.000], mean observation: 0.199 [-2.260, 2.514], loss: --, mean_absolute_error: --, mean_q: --
    2626/1500000: episode: 21, duration: 0.111s, episode steps: 91, steps per second: 822, episode reward: -575.779, mean reward: -6.327 [-100.000, 0.812], mean action: 1.956 [0.000, 3.000], mean observation: 0.224 [-2.149, 2.677], loss: --, mean_absolute_error: --, mean_q: --
    2708/1500000: episode: 22, duration: 0.101s, episode steps: 82, steps per second: 812, episode reward: -476.980, mean reward: -5.817 [-100.000, 0.644], mean action: 1.976 [0.000, 3.000], mean observation: 0.100 [-2.312, 1.828], loss: --, mean_absolute_error: --, mean_q: --
    2823/1500000: episode: 23, duration: 0.135s, episode steps: 115, steps per second: 851, episode reward: -637.939, mean reward: -5.547 [-100.000, 1.660], mean action: 1.965 [0.000, 3.000], mean observation: 0.258 [-2.062, 2.977], loss: --, mean_absolute_error: --, mean_q: --
    3088/1500000: episode: 24, duration: 0.416s, episode steps: 265, steps per second: 637, episode reward: -1415.164, mean reward: -5.340 [-100.000, 3.572], mean action: 1.955 [0.000, 3.000], mean observation: 0.643 [-0.760, 8.358], loss: --, mean_absolute_error: --, mean_q: --
    3174/1500000: episode: 25, duration: 0.095s, episode steps: 86, steps per second: 909, episode reward: -375.298, mean reward: -4.364 [-100.000, 1.937], mean action: 2.116 [0.000, 3.000], mean observation: 0.245 [-1.046, 1.849], loss: --, mean_absolute_error: --, mean_q: --
    3331/1500000: episode: 26, duration: 0.198s, episode steps: 157, steps per second: 794, episode reward: -704.974, mean reward: -4.490 [-100.000, 3.663], mean action: 1.879 [0.000, 3.000], mean observation: 0.493 [-0.201, 3.837], loss: --, mean_absolute_error: --, mean_q: --
    3450/1500000: episode: 27, duration: 0.157s, episode steps: 119, steps per second: 756, episode reward: -508.081, mean reward: -4.270 [-100.000, 1.861], mean action: 1.891 [0.000, 3.000], mean observation: 0.385 [-0.267, 2.269], loss: --, mean_absolute_error: --, mean_q: --
    3763/1500000: episode: 28, duration: 0.547s, episode steps: 313, steps per second: 573, episode reward: -2061.770, mean reward: -6.587 [-100.000, 2.173], mean action: 1.949 [0.000, 3.000], mean observation: 1.031 [-4.163, 13.230], loss: --, mean_absolute_error: --, mean_q: --
    3868/1500000: episode: 29, duration: 0.126s, episode steps: 105, steps per second: 836, episode reward: -449.190, mean reward: -4.278 [-100.000, 1.536], mean action: 1.971 [1.000, 3.000], mean observation: 0.476 [-0.146, 2.490], loss: --, mean_absolute_error: --, mean_q: --
    3956/1500000: episode: 30, duration: 0.115s, episode steps: 88, steps per second: 769, episode reward: -453.634, mean reward: -5.155 [-100.000, 0.779], mean action: 1.955 [0.000, 3.000], mean observation: 0.068 [-2.252, 1.598], loss: --, mean_absolute_error: --, mean_q: --
    4097/1500000: episode: 31, duration: 0.194s, episode steps: 141, steps per second: 728, episode reward: -677.290, mean reward: -4.803 [-100.000, 1.797], mean action: 1.908 [0.000, 3.000], mean observation: 0.246 [-1.886, 3.157], loss: --, mean_absolute_error: --, mean_q: --
    4182/1500000: episode: 32, duration: 0.170s, episode steps: 85, steps per second: 500, episode reward: -571.808, mean reward: -6.727 [-100.000, -0.527], mean action: 1.988 [1.000, 2.000], mean observation: 0.499 [-0.507, 2.549], loss: --, mean_absolute_error: --, mean_q: --
    4255/1500000: episode: 33, duration: 0.133s, episode steps: 73, steps per second: 549, episode reward: -433.192, mean reward: -5.934 [-100.000, -0.773], mean action: 1.918 [0.000, 2.000], mean observation: 0.051 [-2.343, 1.527], loss: --, mean_absolute_error: --, mean_q: --
    4342/1500000: episode: 34, duration: 0.133s, episode steps: 87, steps per second: 654, episode reward: -459.639, mean reward: -5.283 [-100.000, 1.584], mean action: 1.966 [0.000, 3.000], mean observation: 0.366 [-0.501, 2.195], loss: --, mean_absolute_error: --, mean_q: --
    4516/1500000: episode: 35, duration: 0.265s, episode steps: 174, steps per second: 658, episode reward: -756.432, mean reward: -4.347 [-100.000, 4.318], mean action: 1.983 [0.000, 3.000], mean observation: 0.415 [-0.524, 3.712], loss: --, mean_absolute_error: --, mean_q: --
    4595/1500000: episode: 36, duration: 0.188s, episode steps: 79, steps per second: 421, episode reward: -377.995, mean reward: -4.785 [-100.000, 1.043], mean action: 1.848 [0.000, 2.000], mean observation: 0.011 [-1.971, 1.377], loss: --, mean_absolute_error: --, mean_q: --
    4803/1500000: episode: 37, duration: 0.455s, episode steps: 208, steps per second: 457, episode reward: -1036.974, mean reward: -4.985 [-100.000, 4.168], mean action: 1.971 [0.000, 3.000], mean observation: 0.419 [-2.753, 5.382], loss: --, mean_absolute_error: --, mean_q: --
    4958/1500000: episode: 38, duration: 0.202s, episode steps: 155, steps per second: 767, episode reward: -656.260, mean reward: -4.234 [-100.000, 4.722], mean action: 1.935 [0.000, 3.000], mean observation: 0.213 [-2.141, 2.973], loss: --, mean_absolute_error: --, mean_q: --
    5041/1500000: episode: 39, duration: 0.130s, episode steps: 83, steps per second: 641, episode reward: -375.566, mean reward: -4.525 [-100.000, 0.674], mean action: 2.000 [0.000, 3.000], mean observation: 0.021 [-1.919, 1.442], loss: --, mean_absolute_error: --, mean_q: --
    5121/1500000: episode: 40, duration: 0.190s, episode steps: 80, steps per second: 420, episode reward: -384.923, mean reward: -4.812 [-100.000, 2.328], mean action: 2.175 [0.000, 3.000], mean observation: 0.240 [-1.342, 1.773], loss: --, mean_absolute_error: --, mean_q: --
    5268/1500000: episode: 41, duration: 0.323s, episode steps: 147, steps per second: 455, episode reward: -704.720, mean reward: -4.794 [-100.000, 3.481], mean action: 1.925 [0.000, 3.000], mean observation: 0.214 [-2.844, 2.644], loss: --, mean_absolute_error: --, mean_q: --
    5348/1500000: episode: 42, duration: 0.099s, episode steps: 80, steps per second: 810, episode reward: -444.738, mean reward: -5.559 [-100.000, 0.424], mean action: 2.013 [0.000, 3.000], mean observation: 0.052 [-2.309, 1.533], loss: --, mean_absolute_error: --, mean_q: --
    5643/1500000: episode: 43, duration: 0.504s, episode steps: 295, steps per second: 586, episode reward: -1805.205, mean reward: -6.119 [-100.000, 2.104], mean action: 1.949 [0.000, 3.000], mean observation: 0.773 [-0.953, 10.692], loss: --, mean_absolute_error: --, mean_q: --
    5782/1500000: episode: 44, duration: 0.346s, episode steps: 139, steps per second: 402, episode reward: -814.794, mean reward: -5.862 [-100.000, 1.576], mean action: 1.978 [0.000, 3.000], mean observation: 0.485 [-0.652, 3.768], loss: --, mean_absolute_error: --, mean_q: --
    5853/1500000: episode: 45, duration: 0.080s, episode steps: 71, steps per second: 883, episode reward: -473.689, mean reward: -6.672 [-100.000, 0.078], mean action: 1.930 [0.000, 2.000], mean observation: 0.050 [-2.648, 1.466], loss: --, mean_absolute_error: --, mean_q: --
    5991/1500000: episode: 46, duration: 0.183s, episode steps: 138, steps per second: 753, episode reward: -588.557, mean reward: -4.265 [-100.000, 1.426], mean action: 1.964 [0.000, 3.000], mean observation: 0.480 [-0.103, 3.185], loss: --, mean_absolute_error: --, mean_q: --
    6096/1500000: episode: 47, duration: 0.132s, episode steps: 105, steps per second: 797, episode reward: -405.640, mean reward: -3.863 [-100.000, 2.121], mean action: 1.952 [0.000, 3.000], mean observation: 0.049 [-1.777, 1.573], loss: --, mean_absolute_error: --, mean_q: --
    6351/1500000: episode: 48, duration: 0.613s, episode steps: 255, steps per second: 416, episode reward: -1465.558, mean reward: -5.747 [-100.000, 2.322], mean action: 1.969 [0.000, 3.000], mean observation: 0.759 [-0.919, 8.783], loss: --, mean_absolute_error: --, mean_q: --
    6562/1500000: episode: 49, duration: 0.398s, episode steps: 211, steps per second: 530, episode reward: -1299.381, mean reward: -6.158 [-100.000, 1.759], mean action: 1.938 [0.000, 3.000], mean observation: 0.711 [-3.909, 7.159], loss: --, mean_absolute_error: --, mean_q: --
    6666/1500000: episode: 50, duration: 0.178s, episode steps: 104, steps per second: 583, episode reward: -492.092, mean reward: -4.732 [-100.000, 1.185], mean action: 1.971 [0.000, 3.000], mean observation: 0.144 [-1.538, 2.372], loss: --, mean_absolute_error: --, mean_q: --
    6826/1500000: episode: 51, duration: 0.215s, episode steps: 160, steps per second: 744, episode reward: -816.430, mean reward: -5.103 [-100.000, 1.686], mean action: 1.950 [0.000, 3.000], mean observation: 0.381 [-1.150, 4.808], loss: --, mean_absolute_error: --, mean_q: --
    6964/1500000: episode: 52, duration: 0.236s, episode steps: 138, steps per second: 585, episode reward: -769.418, mean reward: -5.575 [-100.000, 1.754], mean action: 1.993 [0.000, 3.000], mean observation: 0.372 [-1.029, 4.595], loss: --, mean_absolute_error: --, mean_q: --
    7083/1500000: episode: 53, duration: 0.301s, episode steps: 119, steps per second: 396, episode reward: -581.166, mean reward: -4.884 [-100.000, 1.894], mean action: 2.008 [0.000, 3.000], mean observation: 0.210 [-1.777, 2.791], loss: --, mean_absolute_error: --, mean_q: --
    7222/1500000: episode: 54, duration: 0.283s, episode steps: 139, steps per second: 490, episode reward: -758.158, mean reward: -5.454 [-100.000, 1.243], mean action: 1.971 [0.000, 3.000], mean observation: 0.310 [-2.417, 3.384], loss: --, mean_absolute_error: --, mean_q: --
    7404/1500000: episode: 55, duration: 0.499s, episode steps: 182, steps per second: 364, episode reward: -1019.557, mean reward: -5.602 [-100.000, 1.577], mean action: 1.923 [0.000, 3.000], mean observation: 0.498 [-1.085, 6.155], loss: --, mean_absolute_error: --, mean_q: --
    7518/1500000: episode: 56, duration: 0.137s, episode steps: 114, steps per second: 833, episode reward: -562.937, mean reward: -4.938 [-100.000, 2.770], mean action: 1.939 [0.000, 3.000], mean observation: 0.144 [-2.349, 1.986], loss: --, mean_absolute_error: --, mean_q: --
    7654/1500000: episode: 57, duration: 0.184s, episode steps: 136, steps per second: 738, episode reward: -501.725, mean reward: -3.689 [-100.000, 4.381], mean action: 2.000 [0.000, 3.000], mean observation: 0.301 [-0.536, 1.990], loss: --, mean_absolute_error: --, mean_q: --
    7731/1500000: episode: 58, duration: 0.082s, episode steps: 77, steps per second: 939, episode reward: -568.942, mean reward: -7.389 [-100.000, 1.775], mean action: 1.961 [1.000, 2.000], mean observation: 0.086 [-3.092, 1.431], loss: --, mean_absolute_error: --, mean_q: --
    7826/1500000: episode: 59, duration: 0.103s, episode steps: 95, steps per second: 921, episode reward: -491.591, mean reward: -5.175 [-100.000, 2.256], mean action: 1.905 [0.000, 3.000], mean observation: 0.083 [-2.391, 1.525], loss: --, mean_absolute_error: --, mean_q: --
    8058/1500000: episode: 60, duration: 0.411s, episode steps: 232, steps per second: 564, episode reward: -986.704, mean reward: -4.253 [-100.000, 4.120], mean action: 1.940 [0.000, 3.000], mean observation: 0.410 [-2.514, 5.351], loss: --, mean_absolute_error: --, mean_q: --
    8167/1500000: episode: 61, duration: 0.268s, episode steps: 109, steps per second: 407, episode reward: -592.249, mean reward: -5.433 [-100.000, 1.281], mean action: 1.954 [0.000, 3.000], mean observation: 0.373 [-0.687, 2.407], loss: --, mean_absolute_error: --, mean_q: --
    8254/1500000: episode: 62, duration: 0.107s, episode steps: 87, steps per second: 812, episode reward: -392.005, mean reward: -4.506 [-100.000, 0.752], mean action: 1.977 [0.000, 3.000], mean observation: 0.297 [-0.455, 2.183], loss: --, mean_absolute_error: --, mean_q: --
    8351/1500000: episode: 63, duration: 0.105s, episode steps: 97, steps per second: 923, episode reward: -587.890, mean reward: -6.061 [-100.000, 0.630], mean action: 1.907 [0.000, 3.000], mean observation: 0.458 [-0.648, 2.572], loss: --, mean_absolute_error: --, mean_q: --
    8431/1500000: episode: 64, duration: 0.098s, episode steps: 80, steps per second: 820, episode reward: -502.261, mean reward: -6.278 [-100.000, -0.353], mean action: 1.887 [0.000, 2.000], mean observation: 0.373 [-0.847, 2.488], loss: --, mean_absolute_error: --, mean_q: --
    8524/1500000: episode: 65, duration: 0.102s, episode steps: 93, steps per second: 916, episode reward: -421.569, mean reward: -4.533 [-100.000, 1.059], mean action: 1.925 [0.000, 3.000], mean observation: 0.460 [-0.198, 2.175], loss: --, mean_absolute_error: --, mean_q: --
    8627/1500000: episode: 66, duration: 0.142s, episode steps: 103, steps per second: 727, episode reward: -514.242, mean reward: -4.993 [-100.000, 1.098], mean action: 1.961 [0.000, 2.000], mean observation: 0.418 [-0.299, 2.291], loss: --, mean_absolute_error: --, mean_q: --
    8720/1500000: episode: 67, duration: 0.105s, episode steps: 93, steps per second: 887, episode reward: -414.356, mean reward: -4.455 [-100.000, 2.282], mean action: 1.935 [0.000, 3.000], mean observation: 0.010 [-2.289, 1.217], loss: --, mean_absolute_error: --, mean_q: --
    8805/1500000: episode: 68, duration: 0.092s, episode steps: 85, steps per second: 921, episode reward: -582.822, mean reward: -6.857 [-100.000, -0.309], mean action: 1.988 [1.000, 3.000], mean observation: 0.180 [-2.531, 2.263], loss: --, mean_absolute_error: --, mean_q: --
    9023/1500000: episode: 69, duration: 0.324s, episode steps: 218, steps per second: 673, episode reward: -1093.188, mean reward: -5.015 [-100.000, 1.554], mean action: 1.945 [0.000, 3.000], mean observation: 0.509 [-2.540, 6.160], loss: --, mean_absolute_error: --, mean_q: --
    9224/1500000: episode: 70, duration: 0.274s, episode steps: 201, steps per second: 735, episode reward: -767.457, mean reward: -3.818 [-100.000, 3.837], mean action: 1.950 [0.000, 3.000], mean observation: 0.457 [-0.285, 4.345], loss: --, mean_absolute_error: --, mean_q: --
    9335/1500000: episode: 71, duration: 0.136s, episode steps: 111, steps per second: 814, episode reward: -560.046, mean reward: -5.045 [-100.000, 1.854], mean action: 1.937 [0.000, 3.000], mean observation: 0.131 [-2.400, 1.910], loss: --, mean_absolute_error: --, mean_q: --
    9492/1500000: episode: 72, duration: 0.212s, episode steps: 157, steps per second: 741, episode reward: -696.032, mean reward: -4.433 [-100.000, 4.235], mean action: 1.936 [0.000, 3.000], mean observation: 0.368 [-0.642, 3.001], loss: --, mean_absolute_error: --, mean_q: --
    9694/1500000: episode: 73, duration: 0.265s, episode steps: 202, steps per second: 761, episode reward: -938.714, mean reward: -4.647 [-100.000, 3.583], mean action: 1.985 [1.000, 3.000], mean observation: 0.388 [-2.634, 4.759], loss: --, mean_absolute_error: --, mean_q: --
    9803/1500000: episode: 74, duration: 0.123s, episode steps: 109, steps per second: 889, episode reward: -645.138, mean reward: -5.919 [-100.000, 1.807], mean action: 1.982 [0.000, 3.000], mean observation: 0.210 [-2.652, 2.457], loss: --, mean_absolute_error: --, mean_q: --
    9913/1500000: episode: 75, duration: 0.137s, episode steps: 110, steps per second: 804, episode reward: -455.094, mean reward: -4.137 [-100.000, 1.173], mean action: 2.009 [1.000, 3.000], mean observation: 0.116 [-1.111, 2.378], loss: --, mean_absolute_error: --, mean_q: --
   10022/1500000: episode: 76, duration: 0.122s, episode steps: 109, steps per second: 894, episode reward: -474.951, mean reward: -4.357 [-100.000, 3.713], mean action: 1.936 [0.000, 2.000], mean observation: 0.052 [-2.346, 1.355], loss: --, mean_absolute_error: --, mean_q: --
   10134/1500000: episode: 77, duration: 0.151s, episode steps: 112, steps per second: 743, episode reward: -561.149, mean reward: -5.010 [-100.000, 3.524], mean action: 1.938 [0.000, 3.000], mean observation: 0.095 [-2.822, 1.550], loss: --, mean_absolute_error: --, mean_q: --
   10247/1500000: episode: 78, duration: 0.129s, episode steps: 113, steps per second: 873, episode reward: -728.164, mean reward: -6.444 [-100.000, 1.236], mean action: 1.991 [0.000, 3.000], mean observation: 0.503 [-0.699, 3.299], loss: --, mean_absolute_error: --, mean_q: --
   10331/1500000: episode: 79, duration: 0.095s, episode steps: 84, steps per second: 886, episode reward: -445.374, mean reward: -5.302 [-100.000, 0.827], mean action: 1.917 [0.000, 3.000], mean observation: 0.087 [-2.091, 1.822], loss: --, mean_absolute_error: --, mean_q: --
   10511/1500000: episode: 80, duration: 0.247s, episode steps: 180, steps per second: 730, episode reward: -846.672, mean reward: -4.704 [-100.000, 4.234], mean action: 1.956 [0.000, 3.000], mean observation: 0.330 [-2.478, 4.042], loss: --, mean_absolute_error: --, mean_q: --
   10617/1500000: episode: 81, duration: 0.124s, episode steps: 106, steps per second: 852, episode reward: -577.230, mean reward: -5.446 [-100.000, 1.937], mean action: 1.962 [0.000, 2.000], mean observation: 0.161 [-2.215, 2.241], loss: --, mean_absolute_error: --, mean_q: --
   10692/1500000: episode: 82, duration: 0.086s, episode steps: 75, steps per second: 867, episode reward: -453.905, mean reward: -6.052 [-100.000, 0.134], mean action: 2.307 [0.000, 3.000], mean observation: 0.225 [-1.900, 1.811], loss: --, mean_absolute_error: --, mean_q: --
   10812/1500000: episode: 83, duration: 0.171s, episode steps: 120, steps per second: 701, episode reward: -619.775, mean reward: -5.165 [-100.000, 3.687], mean action: 1.933 [0.000, 2.000], mean observation: 0.141 [-3.065, 1.571], loss: --, mean_absolute_error: --, mean_q: --
   10904/1500000: episode: 84, duration: 0.111s, episode steps: 92, steps per second: 829, episode reward: -443.058, mean reward: -4.816 [-100.000, 3.138], mean action: 1.913 [0.000, 3.000], mean observation: 0.300 [-0.506, 2.298], loss: --, mean_absolute_error: --, mean_q: --
   11031/1500000: episode: 85, duration: 0.146s, episode steps: 127, steps per second: 870, episode reward: -610.603, mean reward: -4.808 [-100.000, 4.547], mean action: 1.866 [0.000, 3.000], mean observation: 0.146 [-2.927, 1.756], loss: --, mean_absolute_error: --, mean_q: --
   11110/1500000: episode: 86, duration: 0.107s, episode steps: 79, steps per second: 739, episode reward: -511.076, mean reward: -6.469 [-100.000, 0.245], mean action: 1.987 [0.000, 3.000], mean observation: 0.080 [-2.716, 1.460], loss: --, mean_absolute_error: --, mean_q: --
   11301/1500000: episode: 87, duration: 0.259s, episode steps: 191, steps per second: 738, episode reward: -1110.489, mean reward: -5.814 [-100.000, 1.427], mean action: 1.921 [0.000, 3.000], mean observation: 0.573 [-2.720, 6.260], loss: --, mean_absolute_error: --, mean_q: --
   11392/1500000: episode: 88, duration: 0.110s, episode steps: 91, steps per second: 824, episode reward: -471.945, mean reward: -5.186 [-100.000, 0.984], mean action: 1.890 [0.000, 2.000], mean observation: 0.066 [-2.320, 1.558], loss: --, mean_absolute_error: --, mean_q: --
   11493/1500000: episode: 89, duration: 0.128s, episode steps: 101, steps per second: 789, episode reward: -575.038, mean reward: -5.693 [-100.000, 2.297], mean action: 1.921 [0.000, 3.000], mean observation: 0.146 [-2.429, 2.082], loss: --, mean_absolute_error: --, mean_q: --
   11603/1500000: episode: 90, duration: 0.151s, episode steps: 110, steps per second: 727, episode reward: -484.084, mean reward: -4.401 [-100.000, 3.183], mean action: 1.927 [0.000, 3.000], mean observation: 0.368 [-0.344, 2.065], loss: --, mean_absolute_error: --, mean_q: --
   11690/1500000: episode: 91, duration: 0.171s, episode steps: 87, steps per second: 508, episode reward: -273.124, mean reward: -3.139 [-100.000, 1.267], mean action: 1.885 [0.000, 3.000], mean observation: 0.355 [-0.355, 1.359], loss: --, mean_absolute_error: --, mean_q: --
   11818/1500000: episode: 92, duration: 0.208s, episode steps: 128, steps per second: 616, episode reward: -637.795, mean reward: -4.983 [-100.000, 0.795], mean action: 1.953 [0.000, 3.000], mean observation: 0.494 [-0.256, 3.247], loss: --, mean_absolute_error: --, mean_q: --
   12040/1500000: episode: 93, duration: 0.493s, episode steps: 222, steps per second: 451, episode reward: -1317.755, mean reward: -5.936 [-100.000, 0.438], mean action: 1.946 [0.000, 3.000], mean observation: 0.669 [-3.673, 7.225], loss: --, mean_absolute_error: --, mean_q: --
   12117/1500000: episode: 94, duration: 0.202s, episode steps: 77, steps per second: 382, episode reward: -486.874, mean reward: -6.323 [-100.000, -1.136], mean action: 1.909 [0.000, 2.000], mean observation: 0.129 [-2.284, 1.980], loss: --, mean_absolute_error: --, mean_q: --
   12200/1500000: episode: 95, duration: 0.193s, episode steps: 83, steps per second: 430, episode reward: -465.611, mean reward: -5.610 [-100.000, 0.555], mean action: 1.940 [0.000, 3.000], mean observation: 0.054 [-2.414, 1.491], loss: --, mean_absolute_error: --, mean_q: --
   12283/1500000: episode: 96, duration: 0.102s, episode steps: 83, steps per second: 817, episode reward: -511.839, mean reward: -6.167 [-100.000, 0.225], mean action: 1.880 [0.000, 3.000], mean observation: 0.449 [-0.641, 2.291], loss: --, mean_absolute_error: --, mean_q: --
   12368/1500000: episode: 97, duration: 0.104s, episode steps: 85, steps per second: 820, episode reward: -527.396, mean reward: -6.205 [-100.000, -0.018], mean action: 1.953 [0.000, 3.000], mean observation: 0.163 [-2.257, 2.215], loss: --, mean_absolute_error: --, mean_q: --
   12461/1500000: episode: 98, duration: 0.128s, episode steps: 93, steps per second: 728, episode reward: -336.382, mean reward: -3.617 [-100.000, 2.161], mean action: 1.946 [0.000, 3.000], mean observation: -0.018 [-1.718, 1.232], loss: --, mean_absolute_error: --, mean_q: --
   12548/1500000: episode: 99, duration: 0.095s, episode steps: 87, steps per second: 916, episode reward: -382.969, mean reward: -4.402 [-100.000, 2.144], mean action: 1.885 [0.000, 3.000], mean observation: -0.017 [-2.203, 1.065], loss: --, mean_absolute_error: --, mean_q: --
   12686/1500000: episode: 100, duration: 0.172s, episode steps: 138, steps per second: 804, episode reward: -782.009, mean reward: -5.667 [-100.000, 1.446], mean action: 1.971 [0.000, 3.000], mean observation: 0.346 [-2.020, 3.953], loss: --, mean_absolute_error: --, mean_q: --
   12768/1500000: episode: 101, duration: 0.090s, episode steps: 82, steps per second: 913, episode reward: -327.430, mean reward: -3.993 [-100.000, 1.878], mean action: 1.951 [0.000, 3.000], mean observation: -0.019 [-1.828, 1.246], loss: --, mean_absolute_error: --, mean_q: --
   12962/1500000: episode: 102, duration: 0.262s, episode steps: 194, steps per second: 741, episode reward: -885.461, mean reward: -4.564 [-100.000, 2.376], mean action: 1.933 [0.000, 3.000], mean observation: 0.502 [-0.272, 4.789], loss: --, mean_absolute_error: --, mean_q: --
   13075/1500000: episode: 103, duration: 0.134s, episode steps: 113, steps per second: 846, episode reward: -646.540, mean reward: -5.722 [-100.000, 0.967], mean action: 1.991 [0.000, 3.000], mean observation: 0.369 [-0.891, 2.540], loss: --, mean_absolute_error: --, mean_q: --
   13203/1500000: episode: 104, duration: 0.160s, episode steps: 128, steps per second: 799, episode reward: -420.151, mean reward: -3.282 [-100.000, 3.720], mean action: 1.945 [0.000, 3.000], mean observation: 0.321 [-0.519, 1.879], loss: --, mean_absolute_error: --, mean_q: --
   13298/1500000: episode: 105, duration: 0.103s, episode steps: 95, steps per second: 922, episode reward: -584.458, mean reward: -6.152 [-100.000, 2.355], mean action: 2.316 [0.000, 3.000], mean observation: 0.068 [-3.055, 1.723], loss: --, mean_absolute_error: --, mean_q: --
   13380/1500000: episode: 106, duration: 0.090s, episode steps: 82, steps per second: 908, episode reward: -434.183, mean reward: -5.295 [-100.000, 1.231], mean action: 1.878 [0.000, 2.000], mean observation: 0.008 [-2.517, 1.121], loss: --, mean_absolute_error: --, mean_q: --
   13481/1500000: episode: 107, duration: 0.122s, episode steps: 101, steps per second: 829, episode reward: -439.873, mean reward: -4.355 [-100.000, 3.061], mean action: 1.980 [1.000, 3.000], mean observation: 0.031 [-2.217, 1.324], loss: --, mean_absolute_error: --, mean_q: --
   13563/1500000: episode: 108, duration: 0.089s, episode steps: 82, steps per second: 921, episode reward: -502.174, mean reward: -6.124 [-100.000, 2.077], mean action: 1.963 [0.000, 3.000], mean observation: 0.060 [-2.742, 1.392], loss: --, mean_absolute_error: --, mean_q: --
   13703/1500000: episode: 109, duration: 0.191s, episode steps: 140, steps per second: 731, episode reward: -756.640, mean reward: -5.405 [-100.000, 1.959], mean action: 1.979 [0.000, 3.000], mean observation: 0.466 [-0.392, 3.489], loss: --, mean_absolute_error: --, mean_q: --
   13885/1500000: episode: 110, duration: 0.245s, episode steps: 182, steps per second: 741, episode reward: -889.205, mean reward: -4.886 [-100.000, 3.570], mean action: 1.951 [0.000, 3.000], mean observation: 0.325 [-3.258, 3.707], loss: --, mean_absolute_error: --, mean_q: --
   14023/1500000: episode: 111, duration: 0.189s, episode steps: 138, steps per second: 731, episode reward: -804.517, mean reward: -5.830 [-100.000, 1.209], mean action: 1.971 [0.000, 3.000], mean observation: 0.432 [-0.879, 3.321], loss: --, mean_absolute_error: --, mean_q: --
   14149/1500000: episode: 112, duration: 0.201s, episode steps: 126, steps per second: 626, episode reward: -484.289, mean reward: -3.844 [-100.000, 3.483], mean action: 1.968 [0.000, 3.000], mean observation: 0.090 [-2.267, 1.643], loss: --, mean_absolute_error: --, mean_q: --
   14295/1500000: episode: 113, duration: 0.188s, episode steps: 146, steps per second: 776, episode reward: -778.640, mean reward: -5.333 [-100.000, 1.089], mean action: 1.952 [0.000, 3.000], mean observation: 0.530 [-0.689, 4.044], loss: --, mean_absolute_error: --, mean_q: --
   14465/1500000: episode: 114, duration: 0.235s, episode steps: 170, steps per second: 723, episode reward: -864.002, mean reward: -5.082 [-100.000, 1.617], mean action: 1.976 [0.000, 3.000], mean observation: 0.598 [-0.140, 4.972], loss: --, mean_absolute_error: --, mean_q: --
   14745/1500000: episode: 115, duration: 0.421s, episode steps: 280, steps per second: 665, episode reward: -1531.359, mean reward: -5.469 [-100.000, 2.041], mean action: 1.961 [0.000, 3.000], mean observation: 0.669 [-1.364, 8.539], loss: --, mean_absolute_error: --, mean_q: --
   14818/1500000: episode: 116, duration: 0.088s, episode steps: 73, steps per second: 825, episode reward: -458.618, mean reward: -6.282 [-100.000, -0.106], mean action: 1.904 [0.000, 2.000], mean observation: 0.032 [-2.571, 1.383], loss: --, mean_absolute_error: --, mean_q: --
   15019/1500000: episode: 117, duration: 0.298s, episode steps: 201, steps per second: 674, episode reward: -860.444, mean reward: -4.281 [-100.000, 3.151], mean action: 1.960 [0.000, 3.000], mean observation: 0.357 [-1.133, 5.027], loss: --, mean_absolute_error: --, mean_q: --
   15116/1500000: episode: 118, duration: 0.125s, episode steps: 97, steps per second: 779, episode reward: -605.430, mean reward: -6.242 [-100.000, 0.719], mean action: 1.918 [0.000, 2.000], mean observation: 0.220 [-2.240, 2.651], loss: --, mean_absolute_error: --, mean_q: --
   15192/1500000: episode: 119, duration: 0.086s, episode steps: 76, steps per second: 886, episode reward: -485.679, mean reward: -6.391 [-100.000, 0.621], mean action: 1.921 [0.000, 3.000], mean observation: 0.410 [-0.650, 2.491], loss: --, mean_absolute_error: --, mean_q: --
   15278/1500000: episode: 120, duration: 0.102s, episode steps: 86, steps per second: 842, episode reward: -503.739, mean reward: -5.857 [-100.000, 0.744], mean action: 2.023 [0.000, 3.000], mean observation: 0.140 [-2.276, 2.058], loss: --, mean_absolute_error: --, mean_q: --
   15374/1500000: episode: 121, duration: 0.106s, episode steps: 96, steps per second: 901, episode reward: -448.204, mean reward: -4.669 [-100.000, 2.050], mean action: 2.146 [0.000, 3.000], mean observation: 0.249 [-1.182, 1.956], loss: --, mean_absolute_error: --, mean_q: --
   15507/1500000: episode: 122, duration: 0.159s, episode steps: 133, steps per second: 838, episode reward: -609.592, mean reward: -4.583 [-100.000, 3.256], mean action: 1.940 [0.000, 3.000], mean observation: 0.193 [-2.270, 2.497], loss: --, mean_absolute_error: --, mean_q: --
   15622/1500000: episode: 123, duration: 0.137s, episode steps: 115, steps per second: 839, episode reward: -509.883, mean reward: -4.434 [-100.000, 1.596], mean action: 1.974 [0.000, 3.000], mean observation: 0.136 [-1.518, 2.372], loss: --, mean_absolute_error: --, mean_q: --
   15781/1500000: episode: 124, duration: 0.213s, episode steps: 159, steps per second: 746, episode reward: -683.142, mean reward: -4.296 [-100.000, 4.147], mean action: 1.918 [0.000, 3.000], mean observation: 0.212 [-2.886, 2.604], loss: --, mean_absolute_error: --, mean_q: --
   15867/1500000: episode: 125, duration: 0.098s, episode steps: 86, steps per second: 878, episode reward: -371.792, mean reward: -4.323 [-100.000, 1.566], mean action: 1.907 [0.000, 2.000], mean observation: 0.316 [-0.470, 1.982], loss: --, mean_absolute_error: --, mean_q: --
   16222/1500000: episode: 126, duration: 0.864s, episode steps: 355, steps per second: 411, episode reward: -2190.829, mean reward: -6.171 [-100.000, 2.027], mean action: 1.952 [0.000, 3.000], mean observation: 1.057 [-3.707, 14.741], loss: --, mean_absolute_error: --, mean_q: --
   16334/1500000: episode: 127, duration: 0.125s, episode steps: 112, steps per second: 898, episode reward: -577.162, mean reward: -5.153 [-100.000, 1.145], mean action: 1.938 [0.000, 3.000], mean observation: 0.164 [-2.118, 2.321], loss: --, mean_absolute_error: --, mean_q: --
   16444/1500000: episode: 128, duration: 0.147s, episode steps: 110, steps per second: 748, episode reward: -558.076, mean reward: -5.073 [-100.000, 1.584], mean action: 1.964 [0.000, 3.000], mean observation: 0.352 [-0.751, 2.246], loss: --, mean_absolute_error: --, mean_q: --
   16515/1500000: episode: 129, duration: 0.078s, episode steps: 71, steps per second: 908, episode reward: -466.147, mean reward: -6.565 [-100.000, -0.479], mean action: 1.958 [0.000, 3.000], mean observation: 0.340 [-1.009, 2.345], loss: --, mean_absolute_error: --, mean_q: --
   16604/1500000: episode: 130, duration: 0.097s, episode steps: 89, steps per second: 915, episode reward: -532.898, mean reward: -5.988 [-100.000, 1.232], mean action: 1.921 [0.000, 2.000], mean observation: 0.109 [-2.589, 1.746], loss: --, mean_absolute_error: --, mean_q: --
   16690/1500000: episode: 131, duration: 0.102s, episode steps: 86, steps per second: 841, episode reward: -462.807, mean reward: -5.381 [-100.000, 0.735], mean action: 2.000 [1.000, 3.000], mean observation: 0.387 [-0.563, 2.141], loss: --, mean_absolute_error: --, mean_q: --
   16778/1500000: episode: 132, duration: 0.096s, episode steps: 88, steps per second: 913, episode reward: -511.019, mean reward: -5.807 [-100.000, 0.229], mean action: 1.886 [0.000, 3.000], mean observation: 0.150 [-2.321, 1.970], loss: --, mean_absolute_error: --, mean_q: --
   16988/1500000: episode: 133, duration: 0.278s, episode steps: 210, steps per second: 756, episode reward: -1253.640, mean reward: -5.970 [-100.000, 2.306], mean action: 1.929 [0.000, 3.000], mean observation: 0.779 [-0.248, 7.813], loss: --, mean_absolute_error: --, mean_q: --
   17104/1500000: episode: 134, duration: 0.132s, episode steps: 116, steps per second: 881, episode reward: -544.926, mean reward: -4.698 [-100.000, 1.839], mean action: 1.922 [0.000, 3.000], mean observation: 0.443 [-0.368, 2.628], loss: --, mean_absolute_error: --, mean_q: --
   17198/1500000: episode: 135, duration: 0.120s, episode steps: 94, steps per second: 783, episode reward: -411.899, mean reward: -4.382 [-100.000, 3.416], mean action: 1.936 [0.000, 3.000], mean observation: 0.304 [-0.535, 2.088], loss: --, mean_absolute_error: --, mean_q: --
   17278/1500000: episode: 136, duration: 0.093s, episode steps: 80, steps per second: 857, episode reward: -412.097, mean reward: -5.151 [-100.000, 1.926], mean action: 1.938 [0.000, 3.000], mean observation: -0.010 [-2.459, 1.067], loss: --, mean_absolute_error: --, mean_q: --
   17377/1500000: episode: 137, duration: 0.126s, episode steps: 99, steps per second: 785, episode reward: -447.273, mean reward: -4.518 [-100.000, 2.382], mean action: 2.232 [0.000, 3.000], mean observation: 0.155 [-1.917, 1.592], loss: --, mean_absolute_error: --, mean_q: --
   17545/1500000: episode: 138, duration: 0.216s, episode steps: 168, steps per second: 777, episode reward: -925.823, mean reward: -5.511 [-100.000, 1.777], mean action: 1.917 [0.000, 3.000], mean observation: 0.580 [-0.497, 5.088], loss: --, mean_absolute_error: --, mean_q: --
   17666/1500000: episode: 139, duration: 0.146s, episode steps: 121, steps per second: 830, episode reward: -651.716, mean reward: -5.386 [-100.000, -0.323], mean action: 1.950 [0.000, 2.000], mean observation: 0.594 [-0.124, 3.675], loss: --, mean_absolute_error: --, mean_q: --
   17824/1500000: episode: 140, duration: 0.206s, episode steps: 158, steps per second: 767, episode reward: -567.770, mean reward: -3.593 [-100.000, 3.269], mean action: 1.930 [0.000, 3.000], mean observation: 0.184 [-1.398, 2.940], loss: --, mean_absolute_error: --, mean_q: --
   17931/1500000: episode: 141, duration: 0.132s, episode steps: 107, steps per second: 810, episode reward: -488.875, mean reward: -4.569 [-100.000, 1.183], mean action: 1.935 [0.000, 3.000], mean observation: 0.441 [-0.215, 2.417], loss: --, mean_absolute_error: --, mean_q: --
   18219/1500000: episode: 142, duration: 0.463s, episode steps: 288, steps per second: 622, episode reward: -1518.349, mean reward: -5.272 [-100.000, 3.039], mean action: 1.934 [0.000, 3.000], mean observation: 0.618 [-0.761, 8.905], loss: --, mean_absolute_error: --, mean_q: --
   18345/1500000: episode: 143, duration: 0.142s, episode steps: 126, steps per second: 888, episode reward: -567.498, mean reward: -4.504 [-100.000, 1.704], mean action: 1.913 [0.000, 3.000], mean observation: 0.513 [-0.118, 3.185], loss: --, mean_absolute_error: --, mean_q: --
   18461/1500000: episode: 144, duration: 0.137s, episode steps: 116, steps per second: 844, episode reward: -515.534, mean reward: -4.444 [-100.000, 1.625], mean action: 1.914 [0.000, 3.000], mean observation: 0.446 [-0.325, 2.611], loss: --, mean_absolute_error: --, mean_q: --
   18653/1500000: episode: 145, duration: 0.255s, episode steps: 192, steps per second: 754, episode reward: -952.089, mean reward: -4.959 [-100.000, 3.789], mean action: 2.005 [0.000, 3.000], mean observation: 0.451 [-1.020, 4.541], loss: --, mean_absolute_error: --, mean_q: --
   18737/1500000: episode: 146, duration: 0.106s, episode steps: 84, steps per second: 795, episode reward: -387.468, mean reward: -4.613 [-100.000, 0.602], mean action: 1.952 [0.000, 3.000], mean observation: 0.033 [-1.954, 1.530], loss: --, mean_absolute_error: --, mean_q: --
   18818/1500000: episode: 147, duration: 0.086s, episode steps: 81, steps per second: 937, episode reward: -556.759, mean reward: -6.874 [-100.000, -0.956], mean action: 2.037 [2.000, 3.000], mean observation: 0.406 [-0.865, 2.536], loss: --, mean_absolute_error: --, mean_q: --
   18920/1500000: episode: 148, duration: 0.123s, episode steps: 102, steps per second: 827, episode reward: -533.957, mean reward: -5.235 [-100.000, 2.047], mean action: 1.931 [0.000, 3.000], mean observation: 0.093 [-2.699, 1.484], loss: --, mean_absolute_error: --, mean_q: --
   19171/1500000: episode: 149, duration: 0.361s, episode steps: 251, steps per second: 696, episode reward: -1309.406, mean reward: -5.217 [-100.000, 3.044], mean action: 1.888 [0.000, 3.000], mean observation: 0.630 [-0.666, 7.611], loss: --, mean_absolute_error: --, mean_q: --
   19251/1500000: episode: 150, duration: 0.086s, episode steps: 80, steps per second: 930, episode reward: -432.381, mean reward: -5.405 [-100.000, 2.186], mean action: 2.337 [0.000, 3.000], mean observation: 0.192 [-1.925, 1.776], loss: --, mean_absolute_error: --, mean_q: --
   19341/1500000: episode: 151, duration: 0.101s, episode steps: 90, steps per second: 887, episode reward: -582.976, mean reward: -6.478 [-100.000, 1.174], mean action: 1.978 [1.000, 3.000], mean observation: 0.128 [-2.879, 1.751], loss: --, mean_absolute_error: --, mean_q: --
   19542/1500000: episode: 152, duration: 0.290s, episode steps: 201, steps per second: 694, episode reward: -1191.586, mean reward: -5.928 [-100.000, 1.883], mean action: 1.965 [0.000, 3.000], mean observation: 0.572 [-2.401, 6.788], loss: --, mean_absolute_error: --, mean_q: --
   19712/1500000: episode: 153, duration: 0.207s, episode steps: 170, steps per second: 823, episode reward: -941.249, mean reward: -5.537 [-100.000, 2.041], mean action: 1.971 [0.000, 3.000], mean observation: 0.400 [-3.897, 3.846], loss: --, mean_absolute_error: --, mean_q: --
   19834/1500000: episode: 154, duration: 0.155s, episode steps: 122, steps per second: 785, episode reward: -704.845, mean reward: -5.777 [-100.000, 1.303], mean action: 1.967 [0.000, 3.000], mean observation: 0.432 [-0.894, 2.964], loss: --, mean_absolute_error: --, mean_q: --
   20079/1500000: episode: 155, duration: 0.598s, episode steps: 245, steps per second: 410, episode reward: -1473.894, mean reward: -6.016 [-100.000, 1.737], mean action: 1.963 [0.000, 3.000], mean observation: 0.732 [-3.786, 8.439], loss: --, mean_absolute_error: --, mean_q: --
   20176/1500000: episode: 156, duration: 0.107s, episode steps: 97, steps per second: 909, episode reward: -594.391, mean reward: -6.128 [-100.000, 0.465], mean action: 1.948 [0.000, 3.000], mean observation: 0.199 [-2.533, 2.339], loss: --, mean_absolute_error: --, mean_q: --
   20276/1500000: episode: 157, duration: 0.121s, episode steps: 100, steps per second: 824, episode reward: -550.628, mean reward: -5.506 [-100.000, 2.575], mean action: 2.000 [1.000, 3.000], mean observation: 0.325 [-0.644, 2.532], loss: --, mean_absolute_error: --, mean_q: --
   20418/1500000: episode: 158, duration: 0.166s, episode steps: 142, steps per second: 855, episode reward: -696.675, mean reward: -4.906 [-100.000, 2.131], mean action: 1.979 [0.000, 3.000], mean observation: 0.275 [-1.887, 3.420], loss: --, mean_absolute_error: --, mean_q: --
   20504/1500000: episode: 159, duration: 0.108s, episode steps: 86, steps per second: 800, episode reward: -311.050, mean reward: -3.617 [-100.000, 1.728], mean action: 1.930 [0.000, 3.000], mean observation: 0.308 [-0.566, 1.743], loss: --, mean_absolute_error: --, mean_q: --
   20595/1500000: episode: 160, duration: 0.099s, episode steps: 91, steps per second: 919, episode reward: -345.501, mean reward: -3.797 [-100.000, 0.752], mean action: 1.934 [0.000, 3.000], mean observation: 0.290 [-0.433, 1.886], loss: --, mean_absolute_error: --, mean_q: --
   20718/1500000: episode: 161, duration: 0.168s, episode steps: 123, steps per second: 732, episode reward: -572.624, mean reward: -4.655 [-100.000, 4.308], mean action: 1.967 [0.000, 3.000], mean observation: 0.316 [-0.655, 2.442], loss: --, mean_absolute_error: --, mean_q: --
   20860/1500000: episode: 162, duration: 0.174s, episode steps: 142, steps per second: 817, episode reward: -421.137, mean reward: -2.966 [-100.000, 3.701], mean action: 1.915 [0.000, 3.000], mean observation: 0.337 [-0.542, 2.121], loss: --, mean_absolute_error: --, mean_q: --
   20982/1500000: episode: 163, duration: 0.149s, episode steps: 122, steps per second: 816, episode reward: -554.636, mean reward: -4.546 [-100.000, 1.495], mean action: 1.902 [0.000, 3.000], mean observation: 0.568 [-0.144, 3.150], loss: --, mean_absolute_error: --, mean_q: --
   21130/1500000: episode: 164, duration: 0.179s, episode steps: 148, steps per second: 828, episode reward: -548.810, mean reward: -3.708 [-100.000, 3.942], mean action: 1.939 [0.000, 3.000], mean observation: 0.145 [-1.636, 2.516], loss: --, mean_absolute_error: --, mean_q: --
   21207/1500000: episode: 165, duration: 0.094s, episode steps: 77, steps per second: 822, episode reward: -421.267, mean reward: -5.471 [-100.000, 0.351], mean action: 1.987 [0.000, 3.000], mean observation: 0.334 [-0.720, 2.322], loss: --, mean_absolute_error: --, mean_q: --
   21353/1500000: episode: 166, duration: 0.167s, episode steps: 146, steps per second: 873, episode reward: -798.480, mean reward: -5.469 [-100.000, 2.071], mean action: 1.986 [0.000, 3.000], mean observation: 0.460 [-0.605, 3.684], loss: --, mean_absolute_error: --, mean_q: --
   21620/1500000: episode: 167, duration: 0.481s, episode steps: 267, steps per second: 555, episode reward: -1441.875, mean reward: -5.400 [-100.000, 3.877], mean action: 1.951 [0.000, 3.000], mean observation: 0.623 [-2.355, 8.851], loss: --, mean_absolute_error: --, mean_q: --
   21723/1500000: episode: 168, duration: 0.113s, episode steps: 103, steps per second: 909, episode reward: -625.735, mean reward: -6.075 [-100.000, 0.208], mean action: 1.981 [0.000, 3.000], mean observation: 0.452 [-0.579, 2.644], loss: --, mean_absolute_error: --, mean_q: --
   21909/1500000: episode: 169, duration: 0.240s, episode steps: 186, steps per second: 776, episode reward: -773.234, mean reward: -4.157 [-100.000, 2.286], mean action: 1.952 [0.000, 3.000], mean observation: 0.476 [-0.557, 4.134], loss: --, mean_absolute_error: --, mean_q: --
   22095/1500000: episode: 170, duration: 0.240s, episode steps: 186, steps per second: 775, episode reward: -1051.863, mean reward: -5.655 [-100.000, 1.803], mean action: 1.903 [0.000, 3.000], mean observation: 0.471 [-3.382, 4.991], loss: --, mean_absolute_error: --, mean_q: --
   22256/1500000: episode: 171, duration: 0.220s, episode steps: 161, steps per second: 733, episode reward: -868.702, mean reward: -5.396 [-100.000, 1.711], mean action: 1.963 [0.000, 3.000], mean observation: 0.421 [-1.769, 4.831], loss: --, mean_absolute_error: --, mean_q: --
   22472/1500000: episode: 172, duration: 0.327s, episode steps: 216, steps per second: 661, episode reward: -1323.622, mean reward: -6.128 [-100.000, 1.876], mean action: 1.907 [0.000, 3.000], mean observation: 0.651 [-1.729, 7.057], loss: --, mean_absolute_error: --, mean_q: --
   22565/1500000: episode: 173, duration: 0.101s, episode steps: 93, steps per second: 923, episode reward: -587.285, mean reward: -6.315 [-100.000, -0.415], mean action: 1.935 [0.000, 2.000], mean observation: 0.192 [-2.417, 2.332], loss: --, mean_absolute_error: --, mean_q: --
   22706/1500000: episode: 174, duration: 0.171s, episode steps: 141, steps per second: 826, episode reward: -829.126, mean reward: -5.880 [-100.000, 2.223], mean action: 1.943 [0.000, 3.000], mean observation: 0.336 [-2.426, 3.765], loss: --, mean_absolute_error: --, mean_q: --
   22898/1500000: episode: 175, duration: 0.234s, episode steps: 192, steps per second: 820, episode reward: -1125.266, mean reward: -5.861 [-100.000, 2.601], mean action: 1.880 [0.000, 3.000], mean observation: 0.548 [-2.948, 6.081], loss: --, mean_absolute_error: --, mean_q: --
   23005/1500000: episode: 176, duration: 0.122s, episode steps: 107, steps per second: 880, episode reward: -648.116, mean reward: -6.057 [-100.000, 2.086], mean action: 2.019 [0.000, 3.000], mean observation: 0.400 [-0.896, 2.584], loss: --, mean_absolute_error: --, mean_q: --
   23085/1500000: episode: 177, duration: 0.104s, episode steps: 80, steps per second: 768, episode reward: -507.053, mean reward: -6.338 [-100.000, -1.116], mean action: 1.988 [1.000, 2.000], mean observation: 0.165 [-2.123, 2.322], loss: --, mean_absolute_error: --, mean_q: --
   23164/1500000: episode: 178, duration: 0.099s, episode steps: 79, steps per second: 802, episode reward: -371.116, mean reward: -4.698 [-100.000, 1.130], mean action: 1.899 [0.000, 2.000], mean observation: 0.340 [-0.383, 2.045], loss: --, mean_absolute_error: --, mean_q: --
   23241/1500000: episode: 179, duration: 0.110s, episode steps: 77, steps per second: 697, episode reward: -449.206, mean reward: -5.834 [-100.000, 0.567], mean action: 1.974 [0.000, 3.000], mean observation: 0.398 [-0.535, 2.255], loss: --, mean_absolute_error: --, mean_q: --
   23446/1500000: episode: 180, duration: 0.290s, episode steps: 205, steps per second: 708, episode reward: -1043.576, mean reward: -5.091 [-100.000, 2.075], mean action: 1.873 [0.000, 3.000], mean observation: 0.477 [-4.136, 4.415], loss: --, mean_absolute_error: --, mean_q: --
   23608/1500000: episode: 181, duration: 0.195s, episode steps: 162, steps per second: 832, episode reward: -692.733, mean reward: -4.276 [-100.000, 4.474], mean action: 1.951 [0.000, 3.000], mean observation: 0.250 [-2.457, 3.088], loss: --, mean_absolute_error: --, mean_q: --
   23815/1500000: episode: 182, duration: 0.380s, episode steps: 207, steps per second: 545, episode reward: -1007.322, mean reward: -4.866 [-100.000, 2.169], mean action: 1.937 [0.000, 3.000], mean observation: 0.481 [-0.736, 5.138], loss: --, mean_absolute_error: --, mean_q: --
   23947/1500000: episode: 183, duration: 0.273s, episode steps: 132, steps per second: 484, episode reward: -789.330, mean reward: -5.980 [-100.000, 1.611], mean action: 1.939 [0.000, 3.000], mean observation: 0.320 [-2.893, 3.237], loss: --, mean_absolute_error: --, mean_q: --
   24184/1500000: episode: 184, duration: 0.343s, episode steps: 237, steps per second: 692, episode reward: -1249.275, mean reward: -5.271 [-100.000, 2.458], mean action: 1.920 [0.000, 3.000], mean observation: 0.555 [-3.733, 6.539], loss: --, mean_absolute_error: --, mean_q: --
   24299/1500000: episode: 185, duration: 0.131s, episode steps: 115, steps per second: 879, episode reward: -605.772, mean reward: -5.268 [-100.000, 1.167], mean action: 1.913 [0.000, 3.000], mean observation: 0.253 [-1.858, 2.979], loss: --, mean_absolute_error: --, mean_q: --
   24393/1500000: episode: 186, duration: 0.113s, episode steps: 94, steps per second: 832, episode reward: -413.048, mean reward: -4.394 [-100.000, 2.335], mean action: 1.947 [0.000, 3.000], mean observation: 0.306 [-0.585, 2.072], loss: --, mean_absolute_error: --, mean_q: --
   24499/1500000: episode: 187, duration: 0.128s, episode steps: 106, steps per second: 827, episode reward: -521.274, mean reward: -4.918 [-100.000, 1.234], mean action: 1.887 [0.000, 3.000], mean observation: 0.428 [-0.382, 2.372], loss: --, mean_absolute_error: --, mean_q: --
   24587/1500000: episode: 188, duration: 0.104s, episode steps: 88, steps per second: 849, episode reward: -463.159, mean reward: -5.263 [-100.000, 0.493], mean action: 1.989 [1.000, 2.000], mean observation: 0.376 [-0.480, 2.148], loss: --, mean_absolute_error: --, mean_q: --
   24723/1500000: episode: 189, duration: 0.160s, episode steps: 136, steps per second: 849, episode reward: -744.413, mean reward: -5.474 [-100.000, 1.296], mean action: 1.926 [0.000, 3.000], mean observation: 0.320 [-1.707, 3.825], loss: --, mean_absolute_error: --, mean_q: --
   24951/1500000: episode: 190, duration: 0.335s, episode steps: 228, steps per second: 680, episode reward: -1186.861, mean reward: -5.206 [-100.000, 3.131], mean action: 1.930 [0.000, 3.000], mean observation: 0.531 [-1.018, 6.276], loss: --, mean_absolute_error: --, mean_q: --
   25148/1500000: episode: 191, duration: 0.284s, episode steps: 197, steps per second: 694, episode reward: -706.983, mean reward: -3.589 [-100.000, 4.021], mean action: 1.909 [0.000, 3.000], mean observation: 0.472 [-0.308, 4.179], loss: --, mean_absolute_error: --, mean_q: --
   25310/1500000: episode: 192, duration: 0.213s, episode steps: 162, steps per second: 761, episode reward: -947.829, mean reward: -5.851 [-100.000, 2.581], mean action: 1.975 [0.000, 3.000], mean observation: 0.407 [-2.996, 4.382], loss: --, mean_absolute_error: --, mean_q: --
   25386/1500000: episode: 193, duration: 0.090s, episode steps: 76, steps per second: 840, episode reward: -406.613, mean reward: -5.350 [-100.000, 2.619], mean action: 2.013 [1.000, 3.000], mean observation: -0.014 [-2.448, 1.137], loss: --, mean_absolute_error: --, mean_q: --
   25466/1500000: episode: 194, duration: 0.088s, episode steps: 80, steps per second: 913, episode reward: -513.537, mean reward: -6.419 [-100.000, -1.423], mean action: 1.962 [1.000, 2.000], mean observation: 0.100 [-2.581, 1.709], loss: --, mean_absolute_error: --, mean_q: --
   25573/1500000: episode: 195, duration: 0.130s, episode steps: 107, steps per second: 824, episode reward: -505.734, mean reward: -4.726 [-100.000, 0.919], mean action: 1.991 [1.000, 3.000], mean observation: 0.364 [-0.497, 2.022], loss: --, mean_absolute_error: --, mean_q: --
   25656/1500000: episode: 196, duration: 0.090s, episode steps: 83, steps per second: 923, episode reward: -519.708, mean reward: -6.262 [-100.000, -0.361], mean action: 1.952 [0.000, 3.000], mean observation: 0.168 [-2.276, 2.143], loss: --, mean_absolute_error: --, mean_q: --
   25933/1500000: episode: 197, duration: 0.408s, episode steps: 277, steps per second: 678, episode reward: -1681.742, mean reward: -6.071 [-100.000, 1.522], mean action: 1.957 [0.000, 3.000], mean observation: 1.001 [-0.558, 11.357], loss: --, mean_absolute_error: --, mean_q: --
   26010/1500000: episode: 198, duration: 0.090s, episode steps: 77, steps per second: 855, episode reward: -458.766, mean reward: -5.958 [-100.000, 0.597], mean action: 2.000 [1.000, 3.000], mean observation: 0.347 [-0.693, 2.431], loss: --, mean_absolute_error: --, mean_q: --
   26132/1500000: episode: 199, duration: 0.145s, episode steps: 122, steps per second: 842, episode reward: -581.455, mean reward: -4.766 [-100.000, 2.002], mean action: 1.943 [0.000, 3.000], mean observation: 0.391 [-0.455, 2.472], loss: --, mean_absolute_error: --, mean_q: --
   26334/1500000: episode: 200, duration: 0.267s, episode steps: 202, steps per second: 758, episode reward: -831.606, mean reward: -4.117 [-100.000, 2.303], mean action: 1.985 [0.000, 3.000], mean observation: 0.244 [-1.123, 3.981], loss: --, mean_absolute_error: --, mean_q: --
   26436/1500000: episode: 201, duration: 0.114s, episode steps: 102, steps per second: 892, episode reward: -483.243, mean reward: -4.738 [-100.000, 1.230], mean action: 2.020 [1.000, 3.000], mean observation: 0.291 [-0.722, 2.389], loss: --, mean_absolute_error: --, mean_q: --
   26512/1500000: episode: 202, duration: 0.081s, episode steps: 76, steps per second: 942, episode reward: -508.740, mean reward: -6.694 [-100.000, -0.303], mean action: 1.947 [0.000, 3.000], mean observation: 0.362 [-0.866, 2.636], loss: --, mean_absolute_error: --, mean_q: --
   26652/1500000: episode: 203, duration: 0.178s, episode steps: 140, steps per second: 787, episode reward: -496.911, mean reward: -3.549 [-100.000, 4.692], mean action: 1.914 [0.000, 3.000], mean observation: 0.089 [-2.406, 1.514], loss: --, mean_absolute_error: --, mean_q: --
   26735/1500000: episode: 204, duration: 0.092s, episode steps: 83, steps per second: 901, episode reward: -397.589, mean reward: -4.790 [-100.000, 0.598], mean action: 1.976 [1.000, 3.000], mean observation: 0.332 [-0.449, 2.147], loss: --, mean_absolute_error: --, mean_q: --
   26809/1500000: episode: 205, duration: 0.091s, episode steps: 74, steps per second: 813, episode reward: -529.401, mean reward: -7.154 [-100.000, -0.759], mean action: 1.973 [0.000, 3.000], mean observation: 0.417 [-0.924, 2.677], loss: --, mean_absolute_error: --, mean_q: --
   27029/1500000: episode: 206, duration: 0.294s, episode steps: 220, steps per second: 747, episode reward: -1281.407, mean reward: -5.825 [-100.000, 1.806], mean action: 1.909 [0.000, 3.000], mean observation: 0.661 [-2.921, 7.537], loss: --, mean_absolute_error: --, mean_q: --
   27159/1500000: episode: 207, duration: 0.148s, episode steps: 130, steps per second: 876, episode reward: -649.154, mean reward: -4.993 [-100.000, 3.312], mean action: 1.985 [0.000, 3.000], mean observation: 0.261 [-1.257, 2.884], loss: --, mean_absolute_error: --, mean_q: --
   27255/1500000: episode: 208, duration: 0.116s, episode steps: 96, steps per second: 827, episode reward: -428.407, mean reward: -4.463 [-100.000, 3.007], mean action: 1.969 [0.000, 3.000], mean observation: 0.032 [-2.301, 1.324], loss: --, mean_absolute_error: --, mean_q: --
   27348/1500000: episode: 209, duration: 0.102s, episode steps: 93, steps per second: 915, episode reward: -599.856, mean reward: -6.450 [-100.000, 1.272], mean action: 1.968 [0.000, 2.000], mean observation: 0.504 [-0.565, 2.748], loss: --, mean_absolute_error: --, mean_q: --
   27433/1500000: episode: 210, duration: 0.090s, episode steps: 85, steps per second: 941, episode reward: -560.840, mean reward: -6.598 [-100.000, -0.158], mean action: 2.000 [1.000, 3.000], mean observation: 0.465 [-0.656, 2.397], loss: --, mean_absolute_error: --, mean_q: --
   27725/1500000: episode: 211, duration: 0.531s, episode steps: 292, steps per second: 550, episode reward: -1686.798, mean reward: -5.777 [-100.000, 1.865], mean action: 2.000 [0.000, 3.000], mean observation: 0.794 [-0.709, 10.622], loss: --, mean_absolute_error: --, mean_q: --
   27846/1500000: episode: 212, duration: 0.296s, episode steps: 121, steps per second: 409, episode reward: -636.587, mean reward: -5.261 [-100.000, 0.814], mean action: 1.983 [0.000, 3.000], mean observation: 0.387 [-0.868, 2.542], loss: --, mean_absolute_error: --, mean_q: --
   27916/1500000: episode: 213, duration: 0.090s, episode steps: 70, steps per second: 778, episode reward: -489.848, mean reward: -6.998 [-100.000, -0.707], mean action: 1.957 [0.000, 3.000], mean observation: 0.418 [-0.712, 2.572], loss: --, mean_absolute_error: --, mean_q: --
   28021/1500000: episode: 214, duration: 0.114s, episode steps: 105, steps per second: 920, episode reward: -682.415, mean reward: -6.499 [-100.000, 0.947], mean action: 1.962 [0.000, 3.000], mean observation: 0.261 [-2.831, 2.577], loss: --, mean_absolute_error: --, mean_q: --
   28114/1500000: episode: 215, duration: 0.115s, episode steps: 93, steps per second: 807, episode reward: -358.331, mean reward: -3.853 [-100.000, 0.924], mean action: 1.978 [0.000, 3.000], mean observation: 0.029 [-1.537, 1.583], loss: --, mean_absolute_error: --, mean_q: --
   28239/1500000: episode: 216, duration: 0.155s, episode steps: 125, steps per second: 805, episode reward: -602.486, mean reward: -4.820 [-100.000, 3.307], mean action: 1.952 [0.000, 3.000], mean observation: 0.343 [-0.490, 2.431], loss: --, mean_absolute_error: --, mean_q: --
   28343/1500000: episode: 217, duration: 0.131s, episode steps: 104, steps per second: 792, episode reward: -587.586, mean reward: -5.650 [-100.000, 0.958], mean action: 1.971 [0.000, 3.000], mean observation: 0.222 [-1.938, 2.768], loss: --, mean_absolute_error: --, mean_q: --
   28428/1500000: episode: 218, duration: 0.091s, episode steps: 85, steps per second: 931, episode reward: -530.400, mean reward: -6.240 [-100.000, 0.199], mean action: 2.000 [0.000, 3.000], mean observation: 0.415 [-0.719, 2.299], loss: --, mean_absolute_error: --, mean_q: --
   28584/1500000: episode: 219, duration: 0.191s, episode steps: 156, steps per second: 818, episode reward: -908.279, mean reward: -5.822 [-100.000, 1.905], mean action: 1.949 [0.000, 3.000], mean observation: 0.401 [-2.727, 4.382], loss: --, mean_absolute_error: --, mean_q: --
   28670/1500000: episode: 220, duration: 0.092s, episode steps: 86, steps per second: 930, episode reward: -402.980, mean reward: -4.686 [-100.000, 0.156], mean action: 1.930 [0.000, 3.000], mean observation: 0.418 [-0.214, 1.834], loss: --, mean_absolute_error: --, mean_q: --
   28793/1500000: episode: 221, duration: 0.146s, episode steps: 123, steps per second: 840, episode reward: -659.835, mean reward: -5.365 [-100.000, 1.739], mean action: 1.959 [0.000, 3.000], mean observation: 0.441 [-0.462, 2.959], loss: --, mean_absolute_error: --, mean_q: --
   28882/1500000: episode: 222, duration: 0.099s, episode steps: 89, steps per second: 898, episode reward: -526.303, mean reward: -5.914 [-100.000, 0.390], mean action: 1.933 [0.000, 3.000], mean observation: 0.131 [-2.433, 1.904], loss: --, mean_absolute_error: --, mean_q: --
   29059/1500000: episode: 223, duration: 0.293s, episode steps: 177, steps per second: 604, episode reward: -970.683, mean reward: -5.484 [-100.000, 0.995], mean action: 1.938 [0.000, 3.000], mean observation: 0.495 [-1.746, 5.777], loss: --, mean_absolute_error: --, mean_q: --
   29248/1500000: episode: 224, duration: 0.247s, episode steps: 189, steps per second: 765, episode reward: -838.541, mean reward: -4.437 [-100.000, 3.304], mean action: 1.963 [0.000, 3.000], mean observation: 0.309 [-3.429, 3.340], loss: --, mean_absolute_error: --, mean_q: --
   29332/1500000: episode: 225, duration: 0.101s, episode steps: 84, steps per second: 835, episode reward: -460.465, mean reward: -5.482 [-100.000, 0.995], mean action: 2.000 [1.000, 3.000], mean observation: 0.063 [-2.342, 1.612], loss: --, mean_absolute_error: --, mean_q: --
   29498/1500000: episode: 226, duration: 0.216s, episode steps: 166, steps per second: 767, episode reward: -663.591, mean reward: -3.998 [-100.000, 3.430], mean action: 1.970 [0.000, 3.000], mean observation: 0.445 [-0.486, 3.524], loss: --, mean_absolute_error: --, mean_q: --
   29719/1500000: episode: 227, duration: 0.336s, episode steps: 221, steps per second: 657, episode reward: -1079.770, mean reward: -4.886 [-100.000, 2.017], mean action: 1.932 [0.000, 3.000], mean observation: 0.458 [-1.265, 6.444], loss: --, mean_absolute_error: --, mean_q: --
   29850/1500000: episode: 228, duration: 0.154s, episode steps: 131, steps per second: 849, episode reward: -640.428, mean reward: -4.889 [-100.000, 1.687], mean action: 1.947 [0.000, 3.000], mean observation: 0.450 [-0.507, 3.060], loss: --, mean_absolute_error: --, mean_q: --
   29957/1500000: episode: 229, duration: 0.127s, episode steps: 107, steps per second: 840, episode reward: -551.730, mean reward: -5.156 [-100.000, 0.840], mean action: 1.925 [0.000, 3.000], mean observation: 0.210 [-1.618, 2.831], loss: --, mean_absolute_error: --, mean_q: --
   30249/1500000: episode: 230, duration: 0.447s, episode steps: 292, steps per second: 653, episode reward: -1623.165, mean reward: -5.559 [-100.000, 4.320], mean action: 1.945 [0.000, 3.000], mean observation: 0.697 [-0.965, 9.705], loss: --, mean_absolute_error: --, mean_q: --
   30328/1500000: episode: 231, duration: 0.087s, episode steps: 79, steps per second: 908, episode reward: -402.846, mean reward: -5.099 [-100.000, 2.705], mean action: 1.962 [0.000, 3.000], mean observation: 0.011 [-2.231, 1.339], loss: --, mean_absolute_error: --, mean_q: --
   30435/1500000: episode: 232, duration: 0.126s, episode steps: 107, steps per second: 846, episode reward: -441.028, mean reward: -4.122 [-100.000, 1.103], mean action: 1.963 [0.000, 3.000], mean observation: 0.102 [-1.635, 1.961], loss: --, mean_absolute_error: --, mean_q: --
   30544/1500000: episode: 233, duration: 0.135s, episode steps: 109, steps per second: 809, episode reward: -520.342, mean reward: -4.774 [-100.000, 1.450], mean action: 1.917 [0.000, 2.000], mean observation: 0.444 [-0.197, 2.501], loss: --, mean_absolute_error: --, mean_q: --
   30628/1500000: episode: 234, duration: 0.099s, episode steps: 84, steps per second: 853, episode reward: -313.543, mean reward: -3.733 [-100.000, 1.607], mean action: 2.143 [0.000, 3.000], mean observation: 0.250 [-0.781, 1.758], loss: --, mean_absolute_error: --, mean_q: --
   30703/1500000: episode: 235, duration: 0.083s, episode steps: 75, steps per second: 898, episode reward: -544.828, mean reward: -7.264 [-100.000, -0.704], mean action: 2.013 [0.000, 3.000], mean observation: 0.428 [-0.896, 2.661], loss: --, mean_absolute_error: --, mean_q: --
   30806/1500000: episode: 236, duration: 0.116s, episode steps: 103, steps per second: 889, episode reward: -598.991, mean reward: -5.815 [-100.000, 0.718], mean action: 2.000 [0.000, 3.000], mean observation: 0.355 [-0.869, 2.601], loss: --, mean_absolute_error: --, mean_q: --
   31015/1500000: episode: 237, duration: 0.275s, episode steps: 209, steps per second: 759, episode reward: -1267.539, mean reward: -6.065 [-100.000, 2.044], mean action: 1.962 [0.000, 3.000], mean observation: 0.838 [-0.198, 8.284], loss: --, mean_absolute_error: --, mean_q: --
   31147/1500000: episode: 238, duration: 0.166s, episode steps: 132, steps per second: 796, episode reward: -802.077, mean reward: -6.076 [-100.000, 2.386], mean action: 1.977 [0.000, 3.000], mean observation: 0.360 [-2.587, 3.810], loss: --, mean_absolute_error: --, mean_q: --
   31262/1500000: episode: 239, duration: 0.170s, episode steps: 115, steps per second: 676, episode reward: -390.591, mean reward: -3.396 [-100.000, 3.742], mean action: 1.957 [0.000, 3.000], mean observation: 0.294 [-0.518, 1.667], loss: --, mean_absolute_error: --, mean_q: --
   31342/1500000: episode: 240, duration: 0.086s, episode steps: 80, steps per second: 933, episode reward: -397.449, mean reward: -4.968 [-100.000, 0.578], mean action: 1.962 [0.000, 3.000], mean observation: 0.033 [-2.019, 1.525], loss: --, mean_absolute_error: --, mean_q: --
   31439/1500000: episode: 241, duration: 0.109s, episode steps: 97, steps per second: 891, episode reward: -515.667, mean reward: -5.316 [-100.000, 0.375], mean action: 1.948 [0.000, 3.000], mean observation: 0.105 [-2.331, 1.726], loss: --, mean_absolute_error: --, mean_q: --
   31525/1500000: episode: 242, duration: 0.104s, episode steps: 86, steps per second: 829, episode reward: -404.002, mean reward: -4.698 [-100.000, 1.766], mean action: 2.209 [0.000, 3.000], mean observation: 0.238 [-1.205, 1.885], loss: --, mean_absolute_error: --, mean_q: --
   31612/1500000: episode: 243, duration: 0.098s, episode steps: 87, steps per second: 887, episode reward: -533.098, mean reward: -6.128 [-100.000, 1.591], mean action: 1.989 [1.000, 3.000], mean observation: 0.090 [-2.586, 1.638], loss: --, mean_absolute_error: --, mean_q: --
   31750/1500000: episode: 244, duration: 0.211s, episode steps: 138, steps per second: 655, episode reward: -637.888, mean reward: -4.622 [-100.000, 2.169], mean action: 1.942 [0.000, 3.000], mean observation: 0.415 [-0.434, 2.925], loss: --, mean_absolute_error: --, mean_q: --
   31971/1500000: episode: 245, duration: 0.566s, episode steps: 221, steps per second: 390, episode reward: -1002.021, mean reward: -4.534 [-100.000, 3.258], mean action: 1.946 [0.000, 3.000], mean observation: 0.630 [-0.355, 6.167], loss: --, mean_absolute_error: --, mean_q: --
   32122/1500000: episode: 246, duration: 0.192s, episode steps: 151, steps per second: 786, episode reward: -952.560, mean reward: -6.308 [-100.000, 2.096], mean action: 1.974 [1.000, 3.000], mean observation: 0.487 [-2.702, 4.964], loss: --, mean_absolute_error: --, mean_q: --
   32196/1500000: episode: 247, duration: 0.095s, episode steps: 74, steps per second: 778, episode reward: -529.443, mean reward: -7.155 [-100.000, -0.444], mean action: 1.959 [0.000, 3.000], mean observation: 0.110 [-2.699, 1.667], loss: --, mean_absolute_error: --, mean_q: --
   32271/1500000: episode: 248, duration: 0.083s, episode steps: 75, steps per second: 902, episode reward: -392.314, mean reward: -5.231 [-100.000, 1.145], mean action: 1.920 [0.000, 3.000], mean observation: -0.013 [-2.328, 1.168], loss: --, mean_absolute_error: --, mean_q: --
   32361/1500000: episode: 249, duration: 0.103s, episode steps: 90, steps per second: 876, episode reward: -450.393, mean reward: -5.004 [-100.000, 2.211], mean action: 1.989 [0.000, 3.000], mean observation: 0.033 [-2.469, 1.305], loss: --, mean_absolute_error: --, mean_q: --
   32526/1500000: episode: 250, duration: 0.219s, episode steps: 165, steps per second: 752, episode reward: -744.957, mean reward: -4.515 [-100.000, 3.891], mean action: 1.921 [0.000, 3.000], mean observation: 0.191 [-3.978, 1.760], loss: --, mean_absolute_error: --, mean_q: --
   32632/1500000: episode: 251, duration: 0.125s, episode steps: 106, steps per second: 850, episode reward: -597.577, mean reward: -5.638 [-100.000, 0.994], mean action: 1.962 [0.000, 3.000], mean observation: 0.390 [-0.714, 2.412], loss: --, mean_absolute_error: --, mean_q: --
   32859/1500000: episode: 252, duration: 0.320s, episode steps: 227, steps per second: 709, episode reward: -1128.095, mean reward: -4.970 [-100.000, 2.466], mean action: 1.952 [0.000, 3.000], mean observation: 0.582 [-0.341, 6.576], loss: --, mean_absolute_error: --, mean_q: --
   33053/1500000: episode: 253, duration: 0.254s, episode steps: 194, steps per second: 764, episode reward: -753.125, mean reward: -3.882 [-100.000, 3.746], mean action: 1.876 [0.000, 3.000], mean observation: 0.314 [-0.900, 3.268], loss: --, mean_absolute_error: --, mean_q: --
   33126/1500000: episode: 254, duration: 0.081s, episode steps: 73, steps per second: 897, episode reward: -501.639, mean reward: -6.872 [-100.000, 0.107], mean action: 1.932 [0.000, 3.000], mean observation: 0.368 [-0.954, 2.634], loss: --, mean_absolute_error: --, mean_q: --
   33248/1500000: episode: 255, duration: 0.155s, episode steps: 122, steps per second: 785, episode reward: -680.494, mean reward: -5.578 [-100.000, 2.195], mean action: 1.967 [0.000, 3.000], mean observation: 0.230 [-2.546, 2.683], loss: --, mean_absolute_error: --, mean_q: --
   33340/1500000: episode: 256, duration: 0.119s, episode steps: 92, steps per second: 770, episode reward: -429.499, mean reward: -4.668 [-100.000, 1.248], mean action: 1.946 [0.000, 3.000], mean observation: 0.333 [-0.509, 2.082], loss: --, mean_absolute_error: --, mean_q: --
   33424/1500000: episode: 257, duration: 0.102s, episode steps: 84, steps per second: 826, episode reward: -511.056, mean reward: -6.084 [-100.000, -0.106], mean action: 2.000 [0.000, 3.000], mean observation: 0.392 [-0.717, 2.306], loss: --, mean_absolute_error: --, mean_q: --
   33497/1500000: episode: 258, duration: 0.092s, episode steps: 73, steps per second: 791, episode reward: -515.498, mean reward: -7.062 [-100.000, -0.681], mean action: 2.000 [0.000, 3.000], mean observation: 0.395 [-0.839, 2.641], loss: --, mean_absolute_error: --, mean_q: --
   33590/1500000: episode: 259, duration: 0.101s, episode steps: 93, steps per second: 920, episode reward: -426.902, mean reward: -4.590 [-100.000, 1.252], mean action: 1.935 [0.000, 3.000], mean observation: 0.456 [-0.219, 2.172], loss: --, mean_absolute_error: --, mean_q: --
   33667/1500000: episode: 260, duration: 0.101s, episode steps: 77, steps per second: 759, episode reward: -509.923, mean reward: -6.622 [-100.000, -0.401], mean action: 1.883 [0.000, 2.000], mean observation: 0.128 [-2.464, 1.863], loss: --, mean_absolute_error: --, mean_q: --
   33840/1500000: episode: 261, duration: 0.443s, episode steps: 173, steps per second: 391, episode reward: -1114.077, mean reward: -6.440 [-100.000, 1.111], mean action: 1.965 [0.000, 3.000], mean observation: 0.659 [-0.537, 6.054], loss: --, mean_absolute_error: --, mean_q: --
   33920/1500000: episode: 262, duration: 0.197s, episode steps: 80, steps per second: 405, episode reward: -517.870, mean reward: -6.473 [-100.000, 0.657], mean action: 1.925 [0.000, 3.000], mean observation: 0.121 [-2.543, 1.746], loss: --, mean_absolute_error: --, mean_q: --
   34026/1500000: episode: 263, duration: 0.282s, episode steps: 106, steps per second: 377, episode reward: -674.107, mean reward: -6.359 [-100.000, 1.277], mean action: 1.934 [0.000, 3.000], mean observation: 0.493 [-0.697, 3.056], loss: --, mean_absolute_error: --, mean_q: --
   34356/1500000: episode: 264, duration: 1.215s, episode steps: 330, steps per second: 272, episode reward: -1988.920, mean reward: -6.027 [-100.000, 3.570], mean action: 1.976 [0.000, 3.000], mean observation: 0.862 [-1.948, 13.292], loss: --, mean_absolute_error: --, mean_q: --
   34504/1500000: episode: 265, duration: 0.444s, episode steps: 148, steps per second: 333, episode reward: -668.912, mean reward: -4.520 [-100.000, 2.817], mean action: 1.966 [0.000, 3.000], mean observation: 0.420 [-0.328, 3.192], loss: --, mean_absolute_error: --, mean_q: --
   34579/1500000: episode: 266, duration: 0.222s, episode steps: 75, steps per second: 337, episode reward: -557.679, mean reward: -7.436 [-100.000, -0.863], mean action: 2.000 [2.000, 2.000], mean observation: 0.434 [-0.883, 2.679], loss: --, mean_absolute_error: --, mean_q: --
   34662/1500000: episode: 267, duration: 0.293s, episode steps: 83, steps per second: 284, episode reward: -477.588, mean reward: -5.754 [-100.000, 0.633], mean action: 1.988 [0.000, 3.000], mean observation: 0.315 [-0.931, 2.364], loss: --, mean_absolute_error: --, mean_q: --
   34743/1500000: episode: 268, duration: 0.331s, episode steps: 81, steps per second: 245, episode reward: -460.126, mean reward: -5.681 [-100.000, 0.066], mean action: 1.877 [0.000, 3.000], mean observation: 0.381 [-0.606, 2.211], loss: --, mean_absolute_error: --, mean_q: --
   34823/1500000: episode: 269, duration: 0.332s, episode steps: 80, steps per second: 241, episode reward: -449.579, mean reward: -5.620 [-100.000, 0.898], mean action: 1.950 [0.000, 3.000], mean observation: 0.037 [-2.500, 1.386], loss: --, mean_absolute_error: --, mean_q: --
   35008/1500000: episode: 270, duration: 0.789s, episode steps: 185, steps per second: 234, episode reward: -993.611, mean reward: -5.371 [-100.000, 2.064], mean action: 1.935 [0.000, 3.000], mean observation: 0.693 [-0.134, 6.098], loss: --, mean_absolute_error: --, mean_q: --
   35116/1500000: episode: 271, duration: 0.375s, episode steps: 108, steps per second: 288, episode reward: -634.538, mean reward: -5.875 [-100.000, 1.533], mean action: 2.009 [0.000, 3.000], mean observation: 0.288 [-1.206, 2.801], loss: --, mean_absolute_error: --, mean_q: --
   35298/1500000: episode: 272, duration: 0.452s, episode steps: 182, steps per second: 403, episode reward: -998.219, mean reward: -5.485 [-100.000, 3.186], mean action: 2.016 [1.000, 3.000], mean observation: 0.482 [-1.266, 4.685], loss: --, mean_absolute_error: --, mean_q: --
   35380/1500000: episode: 273, duration: 0.147s, episode steps: 82, steps per second: 556, episode reward: -467.134, mean reward: -5.697 [-100.000, 1.139], mean action: 2.024 [0.000, 3.000], mean observation: 0.316 [-0.849, 2.367], loss: --, mean_absolute_error: --, mean_q: --
   35534/1500000: episode: 274, duration: 0.357s, episode steps: 154, steps per second: 431, episode reward: -715.641, mean reward: -4.647 [-100.000, 4.056], mean action: 1.948 [0.000, 3.000], mean observation: 0.222 [-2.929, 2.732], loss: --, mean_absolute_error: --, mean_q: --
   35920/1500000: episode: 275, duration: 0.956s, episode steps: 386, steps per second: 404, episode reward: -2291.748, mean reward: -5.937 [-100.000, 3.428], mean action: 1.946 [0.000, 3.000], mean observation: 1.055 [-3.732, 15.365], loss: --, mean_absolute_error: --, mean_q: --
   35992/1500000: episode: 276, duration: 0.102s, episode steps: 72, steps per second: 705, episode reward: -484.282, mean reward: -6.726 [-100.000, 0.441], mean action: 2.028 [1.000, 3.000], mean observation: 0.380 [-0.685, 2.643], loss: --, mean_absolute_error: --, mean_q: --
   36171/1500000: episode: 277, duration: 0.293s, episode steps: 179, steps per second: 612, episode reward: -725.006, mean reward: -4.050 [-100.000, 1.932], mean action: 1.955 [0.000, 3.000], mean observation: 0.507 [-0.128, 4.177], loss: --, mean_absolute_error: --, mean_q: --
   36248/1500000: episode: 278, duration: 0.119s, episode steps: 77, steps per second: 648, episode reward: -537.877, mean reward: -6.985 [-100.000, -0.495], mean action: 1.883 [0.000, 3.000], mean observation: 0.429 [-0.972, 2.550], loss: --, mean_absolute_error: --, mean_q: --
   36345/1500000: episode: 279, duration: 0.154s, episode steps: 97, steps per second: 631, episode reward: -551.485, mean reward: -5.685 [-100.000, 0.861], mean action: 1.979 [0.000, 3.000], mean observation: 0.145 [-2.395, 2.035], loss: --, mean_absolute_error: --, mean_q: --
   36438/1500000: episode: 280, duration: 0.134s, episode steps: 93, steps per second: 693, episode reward: -566.335, mean reward: -6.090 [-100.000, -0.324], mean action: 1.946 [0.000, 3.000], mean observation: 0.168 [-2.502, 2.161], loss: --, mean_absolute_error: --, mean_q: --
   36516/1500000: episode: 281, duration: 0.115s, episode steps: 78, steps per second: 676, episode reward: -434.733, mean reward: -5.573 [-100.000, 0.486], mean action: 2.103 [0.000, 3.000], mean observation: 0.301 [-1.080, 2.151], loss: --, mean_absolute_error: --, mean_q: --
   36604/1500000: episode: 282, duration: 0.118s, episode steps: 88, steps per second: 748, episode reward: -498.497, mean reward: -5.665 [-100.000, 1.054], mean action: 1.966 [0.000, 3.000], mean observation: 0.465 [-0.428, 2.279], loss: --, mean_absolute_error: --, mean_q: --
   36696/1500000: episode: 283, duration: 0.117s, episode steps: 92, steps per second: 789, episode reward: -561.039, mean reward: -6.098 [-100.000, 0.236], mean action: 1.967 [0.000, 3.000], mean observation: 0.174 [-2.111, 2.333], loss: --, mean_absolute_error: --, mean_q: --
   36799/1500000: episode: 284, duration: 0.145s, episode steps: 103, steps per second: 709, episode reward: -589.012, mean reward: -5.719 [-100.000, 0.036], mean action: 1.932 [0.000, 3.000], mean observation: 0.192 [-2.336, 2.346], loss: --, mean_absolute_error: --, mean_q: --
   36889/1500000: episode: 285, duration: 0.112s, episode steps: 90, steps per second: 801, episode reward: -413.638, mean reward: -4.596 [-100.000, 1.026], mean action: 1.922 [0.000, 3.000], mean observation: 0.058 [-1.979, 1.571], loss: --, mean_absolute_error: --, mean_q: --
   36972/1500000: episode: 286, duration: 0.093s, episode steps: 83, steps per second: 897, episode reward: -426.412, mean reward: -5.137 [-100.000, 1.001], mean action: 1.940 [0.000, 3.000], mean observation: 0.029 [-2.187, 1.439], loss: --, mean_absolute_error: --, mean_q: --
   37081/1500000: episode: 287, duration: 0.139s, episode steps: 109, steps per second: 783, episode reward: -614.487, mean reward: -5.637 [-100.000, 0.365], mean action: 1.881 [0.000, 2.000], mean observation: 0.481 [-0.438, 2.915], loss: --, mean_absolute_error: --, mean_q: --
   37202/1500000: episode: 288, duration: 0.166s, episode steps: 121, steps per second: 728, episode reward: -421.626, mean reward: -3.485 [-100.000, 1.981], mean action: 1.983 [0.000, 3.000], mean observation: 0.402 [-0.380, 2.216], loss: --, mean_absolute_error: --, mean_q: --
   37379/1500000: episode: 289, duration: 0.296s, episode steps: 177, steps per second: 598, episode reward: -1039.798, mean reward: -5.875 [-100.000, 2.168], mean action: 1.910 [0.000, 3.000], mean observation: 0.481 [-3.493, 4.863], loss: --, mean_absolute_error: --, mean_q: --
   37505/1500000: episode: 290, duration: 0.194s, episode steps: 126, steps per second: 651, episode reward: -504.414, mean reward: -4.003 [-100.000, 2.677], mean action: 1.937 [0.000, 3.000], mean observation: 0.109 [-2.111, 1.844], loss: --, mean_absolute_error: --, mean_q: --
   37601/1500000: episode: 291, duration: 0.115s, episode steps: 96, steps per second: 838, episode reward: -635.787, mean reward: -6.623 [-100.000, 0.375], mean action: 1.885 [0.000, 3.000], mean observation: 0.160 [-2.961, 1.767], loss: --, mean_absolute_error: --, mean_q: --
   37680/1500000: episode: 292, duration: 0.084s, episode steps: 79, steps per second: 940, episode reward: -386.504, mean reward: -4.892 [-100.000, 1.001], mean action: 2.316 [0.000, 3.000], mean observation: 0.182 [-1.624, 1.721], loss: --, mean_absolute_error: --, mean_q: --
   37764/1500000: episode: 293, duration: 0.173s, episode steps: 84, steps per second: 485, episode reward: -419.528, mean reward: -4.994 [-100.000, 0.260], mean action: 1.976 [1.000, 3.000], mean observation: 0.326 [-0.600, 2.179], loss: --, mean_absolute_error: --, mean_q: --
   37870/1500000: episode: 294, duration: 0.255s, episode steps: 106, steps per second: 416, episode reward: -710.947, mean reward: -6.707 [-100.000, 1.312], mean action: 1.915 [0.000, 3.000], mean observation: 0.478 [-0.783, 3.045], loss: --, mean_absolute_error: --, mean_q: --
   38068/1500000: episode: 295, duration: 0.303s, episode steps: 198, steps per second: 653, episode reward: -1178.228, mean reward: -5.951 [-100.000, 2.119], mean action: 1.965 [0.000, 3.000], mean observation: 0.546 [-3.988, 5.645], loss: --, mean_absolute_error: --, mean_q: --
   38173/1500000: episode: 296, duration: 0.125s, episode steps: 105, steps per second: 837, episode reward: -529.574, mean reward: -5.044 [-100.000, 1.572], mean action: 1.962 [0.000, 3.000], mean observation: 0.376 [-0.394, 2.127], loss: --, mean_absolute_error: --, mean_q: --
   38273/1500000: episode: 297, duration: 0.120s, episode steps: 100, steps per second: 834, episode reward: -331.228, mean reward: -3.312 [-100.000, 1.102], mean action: 1.930 [0.000, 3.000], mean observation: 0.315 [-0.438, 1.544], loss: --, mean_absolute_error: --, mean_q: --
   38504/1500000: episode: 298, duration: 0.366s, episode steps: 231, steps per second: 630, episode reward: -1168.790, mean reward: -5.060 [-100.000, 2.431], mean action: 1.944 [0.000, 3.000], mean observation: 0.514 [-0.691, 6.223], loss: --, mean_absolute_error: --, mean_q: --
   38616/1500000: episode: 299, duration: 0.130s, episode steps: 112, steps per second: 860, episode reward: -441.094, mean reward: -3.938 [-100.000, 1.516], mean action: 1.946 [0.000, 3.000], mean observation: 0.372 [-0.315, 1.957], loss: --, mean_absolute_error: --, mean_q: --
   38708/1500000: episode: 300, duration: 0.117s, episode steps: 92, steps per second: 786, episode reward: -404.838, mean reward: -4.400 [-100.000, 0.790], mean action: 1.870 [0.000, 3.000], mean observation: 0.292 [-0.611, 2.099], loss: --, mean_absolute_error: --, mean_q: --
   38931/1500000: episode: 301, duration: 0.307s, episode steps: 223, steps per second: 727, episode reward: -1356.481, mean reward: -6.083 [-100.000, 1.418], mean action: 1.964 [0.000, 3.000], mean observation: 0.666 [-0.901, 7.562], loss: --, mean_absolute_error: --, mean_q: --
   39011/1500000: episode: 302, duration: 0.087s, episode steps: 80, steps per second: 919, episode reward: -425.401, mean reward: -5.318 [-100.000, 0.218], mean action: 1.938 [0.000, 3.000], mean observation: 0.330 [-0.667, 2.296], loss: --, mean_absolute_error: --, mean_q: --
   39215/1500000: episode: 303, duration: 0.300s, episode steps: 204, steps per second: 680, episode reward: -1138.029, mean reward: -5.579 [-100.000, 2.615], mean action: 1.980 [0.000, 3.000], mean observation: 0.503 [-2.787, 5.951], loss: --, mean_absolute_error: --, mean_q: --
   39390/1500000: episode: 304, duration: 0.214s, episode steps: 175, steps per second: 817, episode reward: -977.481, mean reward: -5.586 [-100.000, 2.029], mean action: 1.954 [0.000, 3.000], mean observation: 0.458 [-1.461, 4.389], loss: --, mean_absolute_error: --, mean_q: --
   39466/1500000: episode: 305, duration: 0.093s, episode steps: 76, steps per second: 814, episode reward: -420.871, mean reward: -5.538 [-100.000, 0.808], mean action: 1.961 [0.000, 3.000], mean observation: 0.014 [-2.388, 1.330], loss: --, mean_absolute_error: --, mean_q: --
   39576/1500000: episode: 306, duration: 0.132s, episode steps: 110, steps per second: 834, episode reward: -458.727, mean reward: -4.170 [-100.000, 1.791], mean action: 1.945 [0.000, 3.000], mean observation: 0.093 [-1.813, 1.902], loss: --, mean_absolute_error: --, mean_q: --
   39720/1500000: episode: 307, duration: 0.171s, episode steps: 144, steps per second: 844, episode reward: -792.120, mean reward: -5.501 [-100.000, 2.302], mean action: 1.958 [0.000, 3.000], mean observation: 0.332 [-2.252, 3.747], loss: --, mean_absolute_error: --, mean_q: --
   39828/1500000: episode: 308, duration: 0.131s, episode steps: 108, steps per second: 823, episode reward: -567.452, mean reward: -5.254 [-100.000, 1.075], mean action: 1.917 [0.000, 3.000], mean observation: 0.380 [-0.634, 2.230], loss: --, mean_absolute_error: --, mean_q: --
   40046/1500000: episode: 309, duration: 0.298s, episode steps: 218, steps per second: 731, episode reward: -1239.891, mean reward: -5.688 [-100.000, 2.002], mean action: 1.954 [0.000, 3.000], mean observation: 0.627 [-3.813, 6.659], loss: --, mean_absolute_error: --, mean_q: --
   40208/1500000: episode: 310, duration: 0.215s, episode steps: 162, steps per second: 755, episode reward: -634.244, mean reward: -3.915 [-100.000, 3.609], mean action: 1.883 [0.000, 3.000], mean observation: 0.158 [-3.228, 1.852], loss: --, mean_absolute_error: --, mean_q: --
   40300/1500000: episode: 311, duration: 0.104s, episode steps: 92, steps per second: 884, episode reward: -308.281, mean reward: -3.351 [-100.000, 2.506], mean action: 2.022 [1.000, 3.000], mean observation: -0.009 [-1.467, 1.393], loss: --, mean_absolute_error: --, mean_q: --
   40380/1500000: episode: 312, duration: 0.086s, episode steps: 80, steps per second: 934, episode reward: -501.150, mean reward: -6.264 [-100.000, 0.036], mean action: 1.975 [0.000, 2.000], mean observation: 0.088 [-2.567, 1.641], loss: --, mean_absolute_error: --, mean_q: --
   40459/1500000: episode: 313, duration: 0.096s, episode steps: 79, steps per second: 826, episode reward: -347.660, mean reward: -4.401 [-100.000, 0.775], mean action: 1.975 [0.000, 3.000], mean observation: -0.025 [-2.075, 1.189], loss: --, mean_absolute_error: --, mean_q: --
   40635/1500000: episode: 314, duration: 0.219s, episode steps: 176, steps per second: 804, episode reward: -583.125, mean reward: -3.313 [-100.000, 3.431], mean action: 1.977 [0.000, 3.000], mean observation: 0.173 [-1.438, 2.942], loss: --, mean_absolute_error: --, mean_q: --
   40736/1500000: episode: 315, duration: 0.119s, episode steps: 101, steps per second: 848, episode reward: -481.366, mean reward: -4.766 [-100.000, 1.065], mean action: 1.960 [0.000, 3.000], mean observation: 0.114 [-1.858, 2.077], loss: --, mean_absolute_error: --, mean_q: --
   40888/1500000: episode: 316, duration: 0.189s, episode steps: 152, steps per second: 802, episode reward: -943.491, mean reward: -6.207 [-100.000, 1.755], mean action: 1.967 [0.000, 3.000], mean observation: 0.480 [-2.705, 4.920], loss: --, mean_absolute_error: --, mean_q: --
   40996/1500000: episode: 317, duration: 0.123s, episode steps: 108, steps per second: 878, episode reward: -406.271, mean reward: -3.762 [-100.000, 1.221], mean action: 1.991 [0.000, 3.000], mean observation: 0.064 [-1.550, 1.738], loss: --, mean_absolute_error: --, mean_q: --
   41100/1500000: episode: 318, duration: 0.124s, episode steps: 104, steps per second: 837, episode reward: -525.920, mean reward: -5.057 [-100.000, 1.274], mean action: 1.962 [0.000, 3.000], mean observation: 0.161 [-1.870, 2.372], loss: --, mean_absolute_error: --, mean_q: --
   41188/1500000: episode: 319, duration: 0.094s, episode steps: 88, steps per second: 931, episode reward: -445.405, mean reward: -5.061 [-100.000, 1.081], mean action: 1.966 [0.000, 3.000], mean observation: 0.477 [-0.331, 2.222], loss: --, mean_absolute_error: --, mean_q: --
   41275/1500000: episode: 320, duration: 0.100s, episode steps: 87, steps per second: 872, episode reward: -481.936, mean reward: -5.539 [-100.000, 1.032], mean action: 1.966 [0.000, 3.000], mean observation: 0.120 [-1.995, 2.049], loss: --, mean_absolute_error: --, mean_q: --
   41355/1500000: episode: 321, duration: 0.095s, episode steps: 80, steps per second: 843, episode reward: -519.537, mean reward: -6.494 [-100.000, -0.003], mean action: 1.988 [0.000, 3.000], mean observation: 0.479 [-0.558, 2.257], loss: --, mean_absolute_error: --, mean_q: --
   41441/1500000: episode: 322, duration: 0.095s, episode steps: 86, steps per second: 902, episode reward: -438.593, mean reward: -5.100 [-100.000, 1.994], mean action: 2.047 [0.000, 3.000], mean observation: 0.285 [-0.974, 2.119], loss: --, mean_absolute_error: --, mean_q: --
   41785/1500000: episode: 323, duration: 0.669s, episode steps: 344, steps per second: 514, episode reward: -1866.025, mean reward: -5.424 [-100.000, 2.827], mean action: 1.959 [0.000, 3.000], mean observation: 0.769 [-2.713, 12.106], loss: --, mean_absolute_error: --, mean_q: --
   41921/1500000: episode: 324, duration: 0.312s, episode steps: 136, steps per second: 436, episode reward: -505.406, mean reward: -3.716 [-100.000, 4.185], mean action: 1.904 [0.000, 3.000], mean observation: 0.098 [-2.207, 1.776], loss: --, mean_absolute_error: --, mean_q: --
   42040/1500000: episode: 325, duration: 0.138s, episode steps: 119, steps per second: 862, episode reward: -430.651, mean reward: -3.619 [-100.000, 2.480], mean action: 1.916 [0.000, 2.000], mean observation: 0.305 [-0.467, 1.729], loss: --, mean_absolute_error: --, mean_q: --
   42128/1500000: episode: 326, duration: 0.112s, episode steps: 88, steps per second: 787, episode reward: -488.718, mean reward: -5.554 [-100.000, 0.990], mean action: 1.955 [0.000, 3.000], mean observation: 0.110 [-2.321, 1.771], loss: --, mean_absolute_error: --, mean_q: --
   42281/1500000: episode: 327, duration: 0.184s, episode steps: 153, steps per second: 831, episode reward: -695.924, mean reward: -4.549 [-100.000, 1.901], mean action: 1.980 [0.000, 3.000], mean observation: 0.470 [-0.386, 3.538], loss: --, mean_absolute_error: --, mean_q: --
   42366/1500000: episode: 328, duration: 0.108s, episode steps: 85, steps per second: 785, episode reward: -568.624, mean reward: -6.690 [-100.000, 0.492], mean action: 1.976 [0.000, 3.000], mean observation: 0.368 [-1.016, 2.792], loss: --, mean_absolute_error: --, mean_q: --
   42483/1500000: episode: 329, duration: 0.144s, episode steps: 117, steps per second: 815, episode reward: -666.762, mean reward: -5.699 [-100.000, 1.272], mean action: 2.017 [0.000, 3.000], mean observation: 0.398 [-0.885, 2.597], loss: --, mean_absolute_error: --, mean_q: --
   42568/1500000: episode: 330, duration: 0.114s, episode steps: 85, steps per second: 747, episode reward: -480.406, mean reward: -5.652 [-100.000, 0.954], mean action: 1.941 [0.000, 3.000], mean observation: 0.077 [-2.329, 1.678], loss: --, mean_absolute_error: --, mean_q: --
   42654/1500000: episode: 331, duration: 0.099s, episode steps: 86, steps per second: 870, episode reward: -563.757, mean reward: -6.555 [-100.000, -0.866], mean action: 1.977 [0.000, 3.000], mean observation: 0.173 [-2.461, 2.237], loss: --, mean_absolute_error: --, mean_q: --
   42736/1500000: episode: 332, duration: 0.088s, episode steps: 82, steps per second: 937, episode reward: -461.630, mean reward: -5.630 [-100.000, 1.189], mean action: 2.341 [0.000, 3.000], mean observation: 0.092 [-2.471, 1.542], loss: --, mean_absolute_error: --, mean_q: --
   42817/1500000: episode: 333, duration: 0.109s, episode steps: 81, steps per second: 741, episode reward: -589.863, mean reward: -7.282 [-100.000, 2.007], mean action: 2.383 [1.000, 3.000], mean observation: 0.040 [-3.128, 1.890], loss: --, mean_absolute_error: --, mean_q: --
   42982/1500000: episode: 334, duration: 0.263s, episode steps: 165, steps per second: 628, episode reward: -983.091, mean reward: -5.958 [-100.000, 1.740], mean action: 1.939 [0.000, 3.000], mean observation: 0.531 [-0.889, 4.802], loss: --, mean_absolute_error: --, mean_q: --
   43082/1500000: episode: 335, duration: 0.111s, episode steps: 100, steps per second: 901, episode reward: -591.041, mean reward: -5.910 [-100.000, 0.486], mean action: 2.000 [0.000, 3.000], mean observation: 0.463 [-0.502, 2.605], loss: --, mean_absolute_error: --, mean_q: --
   43274/1500000: episode: 336, duration: 0.255s, episode steps: 192, steps per second: 752, episode reward: -1163.859, mean reward: -6.062 [-100.000, 1.823], mean action: 1.990 [0.000, 3.000], mean observation: 0.694 [-0.544, 6.627], loss: --, mean_absolute_error: --, mean_q: --
   43386/1500000: episode: 337, duration: 0.124s, episode steps: 112, steps per second: 903, episode reward: -591.055, mean reward: -5.277 [-100.000, 1.645], mean action: 1.938 [0.000, 3.000], mean observation: 0.464 [-0.418, 2.773], loss: --, mean_absolute_error: --, mean_q: --
   43499/1500000: episode: 338, duration: 0.131s, episode steps: 113, steps per second: 859, episode reward: -645.051, mean reward: -5.708 [-100.000, 4.059], mean action: 2.212 [0.000, 3.000], mean observation: 0.041 [-3.134, 1.957], loss: --, mean_absolute_error: --, mean_q: --
   43580/1500000: episode: 339, duration: 0.086s, episode steps: 81, steps per second: 938, episode reward: -375.879, mean reward: -4.640 [-100.000, 0.843], mean action: 1.926 [0.000, 3.000], mean observation: 0.354 [-0.358, 1.992], loss: --, mean_absolute_error: --, mean_q: --
   43655/1500000: episode: 340, duration: 0.101s, episode steps: 75, steps per second: 743, episode reward: -382.584, mean reward: -5.101 [-100.000, 1.835], mean action: 1.960 [0.000, 3.000], mean observation: -0.038 [-2.445, 1.037], loss: --, mean_absolute_error: --, mean_q: --
   43737/1500000: episode: 341, duration: 0.101s, episode steps: 82, steps per second: 808, episode reward: -476.281, mean reward: -5.808 [-100.000, 0.466], mean action: 1.915 [0.000, 3.000], mean observation: 0.091 [-2.295, 1.737], loss: --, mean_absolute_error: --, mean_q: --
   43955/1500000: episode: 342, duration: 0.291s, episode steps: 218, steps per second: 749, episode reward: -1127.322, mean reward: -5.171 [-100.000, 1.447], mean action: 1.950 [0.000, 3.000], mean observation: 0.549 [-3.503, 5.969], loss: --, mean_absolute_error: --, mean_q: --
   44039/1500000: episode: 343, duration: 0.097s, episode steps: 84, steps per second: 862, episode reward: -568.054, mean reward: -6.763 [-100.000, 0.043], mean action: 1.917 [0.000, 2.000], mean observation: 0.164 [-2.475, 2.130], loss: --, mean_absolute_error: --, mean_q: --
   44140/1500000: episode: 344, duration: 0.118s, episode steps: 101, steps per second: 856, episode reward: -528.612, mean reward: -5.234 [-100.000, 1.435], mean action: 1.931 [0.000, 3.000], mean observation: 0.157 [-1.965, 2.315], loss: --, mean_absolute_error: --, mean_q: --
   44215/1500000: episode: 345, duration: 0.095s, episode steps: 75, steps per second: 786, episode reward: -382.464, mean reward: -5.100 [-100.000, 1.212], mean action: 2.000 [0.000, 3.000], mean observation: 0.320 [-0.645, 2.155], loss: --, mean_absolute_error: --, mean_q: --
   44429/1500000: episode: 346, duration: 0.312s, episode steps: 214, steps per second: 685, episode reward: -961.196, mean reward: -4.492 [-100.000, 4.385], mean action: 1.953 [0.000, 3.000], mean observation: 0.382 [-2.800, 4.936], loss: --, mean_absolute_error: --, mean_q: --
   44642/1500000: episode: 347, duration: 0.287s, episode steps: 213, steps per second: 742, episode reward: -961.605, mean reward: -4.515 [-100.000, 1.869], mean action: 1.986 [0.000, 3.000], mean observation: 0.300 [-1.202, 4.235], loss: --, mean_absolute_error: --, mean_q: --
   44742/1500000: episode: 348, duration: 0.112s, episode steps: 100, steps per second: 892, episode reward: -655.524, mean reward: -6.555 [-100.000, 0.981], mean action: 1.970 [0.000, 3.000], mean observation: 0.507 [-0.641, 2.992], loss: --, mean_absolute_error: --, mean_q: --
   44834/1500000: episode: 349, duration: 0.105s, episode steps: 92, steps per second: 878, episode reward: -497.128, mean reward: -5.404 [-100.000, 1.330], mean action: 1.946 [0.000, 3.000], mean observation: 0.313 [-0.710, 2.477], loss: --, mean_absolute_error: --, mean_q: --
   44998/1500000: episode: 350, duration: 0.196s, episode steps: 164, steps per second: 835, episode reward: -888.702, mean reward: -5.419 [-100.000, 1.956], mean action: 1.915 [0.000, 3.000], mean observation: 0.458 [-2.012, 5.111], loss: --, mean_absolute_error: --, mean_q: --
   45120/1500000: episode: 351, duration: 0.152s, episode steps: 122, steps per second: 801, episode reward: -503.839, mean reward: -4.130 [-100.000, 2.786], mean action: 1.951 [0.000, 3.000], mean observation: 0.132 [-1.654, 2.274], loss: --, mean_absolute_error: --, mean_q: --
   45200/1500000: episode: 352, duration: 0.092s, episode steps: 80, steps per second: 866, episode reward: -525.047, mean reward: -6.563 [-100.000, 0.222], mean action: 2.000 [1.000, 3.000], mean observation: 0.439 [-0.651, 2.431], loss: --, mean_absolute_error: --, mean_q: --
   45314/1500000: episode: 353, duration: 0.139s, episode steps: 114, steps per second: 818, episode reward: -461.206, mean reward: -4.046 [-100.000, 4.283], mean action: 1.877 [0.000, 3.000], mean observation: 0.056 [-2.277, 1.458], loss: --, mean_absolute_error: --, mean_q: --
   45410/1500000: episode: 354, duration: 0.108s, episode steps: 96, steps per second: 889, episode reward: -441.851, mean reward: -4.603 [-100.000, 1.214], mean action: 1.969 [0.000, 3.000], mean observation: 0.390 [-0.352, 1.906], loss: --, mean_absolute_error: --, mean_q: --
   45494/1500000: episode: 355, duration: 0.114s, episode steps: 84, steps per second: 735, episode reward: -362.196, mean reward: -4.312 [-100.000, 1.701], mean action: 2.250 [0.000, 3.000], mean observation: 0.219 [-1.160, 1.766], loss: --, mean_absolute_error: --, mean_q: --
   45612/1500000: episode: 356, duration: 0.132s, episode steps: 118, steps per second: 891, episode reward: -584.122, mean reward: -4.950 [-100.000, 1.681], mean action: 1.941 [0.000, 3.000], mean observation: 0.445 [-0.452, 2.709], loss: --, mean_absolute_error: --, mean_q: --
   45713/1500000: episode: 357, duration: 0.119s, episode steps: 101, steps per second: 847, episode reward: -540.879, mean reward: -5.355 [-100.000, 0.502], mean action: 1.901 [0.000, 3.000], mean observation: 0.210 [-1.698, 2.770], loss: --, mean_absolute_error: --, mean_q: --
   45834/1500000: episode: 358, duration: 0.143s, episode steps: 121, steps per second: 849, episode reward: -562.967, mean reward: -4.653 [-100.000, 1.398], mean action: 1.967 [0.000, 3.000], mean observation: 0.436 [-0.256, 2.672], loss: --, mean_absolute_error: --, mean_q: --
   45919/1500000: episode: 359, duration: 0.170s, episode steps: 85, steps per second: 501, episode reward: -509.938, mean reward: -5.999 [-100.000, 0.111], mean action: 1.976 [0.000, 3.000], mean observation: 0.366 [-0.534, 2.532], loss: --, mean_absolute_error: --, mean_q: --
   46002/1500000: episode: 360, duration: 0.194s, episode steps: 83, steps per second: 428, episode reward: -408.565, mean reward: -4.922 [-100.000, 0.078], mean action: 1.892 [0.000, 3.000], mean observation: 0.378 [-0.359, 2.083], loss: --, mean_absolute_error: --, mean_q: --
   46112/1500000: episode: 361, duration: 0.181s, episode steps: 110, steps per second: 608, episode reward: -516.623, mean reward: -4.697 [-100.000, 1.484], mean action: 1.955 [0.000, 3.000], mean observation: 0.428 [-0.490, 2.434], loss: --, mean_absolute_error: --, mean_q: --
   46189/1500000: episode: 362, duration: 0.085s, episode steps: 77, steps per second: 904, episode reward: -448.965, mean reward: -5.831 [-100.000, 0.479], mean action: 1.974 [0.000, 2.000], mean observation: 0.053 [-2.281, 1.556], loss: --, mean_absolute_error: --, mean_q: --
   46394/1500000: episode: 363, duration: 0.273s, episode steps: 205, steps per second: 750, episode reward: -1242.090, mean reward: -6.059 [-100.000, 1.596], mean action: 1.971 [0.000, 3.000], mean observation: 0.584 [-1.797, 7.243], loss: --, mean_absolute_error: --, mean_q: --
   46627/1500000: episode: 364, duration: 0.327s, episode steps: 233, steps per second: 712, episode reward: -1230.947, mean reward: -5.283 [-100.000, 1.338], mean action: 1.948 [0.000, 3.000], mean observation: 0.570 [-4.814, 5.412], loss: --, mean_absolute_error: --, mean_q: --
   46740/1500000: episode: 365, duration: 0.129s, episode steps: 113, steps per second: 874, episode reward: -542.504, mean reward: -4.801 [-100.000, 2.291], mean action: 1.982 [0.000, 3.000], mean observation: 0.331 [-0.715, 2.252], loss: --, mean_absolute_error: --, mean_q: --
   46839/1500000: episode: 366, duration: 0.117s, episode steps: 99, steps per second: 844, episode reward: -625.282, mean reward: -6.316 [-100.000, 0.335], mean action: 1.970 [0.000, 3.000], mean observation: 0.246 [-2.399, 2.688], loss: --, mean_absolute_error: --, mean_q: --
   46968/1500000: episode: 367, duration: 0.147s, episode steps: 129, steps per second: 875, episode reward: -381.066, mean reward: -2.954 [-100.000, 3.489], mean action: 1.930 [0.000, 3.000], mean observation: 0.314 [-0.508, 1.783], loss: --, mean_absolute_error: --, mean_q: --
   47165/1500000: episode: 368, duration: 0.276s, episode steps: 197, steps per second: 715, episode reward: -1115.442, mean reward: -5.662 [-100.000, 1.802], mean action: 1.944 [0.000, 3.000], mean observation: 0.498 [-3.166, 5.623], loss: --, mean_absolute_error: --, mean_q: --
   47326/1500000: episode: 369, duration: 0.196s, episode steps: 161, steps per second: 823, episode reward: -795.797, mean reward: -4.943 [-100.000, 2.709], mean action: 1.957 [0.000, 3.000], mean observation: 0.316 [-2.277, 3.796], loss: --, mean_absolute_error: --, mean_q: --
   47404/1500000: episode: 370, duration: 0.107s, episode steps: 78, steps per second: 732, episode reward: -368.277, mean reward: -4.722 [-100.000, 1.222], mean action: 1.923 [0.000, 3.000], mean observation: -0.050 [-2.341, 0.929], loss: --, mean_absolute_error: --, mean_q: --
   47482/1500000: episode: 371, duration: 0.090s, episode steps: 78, steps per second: 866, episode reward: -522.810, mean reward: -6.703 [-100.000, 0.730], mean action: 1.910 [0.000, 3.000], mean observation: 0.423 [-0.816, 2.558], loss: --, mean_absolute_error: --, mean_q: --
   47621/1500000: episode: 372, duration: 0.172s, episode steps: 139, steps per second: 810, episode reward: -765.283, mean reward: -5.506 [-100.000, 2.261], mean action: 2.000 [1.000, 3.000], mean observation: 0.348 [-2.276, 3.840], loss: --, mean_absolute_error: --, mean_q: --
   47749/1500000: episode: 373, duration: 0.150s, episode steps: 128, steps per second: 855, episode reward: -724.912, mean reward: -5.663 [-100.000, 1.088], mean action: 1.945 [0.000, 3.000], mean observation: 0.469 [-0.514, 3.306], loss: --, mean_absolute_error: --, mean_q: --
   47856/1500000: episode: 374, duration: 0.127s, episode steps: 107, steps per second: 845, episode reward: -643.995, mean reward: -6.019 [-100.000, 3.019], mean action: 2.290 [0.000, 3.000], mean observation: 0.067 [-3.196, 1.885], loss: --, mean_absolute_error: --, mean_q: --
   47987/1500000: episode: 375, duration: 0.150s, episode steps: 131, steps per second: 876, episode reward: -715.290, mean reward: -5.460 [-100.000, 1.367], mean action: 1.939 [0.000, 3.000], mean observation: 0.272 [-2.343, 3.056], loss: --, mean_absolute_error: --, mean_q: --
   48171/1500000: episode: 376, duration: 0.250s, episode steps: 184, steps per second: 737, episode reward: -752.273, mean reward: -4.088 [-100.000, 3.799], mean action: 1.946 [0.000, 3.000], mean observation: 0.324 [-0.791, 3.167], loss: --, mean_absolute_error: --, mean_q: --
   48273/1500000: episode: 377, duration: 0.118s, episode steps: 102, steps per second: 862, episode reward: -430.531, mean reward: -4.221 [-100.000, 2.810], mean action: 1.971 [0.000, 3.000], mean observation: 0.023 [-2.301, 1.249], loss: --, mean_absolute_error: --, mean_q: --
   48357/1500000: episode: 378, duration: 0.092s, episode steps: 84, steps per second: 914, episode reward: -567.472, mean reward: -6.756 [-100.000, 0.574], mean action: 1.988 [0.000, 3.000], mean observation: 0.198 [-2.470, 2.277], loss: --, mean_absolute_error: --, mean_q: --
   48464/1500000: episode: 379, duration: 0.128s, episode steps: 107, steps per second: 839, episode reward: -545.090, mean reward: -5.094 [-100.000, 1.067], mean action: 1.935 [0.000, 3.000], mean observation: 0.189 [-1.782, 2.535], loss: --, mean_absolute_error: --, mean_q: --
   48615/1500000: episode: 380, duration: 0.174s, episode steps: 151, steps per second: 870, episode reward: -822.419, mean reward: -5.446 [-100.000, 3.262], mean action: 1.914 [0.000, 2.000], mean observation: 0.315 [-3.142, 3.273], loss: --, mean_absolute_error: --, mean_q: --
   48694/1500000: episode: 381, duration: 0.095s, episode steps: 79, steps per second: 829, episode reward: -452.442, mean reward: -5.727 [-100.000, 0.114], mean action: 2.000 [0.000, 3.000], mean observation: 0.388 [-0.558, 2.235], loss: --, mean_absolute_error: --, mean_q: --
   48837/1500000: episode: 382, duration: 0.176s, episode steps: 143, steps per second: 813, episode reward: -724.196, mean reward: -5.064 [-100.000, 0.852], mean action: 1.930 [0.000, 3.000], mean observation: 0.319 [-1.104, 4.073], loss: --, mean_absolute_error: --, mean_q: --
   49002/1500000: episode: 383, duration: 0.208s, episode steps: 165, steps per second: 794, episode reward: -610.672, mean reward: -3.701 [-100.000, 2.601], mean action: 1.952 [0.000, 3.000], mean observation: 0.184 [-1.007, 3.298], loss: --, mean_absolute_error: --, mean_q: --
   49083/1500000: episode: 384, duration: 0.107s, episode steps: 81, steps per second: 755, episode reward: -474.743, mean reward: -5.861 [-100.000, 0.755], mean action: 1.914 [0.000, 2.000], mean observation: 0.392 [-0.540, 2.266], loss: --, mean_absolute_error: --, mean_q: --
   49214/1500000: episode: 385, duration: 0.181s, episode steps: 131, steps per second: 725, episode reward: -676.794, mean reward: -5.166 [-100.000, 1.332], mean action: 1.977 [0.000, 3.000], mean observation: 0.314 [-1.003, 4.004], loss: --, mean_absolute_error: --, mean_q: --
   49314/1500000: episode: 386, duration: 0.114s, episode steps: 100, steps per second: 877, episode reward: -653.613, mean reward: -6.536 [-100.000, 1.597], mean action: 2.000 [0.000, 3.000], mean observation: 0.258 [-2.357, 2.912], loss: --, mean_absolute_error: --, mean_q: --
   49412/1500000: episode: 387, duration: 0.114s, episode steps: 98, steps per second: 858, episode reward: -538.558, mean reward: -5.495 [-100.000, 0.656], mean action: 1.939 [0.000, 3.000], mean observation: 0.124 [-2.262, 1.936], loss: --, mean_absolute_error: --, mean_q: --
   49559/1500000: episode: 388, duration: 0.174s, episode steps: 147, steps per second: 847, episode reward: -650.021, mean reward: -4.422 [-100.000, 2.197], mean action: 1.986 [0.000, 3.000], mean observation: 0.359 [-0.525, 2.682], loss: --, mean_absolute_error: --, mean_q: --
   49638/1500000: episode: 389, duration: 0.097s, episode steps: 79, steps per second: 817, episode reward: -486.209, mean reward: -6.155 [-100.000, -0.209], mean action: 1.924 [0.000, 3.000], mean observation: 0.081 [-2.416, 1.583], loss: --, mean_absolute_error: --, mean_q: --
   49768/1500000: episode: 390, duration: 0.211s, episode steps: 130, steps per second: 616, episode reward: -594.307, mean reward: -4.572 [-100.000, 3.000], mean action: 1.969 [0.000, 3.000], mean observation: 0.407 [-0.250, 2.730], loss: --, mean_absolute_error: --, mean_q: --
   50025/1500000: episode: 391, duration: 2.362s, episode steps: 257, steps per second: 109, episode reward: -1126.453, mean reward: -4.383 [-100.000, 4.486], mean action: 1.961 [0.000, 3.000], mean observation: 0.554 [-0.640, 6.667], loss: 59.425007, mean_absolute_error: 1.399732, mean_q: 0.188995
   50135/1500000: episode: 392, duration: 1.241s, episode steps: 110, steps per second: 89, episode reward: -370.414, mean reward: -3.367 [-100.000, 4.639], mean action: 1.045 [0.000, 3.000], mean observation: 0.218 [-1.535, 2.515], loss: 58.810726, mean_absolute_error: 1.384294, mean_q: 0.061553
   50209/1500000: episode: 393, duration: 0.792s, episode steps: 74, steps per second: 93, episode reward: -346.063, mean reward: -4.677 [-100.000, 1.445], mean action: 0.581 [0.000, 3.000], mean observation: 0.134 [-1.892, 1.979], loss: 55.017918, mean_absolute_error: 1.287234, mean_q: 0.045957
   50291/1500000: episode: 394, duration: 0.860s, episode steps: 82, steps per second: 95, episode reward: -377.893, mean reward: -4.608 [-100.000, 0.759], mean action: 0.549 [0.000, 3.000], mean observation: 0.164 [-1.849, 2.115], loss: 47.129009, mean_absolute_error: 1.213315, mean_q: 0.054725
   50372/1500000: episode: 395, duration: 1.067s, episode steps: 81, steps per second: 76, episode reward: -378.449, mean reward: -4.672 [-100.000, 2.127], mean action: 0.543 [0.000, 3.000], mean observation: 0.120 [-2.002, 2.080], loss: 34.437199, mean_absolute_error: 1.092992, mean_q: 0.076752
   50456/1500000: episode: 396, duration: 0.889s, episode steps: 84, steps per second: 95, episode reward: -325.062, mean reward: -3.870 [-100.000, 4.361], mean action: 0.512 [0.000, 3.000], mean observation: 0.105 [-1.868, 1.927], loss: 55.319504, mean_absolute_error: 1.151211, mean_q: 0.112945
   50508/1500000: episode: 397, duration: 0.541s, episode steps: 52, steps per second: 96, episode reward: -303.698, mean reward: -5.840 [-100.000, 0.532], mean action: 0.981 [0.000, 3.000], mean observation: 0.119 [-1.967, 1.869], loss: 26.130564, mean_absolute_error: 0.954251, mean_q: 0.131046
   50578/1500000: episode: 398, duration: 0.745s, episode steps: 70, steps per second: 94, episode reward: -263.973, mean reward: -3.771 [-100.000, 3.839], mean action: 0.714 [0.000, 3.000], mean observation: 0.078 [-1.674, 6.733], loss: 57.602402, mean_absolute_error: 1.043853, mean_q: 0.146282
   50668/1500000: episode: 399, duration: 0.939s, episode steps: 90, steps per second: 96, episode reward: -288.423, mean reward: -3.205 [-100.000, 5.141], mean action: 0.178 [0.000, 3.000], mean observation: -0.050 [-1.865, 2.749], loss: 43.088272, mean_absolute_error: 0.963634, mean_q: 0.187245
   50757/1500000: episode: 400, duration: 0.966s, episode steps: 89, steps per second: 92, episode reward: -155.241, mean reward: -1.744 [-100.000, 16.370], mean action: 0.079 [0.000, 3.000], mean observation: -0.018 [-1.895, 1.002], loss: 41.825089, mean_absolute_error: 0.921279, mean_q: 0.218110
   50845/1500000: episode: 401, duration: 1.046s, episode steps: 88, steps per second: 84, episode reward: -148.152, mean reward: -1.684 [-100.000, 16.839], mean action: 0.352 [0.000, 3.000], mean observation: -0.042 [-1.737, 2.543], loss: 34.256027, mean_absolute_error: 0.858243, mean_q: 0.235484
   50934/1500000: episode: 402, duration: 1.074s, episode steps: 89, steps per second: 83, episode reward: -174.935, mean reward: -1.966 [-100.000, 7.027], mean action: 0.213 [0.000, 3.000], mean observation: -0.058 [-1.822, 1.000], loss: 41.571121, mean_absolute_error: 0.909863, mean_q: 0.264688
   51014/1500000: episode: 403, duration: 0.862s, episode steps: 80, steps per second: 93, episode reward: -172.139, mean reward: -2.152 [-100.000, 18.128], mean action: 0.150 [0.000, 3.000], mean observation: -0.071 [-1.833, 1.000], loss: 35.906860, mean_absolute_error: 0.913892, mean_q: 0.278821
   51082/1500000: episode: 404, duration: 0.720s, episode steps: 68, steps per second: 94, episode reward: -176.140, mean reward: -2.590 [-100.000, 7.805], mean action: 0.176 [0.000, 3.000], mean observation: -0.070 [-6.076, 1.000], loss: 47.066898, mean_absolute_error: 0.981953, mean_q: 0.278446
   51153/1500000: episode: 405, duration: 0.725s, episode steps: 71, steps per second: 98, episode reward: -200.629, mean reward: -2.826 [-100.000, 0.484], mean action: 0.085 [0.000, 2.000], mean observation: -0.010 [-5.796, 1.000], loss: 34.610580, mean_absolute_error: 0.921929, mean_q: 0.260157
   51215/1500000: episode: 406, duration: 0.673s, episode steps: 62, steps per second: 92, episode reward: -191.120, mean reward: -3.083 [-100.000, 6.234], mean action: 0.048 [0.000, 2.000], mean observation: -0.104 [-5.700, 1.000], loss: 31.439386, mean_absolute_error: 0.921651, mean_q: 0.253040
   51299/1500000: episode: 407, duration: 0.896s, episode steps: 84, steps per second: 94, episode reward: -217.872, mean reward: -2.594 [-100.000, 32.542], mean action: 0.167 [0.000, 3.000], mean observation: -0.075 [-2.878, 1.000], loss: 28.773556, mean_absolute_error: 0.919170, mean_q: 0.245574
   51365/1500000: episode: 408, duration: 0.898s, episode steps: 66, steps per second: 74, episode reward: -194.547, mean reward: -2.948 [-100.000, 6.305], mean action: 0.258 [0.000, 3.000], mean observation: -0.015 [-1.775, 1.209], loss: 38.655132, mean_absolute_error: 0.987014, mean_q: 0.244327
   51442/1500000: episode: 409, duration: 0.832s, episode steps: 77, steps per second: 93, episode reward: -220.895, mean reward: -2.869 [-100.000, 13.668], mean action: 0.169 [0.000, 3.000], mean observation: -0.004 [-1.797, 1.020], loss: 47.076729, mean_absolute_error: 1.023101, mean_q: 0.229736
   51516/1500000: episode: 410, duration: 0.756s, episode steps: 74, steps per second: 98, episode reward: -194.106, mean reward: -2.623 [-100.000, 6.961], mean action: 0.068 [0.000, 3.000], mean observation: -0.078 [-6.284, 1.000], loss: 40.176971, mean_absolute_error: 1.000603, mean_q: 0.210983
   51575/1500000: episode: 411, duration: 0.619s, episode steps: 59, steps per second: 95, episode reward: -162.710, mean reward: -2.758 [-100.000, 6.945], mean action: 0.102 [0.000, 2.000], mean observation: -0.055 [-1.795, 1.000], loss: 35.562431, mean_absolute_error: 0.977613, mean_q: 0.193169
   51630/1500000: episode: 412, duration: 0.576s, episode steps: 55, steps per second: 96, episode reward: -133.998, mean reward: -2.436 [-100.000, 8.832], mean action: 0.236 [0.000, 3.000], mean observation: -0.084 [-6.331, 1.000], loss: 28.321016, mean_absolute_error: 0.953335, mean_q: 0.186112
   51687/1500000: episode: 413, duration: 0.658s, episode steps: 57, steps per second: 87, episode reward: -171.310, mean reward: -3.005 [-100.000, 7.081], mean action: 0.228 [0.000, 3.000], mean observation: -0.037 [-1.790, 5.954], loss: 33.757858, mean_absolute_error: 0.952450, mean_q: 0.165258
   51765/1500000: episode: 414, duration: 0.835s, episode steps: 78, steps per second: 93, episode reward: -199.984, mean reward: -2.564 [-100.000, 8.184], mean action: 0.205 [0.000, 3.000], mean observation: 0.052 [-1.713, 5.888], loss: 45.669334, mean_absolute_error: 1.041738, mean_q: 0.169907
   51813/1500000: episode: 415, duration: 0.508s, episode steps: 48, steps per second: 95, episode reward: -180.660, mean reward: -3.764 [-100.000, 6.397], mean action: 0.146 [0.000, 3.000], mean observation: 0.016 [-1.803, 4.262], loss: 47.940701, mean_absolute_error: 1.072779, mean_q: 0.153897
   51871/1500000: episode: 416, duration: 0.859s, episode steps: 58, steps per second: 67, episode reward: -212.871, mean reward: -3.670 [-100.000, 5.481], mean action: 0.103 [0.000, 3.000], mean observation: -0.001 [-1.882, 5.797], loss: 47.414688, mean_absolute_error: 1.052487, mean_q: 0.126655
   51939/1500000: episode: 417, duration: 0.745s, episode steps: 68, steps per second: 91, episode reward: -199.657, mean reward: -2.936 [-100.000, 6.764], mean action: 0.103 [0.000, 3.000], mean observation: -0.067 [-7.507, 1.000], loss: 37.499302, mean_absolute_error: 0.996533, mean_q: 0.126043
   51998/1500000: episode: 418, duration: 0.644s, episode steps: 59, steps per second: 92, episode reward: -178.148, mean reward: -3.019 [-100.000, 40.793], mean action: 0.254 [0.000, 3.000], mean observation: -0.137 [-3.107, 1.000], loss: 51.707684, mean_absolute_error: 1.091683, mean_q: 0.128763
   52071/1500000: episode: 419, duration: 0.807s, episode steps: 73, steps per second: 90, episode reward: -167.024, mean reward: -2.288 [-100.000, 12.230], mean action: 0.137 [0.000, 3.000], mean observation: 0.008 [-1.783, 1.000], loss: 44.917088, mean_absolute_error: 1.072385, mean_q: 0.105864
   52130/1500000: episode: 420, duration: 0.647s, episode steps: 59, steps per second: 91, episode reward: -156.048, mean reward: -2.645 [-100.000, 11.440], mean action: 0.220 [0.000, 3.000], mean observation: -0.111 [-4.725, 1.000], loss: 38.811367, mean_absolute_error: 1.035095, mean_q: 0.068660
   52195/1500000: episode: 421, duration: 0.721s, episode steps: 65, steps per second: 90, episode reward: -143.152, mean reward: -2.202 [-100.000, 16.943], mean action: 0.138 [0.000, 3.000], mean observation: -0.041 [-1.806, 1.000], loss: 35.310108, mean_absolute_error: 1.027954, mean_q: 0.030329
   52285/1500000: episode: 422, duration: 1.143s, episode steps: 90, steps per second: 79, episode reward: -199.628, mean reward: -2.218 [-100.000, 13.988], mean action: 0.489 [0.000, 3.000], mean observation: 0.123 [-2.793, 1.000], loss: 57.570778, mean_absolute_error: 1.134499, mean_q: -0.017942
   52355/1500000: episode: 423, duration: 0.943s, episode steps: 70, steps per second: 74, episode reward: -162.587, mean reward: -2.323 [-100.000, 17.743], mean action: 0.100 [0.000, 3.000], mean observation: -0.042 [-1.781, 1.000], loss: 45.071728, mean_absolute_error: 1.132472, mean_q: -0.040711
   52431/1500000: episode: 424, duration: 0.777s, episode steps: 76, steps per second: 98, episode reward: -184.288, mean reward: -2.425 [-100.000, 7.170], mean action: 0.211 [0.000, 3.000], mean observation: -0.029 [-1.762, 1.000], loss: 42.276539, mean_absolute_error: 1.100211, mean_q: -0.081754
   52510/1500000: episode: 425, duration: 0.854s, episode steps: 79, steps per second: 93, episode reward: -187.932, mean reward: -2.379 [-100.000, 6.687], mean action: 0.076 [0.000, 2.000], mean observation: -0.039 [-1.787, 1.000], loss: 33.889656, mean_absolute_error: 1.097290, mean_q: -0.128078
   52618/1500000: episode: 426, duration: 1.434s, episode steps: 108, steps per second: 75, episode reward: -270.150, mean reward: -2.501 [-100.000, 89.838], mean action: 0.296 [0.000, 3.000], mean observation: 0.157 [-1.751, 2.716], loss: 35.709919, mean_absolute_error: 1.083542, mean_q: -0.151795
   52690/1500000: episode: 427, duration: 0.999s, episode steps: 72, steps per second: 72, episode reward: -213.732, mean reward: -2.969 [-100.000, 6.054], mean action: 0.139 [0.000, 3.000], mean observation: -0.095 [-1.819, 1.000], loss: 33.525440, mean_absolute_error: 1.128702, mean_q: -0.184776
   52745/1500000: episode: 428, duration: 0.605s, episode steps: 55, steps per second: 91, episode reward: -133.302, mean reward: -2.424 [-100.000, 17.076], mean action: 0.200 [0.000, 3.000], mean observation: -0.088 [-1.832, 1.000], loss: 42.997570, mean_absolute_error: 1.173678, mean_q: -0.193788
   52831/1500000: episode: 429, duration: 1.126s, episode steps: 86, steps per second: 76, episode reward: -145.589, mean reward: -1.693 [-100.000, 14.521], mean action: 0.209 [0.000, 3.000], mean observation: -0.002 [-1.746, 1.000], loss: 33.041355, mean_absolute_error: 1.132845, mean_q: -0.193519
   52916/1500000: episode: 430, duration: 0.885s, episode steps: 85, steps per second: 96, episode reward: -265.253, mean reward: -3.121 [-100.000, 32.626], mean action: 0.482 [0.000, 3.000], mean observation: 0.153 [-1.759, 5.556], loss: 25.978176, mean_absolute_error: 1.143120, mean_q: -0.247735
   52974/1500000: episode: 431, duration: 0.611s, episode steps: 58, steps per second: 95, episode reward: -147.733, mean reward: -2.547 [-100.000, 5.057], mean action: 0.862 [0.000, 3.000], mean observation: -0.147 [-1.809, 5.091], loss: 38.837482, mean_absolute_error: 1.198136, mean_q: -0.255588
   53032/1500000: episode: 432, duration: 0.650s, episode steps: 58, steps per second: 89, episode reward: -168.400, mean reward: -2.903 [-100.000, 7.333], mean action: 1.000 [0.000, 3.000], mean observation: -0.069 [-6.186, 1.000], loss: 35.985596, mean_absolute_error: 1.217317, mean_q: -0.289630
   53112/1500000: episode: 433, duration: 0.866s, episode steps: 80, steps per second: 92, episode reward: -176.784, mean reward: -2.210 [-100.000, 48.151], mean action: 0.912 [0.000, 3.000], mean observation: 0.048 [-1.995, 1.000], loss: 28.805944, mean_absolute_error: 1.167552, mean_q: -0.303420
   53174/1500000: episode: 434, duration: 0.754s, episode steps: 62, steps per second: 82, episode reward: -177.354, mean reward: -2.861 [-100.000, 8.483], mean action: 1.113 [0.000, 3.000], mean observation: -0.044 [-1.823, 1.000], loss: 47.927921, mean_absolute_error: 1.285178, mean_q: -0.350415
   53261/1500000: episode: 435, duration: 1.246s, episode steps: 87, steps per second: 70, episode reward: -381.929, mean reward: -4.390 [-100.000, 117.396], mean action: 1.264 [0.000, 3.000], mean observation: 0.221 [-1.733, 3.544], loss: 25.852648, mean_absolute_error: 1.215107, mean_q: -0.398949
   53320/1500000: episode: 436, duration: 0.718s, episode steps: 59, steps per second: 82, episode reward: -147.937, mean reward: -2.507 [-100.000, 10.629], mean action: 1.017 [0.000, 3.000], mean observation: -0.147 [-6.254, 1.000], loss: 37.477703, mean_absolute_error: 1.301029, mean_q: -0.437291
   53400/1500000: episode: 437, duration: 0.858s, episode steps: 80, steps per second: 93, episode reward: -294.068, mean reward: -3.676 [-100.000, 116.580], mean action: 1.238 [0.000, 3.000], mean observation: 0.236 [-1.808, 2.968], loss: 32.771263, mean_absolute_error: 1.301138, mean_q: -0.464342
   53481/1500000: episode: 438, duration: 0.977s, episode steps: 81, steps per second: 83, episode reward: -186.242, mean reward: -2.299 [-100.000, 5.851], mean action: 1.420 [0.000, 3.000], mean observation: -0.052 [-1.808, 6.120], loss: 27.558084, mean_absolute_error: 1.288856, mean_q: -0.505482
   53568/1500000: episode: 439, duration: 1.017s, episode steps: 87, steps per second: 86, episode reward: -269.168, mean reward: -3.094 [-100.000, 7.442], mean action: 1.506 [0.000, 3.000], mean observation: 0.153 [-1.757, 4.049], loss: 54.392944, mean_absolute_error: 1.474198, mean_q: -0.534822
   53654/1500000: episode: 440, duration: 1.092s, episode steps: 86, steps per second: 79, episode reward: -140.685, mean reward: -1.636 [-100.000, 49.531], mean action: 1.256 [0.000, 3.000], mean observation: -0.157 [-2.163, 1.839], loss: 30.763067, mean_absolute_error: 1.349427, mean_q: -0.540115
   53734/1500000: episode: 441, duration: 1.294s, episode steps: 80, steps per second: 62, episode reward: -252.568, mean reward: -3.157 [-100.000, 26.597], mean action: 1.488 [0.000, 3.000], mean observation: 0.163 [-1.749, 4.338], loss: 34.488384, mean_absolute_error: 1.404383, mean_q: -0.577183
   53811/1500000: episode: 442, duration: 1.086s, episode steps: 77, steps per second: 71, episode reward: -148.860, mean reward: -1.933 [-100.000, 29.601], mean action: 1.506 [0.000, 3.000], mean observation: -0.109 [-1.804, 4.353], loss: 44.508781, mean_absolute_error: 1.473177, mean_q: -0.614610
   53886/1500000: episode: 443, duration: 0.957s, episode steps: 75, steps per second: 78, episode reward: -205.303, mean reward: -2.737 [-100.000, 6.133], mean action: 1.493 [0.000, 3.000], mean observation: -0.080 [-1.810, 5.929], loss: 20.990295, mean_absolute_error: 1.371581, mean_q: -0.690888
   53986/1500000: episode: 444, duration: 1.573s, episode steps: 100, steps per second: 64, episode reward: -257.297, mean reward: -2.573 [-100.000, 7.341], mean action: 1.190 [0.000, 3.000], mean observation: -0.150 [-5.531, 1.008], loss: 40.147648, mean_absolute_error: 1.472867, mean_q: -0.714015
   54067/1500000: episode: 445, duration: 0.988s, episode steps: 81, steps per second: 82, episode reward: -214.010, mean reward: -2.642 [-100.000, 28.605], mean action: 1.519 [0.000, 3.000], mean observation: -0.145 [-5.484, 1.000], loss: 36.836334, mean_absolute_error: 1.531126, mean_q: -0.793274
   54154/1500000: episode: 446, duration: 1.360s, episode steps: 87, steps per second: 64, episode reward: -80.585, mean reward: -0.926 [-100.000, 89.100], mean action: 1.218 [0.000, 3.000], mean observation: 0.157 [-1.776, 1.681], loss: 40.159142, mean_absolute_error: 1.605796, mean_q: -0.838053
   54240/1500000: episode: 447, duration: 0.935s, episode steps: 86, steps per second: 92, episode reward: -353.967, mean reward: -4.116 [-100.000, 109.937], mean action: 1.709 [0.000, 3.000], mean observation: 0.258 [-1.721, 3.260], loss: 21.658577, mean_absolute_error: 1.500543, mean_q: -0.856593
   54307/1500000: episode: 448, duration: 1.060s, episode steps: 67, steps per second: 63, episode reward: -261.968, mean reward: -3.910 [-100.000, 23.512], mean action: 1.866 [0.000, 3.000], mean observation: -0.232 [-4.545, 1.000], loss: 39.287529, mean_absolute_error: 1.647272, mean_q: -0.911168
   54384/1500000: episode: 449, duration: 1.030s, episode steps: 77, steps per second: 75, episode reward: -215.342, mean reward: -2.797 [-100.000, 6.516], mean action: 1.714 [0.000, 3.000], mean observation: -0.119 [-1.698, 5.443], loss: 25.629061, mean_absolute_error: 1.578270, mean_q: -0.939916
   54443/1500000: episode: 450, duration: 0.711s, episode steps: 59, steps per second: 83, episode reward: -253.416, mean reward: -4.295 [-100.000, 6.621], mean action: 2.051 [0.000, 3.000], mean observation: -0.268 [-5.721, 1.000], loss: 42.816666, mean_absolute_error: 1.694747, mean_q: -0.956024
   54509/1500000: episode: 451, duration: 0.967s, episode steps: 66, steps per second: 68, episode reward: -51.687, mean reward: -0.783 [-100.000, 104.187], mean action: 1.742 [0.000, 3.000], mean observation: 0.095 [-1.770, 1.994], loss: 35.338253, mean_absolute_error: 1.715370, mean_q: -0.978209
   54583/1500000: episode: 452, duration: 1.312s, episode steps: 74, steps per second: 56, episode reward: -295.896, mean reward: -3.999 [-100.000, 9.795], mean action: 1.541 [0.000, 3.000], mean observation: 0.195 [-1.856, 4.087], loss: 30.940647, mean_absolute_error: 1.693820, mean_q: -1.029344
   54648/1500000: episode: 453, duration: 0.713s, episode steps: 65, steps per second: 91, episode reward: -194.524, mean reward: -2.993 [-100.000, 11.165], mean action: 2.138 [0.000, 3.000], mean observation: -0.132 [-1.687, 3.740], loss: 20.600262, mean_absolute_error: 1.662820, mean_q: -1.075880
   54708/1500000: episode: 454, duration: 0.800s, episode steps: 60, steps per second: 75, episode reward: -221.425, mean reward: -3.690 [-100.000, 36.805], mean action: 2.083 [0.000, 3.000], mean observation: -0.275 [-6.309, 1.000], loss: 31.313192, mean_absolute_error: 1.736082, mean_q: -1.085643
   54774/1500000: episode: 455, duration: 0.813s, episode steps: 66, steps per second: 81, episode reward: -183.380, mean reward: -2.778 [-100.000, 7.966], mean action: 2.000 [0.000, 3.000], mean observation: 0.014 [-5.968, 1.000], loss: 32.960228, mean_absolute_error: 1.790267, mean_q: -1.144277
   54837/1500000: episode: 456, duration: 0.703s, episode steps: 63, steps per second: 90, episode reward: -247.227, mean reward: -3.924 [-100.000, 7.370], mean action: 1.873 [1.000, 3.000], mean observation: 0.176 [-3.402, 1.072], loss: 30.737871, mean_absolute_error: 1.796231, mean_q: -1.147334
   54903/1500000: episode: 457, duration: 0.894s, episode steps: 66, steps per second: 74, episode reward: -226.840, mean reward: -3.437 [-100.000, 6.628], mean action: 2.136 [0.000, 3.000], mean observation: -0.120 [-1.792, 3.083], loss: 39.454342, mean_absolute_error: 1.868657, mean_q: -1.170492
   54976/1500000: episode: 458, duration: 1.406s, episode steps: 73, steps per second: 52, episode reward: -263.303, mean reward: -3.607 [-100.000, 55.851], mean action: 1.822 [0.000, 3.000], mean observation: -0.238 [-4.218, 1.000], loss: 42.258488, mean_absolute_error: 1.927446, mean_q: -1.246864
   55059/1500000: episode: 459, duration: 1.221s, episode steps: 83, steps per second: 68, episode reward: -93.934, mean reward: -1.132 [-100.000, 92.700], mean action: 1.904 [0.000, 3.000], mean observation: -0.118 [-1.775, 1.780], loss: 42.193287, mean_absolute_error: 1.950008, mean_q: -1.296661
   55116/1500000: episode: 460, duration: 0.731s, episode steps: 57, steps per second: 78, episode reward: -181.773, mean reward: -3.189 [-100.000, 7.355], mean action: 2.175 [1.000, 3.000], mean observation: -0.106 [-1.831, 3.042], loss: 42.043365, mean_absolute_error: 2.004810, mean_q: -1.327805
   55180/1500000: episode: 461, duration: 0.833s, episode steps: 64, steps per second: 77, episode reward: -193.870, mean reward: -3.029 [-100.000, 7.868], mean action: 1.984 [0.000, 3.000], mean observation: 0.048 [-6.037, 1.000], loss: 30.747810, mean_absolute_error: 1.988737, mean_q: -1.393499
   55234/1500000: episode: 462, duration: 0.654s, episode steps: 54, steps per second: 83, episode reward: -198.488, mean reward: -3.676 [-100.000, 8.077], mean action: 1.852 [1.000, 3.000], mean observation: 0.117 [-5.105, 1.000], loss: 30.892031, mean_absolute_error: 1.956512, mean_q: -1.395166
   55292/1500000: episode: 463, duration: 0.757s, episode steps: 58, steps per second: 77, episode reward: -193.651, mean reward: -3.339 [-100.000, 21.378], mean action: 1.862 [1.000, 3.000], mean observation: 0.132 [-5.653, 1.000], loss: 38.086838, mean_absolute_error: 2.045894, mean_q: -1.474696
   55373/1500000: episode: 464, duration: 1.391s, episode steps: 81, steps per second: 58, episode reward: -138.338, mean reward: -1.708 [-100.000, 66.092], mean action: 1.852 [0.000, 3.000], mean observation: -0.140 [-1.766, 5.152], loss: 41.883438, mean_absolute_error: 2.081432, mean_q: -1.475739
   55425/1500000: episode: 465, duration: 0.721s, episode steps: 52, steps per second: 72, episode reward: -250.931, mean reward: -4.826 [-100.000, 4.514], mean action: 2.173 [0.000, 3.000], mean observation: -0.280 [-5.089, 1.000], loss: 39.633621, mean_absolute_error: 2.141327, mean_q: -1.565197
   55495/1500000: episode: 466, duration: 0.896s, episode steps: 70, steps per second: 78, episode reward: -264.490, mean reward: -3.778 [-100.000, 7.013], mean action: 2.057 [0.000, 3.000], mean observation: -0.197 [-1.717, 1.000], loss: 41.432552, mean_absolute_error: 2.198654, mean_q: -1.608773
   55578/1500000: episode: 467, duration: 1.192s, episode steps: 83, steps per second: 70, episode reward: -148.384, mean reward: -1.788 [-100.000, 16.351], mean action: 1.988 [0.000, 3.000], mean observation: 0.031 [-1.715, 1.000], loss: 34.087906, mean_absolute_error: 2.200547, mean_q: -1.662059
   55653/1500000: episode: 468, duration: 0.877s, episode steps: 75, steps per second: 86, episode reward: -207.248, mean reward: -2.763 [-100.000, 26.026], mean action: 1.987 [0.000, 3.000], mean observation: -0.119 [-1.773, 2.212], loss: 38.033188, mean_absolute_error: 2.213418, mean_q: -1.705366
   55712/1500000: episode: 469, duration: 0.850s, episode steps: 59, steps per second: 69, episode reward: -191.954, mean reward: -3.253 [-100.000, 7.505], mean action: 2.051 [0.000, 3.000], mean observation: -0.112 [-1.763, 5.197], loss: 40.173431, mean_absolute_error: 2.225541, mean_q: -1.704454
   55799/1500000: episode: 470, duration: 1.731s, episode steps: 87, steps per second: 50, episode reward: -228.070, mean reward: -2.621 [-100.000, 10.581], mean action: 1.931 [0.000, 3.000], mean observation: -0.132 [-4.349, 1.000], loss: 39.506580, mean_absolute_error: 2.302456, mean_q: -1.781609
   55870/1500000: episode: 471, duration: 1.119s, episode steps: 71, steps per second: 63, episode reward: -200.034, mean reward: -2.817 [-100.000, 7.618], mean action: 1.930 [0.000, 3.000], mean observation: 0.049 [-6.190, 1.000], loss: 40.840023, mean_absolute_error: 2.337495, mean_q: -1.809179
   55960/1500000: episode: 472, duration: 0.973s, episode steps: 90, steps per second: 92, episode reward: -177.821, mean reward: -1.976 [-100.000, 8.248], mean action: 1.900 [0.000, 3.000], mean observation: 0.098 [-6.029, 1.007], loss: 30.825075, mean_absolute_error: 2.306258, mean_q: -1.842786
   56056/1500000: episode: 473, duration: 1.032s, episode steps: 96, steps per second: 93, episode reward: -251.436, mean reward: -2.619 [-100.000, 7.328], mean action: 1.781 [0.000, 3.000], mean observation: 0.191 [-1.926, 1.017], loss: 29.952669, mean_absolute_error: 2.388005, mean_q: -1.926702
   56127/1500000: episode: 474, duration: 0.758s, episode steps: 71, steps per second: 94, episode reward: -200.178, mean reward: -2.819 [-100.000, 12.567], mean action: 2.056 [1.000, 3.000], mean observation: -0.103 [-1.681, 4.937], loss: 27.547819, mean_absolute_error: 2.358228, mean_q: -1.959591
   56191/1500000: episode: 475, duration: 0.695s, episode steps: 64, steps per second: 92, episode reward: -207.163, mean reward: -3.237 [-100.000, 6.506], mean action: 2.016 [0.000, 3.000], mean observation: -0.117 [-1.743, 5.058], loss: 20.216557, mean_absolute_error: 2.370876, mean_q: -2.051106
   56260/1500000: episode: 476, duration: 0.903s, episode steps: 69, steps per second: 76, episode reward: -233.868, mean reward: -3.389 [-100.000, 16.351], mean action: 2.043 [0.000, 3.000], mean observation: -0.185 [-1.727, 4.470], loss: 32.224007, mean_absolute_error: 2.415065, mean_q: -2.012587
   56332/1500000: episode: 477, duration: 0.830s, episode steps: 72, steps per second: 87, episode reward: -77.670, mean reward: -1.079 [-100.000, 87.405], mean action: 1.986 [0.000, 3.000], mean observation: -0.163 [-2.177, 1.996], loss: 36.084892, mean_absolute_error: 2.491766, mean_q: -2.093855
   56416/1500000: episode: 478, duration: 0.890s, episode steps: 84, steps per second: 94, episode reward: -200.829, mean reward: -2.391 [-100.000, 7.570], mean action: 1.929 [0.000, 3.000], mean observation: 0.111 [-5.615, 1.000], loss: 43.246964, mean_absolute_error: 2.691780, mean_q: -2.239641
   56494/1500000: episode: 479, duration: 0.830s, episode steps: 78, steps per second: 94, episode reward: -313.518, mean reward: -4.019 [-100.000, 5.966], mean action: 1.962 [0.000, 3.000], mean observation: -0.256 [-1.845, 1.428], loss: 33.794521, mean_absolute_error: 2.601846, mean_q: -2.225852
   56572/1500000: episode: 480, duration: 0.895s, episode steps: 78, steps per second: 87, episode reward: -198.883, mean reward: -2.550 [-100.000, 7.903], mean action: 2.013 [1.000, 3.000], mean observation: 0.111 [-6.275, 1.000], loss: 29.661295, mean_absolute_error: 2.591386, mean_q: -2.261217
   56625/1500000: episode: 481, duration: 0.589s, episode steps: 53, steps per second: 90, episode reward: -137.827, mean reward: -2.601 [-100.000, 17.722], mean action: 2.057 [1.000, 3.000], mean observation: -0.013 [-1.884, 1.000], loss: 49.941299, mean_absolute_error: 2.753686, mean_q: -2.309592
   56708/1500000: episode: 482, duration: 0.904s, episode steps: 83, steps per second: 92, episode reward: -156.958, mean reward: -1.891 [-100.000, 11.030], mean action: 2.000 [0.000, 3.000], mean observation: 0.068 [-1.806, 1.000], loss: 27.487764, mean_absolute_error: 2.691267, mean_q: -2.397240
   56768/1500000: episode: 483, duration: 0.878s, episode steps: 60, steps per second: 68, episode reward: -151.429, mean reward: -2.524 [-100.000, 17.675], mean action: 2.000 [0.000, 3.000], mean observation: 0.011 [-1.825, 1.000], loss: 29.803305, mean_absolute_error: 2.717235, mean_q: -2.411932
   56853/1500000: episode: 484, duration: 0.927s, episode steps: 85, steps per second: 92, episode reward: -7.220, mean reward: -0.085 [-100.000, 126.754], mean action: 1.776 [0.000, 3.000], mean observation: -0.145 [-2.460, 1.658], loss: 29.118435, mean_absolute_error: 2.758358, mean_q: -2.467515
   56921/1500000: episode: 485, duration: 0.748s, episode steps: 68, steps per second: 91, episode reward: -167.988, mean reward: -2.470 [-100.000, 7.919], mean action: 2.029 [1.000, 3.000], mean observation: 0.053 [-5.627, 1.000], loss: 29.880587, mean_absolute_error: 2.860589, mean_q: -2.591810
   56978/1500000: episode: 486, duration: 0.643s, episode steps: 57, steps per second: 89, episode reward: -151.321, mean reward: -2.655 [-100.000, 16.643], mean action: 2.000 [0.000, 3.000], mean observation: -0.022 [-1.822, 1.000], loss: 38.210732, mean_absolute_error: 2.957727, mean_q: -2.672428
   57066/1500000: episode: 487, duration: 0.963s, episode steps: 88, steps per second: 91, episode reward: -202.702, mean reward: -2.303 [-100.000, 7.216], mean action: 1.932 [0.000, 3.000], mean observation: 0.130 [-5.718, 1.048], loss: 30.403204, mean_absolute_error: 2.929827, mean_q: -2.657124
   57125/1500000: episode: 488, duration: 0.627s, episode steps: 59, steps per second: 94, episode reward: -156.218, mean reward: -2.648 [-100.000, 7.591], mean action: 2.034 [1.000, 3.000], mean observation: -0.001 [-5.501, 1.000], loss: 20.551441, mean_absolute_error: 2.939788, mean_q: -2.789229
   57204/1500000: episode: 489, duration: 1.069s, episode steps: 79, steps per second: 74, episode reward: -315.987, mean reward: -4.000 [-100.000, 2.021], mean action: 2.025 [0.000, 3.000], mean observation: -0.250 [-1.851, 1.000], loss: 38.259853, mean_absolute_error: 3.051160, mean_q: -2.840689
   57291/1500000: episode: 490, duration: 1.338s, episode steps: 87, steps per second: 65, episode reward: -213.675, mean reward: -2.456 [-100.000, 16.652], mean action: 1.897 [0.000, 3.000], mean observation: 0.139 [-5.299, 1.098], loss: 38.211533, mean_absolute_error: 3.116807, mean_q: -2.922280
   57352/1500000: episode: 491, duration: 0.886s, episode steps: 61, steps per second: 69, episode reward: -201.874, mean reward: -3.309 [-100.000, 7.071], mean action: 2.049 [1.000, 3.000], mean observation: -0.052 [-1.824, 5.939], loss: 36.850994, mean_absolute_error: 3.186110, mean_q: -3.003606
   57432/1500000: episode: 492, duration: 0.874s, episode steps: 80, steps per second: 92, episode reward: -194.859, mean reward: -2.436 [-100.000, 7.034], mean action: 2.050 [1.000, 3.000], mean observation: -0.017 [-1.820, 6.142], loss: 49.588280, mean_absolute_error: 3.299598, mean_q: -3.033266
   57518/1500000: episode: 493, duration: 1.139s, episode steps: 86, steps per second: 76, episode reward: -172.400, mean reward: -2.005 [-100.000, 5.480], mean action: 1.953 [0.000, 3.000], mean observation: 0.052 [-5.709, 1.015], loss: 31.985281, mean_absolute_error: 3.305377, mean_q: -3.104680
   57581/1500000: episode: 494, duration: 0.844s, episode steps: 63, steps per second: 75, episode reward: -165.716, mean reward: -2.630 [-100.000, 6.010], mean action: 2.048 [0.000, 3.000], mean observation: -0.011 [-1.702, 4.883], loss: 27.997326, mean_absolute_error: 3.429849, mean_q: -3.335749
   57654/1500000: episode: 495, duration: 1.160s, episode steps: 73, steps per second: 63, episode reward: -187.540, mean reward: -2.569 [-100.000, 7.368], mean action: 1.863 [0.000, 3.000], mean observation: 0.090 [-5.683, 1.000], loss: 43.306793, mean_absolute_error: 3.431067, mean_q: -3.221251
   57716/1500000: episode: 496, duration: 0.833s, episode steps: 62, steps per second: 74, episode reward: -219.484, mean reward: -3.540 [-100.000, 8.301], mean action: 2.081 [0.000, 3.000], mean observation: -0.086 [-1.766, 3.947], loss: 52.130596, mean_absolute_error: 3.593209, mean_q: -3.378623
   57786/1500000: episode: 497, duration: 0.740s, episode steps: 70, steps per second: 95, episode reward: -182.338, mean reward: -2.605 [-100.000, 6.527], mean action: 2.029 [0.000, 3.000], mean observation: 0.026 [-1.769, 5.712], loss: 30.689083, mean_absolute_error: 3.477365, mean_q: -3.402199
   57865/1500000: episode: 498, duration: 1.021s, episode steps: 79, steps per second: 77, episode reward: -173.664, mean reward: -2.198 [-100.000, 59.922], mean action: 2.063 [0.000, 3.000], mean observation: -0.079 [-1.799, 1.000], loss: 48.623058, mean_absolute_error: 3.681598, mean_q: -3.538845
   57950/1500000: episode: 499, duration: 0.926s, episode steps: 85, steps per second: 92, episode reward: -179.932, mean reward: -2.117 [-100.000, 5.253], mean action: 2.024 [0.000, 3.000], mean observation: 0.065 [-5.776, 1.085], loss: 25.887058, mean_absolute_error: 3.570817, mean_q: -3.579002
   58012/1500000: episode: 500, duration: 0.793s, episode steps: 62, steps per second: 78, episode reward: -92.368, mean reward: -1.490 [-100.000, 58.082], mean action: 2.048 [0.000, 3.000], mean observation: -0.035 [-1.799, 2.733], loss: 39.013081, mean_absolute_error: 3.639870, mean_q: -3.603270
   58101/1500000: episode: 501, duration: 1.407s, episode steps: 89, steps per second: 63, episode reward: -281.351, mean reward: -3.161 [-100.000, 26.483], mean action: 2.022 [0.000, 3.000], mean observation: -0.197 [-4.765, 1.002], loss: 42.258274, mean_absolute_error: 3.849972, mean_q: -3.815100
   58173/1500000: episode: 502, duration: 0.903s, episode steps: 72, steps per second: 80, episode reward: -196.963, mean reward: -2.736 [-100.000, 5.853], mean action: 2.014 [0.000, 3.000], mean observation: 0.023 [-1.818, 1.002], loss: 33.646442, mean_absolute_error: 3.842820, mean_q: -3.849543
   58254/1500000: episode: 503, duration: 1.077s, episode steps: 81, steps per second: 75, episode reward: -71.095, mean reward: -0.878 [-100.000, 123.527], mean action: 2.000 [0.000, 3.000], mean observation: -0.078 [-2.387, 1.504], loss: 34.870369, mean_absolute_error: 3.822336, mean_q: -3.850940
   58314/1500000: episode: 504, duration: 0.668s, episode steps: 60, steps per second: 90, episode reward: -168.469, mean reward: -2.808 [-100.000, 8.147], mean action: 1.900 [0.000, 3.000], mean observation: 0.088 [-6.474, 1.000], loss: 44.441395, mean_absolute_error: 4.007106, mean_q: -3.989984
   58368/1500000: episode: 505, duration: 0.747s, episode steps: 54, steps per second: 72, episode reward: -212.214, mean reward: -3.930 [-100.000, 8.291], mean action: 2.074 [1.000, 3.000], mean observation: -0.161 [-1.770, 1.000], loss: 33.440788, mean_absolute_error: 3.956861, mean_q: -4.014086
   58455/1500000: episode: 506, duration: 1.003s, episode steps: 87, steps per second: 87, episode reward: -174.758, mean reward: -2.009 [-100.000, 17.007], mean action: 2.057 [1.000, 3.000], mean observation: 0.058 [-1.826, 5.702], loss: 30.324833, mean_absolute_error: 4.031768, mean_q: -4.139760
   58519/1500000: episode: 507, duration: 0.801s, episode steps: 64, steps per second: 80, episode reward: -255.317, mean reward: -3.989 [-100.000, 6.313], mean action: 2.125 [0.000, 3.000], mean observation: -0.158 [-4.041, 1.000], loss: 41.040756, mean_absolute_error: 4.174934, mean_q: -4.202494
   58585/1500000: episode: 508, duration: 1.056s, episode steps: 66, steps per second: 63, episode reward: -244.292, mean reward: -3.701 [-100.000, 88.543], mean action: 2.061 [0.000, 3.000], mean observation: -0.241 [-3.123, 1.000], loss: 28.487476, mean_absolute_error: 4.148489, mean_q: -4.287316
   58674/1500000: episode: 509, duration: 1.086s, episode steps: 89, steps per second: 82, episode reward: -288.145, mean reward: -3.238 [-100.000, 5.832], mean action: 2.011 [0.000, 3.000], mean observation: -0.184 [-6.182, 1.006], loss: 30.032633, mean_absolute_error: 4.223000, mean_q: -4.426783
   58735/1500000: episode: 510, duration: 0.630s, episode steps: 61, steps per second: 97, episode reward: -173.555, mean reward: -2.845 [-100.000, 5.369], mean action: 2.066 [1.000, 3.000], mean observation: 0.004 [-1.822, 1.000], loss: 29.479473, mean_absolute_error: 4.276637, mean_q: -4.497073
   58803/1500000: episode: 511, duration: 0.720s, episode steps: 68, steps per second: 94, episode reward: -257.678, mean reward: -3.789 [-100.000, 29.539], mean action: 2.103 [0.000, 3.000], mean observation: -0.243 [-4.648, 1.000], loss: 28.994425, mean_absolute_error: 4.367423, mean_q: -4.599257
   58887/1500000: episode: 512, duration: 0.925s, episode steps: 84, steps per second: 91, episode reward: -196.608, mean reward: -2.341 [-100.000, 7.233], mean action: 1.881 [0.000, 3.000], mean observation: 0.147 [-6.380, 1.000], loss: 35.057766, mean_absolute_error: 4.367387, mean_q: -4.553933
   58948/1500000: episode: 513, duration: 0.656s, episode steps: 61, steps per second: 93, episode reward: -177.420, mean reward: -2.909 [-100.000, 6.745], mean action: 2.049 [1.000, 3.000], mean observation: -0.029 [-1.795, 6.152], loss: 27.065498, mean_absolute_error: 4.478332, mean_q: -4.784451
   59032/1500000: episode: 514, duration: 1.111s, episode steps: 84, steps per second: 76, episode reward: -168.674, mean reward: -2.008 [-100.000, 7.990], mean action: 2.012 [0.000, 3.000], mean observation: 0.065 [-5.470, 1.158], loss: 28.580301, mean_absolute_error: 4.639474, mean_q: -4.949777
   59101/1500000: episode: 515, duration: 0.768s, episode steps: 69, steps per second: 90, episode reward: -267.970, mean reward: -3.884 [-100.000, 13.186], mean action: 2.014 [0.000, 3.000], mean observation: -0.211 [-4.355, 1.000], loss: 31.225573, mean_absolute_error: 4.663329, mean_q: -4.963830
   59174/1500000: episode: 516, duration: 0.769s, episode steps: 73, steps per second: 95, episode reward: -203.930, mean reward: -2.794 [-100.000, 13.950], mean action: 1.863 [0.000, 3.000], mean observation: 0.112 [-1.814, 1.000], loss: 44.767578, mean_absolute_error: 4.883918, mean_q: -5.107451
   59249/1500000: episode: 517, duration: 0.802s, episode steps: 75, steps per second: 93, episode reward: -175.358, mean reward: -2.338 [-100.000, 16.114], mean action: 2.040 [0.000, 3.000], mean observation: -0.003 [-1.772, 1.000], loss: 21.663336, mean_absolute_error: 4.728477, mean_q: -5.109682
   59304/1500000: episode: 518, duration: 0.603s, episode steps: 55, steps per second: 91, episode reward: -196.278, mean reward: -3.569 [-100.000, 73.398], mean action: 2.109 [0.000, 3.000], mean observation: -0.274 [-3.237, 1.000], loss: 48.423401, mean_absolute_error: 5.019655, mean_q: -5.273919
   59385/1500000: episode: 519, duration: 0.844s, episode steps: 81, steps per second: 96, episode reward: -207.687, mean reward: -2.564 [-100.000, 7.584], mean action: 1.951 [1.000, 3.000], mean observation: 0.131 [-5.723, 1.000], loss: 27.754545, mean_absolute_error: 4.858405, mean_q: -5.254252
   59471/1500000: episode: 520, duration: 1.617s, episode steps: 86, steps per second: 53, episode reward: -181.080, mean reward: -2.106 [-100.000, 17.854], mean action: 1.942 [0.000, 3.000], mean observation: 0.152 [-1.747, 1.000], loss: 31.074255, mean_absolute_error: 4.932477, mean_q: -5.336555
   59554/1500000: episode: 521, duration: 1.167s, episode steps: 83, steps per second: 71, episode reward: -195.696, mean reward: -2.358 [-100.000, 7.063], mean action: 1.988 [0.000, 3.000], mean observation: -0.014 [-1.823, 5.866], loss: 22.650410, mean_absolute_error: 5.171524, mean_q: -5.674869
   59607/1500000: episode: 522, duration: 0.733s, episode steps: 53, steps per second: 72, episode reward: -176.900, mean reward: -3.338 [-100.000, 9.855], mean action: 2.094 [0.000, 3.000], mean observation: -0.108 [-1.825, 5.258], loss: 39.573063, mean_absolute_error: 5.276630, mean_q: -5.708362
   59666/1500000: episode: 523, duration: 0.755s, episode steps: 59, steps per second: 78, episode reward: -169.104, mean reward: -2.866 [-100.000, 5.270], mean action: 2.034 [0.000, 3.000], mean observation: -0.012 [-6.424, 1.000], loss: 27.676790, mean_absolute_error: 5.270141, mean_q: -5.742574
   59740/1500000: episode: 524, duration: 0.995s, episode steps: 74, steps per second: 74, episode reward: -188.259, mean reward: -2.544 [-100.000, 7.713], mean action: 1.986 [0.000, 3.000], mean observation: 0.086 [-5.672, 1.362], loss: 31.591463, mean_absolute_error: 5.317526, mean_q: -5.782862
   59819/1500000: episode: 525, duration: 0.908s, episode steps: 79, steps per second: 87, episode reward: -225.419, mean reward: -2.853 [-100.000, 6.241], mean action: 2.051 [0.000, 3.000], mean observation: -0.029 [-1.761, 4.747], loss: 29.942394, mean_absolute_error: 5.382263, mean_q: -5.901679
   59886/1500000: episode: 526, duration: 1.130s, episode steps: 67, steps per second: 59, episode reward: -169.469, mean reward: -2.529 [-100.000, 8.126], mean action: 1.970 [1.000, 3.000], mean observation: 0.106 [-6.253, 1.000], loss: 28.403559, mean_absolute_error: 5.562655, mean_q: -6.109855
   59950/1500000: episode: 527, duration: 0.885s, episode steps: 64, steps per second: 72, episode reward: -212.910, mean reward: -3.327 [-100.000, 7.620], mean action: 2.047 [0.000, 3.000], mean observation: -0.061 [-1.791, 5.226], loss: 30.933786, mean_absolute_error: 5.513348, mean_q: -6.041801
   60011/1500000: episode: 528, duration: 0.773s, episode steps: 61, steps per second: 79, episode reward: -172.157, mean reward: -2.822 [-100.000, 54.898], mean action: 1.902 [0.000, 3.000], mean observation: -0.128 [-1.772, 1.000], loss: 36.721535, mean_absolute_error: 5.634295, mean_q: -6.159811
   60099/1500000: episode: 529, duration: 1.035s, episode steps: 88, steps per second: 85, episode reward: -318.198, mean reward: -3.616 [-100.000, 25.612], mean action: 2.011 [0.000, 3.000], mean observation: -0.295 [-5.822, 1.000], loss: 29.507332, mean_absolute_error: 5.728646, mean_q: -6.321544
   60204/1500000: episode: 530, duration: 1.290s, episode steps: 105, steps per second: 81, episode reward: -162.544, mean reward: -1.548 [-100.000, 6.748], mean action: 2.000 [0.000, 3.000], mean observation: 0.047 [-1.433, 4.926], loss: 36.279434, mean_absolute_error: 5.931543, mean_q: -6.547454
   60292/1500000: episode: 531, duration: 0.965s, episode steps: 88, steps per second: 91, episode reward: -256.071, mean reward: -2.910 [-100.000, 41.792], mean action: 2.000 [0.000, 3.000], mean observation: -0.212 [-4.714, 1.000], loss: 27.655251, mean_absolute_error: 5.920018, mean_q: -6.609007
   60380/1500000: episode: 532, duration: 1.147s, episode steps: 88, steps per second: 77, episode reward: -243.215, mean reward: -2.764 [-100.000, 26.524], mean action: 1.852 [0.000, 3.000], mean observation: -0.170 [-4.608, 1.000], loss: 40.712360, mean_absolute_error: 6.216048, mean_q: -6.851166
   60454/1500000: episode: 533, duration: 0.911s, episode steps: 74, steps per second: 81, episode reward: -105.611, mean reward: -1.427 [-100.000, 72.752], mean action: 2.000 [0.000, 3.000], mean observation: -0.091 [-1.737, 3.865], loss: 35.608574, mean_absolute_error: 6.321125, mean_q: -7.003291
   60543/1500000: episode: 534, duration: 0.942s, episode steps: 89, steps per second: 95, episode reward: -227.597, mean reward: -2.557 [-100.000, 32.165], mean action: 1.955 [0.000, 3.000], mean observation: -0.154 [-6.439, 1.000], loss: 30.084892, mean_absolute_error: 6.457476, mean_q: -7.247633
   60675/1500000: episode: 535, duration: 1.347s, episode steps: 132, steps per second: 98, episode reward: -99.493, mean reward: -0.754 [-100.000, 12.629], mean action: 1.985 [0.000, 3.000], mean observation: 0.050 [-1.136, 3.835], loss: 26.150852, mean_absolute_error: 6.567255, mean_q: -7.471325
   60730/1500000: episode: 536, duration: 0.590s, episode steps: 55, steps per second: 93, episode reward: -302.992, mean reward: -5.509 [-100.000, 4.776], mean action: 1.691 [0.000, 3.000], mean observation: -0.327 [-5.220, 1.000], loss: 27.735811, mean_absolute_error: 6.700874, mean_q: -7.562873
   60873/1500000: episode: 537, duration: 2.983s, episode steps: 143, steps per second: 48, episode reward: -85.815, mean reward: -0.600 [-100.000, 10.258], mean action: 1.979 [0.000, 3.000], mean observation: 0.063 [-0.833, 2.918], loss: 27.904907, mean_absolute_error: 6.720094, mean_q: -7.612741
   60957/1500000: episode: 538, duration: 0.874s, episode steps: 84, steps per second: 96, episode reward: -249.047, mean reward: -2.965 [-100.000, 45.850], mean action: 1.821 [0.000, 3.000], mean observation: -0.195 [-4.618, 1.000], loss: 25.474154, mean_absolute_error: 7.014923, mean_q: -7.982346
   61103/1500000: episode: 539, duration: 1.995s, episode steps: 146, steps per second: 73, episode reward: -69.775, mean reward: -0.478 [-100.000, 16.455], mean action: 1.781 [0.000, 3.000], mean observation: 0.101 [-2.954, 1.000], loss: 29.476402, mean_absolute_error: 7.131893, mean_q: -8.141799
   61240/1500000: episode: 540, duration: 1.423s, episode steps: 137, steps per second: 96, episode reward: -79.668, mean reward: -0.582 [-100.000, 14.461], mean action: 2.015 [0.000, 3.000], mean observation: 0.013 [-2.977, 1.000], loss: 45.860630, mean_absolute_error: 7.544256, mean_q: -8.563795
   61459/1500000: episode: 541, duration: 2.578s, episode steps: 219, steps per second: 85, episode reward: -60.398, mean reward: -0.276 [-100.000, 21.774], mean action: 1.936 [0.000, 3.000], mean observation: 0.030 [-0.506, 1.273], loss: 27.751553, mean_absolute_error: 7.619118, mean_q: -8.839703
   61667/1500000: episode: 542, duration: 2.178s, episode steps: 208, steps per second: 95, episode reward: -29.713, mean reward: -0.143 [-100.000, 15.194], mean action: 1.918 [0.000, 3.000], mean observation: 0.032 [-0.637, 2.175], loss: 31.649811, mean_absolute_error: 7.999965, mean_q: -9.241029
   61969/1500000: episode: 543, duration: 3.396s, episode steps: 302, steps per second: 89, episode reward: -40.661, mean reward: -0.135 [-100.000, 13.688], mean action: 1.970 [0.000, 3.000], mean observation: 0.046 [-0.457, 1.000], loss: 29.017792, mean_absolute_error: 8.448207, mean_q: -9.796193
   62269/1500000: episode: 544, duration: 3.501s, episode steps: 300, steps per second: 86, episode reward: -43.901, mean reward: -0.146 [-100.000, 16.915], mean action: 1.857 [0.000, 3.000], mean observation: 0.048 [-0.491, 1.340], loss: 32.714127, mean_absolute_error: 8.986347, mean_q: -10.508897
   62422/1500000: episode: 545, duration: 1.558s, episode steps: 153, steps per second: 98, episode reward: -40.394, mean reward: -0.264 [-100.000, 10.440], mean action: 1.967 [0.000, 3.000], mean observation: 0.069 [-0.694, 2.748], loss: 27.824270, mean_absolute_error: 9.378587, mean_q: -11.080496
   62658/1500000: episode: 546, duration: 2.610s, episode steps: 236, steps per second: 90, episode reward: -97.021, mean reward: -0.411 [-100.000, 13.008], mean action: 1.890 [0.000, 3.000], mean observation: 0.011 [-1.657, 1.000], loss: 26.342823, mean_absolute_error: 9.701246, mean_q: -11.488684
   62851/1500000: episode: 547, duration: 2.235s, episode steps: 193, steps per second: 86, episode reward: -52.368, mean reward: -0.271 [-100.000, 14.169], mean action: 1.715 [0.000, 3.000], mean observation: 0.040 [-0.594, 1.231], loss: 24.829716, mean_absolute_error: 10.240453, mean_q: -12.118629
   63093/1500000: episode: 548, duration: 2.680s, episode steps: 242, steps per second: 90, episode reward: -48.501, mean reward: -0.200 [-100.000, 21.097], mean action: 1.963 [0.000, 3.000], mean observation: 0.046 [-1.393, 1.064], loss: 24.831453, mean_absolute_error: 10.530797, mean_q: -12.551729
   63364/1500000: episode: 549, duration: 3.051s, episode steps: 271, steps per second: 89, episode reward: -53.825, mean reward: -0.199 [-100.000, 12.532], mean action: 1.852 [0.000, 3.000], mean observation: 0.051 [-0.585, 1.707], loss: 21.939165, mean_absolute_error: 10.949037, mean_q: -13.154582
   63856/1500000: episode: 550, duration: 6.073s, episode steps: 492, steps per second: 81, episode reward: 141.551, mean reward: 0.288 [-19.585, 100.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.075 [-0.497, 1.527], loss: 25.910284, mean_absolute_error: 11.659723, mean_q: -14.109997
   64098/1500000: episode: 551, duration: 2.809s, episode steps: 242, steps per second: 86, episode reward: -144.278, mean reward: -0.596 [-100.000, 13.905], mean action: 1.946 [0.000, 3.000], mean observation: -0.039 [-0.786, 1.956], loss: 24.733702, mean_absolute_error: 12.188715, mean_q: -14.756279
   64218/1500000: episode: 552, duration: 1.579s, episode steps: 120, steps per second: 76, episode reward: -171.329, mean reward: -1.428 [-100.000, 9.484], mean action: 1.983 [0.000, 3.000], mean observation: -0.030 [-0.922, 3.124], loss: 21.324793, mean_absolute_error: 12.539355, mean_q: -15.181472
   64427/1500000: episode: 553, duration: 2.304s, episode steps: 209, steps per second: 91, episode reward: -77.997, mean reward: -0.373 [-100.000, 18.051], mean action: 1.785 [0.000, 3.000], mean observation: -0.015 [-0.547, 1.134], loss: 20.638674, mean_absolute_error: 12.982296, mean_q: -15.731278
   64619/1500000: episode: 554, duration: 1.975s, episode steps: 192, steps per second: 97, episode reward: -318.026, mean reward: -1.656 [-100.000, 59.695], mean action: 1.474 [0.000, 3.000], mean observation: -0.013 [-3.485, 1.000], loss: 18.742258, mean_absolute_error: 13.231832, mean_q: -16.030096
   64814/1500000: episode: 555, duration: 2.234s, episode steps: 195, steps per second: 87, episode reward: -275.731, mean reward: -1.414 [-100.000, 10.074], mean action: 1.436 [0.000, 3.000], mean observation: -0.013 [-1.104, 1.714], loss: 23.157793, mean_absolute_error: 13.628386, mean_q: -16.461386
   65003/1500000: episode: 556, duration: 2.349s, episode steps: 189, steps per second: 80, episode reward: -145.481, mean reward: -0.770 [-100.000, 7.558], mean action: 1.656 [0.000, 3.000], mean observation: 0.060 [-0.937, 2.516], loss: 19.665928, mean_absolute_error: 14.016485, mean_q: -17.041723
   65385/1500000: episode: 557, duration: 6.298s, episode steps: 382, steps per second: 61, episode reward: -124.913, mean reward: -0.327 [-100.000, 14.159], mean action: 1.445 [0.000, 3.000], mean observation: 0.034 [-0.623, 1.072], loss: 21.929199, mean_absolute_error: 14.672409, mean_q: -17.845322
   65768/1500000: episode: 558, duration: 4.721s, episode steps: 383, steps per second: 81, episode reward: -95.261, mean reward: -0.249 [-100.000, 35.381], mean action: 1.535 [0.000, 3.000], mean observation: 0.060 [-0.836, 1.226], loss: 17.216938, mean_absolute_error: 15.349945, mean_q: -18.903522
   66107/1500000: episode: 559, duration: 3.908s, episode steps: 339, steps per second: 87, episode reward: -99.047, mean reward: -0.292 [-100.000, 8.386], mean action: 1.835 [0.000, 3.000], mean observation: 0.007 [-1.583, 1.000], loss: 19.705349, mean_absolute_error: 16.024712, mean_q: -19.831758
   66359/1500000: episode: 560, duration: 2.662s, episode steps: 252, steps per second: 95, episode reward: -79.242, mean reward: -0.314 [-100.000, 13.412], mean action: 1.492 [0.000, 3.000], mean observation: 0.144 [-1.222, 1.289], loss: 17.530642, mean_absolute_error: 16.606321, mean_q: -20.598223
   67219/1500000: episode: 561, duration: 10.251s, episode steps: 860, steps per second: 84, episode reward: -659.635, mean reward: -0.767 [-100.000, 4.139], mean action: 1.678 [0.000, 3.000], mean observation: 0.290 [-0.611, 3.585], loss: 16.385628, mean_absolute_error: 17.720041, mean_q: -22.117149
   67953/1500000: episode: 562, duration: 8.874s, episode steps: 734, steps per second: 83, episode reward: -164.149, mean reward: -0.224 [-100.000, 19.753], mean action: 1.798 [0.000, 3.000], mean observation: 0.175 [-0.922, 1.084], loss: 14.570604, mean_absolute_error: 19.171705, mean_q: -24.110687
   68953/1500000: episode: 563, duration: 14.167s, episode steps: 1000, steps per second: 71, episode reward: -590.880, mean reward: -0.591 [-5.709, 3.541], mean action: 1.938 [0.000, 3.000], mean observation: 0.280 [-0.776, 4.901], loss: 14.406005, mean_absolute_error: 20.881924, mean_q: -26.345173
   69950/1500000: episode: 564, duration: 16.298s, episode steps: 997, steps per second: 61, episode reward: 81.904, mean reward: 0.082 [-18.373, 100.000], mean action: 1.671 [0.000, 3.000], mean observation: 0.059 [-1.013, 1.000], loss: 12.612096, mean_absolute_error: 22.284378, mean_q: -28.228245
   70950/1500000: episode: 565, duration: 21.141s, episode steps: 1000, steps per second: 47, episode reward: -338.782, mean reward: -0.339 [-5.466, 9.951], mean action: 1.732 [0.000, 3.000], mean observation: 0.114 [-1.184, 1.483], loss: 10.825586, mean_absolute_error: 23.592730, mean_q: -29.968130
   71399/1500000: episode: 566, duration: 9.738s, episode steps: 449, steps per second: 46, episode reward: -174.783, mean reward: -0.389 [-100.000, 13.958], mean action: 1.857 [0.000, 3.000], mean observation: 0.102 [-1.682, 1.579], loss: 9.979865, mean_absolute_error: 24.210396, mean_q: -30.798271
   72399/1500000: episode: 567, duration: 16.997s, episode steps: 1000, steps per second: 59, episode reward: -67.432, mean reward: -0.067 [-16.445, 11.416], mean action: 1.783 [0.000, 3.000], mean observation: 0.059 [-0.882, 1.000], loss: 10.973502, mean_absolute_error: 24.729235, mean_q: -31.437407
   72993/1500000: episode: 568, duration: 10.329s, episode steps: 594, steps per second: 58, episode reward: -55.794, mean reward: -0.094 [-100.000, 19.091], mean action: 1.662 [0.000, 3.000], mean observation: 0.073 [-0.594, 1.728], loss: 11.671984, mean_absolute_error: 25.256033, mean_q: -32.156166
   73263/1500000: episode: 569, duration: 2.911s, episode steps: 270, steps per second: 93, episode reward: -276.041, mean reward: -1.022 [-100.000, 23.682], mean action: 1.748 [0.000, 3.000], mean observation: -0.004 [-2.033, 1.000], loss: 10.292789, mean_absolute_error: 25.441528, mean_q: -32.392078
   73576/1500000: episode: 570, duration: 3.811s, episode steps: 313, steps per second: 82, episode reward: -247.907, mean reward: -0.792 [-100.000, 19.573], mean action: 1.834 [0.000, 3.000], mean observation: 0.083 [-2.067, 1.053], loss: 10.862203, mean_absolute_error: 25.604742, mean_q: -32.622986
   73899/1500000: episode: 571, duration: 3.527s, episode steps: 323, steps per second: 92, episode reward: -304.194, mean reward: -0.942 [-100.000, 10.482], mean action: 1.882 [0.000, 3.000], mean observation: 0.104 [-0.950, 2.308], loss: 11.569363, mean_absolute_error: 25.632956, mean_q: -32.631821
   74127/1500000: episode: 572, duration: 2.584s, episode steps: 228, steps per second: 88, episode reward: -103.262, mean reward: -0.453 [-100.000, 13.423], mean action: 1.864 [0.000, 3.000], mean observation: 0.088 [-0.839, 2.091], loss: 9.180728, mean_absolute_error: 25.855820, mean_q: -32.883160
   75127/1500000: episode: 573, duration: 14.414s, episode steps: 1000, steps per second: 69, episode reward: -84.042, mean reward: -0.084 [-3.482, 4.614], mean action: 1.805 [0.000, 3.000], mean observation: 0.079 [-0.330, 0.986], loss: 8.814811, mean_absolute_error: 25.908024, mean_q: -32.984470
   75392/1500000: episode: 574, duration: 2.824s, episode steps: 265, steps per second: 94, episode reward: -267.800, mean reward: -1.011 [-100.000, 24.657], mean action: 1.834 [0.000, 3.000], mean observation: -0.000 [-2.131, 1.000], loss: 9.312515, mean_absolute_error: 26.089479, mean_q: -33.254070
   76392/1500000: episode: 575, duration: 14.399s, episode steps: 1000, steps per second: 69, episode reward: 42.393, mean reward: 0.042 [-18.805, 15.193], mean action: 1.967 [0.000, 3.000], mean observation: 0.121 [-0.653, 1.000], loss: 10.174701, mean_absolute_error: 26.275484, mean_q: -33.415005
   77058/1500000: episode: 576, duration: 7.859s, episode steps: 666, steps per second: 85, episode reward: 167.657, mean reward: 0.252 [-21.306, 100.000], mean action: 1.673 [0.000, 3.000], mean observation: 0.111 [-0.881, 1.130], loss: 8.231488, mean_absolute_error: 26.449730, mean_q: -33.693096
   78034/1500000: episode: 577, duration: 12.589s, episode steps: 976, steps per second: 78, episode reward: 46.931, mean reward: 0.048 [-20.457, 100.000], mean action: 1.967 [0.000, 3.000], mean observation: 0.086 [-0.336, 1.000], loss: 9.587203, mean_absolute_error: 26.700680, mean_q: -34.039902
   78732/1500000: episode: 578, duration: 10.700s, episode steps: 698, steps per second: 65, episode reward: 87.989, mean reward: 0.126 [-15.889, 100.000], mean action: 1.778 [0.000, 3.000], mean observation: 0.042 [-0.713, 1.000], loss: 8.619410, mean_absolute_error: 26.405848, mean_q: -33.636303
   79348/1500000: episode: 579, duration: 7.824s, episode steps: 616, steps per second: 79, episode reward: 206.426, mean reward: 0.335 [-19.572, 100.000], mean action: 1.620 [0.000, 3.000], mean observation: 0.104 [-1.284, 1.000], loss: 9.134711, mean_absolute_error: 26.417751, mean_q: -33.636616
   79751/1500000: episode: 580, duration: 4.758s, episode steps: 403, steps per second: 85, episode reward: -179.867, mean reward: -0.446 [-100.000, 10.737], mean action: 1.638 [0.000, 3.000], mean observation: 0.107 [-1.436, 1.000], loss: 8.074837, mean_absolute_error: 26.351505, mean_q: -33.564236
   80751/1500000: episode: 581, duration: 14.820s, episode steps: 1000, steps per second: 67, episode reward: -86.541, mean reward: -0.087 [-3.702, 13.673], mean action: 1.921 [0.000, 3.000], mean observation: 0.067 [-0.235, 1.000], loss: 9.723799, mean_absolute_error: 26.598772, mean_q: -33.858265
   80938/1500000: episode: 582, duration: 2.540s, episode steps: 187, steps per second: 74, episode reward: -220.546, mean reward: -1.179 [-100.000, 13.065], mean action: 1.850 [0.000, 3.000], mean observation: -0.010 [-3.819, 1.324], loss: 7.399083, mean_absolute_error: 26.464270, mean_q: -33.748524
   81938/1500000: episode: 583, duration: 15.517s, episode steps: 1000, steps per second: 64, episode reward: -15.168, mean reward: -0.015 [-22.576, 21.939], mean action: 1.463 [0.000, 3.000], mean observation: 0.080 [-0.293, 1.008], loss: 9.669265, mean_absolute_error: 26.065828, mean_q: -33.228172
   82270/1500000: episode: 584, duration: 3.810s, episode steps: 332, steps per second: 87, episode reward: -287.320, mean reward: -0.865 [-100.000, 35.415], mean action: 1.798 [0.000, 3.000], mean observation: 0.025 [-1.978, 1.000], loss: 10.134679, mean_absolute_error: 25.639025, mean_q: -32.640030
   82570/1500000: episode: 585, duration: 3.365s, episode steps: 300, steps per second: 89, episode reward: -279.788, mean reward: -0.933 [-100.000, 37.626], mean action: 1.687 [0.000, 3.000], mean observation: 0.022 [-2.205, 1.000], loss: 6.836497, mean_absolute_error: 25.963827, mean_q: -33.062897
   83570/1500000: episode: 586, duration: 15.481s, episode steps: 1000, steps per second: 65, episode reward: -45.050, mean reward: -0.045 [-19.141, 23.054], mean action: 1.594 [0.000, 3.000], mean observation: 0.080 [-0.429, 1.000], loss: 8.360324, mean_absolute_error: 25.858074, mean_q: -32.921474
   83708/1500000: episode: 587, duration: 1.640s, episode steps: 138, steps per second: 84, episode reward: -298.302, mean reward: -2.162 [-100.000, 8.060], mean action: 1.370 [0.000, 3.000], mean observation: -0.100 [-3.501, 1.263], loss: 9.302200, mean_absolute_error: 25.234156, mean_q: -32.091358
   84046/1500000: episode: 588, duration: 3.578s, episode steps: 338, steps per second: 94, episode reward: -275.473, mean reward: -0.815 [-100.000, 12.536], mean action: 1.775 [0.000, 3.000], mean observation: 0.067 [-2.490, 1.000], loss: 7.796822, mean_absolute_error: 25.894028, mean_q: -32.946934
   84327/1500000: episode: 589, duration: 3.227s, episode steps: 281, steps per second: 87, episode reward: -306.325, mean reward: -1.090 [-100.000, 24.653], mean action: 1.737 [0.000, 3.000], mean observation: -0.014 [-2.503, 1.000], loss: 9.595451, mean_absolute_error: 25.901913, mean_q: -32.973454
   84574/1500000: episode: 590, duration: 2.610s, episode steps: 247, steps per second: 95, episode reward: -71.386, mean reward: -0.289 [-100.000, 36.800], mean action: 1.721 [0.000, 3.000], mean observation: 0.064 [-0.931, 1.259], loss: 8.093679, mean_absolute_error: 25.494217, mean_q: -32.441555
   85574/1500000: episode: 591, duration: 12.848s, episode steps: 1000, steps per second: 78, episode reward: -44.029, mean reward: -0.044 [-21.454, 26.076], mean action: 1.738 [0.000, 3.000], mean observation: 0.078 [-0.295, 1.000], loss: 8.802418, mean_absolute_error: 25.333490, mean_q: -32.172188
   86574/1500000: episode: 592, duration: 13.583s, episode steps: 1000, steps per second: 74, episode reward: -69.493, mean reward: -0.069 [-23.135, 25.445], mean action: 1.842 [0.000, 3.000], mean observation: 0.068 [-0.316, 1.000], loss: 8.025301, mean_absolute_error: 24.981205, mean_q: -31.650526
   87574/1500000: episode: 593, duration: 14.366s, episode steps: 1000, steps per second: 70, episode reward: -59.123, mean reward: -0.059 [-24.172, 26.883], mean action: 1.837 [0.000, 3.000], mean observation: 0.068 [-0.214, 1.000], loss: 8.962729, mean_absolute_error: 24.609274, mean_q: -31.210270
   88574/1500000: episode: 594, duration: 13.618s, episode steps: 1000, steps per second: 73, episode reward: -31.942, mean reward: -0.032 [-24.081, 22.689], mean action: 1.710 [0.000, 3.000], mean observation: 0.060 [-0.664, 1.000], loss: 8.580969, mean_absolute_error: 24.614048, mean_q: -31.188641
   88951/1500000: episode: 595, duration: 4.358s, episode steps: 377, steps per second: 87, episode reward: -257.964, mean reward: -0.684 [-100.000, 12.216], mean action: 1.950 [0.000, 3.000], mean observation: 0.081 [-1.072, 1.916], loss: 10.268632, mean_absolute_error: 24.260632, mean_q: -30.701965
   89475/1500000: episode: 596, duration: 5.878s, episode steps: 524, steps per second: 89, episode reward: -281.351, mean reward: -0.537 [-100.000, 7.614], mean action: 1.958 [0.000, 3.000], mean observation: 0.081 [-0.883, 1.953], loss: 6.882380, mean_absolute_error: 24.393824, mean_q: -30.914204
   89721/1500000: episode: 597, duration: 2.854s, episode steps: 246, steps per second: 86, episode reward: -208.765, mean reward: -0.849 [-100.000, 11.944], mean action: 1.740 [0.000, 3.000], mean observation: 0.058 [-1.024, 1.468], loss: 8.386585, mean_absolute_error: 23.897812, mean_q: -30.332985
   90721/1500000: episode: 598, duration: 13.731s, episode steps: 1000, steps per second: 73, episode reward: -16.899, mean reward: -0.017 [-23.146, 23.120], mean action: 1.442 [0.000, 3.000], mean observation: 0.068 [-1.450, 1.000], loss: 8.141701, mean_absolute_error: 23.878153, mean_q: -30.300306
   91002/1500000: episode: 599, duration: 3.151s, episode steps: 281, steps per second: 89, episode reward: -328.303, mean reward: -1.168 [-100.000, 7.120], mean action: 1.762 [0.000, 3.000], mean observation: 0.023 [-2.269, 1.000], loss: 10.659987, mean_absolute_error: 24.032413, mean_q: -30.399084
   92002/1500000: episode: 600, duration: 12.345s, episode steps: 1000, steps per second: 81, episode reward: -134.047, mean reward: -0.134 [-3.650, 4.499], mean action: 1.935 [0.000, 3.000], mean observation: 0.102 [-0.272, 0.971], loss: 8.873932, mean_absolute_error: 23.670403, mean_q: -29.847622
   93002/1500000: episode: 601, duration: 13.280s, episode steps: 1000, steps per second: 75, episode reward: -176.827, mean reward: -0.177 [-3.962, 4.281], mean action: 1.799 [0.000, 3.000], mean observation: 0.122 [-0.251, 0.946], loss: 8.567020, mean_absolute_error: 23.197672, mean_q: -29.302954
   93338/1500000: episode: 602, duration: 3.749s, episode steps: 336, steps per second: 90, episode reward: -358.920, mean reward: -1.068 [-100.000, 11.635], mean action: 1.622 [0.000, 3.000], mean observation: 0.034 [-4.103, 1.121], loss: 7.774347, mean_absolute_error: 22.751677, mean_q: -28.798683
   93536/1500000: episode: 603, duration: 2.247s, episode steps: 198, steps per second: 88, episode reward: -373.952, mean reward: -1.889 [-100.000, 3.984], mean action: 1.732 [0.000, 3.000], mean observation: -0.016 [-2.165, 1.523], loss: 10.092570, mean_absolute_error: 22.548405, mean_q: -28.499640
   93767/1500000: episode: 604, duration: 2.851s, episode steps: 231, steps per second: 81, episode reward: -319.227, mean reward: -1.382 [-100.000, 13.015], mean action: 1.978 [0.000, 3.000], mean observation: 0.092 [-1.061, 2.566], loss: 8.208511, mean_absolute_error: 22.442699, mean_q: -28.334572
   93941/1500000: episode: 605, duration: 2.066s, episode steps: 174, steps per second: 84, episode reward: -143.663, mean reward: -0.826 [-100.000, 9.727], mean action: 1.649 [0.000, 3.000], mean observation: -0.019 [-1.096, 1.719], loss: 8.528552, mean_absolute_error: 22.749018, mean_q: -28.761881
   94156/1500000: episode: 606, duration: 5.187s, episode steps: 215, steps per second: 41, episode reward: -96.344, mean reward: -0.448 [-100.000, 18.808], mean action: 1.921 [0.000, 3.000], mean observation: 0.018 [-1.155, 1.368], loss: 11.632692, mean_absolute_error: 22.161036, mean_q: -27.939266
   94493/1500000: episode: 607, duration: 3.938s, episode steps: 337, steps per second: 86, episode reward: -140.894, mean reward: -0.418 [-100.000, 7.985], mean action: 1.944 [0.000, 3.000], mean observation: 0.100 [-1.218, 1.084], loss: 7.529007, mean_absolute_error: 22.378448, mean_q: -28.232485
   94884/1500000: episode: 608, duration: 4.414s, episode steps: 391, steps per second: 89, episode reward: -300.964, mean reward: -0.770 [-100.000, 22.018], mean action: 1.908 [0.000, 3.000], mean observation: 0.013 [-2.382, 1.000], loss: 6.444361, mean_absolute_error: 22.192699, mean_q: -28.036930
   95198/1500000: episode: 609, duration: 3.439s, episode steps: 314, steps per second: 91, episode reward: -137.306, mean reward: -0.437 [-100.000, 15.078], mean action: 1.959 [0.000, 3.000], mean observation: 0.083 [-1.216, 1.066], loss: 7.913503, mean_absolute_error: 22.133114, mean_q: -27.931265
   96198/1500000: episode: 610, duration: 13.445s, episode steps: 1000, steps per second: 74, episode reward: -170.766, mean reward: -0.171 [-4.415, 4.150], mean action: 1.933 [0.000, 3.000], mean observation: 0.122 [-0.282, 0.960], loss: 8.097417, mean_absolute_error: 22.195938, mean_q: -27.948328
   96450/1500000: episode: 611, duration: 2.619s, episode steps: 252, steps per second: 96, episode reward: -473.405, mean reward: -1.879 [-100.000, 5.341], mean action: 1.544 [0.000, 3.000], mean observation: -0.005 [-3.816, 1.235], loss: 9.830195, mean_absolute_error: 22.076145, mean_q: -27.844315
   96802/1500000: episode: 612, duration: 4.118s, episode steps: 352, steps per second: 85, episode reward: -345.922, mean reward: -0.983 [-100.000, 11.262], mean action: 1.807 [0.000, 3.000], mean observation: 0.122 [-0.865, 2.009], loss: 7.177870, mean_absolute_error: 21.533739, mean_q: -27.144747
   97802/1500000: episode: 613, duration: 13.174s, episode steps: 1000, steps per second: 76, episode reward: -132.424, mean reward: -0.132 [-3.799, 3.832], mean action: 1.964 [0.000, 3.000], mean observation: 0.095 [-0.269, 0.946], loss: 7.781865, mean_absolute_error: 21.724703, mean_q: -27.400204
   98145/1500000: episode: 614, duration: 3.977s, episode steps: 343, steps per second: 86, episode reward: -209.161, mean reward: -0.610 [-100.000, 17.238], mean action: 1.930 [0.000, 3.000], mean observation: 0.048 [-1.563, 1.688], loss: 7.251851, mean_absolute_error: 21.432207, mean_q: -27.067200
   98361/1500000: episode: 615, duration: 2.574s, episode steps: 216, steps per second: 84, episode reward: -413.087, mean reward: -1.912 [-100.000, 5.300], mean action: 1.630 [0.000, 3.000], mean observation: -0.059 [-2.298, 0.927], loss: 10.522032, mean_absolute_error: 21.594957, mean_q: -27.237282
   98555/1500000: episode: 616, duration: 2.094s, episode steps: 194, steps per second: 93, episode reward: -413.835, mean reward: -2.133 [-100.000, 4.948], mean action: 1.835 [0.000, 3.000], mean observation: 0.113 [-1.888, 2.051], loss: 8.703298, mean_absolute_error: 21.507828, mean_q: -27.124731
   98702/1500000: episode: 617, duration: 1.541s, episode steps: 147, steps per second: 95, episode reward: -226.979, mean reward: -1.544 [-100.000, 12.174], mean action: 1.626 [0.000, 3.000], mean observation: 0.070 [-1.092, 1.838], loss: 8.730415, mean_absolute_error: 21.133144, mean_q: -26.646017
   98885/1500000: episode: 618, duration: 2.319s, episode steps: 183, steps per second: 79, episode reward: -424.598, mean reward: -2.320 [-100.000, 3.674], mean action: 1.497 [0.000, 3.000], mean observation: -0.073 [-3.336, 0.948], loss: 8.851698, mean_absolute_error: 21.540337, mean_q: -27.152054
   99204/1500000: episode: 619, duration: 6.768s, episode steps: 319, steps per second: 47, episode reward: -331.646, mean reward: -1.040 [-100.000, 6.637], mean action: 1.436 [0.000, 3.000], mean observation: 0.094 [-1.111, 1.335], loss: 7.173295, mean_absolute_error: 21.433958, mean_q: -27.025354
   99441/1500000: episode: 620, duration: 2.880s, episode steps: 237, steps per second: 82, episode reward: -354.744, mean reward: -1.497 [-100.000, 5.161], mean action: 1.502 [0.000, 3.000], mean observation: 0.014 [-1.862, 1.000], loss: 10.470520, mean_absolute_error: 21.200352, mean_q: -26.714247
   99610/1500000: episode: 621, duration: 2.195s, episode steps: 169, steps per second: 77, episode reward: -386.777, mean reward: -2.289 [-100.000, 3.305], mean action: 1.290 [0.000, 3.000], mean observation: -0.046 [-2.162, 1.026], loss: 8.585340, mean_absolute_error: 21.041216, mean_q: -26.506680
   99811/1500000: episode: 622, duration: 2.487s, episode steps: 201, steps per second: 81, episode reward: -383.365, mean reward: -1.907 [-100.000, 4.834], mean action: 1.498 [0.000, 3.000], mean observation: -0.029 [-2.335, 0.944], loss: 9.129493, mean_absolute_error: 21.433229, mean_q: -26.988247
   99997/1500000: episode: 623, duration: 2.089s, episode steps: 186, steps per second: 89, episode reward: -205.848, mean reward: -1.107 [-100.000, 10.979], mean action: 1.554 [0.000, 3.000], mean observation: 0.078 [-1.059, 1.700], loss: 6.196465, mean_absolute_error: 21.441486, mean_q: -27.066753
  100275/1500000: episode: 624, duration: 4.155s, episode steps: 278, steps per second: 67, episode reward: -418.006, mean reward: -1.504 [-100.000, 24.261], mean action: 1.986 [0.000, 3.000], mean observation: 0.117 [-1.615, 3.330], loss: 8.817061, mean_absolute_error: 21.334204, mean_q: -26.900106
  100603/1500000: episode: 625, duration: 8.197s, episode steps: 328, steps per second: 40, episode reward: -283.896, mean reward: -0.866 [-100.000, 10.816], mean action: 1.921 [0.000, 3.000], mean observation: 0.089 [-1.658, 1.247], loss: 6.993651, mean_absolute_error: 21.251936, mean_q: -26.749874
  100976/1500000: episode: 626, duration: 5.331s, episode steps: 373, steps per second: 70, episode reward: -482.614, mean reward: -1.294 [-100.000, 4.772], mean action: 1.874 [0.000, 3.000], mean observation: 0.090 [-1.515, 2.183], loss: 7.842094, mean_absolute_error: 21.571131, mean_q: -27.177523
  101155/1500000: episode: 627, duration: 3.357s, episode steps: 179, steps per second: 53, episode reward: -384.017, mean reward: -2.145 [-100.000, 4.669], mean action: 1.749 [0.000, 3.000], mean observation: -0.072 [-2.354, 2.463], loss: 7.203844, mean_absolute_error: 21.445307, mean_q: -26.978395
  101384/1500000: episode: 628, duration: 4.563s, episode steps: 229, steps per second: 50, episode reward: -135.610, mean reward: -0.592 [-100.000, 30.017], mean action: 2.061 [0.000, 3.000], mean observation: -0.022 [-1.850, 1.000], loss: 9.403137, mean_absolute_error: 21.519211, mean_q: -27.104458
  102207/1500000: episode: 629, duration: 10.692s, episode steps: 823, steps per second: 77, episode reward: -384.887, mean reward: -0.468 [-100.000, 5.951], mean action: 1.795 [0.000, 3.000], mean observation: 0.055 [-3.222, 1.000], loss: 7.452105, mean_absolute_error: 21.372910, mean_q: -26.962275
  102392/1500000: episode: 630, duration: 1.985s, episode steps: 185, steps per second: 93, episode reward: -273.838, mean reward: -1.480 [-100.000, 26.656], mean action: 1.735 [0.000, 3.000], mean observation: -0.028 [-2.118, 1.059], loss: 6.633598, mean_absolute_error: 20.925343, mean_q: -26.386354
  102589/1500000: episode: 631, duration: 2.070s, episode steps: 197, steps per second: 95, episode reward: -239.916, mean reward: -1.218 [-100.000, 6.609], mean action: 2.010 [0.000, 3.000], mean observation: -0.058 [-1.491, 0.968], loss: 11.712655, mean_absolute_error: 21.036900, mean_q: -26.459208
  102726/1500000: episode: 632, duration: 1.721s, episode steps: 137, steps per second: 80, episode reward: -315.276, mean reward: -2.301 [-100.000, 4.026], mean action: 1.343 [0.000, 3.000], mean observation: -0.168 [-2.327, 1.000], loss: 6.214607, mean_absolute_error: 20.634617, mean_q: -25.982346
  103726/1500000: episode: 633, duration: 24.186s, episode steps: 1000, steps per second: 41, episode reward: -145.038, mean reward: -0.145 [-4.291, 4.114], mean action: 1.950 [0.000, 3.000], mean observation: 0.098 [-0.237, 0.942], loss: 8.519110, mean_absolute_error: 20.987835, mean_q: -26.453911
  103967/1500000: episode: 634, duration: 2.846s, episode steps: 241, steps per second: 85, episode reward: -368.657, mean reward: -1.530 [-100.000, 6.081], mean action: 1.701 [0.000, 3.000], mean observation: -0.065 [-1.926, 1.000], loss: 8.514327, mean_absolute_error: 20.986025, mean_q: -26.422916
  104104/1500000: episode: 635, duration: 1.910s, episode steps: 137, steps per second: 72, episode reward: -364.492, mean reward: -2.661 [-100.000, 3.373], mean action: 1.234 [0.000, 3.000], mean observation: -0.166 [-2.101, 1.408], loss: 9.745522, mean_absolute_error: 20.992847, mean_q: -26.452198
  104290/1500000: episode: 636, duration: 2.896s, episode steps: 186, steps per second: 64, episode reward: -475.885, mean reward: -2.559 [-100.000, 4.017], mean action: 1.683 [0.000, 3.000], mean observation: 0.144 [-1.698, 5.158], loss: 6.874297, mean_absolute_error: 21.191896, mean_q: -26.749413
  104430/1500000: episode: 637, duration: 5.086s, episode steps: 140, steps per second: 28, episode reward: -540.707, mean reward: -3.862 [-100.000, 5.148], mean action: 1.300 [0.000, 3.000], mean observation: 0.089 [-2.070, 7.435], loss: 5.866828, mean_absolute_error: 20.798513, mean_q: -26.203087
  104563/1500000: episode: 638, duration: 2.346s, episode steps: 133, steps per second: 57, episode reward: -490.270, mean reward: -3.686 [-100.000, 3.682], mean action: 1.376 [0.000, 3.000], mean observation: 0.195 [-2.000, 6.242], loss: 9.732412, mean_absolute_error: 21.610233, mean_q: -27.228603
  104685/1500000: episode: 639, duration: 1.484s, episode steps: 122, steps per second: 82, episode reward: -254.975, mean reward: -2.090 [-100.000, 10.530], mean action: 2.008 [0.000, 3.000], mean observation: 0.073 [-4.145, 1.217], loss: 8.819798, mean_absolute_error: 20.920616, mean_q: -26.339928
  104926/1500000: episode: 640, duration: 3.243s, episode steps: 241, steps per second: 74, episode reward: -553.032, mean reward: -2.295 [-100.000, 4.946], mean action: 1.755 [0.000, 3.000], mean observation: 0.122 [-1.982, 5.063], loss: 9.958143, mean_absolute_error: 20.553310, mean_q: -25.864975
  105104/1500000: episode: 641, duration: 2.104s, episode steps: 178, steps per second: 85, episode reward: -600.117, mean reward: -3.371 [-100.000, 3.034], mean action: 1.747 [0.000, 3.000], mean observation: 0.132 [-2.201, 3.820], loss: 8.654869, mean_absolute_error: 20.711878, mean_q: -26.002684
  105214/1500000: episode: 642, duration: 1.315s, episode steps: 110, steps per second: 84, episode reward: -395.557, mean reward: -3.596 [-100.000, 10.108], mean action: 1.845 [0.000, 3.000], mean observation: 0.119 [-1.900, 4.086], loss: 7.524710, mean_absolute_error: 21.371809, mean_q: -26.937754
  105294/1500000: episode: 643, duration: 1.070s, episode steps: 80, steps per second: 75, episode reward: -652.685, mean reward: -8.159 [-100.000, 2.176], mean action: 1.425 [0.000, 2.000], mean observation: 0.318 [-2.571, 7.318], loss: 8.985395, mean_absolute_error: 21.231329, mean_q: -26.706226
  105401/1500000: episode: 644, duration: 3.826s, episode steps: 107, steps per second: 28, episode reward: -586.760, mean reward: -5.484 [-100.000, 2.080], mean action: 1.654 [0.000, 3.000], mean observation: 0.261 [-2.364, 3.716], loss: 4.992786, mean_absolute_error: 20.603081, mean_q: -25.898821
  105530/1500000: episode: 645, duration: 2.858s, episode steps: 129, steps per second: 45, episode reward: -306.260, mean reward: -2.374 [-100.000, 84.824], mean action: 1.876 [0.000, 3.000], mean observation: 0.141 [-1.543, 3.545], loss: 7.681534, mean_absolute_error: 20.865330, mean_q: -26.207312
  105645/1500000: episode: 646, duration: 1.670s, episode steps: 115, steps per second: 69, episode reward: -603.214, mean reward: -5.245 [-100.000, 3.611], mean action: 1.539 [0.000, 3.000], mean observation: 0.180 [-2.225, 3.681], loss: 6.254482, mean_absolute_error: 21.167585, mean_q: -26.565384
  105748/1500000: episode: 647, duration: 1.277s, episode steps: 103, steps per second: 81, episode reward: -527.286, mean reward: -5.119 [-100.000, 2.230], mean action: 1.777 [0.000, 3.000], mean observation: 0.246 [-2.207, 3.192], loss: 8.074228, mean_absolute_error: 21.121191, mean_q: -26.478605
  105900/1500000: episode: 648, duration: 1.792s, episode steps: 152, steps per second: 85, episode reward: -506.252, mean reward: -3.331 [-100.000, 4.694], mean action: 1.882 [0.000, 3.000], mean observation: 0.165 [-2.335, 8.397], loss: 10.186496, mean_absolute_error: 21.290733, mean_q: -26.686478
  105989/1500000: episode: 649, duration: 1.091s, episode steps: 89, steps per second: 82, episode reward: -548.481, mean reward: -6.163 [-100.000, 2.362], mean action: 1.753 [0.000, 3.000], mean observation: 0.336 [-4.128, 3.647], loss: 9.430335, mean_absolute_error: 21.689777, mean_q: -27.216570
  106989/1500000: episode: 650, duration: 18.531s, episode steps: 1000, steps per second: 54, episode reward: -115.115, mean reward: -0.115 [-4.314, 4.946], mean action: 1.963 [0.000, 3.000], mean observation: 0.101 [-0.342, 0.930], loss: 7.745490, mean_absolute_error: 21.450949, mean_q: -26.923662
  107170/1500000: episode: 651, duration: 2.645s, episode steps: 181, steps per second: 68, episode reward: -201.499, mean reward: -1.113 [-100.000, 8.896], mean action: 2.033 [0.000, 3.000], mean observation: 0.010 [-1.368, 2.705], loss: 12.614007, mean_absolute_error: 21.604643, mean_q: -27.017954
  107347/1500000: episode: 652, duration: 5.407s, episode steps: 177, steps per second: 33, episode reward: -554.253, mean reward: -3.131 [-100.000, 97.639], mean action: 1.582 [0.000, 3.000], mean observation: 0.198 [-3.970, 4.825], loss: 8.028264, mean_absolute_error: 21.052481, mean_q: -26.345041
  107431/1500000: episode: 653, duration: 1.291s, episode steps: 84, steps per second: 65, episode reward: -677.748, mean reward: -8.068 [-100.000, 1.824], mean action: 1.119 [0.000, 3.000], mean observation: 0.434 [-1.801, 5.431], loss: 13.722487, mean_absolute_error: 21.488245, mean_q: -26.852371
  108431/1500000: episode: 654, duration: 19.793s, episode steps: 1000, steps per second: 51, episode reward: -108.406, mean reward: -0.108 [-4.133, 5.108], mean action: 1.964 [0.000, 3.000], mean observation: 0.104 [-0.402, 0.929], loss: 8.125506, mean_absolute_error: 21.495348, mean_q: -26.924023
  108696/1500000: episode: 655, duration: 3.471s, episode steps: 265, steps per second: 76, episode reward: -248.339, mean reward: -0.937 [-100.000, 7.202], mean action: 1.898 [0.000, 3.000], mean observation: -0.006 [-3.231, 1.000], loss: 8.068468, mean_absolute_error: 21.671286, mean_q: -27.058218
  108958/1500000: episode: 656, duration: 7.513s, episode steps: 262, steps per second: 35, episode reward: -53.094, mean reward: -0.203 [-100.000, 27.163], mean action: 1.840 [0.000, 3.000], mean observation: 0.140 [-1.051, 1.014], loss: 7.867624, mean_absolute_error: 21.522186, mean_q: -26.964050
  109226/1500000: episode: 657, duration: 3.882s, episode steps: 268, steps per second: 69, episode reward: -126.899, mean reward: -0.474 [-100.000, 26.213], mean action: 1.869 [0.000, 3.000], mean observation: 0.111 [-1.651, 1.000], loss: 8.596399, mean_absolute_error: 20.824802, mean_q: -26.046858
  109454/1500000: episode: 658, duration: 2.768s, episode steps: 228, steps per second: 82, episode reward: -249.088, mean reward: -1.092 [-100.000, 18.086], mean action: 2.022 [0.000, 3.000], mean observation: 0.060 [-0.879, 2.036], loss: 11.946987, mean_absolute_error: 20.880556, mean_q: -26.041193
  109810/1500000: episode: 659, duration: 9.426s, episode steps: 356, steps per second: 38, episode reward: -263.848, mean reward: -0.741 [-100.000, 28.968], mean action: 2.008 [0.000, 3.000], mean observation: 0.103 [-0.818, 1.889], loss: 8.569539, mean_absolute_error: 21.317209, mean_q: -26.588923
  110162/1500000: episode: 660, duration: 4.935s, episode steps: 352, steps per second: 71, episode reward: -202.060, mean reward: -0.574 [-100.000, 29.200], mean action: 1.929 [0.000, 3.000], mean observation: 0.032 [-0.802, 2.052], loss: 8.375831, mean_absolute_error: 21.184547, mean_q: -26.467962
  111162/1500000: episode: 661, duration: 26.910s, episode steps: 1000, steps per second: 37, episode reward: -126.729, mean reward: -0.127 [-3.682, 4.404], mean action: 1.951 [0.000, 3.000], mean observation: 0.118 [-0.305, 0.956], loss: 8.618785, mean_absolute_error: 20.778069, mean_q: -25.930077
  112162/1500000: episode: 662, duration: 29.378s, episode steps: 1000, steps per second: 34, episode reward: -125.057, mean reward: -0.125 [-3.654, 4.171], mean action: 1.953 [0.000, 3.000], mean observation: 0.132 [-0.344, 0.977], loss: 7.916736, mean_absolute_error: 20.699066, mean_q: -25.817221
  113091/1500000: episode: 663, duration: 21.102s, episode steps: 929, steps per second: 44, episode reward: 33.828, mean reward: 0.036 [-17.885, 100.000], mean action: 2.047 [0.000, 3.000], mean observation: 0.134 [-0.484, 1.000], loss: 8.645789, mean_absolute_error: 20.343103, mean_q: -25.198011
  113260/1500000: episode: 664, duration: 2.021s, episode steps: 169, steps per second: 84, episode reward: -119.365, mean reward: -0.706 [-100.000, 21.259], mean action: 1.970 [0.000, 3.000], mean observation: -0.052 [-1.012, 2.687], loss: 8.724360, mean_absolute_error: 20.461285, mean_q: -25.223713
  113757/1500000: episode: 665, duration: 14.970s, episode steps: 497, steps per second: 33, episode reward: -116.308, mean reward: -0.234 [-100.000, 37.684], mean action: 1.763 [0.000, 3.000], mean observation: 0.134 [-0.954, 2.275], loss: 12.256786, mean_absolute_error: 19.814445, mean_q: -24.462307
  113866/1500000: episode: 666, duration: 1.775s, episode steps: 109, steps per second: 61, episode reward: -294.139, mean reward: -2.699 [-100.000, 28.294], mean action: 0.991 [0.000, 3.000], mean observation: -0.086 [-4.928, 1.000], loss: 8.371197, mean_absolute_error: 18.895769, mean_q: -23.407619
  114866/1500000: episode: 667, duration: 31.396s, episode steps: 1000, steps per second: 32, episode reward: -87.569, mean reward: -0.088 [-3.189, 4.345], mean action: 1.895 [0.000, 3.000], mean observation: 0.120 [-0.168, 0.967], loss: 9.068826, mean_absolute_error: 19.018148, mean_q: -23.555456
  115290/1500000: episode: 668, duration: 8.960s, episode steps: 424, steps per second: 47, episode reward: 230.478, mean reward: 0.544 [-10.642, 100.000], mean action: 1.210 [0.000, 3.000], mean observation: 0.063 [-1.519, 1.300], loss: 9.023522, mean_absolute_error: 19.057640, mean_q: -23.584560
  116290/1500000: episode: 669, duration: 23.260s, episode steps: 1000, steps per second: 43, episode reward: -157.191, mean reward: -0.157 [-5.106, 5.799], mean action: 1.936 [0.000, 3.000], mean observation: 0.124 [-0.451, 1.166], loss: 15.507875, mean_absolute_error: 18.738344, mean_q: -23.103033
  117290/1500000: episode: 670, duration: 19.692s, episode steps: 1000, steps per second: 51, episode reward: -144.211, mean reward: -0.144 [-5.073, 4.102], mean action: 1.955 [0.000, 3.000], mean observation: 0.119 [-0.462, 0.930], loss: 10.677552, mean_absolute_error: 18.058781, mean_q: -22.301497
  118290/1500000: episode: 671, duration: 18.750s, episode steps: 1000, steps per second: 53, episode reward: -174.377, mean reward: -0.174 [-4.719, 3.827], mean action: 1.941 [0.000, 3.000], mean observation: 0.149 [-0.419, 1.101], loss: 10.924436, mean_absolute_error: 17.683285, mean_q: -21.819960
  118469/1500000: episode: 672, duration: 4.716s, episode steps: 179, steps per second: 38, episode reward: -216.692, mean reward: -1.211 [-100.000, 10.702], mean action: 1.765 [0.000, 3.000], mean observation: 0.098 [-1.288, 2.275], loss: 8.972224, mean_absolute_error: 17.506815, mean_q: -21.627686
  118724/1500000: episode: 673, duration: 4.411s, episode steps: 255, steps per second: 58, episode reward: -389.962, mean reward: -1.529 [-100.000, 5.898], mean action: 1.549 [0.000, 3.000], mean observation: 0.008 [-2.146, 1.271], loss: 11.814147, mean_absolute_error: 17.671700, mean_q: -21.829247
  118930/1500000: episode: 674, duration: 2.372s, episode steps: 206, steps per second: 87, episode reward: -342.670, mean reward: -1.663 [-100.000, 19.208], mean action: 1.393 [0.000, 3.000], mean observation: 0.117 [-1.390, 4.075], loss: 18.766312, mean_absolute_error: 17.389126, mean_q: -21.429640
  119157/1500000: episode: 675, duration: 2.908s, episode steps: 227, steps per second: 78, episode reward: -150.068, mean reward: -0.661 [-100.000, 7.892], mean action: 1.683 [0.000, 3.000], mean observation: 0.054 [-1.240, 1.240], loss: 10.501169, mean_absolute_error: 17.441435, mean_q: -21.553940
  120157/1500000: episode: 676, duration: 17.384s, episode steps: 1000, steps per second: 58, episode reward: -143.698, mean reward: -0.144 [-4.649, 4.818], mean action: 1.936 [0.000, 3.000], mean observation: 0.154 [-0.333, 1.117], loss: 12.237092, mean_absolute_error: 17.430630, mean_q: -21.479635
  121157/1500000: episode: 677, duration: 17.722s, episode steps: 1000, steps per second: 56, episode reward: -175.363, mean reward: -0.175 [-5.307, 3.657], mean action: 1.962 [0.000, 3.000], mean observation: 0.144 [-0.186, 1.000], loss: 9.974003, mean_absolute_error: 17.253376, mean_q: -21.273115
  122157/1500000: episode: 678, duration: 16.759s, episode steps: 1000, steps per second: 60, episode reward: -97.817, mean reward: -0.098 [-4.364, 5.606], mean action: 1.916 [0.000, 3.000], mean observation: 0.129 [-0.297, 0.940], loss: 9.187836, mean_absolute_error: 17.009386, mean_q: -20.921680
  122463/1500000: episode: 679, duration: 5.441s, episode steps: 306, steps per second: 56, episode reward: -129.428, mean reward: -0.423 [-100.000, 17.966], mean action: 1.889 [0.000, 3.000], mean observation: 0.036 [-0.893, 1.764], loss: 10.579354, mean_absolute_error: 16.957136, mean_q: -20.783266
  122863/1500000: episode: 680, duration: 6.776s, episode steps: 400, steps per second: 59, episode reward: -306.053, mean reward: -0.765 [-100.000, 6.363], mean action: 1.798 [0.000, 3.000], mean observation: 0.027 [-3.164, 1.000], loss: 11.364154, mean_absolute_error: 16.562025, mean_q: -20.295004
  123863/1500000: episode: 681, duration: 18.132s, episode steps: 1000, steps per second: 55, episode reward: -145.700, mean reward: -0.146 [-4.374, 4.146], mean action: 1.920 [0.000, 3.000], mean observation: 0.132 [-0.211, 0.934], loss: 9.389518, mean_absolute_error: 16.411623, mean_q: -20.103836
  124863/1500000: episode: 682, duration: 19.699s, episode steps: 1000, steps per second: 51, episode reward: -167.512, mean reward: -0.168 [-4.722, 3.783], mean action: 1.955 [0.000, 3.000], mean observation: 0.128 [-0.212, 0.940], loss: 13.020478, mean_absolute_error: 15.652349, mean_q: -19.115004
  125863/1500000: episode: 683, duration: 18.373s, episode steps: 1000, steps per second: 54, episode reward: -144.925, mean reward: -0.145 [-3.833, 4.463], mean action: 1.952 [0.000, 3.000], mean observation: 0.134 [-0.172, 0.938], loss: 10.749376, mean_absolute_error: 15.347795, mean_q: -18.679543
  126202/1500000: episode: 684, duration: 8.681s, episode steps: 339, steps per second: 39, episode reward: -26.456, mean reward: -0.078 [-100.000, 10.569], mean action: 1.773 [0.000, 3.000], mean observation: 0.090 [-0.775, 1.251], loss: 11.410729, mean_absolute_error: 15.257599, mean_q: -18.510334
  127202/1500000: episode: 685, duration: 17.881s, episode steps: 1000, steps per second: 56, episode reward: -108.623, mean reward: -0.109 [-5.123, 4.708], mean action: 1.958 [0.000, 3.000], mean observation: 0.100 [-0.678, 0.933], loss: 13.602514, mean_absolute_error: 14.784137, mean_q: -17.876385
  127417/1500000: episode: 686, duration: 2.474s, episode steps: 215, steps per second: 87, episode reward: -113.640, mean reward: -0.529 [-100.000, 14.496], mean action: 1.963 [0.000, 3.000], mean observation: 0.085 [-1.027, 1.000], loss: 10.658614, mean_absolute_error: 14.488917, mean_q: -17.527025
  128417/1500000: episode: 687, duration: 17.159s, episode steps: 1000, steps per second: 58, episode reward: -155.303, mean reward: -0.155 [-4.666, 4.016], mean action: 1.956 [0.000, 3.000], mean observation: 0.129 [-0.205, 0.961], loss: 13.443857, mean_absolute_error: 14.483648, mean_q: -17.471605
  129417/1500000: episode: 688, duration: 18.544s, episode steps: 1000, steps per second: 54, episode reward: -104.609, mean reward: -0.105 [-4.096, 4.461], mean action: 1.944 [0.000, 3.000], mean observation: 0.111 [-0.699, 0.938], loss: 10.130560, mean_absolute_error: 13.977432, mean_q: -16.798027
  130417/1500000: episode: 689, duration: 16.618s, episode steps: 1000, steps per second: 60, episode reward: -100.238, mean reward: -0.100 [-3.269, 4.852], mean action: 1.940 [0.000, 3.000], mean observation: 0.117 [-0.257, 0.934], loss: 11.009756, mean_absolute_error: 13.691062, mean_q: -16.376854
  131417/1500000: episode: 690, duration: 18.829s, episode steps: 1000, steps per second: 53, episode reward: -141.748, mean reward: -0.142 [-3.281, 3.686], mean action: 1.945 [0.000, 3.000], mean observation: 0.112 [-0.141, 0.949], loss: 13.319757, mean_absolute_error: 12.993489, mean_q: -15.426332
  132417/1500000: episode: 691, duration: 23.819s, episode steps: 1000, steps per second: 42, episode reward: -92.109, mean reward: -0.092 [-4.178, 4.646], mean action: 1.942 [0.000, 3.000], mean observation: 0.114 [-0.214, 0.943], loss: 11.823568, mean_absolute_error: 12.757656, mean_q: -15.057228
  133417/1500000: episode: 692, duration: 19.355s, episode steps: 1000, steps per second: 52, episode reward: -131.953, mean reward: -0.132 [-4.446, 4.377], mean action: 1.939 [0.000, 3.000], mean observation: 0.105 [-0.396, 0.937], loss: 10.458977, mean_absolute_error: 12.477144, mean_q: -14.664536
  134417/1500000: episode: 693, duration: 25.136s, episode steps: 1000, steps per second: 40, episode reward: -83.449, mean reward: -0.083 [-3.263, 5.542], mean action: 1.962 [0.000, 3.000], mean observation: 0.091 [-0.793, 0.934], loss: 12.490088, mean_absolute_error: 12.131678, mean_q: -14.145035
  135417/1500000: episode: 694, duration: 22.079s, episode steps: 1000, steps per second: 45, episode reward: -127.759, mean reward: -0.128 [-3.394, 3.826], mean action: 1.953 [0.000, 3.000], mean observation: 0.118 [-0.135, 0.997], loss: 11.770507, mean_absolute_error: 11.980881, mean_q: -13.956350
  136417/1500000: episode: 695, duration: 29.884s, episode steps: 1000, steps per second: 33, episode reward: -145.504, mean reward: -0.146 [-3.245, 4.221], mean action: 1.927 [0.000, 3.000], mean observation: 0.112 [-0.165, 0.935], loss: 13.582603, mean_absolute_error: 12.045342, mean_q: -13.918436
  137417/1500000: episode: 696, duration: 35.439s, episode steps: 1000, steps per second: 28, episode reward: -122.723, mean reward: -0.123 [-4.534, 4.679], mean action: 1.966 [0.000, 3.000], mean observation: 0.117 [-0.135, 0.942], loss: 13.859196, mean_absolute_error: 11.751276, mean_q: -13.524163
  138417/1500000: episode: 697, duration: 37.842s, episode steps: 1000, steps per second: 26, episode reward: -136.319, mean reward: -0.136 [-3.830, 3.314], mean action: 1.943 [0.000, 3.000], mean observation: 0.109 [-0.148, 0.946], loss: 9.925021, mean_absolute_error: 11.532721, mean_q: -13.200381
  139417/1500000: episode: 698, duration: 35.543s, episode steps: 1000, steps per second: 28, episode reward: -82.098, mean reward: -0.082 [-3.791, 5.524], mean action: 1.951 [0.000, 3.000], mean observation: 0.092 [-0.794, 0.944], loss: 14.466598, mean_absolute_error: 11.214389, mean_q: -12.642735
  140417/1500000: episode: 699, duration: 36.534s, episode steps: 1000, steps per second: 27, episode reward: -104.522, mean reward: -0.105 [-4.694, 5.739], mean action: 1.940 [0.000, 3.000], mean observation: 0.074 [-0.619, 0.933], loss: 14.376183, mean_absolute_error: 10.792957, mean_q: -11.873745
  141417/1500000: episode: 700, duration: 37.830s, episode steps: 1000, steps per second: 26, episode reward: -91.856, mean reward: -0.092 [-4.333, 5.180], mean action: 1.946 [0.000, 3.000], mean observation: 0.097 [-0.549, 1.002], loss: 14.828714, mean_absolute_error: 10.580407, mean_q: -11.368414
  142417/1500000: episode: 701, duration: 37.747s, episode steps: 1000, steps per second: 26, episode reward: -87.952, mean reward: -0.088 [-3.559, 6.064], mean action: 1.943 [0.000, 3.000], mean observation: 0.112 [-0.425, 0.950], loss: 13.203461, mean_absolute_error: 10.442727, mean_q: -10.965405
  143417/1500000: episode: 702, duration: 32.059s, episode steps: 1000, steps per second: 31, episode reward: -92.441, mean reward: -0.092 [-3.530, 4.451], mean action: 1.947 [0.000, 3.000], mean observation: 0.084 [-0.826, 0.935], loss: 13.579621, mean_absolute_error: 10.062385, mean_q: -10.246249
  144417/1500000: episode: 703, duration: 34.093s, episode steps: 1000, steps per second: 29, episode reward: -165.316, mean reward: -0.165 [-4.209, 4.059], mean action: 1.970 [0.000, 3.000], mean observation: 0.121 [-0.114, 0.942], loss: 17.044924, mean_absolute_error: 10.046890, mean_q: -10.094791
  145417/1500000: episode: 704, duration: 29.738s, episode steps: 1000, steps per second: 34, episode reward: -104.607, mean reward: -0.105 [-3.626, 3.714], mean action: 1.953 [0.000, 3.000], mean observation: 0.116 [-0.228, 0.935], loss: 12.154378, mean_absolute_error: 9.790569, mean_q: -9.646674
  146417/1500000: episode: 705, duration: 27.379s, episode steps: 1000, steps per second: 37, episode reward: -75.106, mean reward: -0.075 [-4.572, 5.961], mean action: 1.948 [0.000, 3.000], mean observation: 0.108 [-0.361, 0.932], loss: 10.540432, mean_absolute_error: 9.584109, mean_q: -9.200170
  147417/1500000: episode: 706, duration: 25.494s, episode steps: 1000, steps per second: 39, episode reward: -112.572, mean reward: -0.113 [-4.281, 4.073], mean action: 1.945 [0.000, 3.000], mean observation: 0.103 [-0.359, 0.929], loss: 13.827798, mean_absolute_error: 9.492413, mean_q: -9.027086
  148417/1500000: episode: 707, duration: 24.049s, episode steps: 1000, steps per second: 42, episode reward: -92.222, mean reward: -0.092 [-4.104, 4.604], mean action: 1.951 [0.000, 3.000], mean observation: 0.117 [-0.581, 0.966], loss: 14.557557, mean_absolute_error: 9.416286, mean_q: -8.709657
  149417/1500000: episode: 708, duration: 29.627s, episode steps: 1000, steps per second: 34, episode reward: -112.630, mean reward: -0.113 [-3.085, 4.420], mean action: 1.951 [0.000, 3.000], mean observation: 0.116 [-0.234, 0.952], loss: 15.878736, mean_absolute_error: 9.337191, mean_q: -8.460166
  150417/1500000: episode: 709, duration: 25.301s, episode steps: 1000, steps per second: 40, episode reward: -98.143, mean reward: -0.098 [-4.559, 4.505], mean action: 1.956 [0.000, 3.000], mean observation: 0.112 [-0.330, 0.931], loss: 14.665427, mean_absolute_error: 9.068422, mean_q: -8.020204
  151417/1500000: episode: 710, duration: 25.900s, episode steps: 1000, steps per second: 39, episode reward: -149.248, mean reward: -0.149 [-3.184, 4.216], mean action: 1.952 [0.000, 3.000], mean observation: 0.111 [-0.160, 0.949], loss: 12.004548, mean_absolute_error: 9.240110, mean_q: -8.336305
  152417/1500000: episode: 711, duration: 23.289s, episode steps: 1000, steps per second: 43, episode reward: -79.040, mean reward: -0.079 [-4.234, 4.456], mean action: 1.943 [0.000, 3.000], mean observation: 0.112 [-0.414, 0.930], loss: 10.295667, mean_absolute_error: 9.279934, mean_q: -8.272773
  153417/1500000: episode: 712, duration: 24.782s, episode steps: 1000, steps per second: 40, episode reward: -107.758, mean reward: -0.108 [-4.488, 4.731], mean action: 1.932 [0.000, 3.000], mean observation: 0.093 [-0.230, 0.959], loss: 13.359696, mean_absolute_error: 9.376503, mean_q: -8.097358
  154417/1500000: episode: 713, duration: 33.173s, episode steps: 1000, steps per second: 30, episode reward: -131.489, mean reward: -0.131 [-3.936, 4.254], mean action: 1.960 [0.000, 3.000], mean observation: 0.097 [-0.330, 0.957], loss: 15.035524, mean_absolute_error: 9.189629, mean_q: -7.460921
  155417/1500000: episode: 714, duration: 33.072s, episode steps: 1000, steps per second: 30, episode reward: -89.917, mean reward: -0.090 [-4.891, 5.801], mean action: 1.972 [0.000, 3.000], mean observation: 0.085 [-0.768, 0.946], loss: 14.308502, mean_absolute_error: 9.069653, mean_q: -6.999191
  156417/1500000: episode: 715, duration: 32.777s, episode steps: 1000, steps per second: 31, episode reward: -79.614, mean reward: -0.080 [-4.338, 5.064], mean action: 1.952 [0.000, 3.000], mean observation: 0.109 [-0.365, 0.933], loss: 12.617546, mean_absolute_error: 9.010283, mean_q: -6.789156
  157417/1500000: episode: 716, duration: 38.342s, episode steps: 1000, steps per second: 26, episode reward: -115.005, mean reward: -0.115 [-3.297, 4.212], mean action: 1.969 [0.000, 3.000], mean observation: 0.100 [-0.471, 0.947], loss: 13.273221, mean_absolute_error: 8.954107, mean_q: -6.459712
  158417/1500000: episode: 717, duration: 35.422s, episode steps: 1000, steps per second: 28, episode reward: -108.582, mean reward: -0.109 [-3.318, 4.109], mean action: 1.943 [0.000, 3.000], mean observation: 0.085 [-0.532, 0.932], loss: 17.191191, mean_absolute_error: 8.926955, mean_q: -6.195858
  159417/1500000: episode: 718, duration: 37.527s, episode steps: 1000, steps per second: 27, episode reward: -118.837, mean reward: -0.119 [-3.452, 4.526], mean action: 1.956 [0.000, 3.000], mean observation: 0.099 [-0.154, 0.986], loss: 18.639503, mean_absolute_error: 8.789119, mean_q: -5.747695
  160417/1500000: episode: 719, duration: 36.290s, episode steps: 1000, steps per second: 28, episode reward: -159.688, mean reward: -0.160 [-4.478, 4.142], mean action: 1.951 [0.000, 3.000], mean observation: 0.096 [-0.190, 0.935], loss: 12.530402, mean_absolute_error: 8.712927, mean_q: -5.193819
  161417/1500000: episode: 720, duration: 36.461s, episode steps: 1000, steps per second: 27, episode reward: -83.270, mean reward: -0.083 [-3.786, 5.044], mean action: 1.936 [0.000, 3.000], mean observation: 0.110 [-0.348, 0.964], loss: 11.847274, mean_absolute_error: 8.590235, mean_q: -4.893559
  162417/1500000: episode: 721, duration: 36.087s, episode steps: 1000, steps per second: 28, episode reward: -115.488, mean reward: -0.115 [-4.269, 4.532], mean action: 1.951 [0.000, 3.000], mean observation: 0.092 [-0.541, 0.924], loss: 14.212078, mean_absolute_error: 8.479131, mean_q: -4.685349
  163417/1500000: episode: 722, duration: 36.781s, episode steps: 1000, steps per second: 27, episode reward: -142.421, mean reward: -0.142 [-4.645, 4.149], mean action: 1.941 [0.000, 3.000], mean observation: 0.086 [-0.156, 0.936], loss: 14.310254, mean_absolute_error: 8.658170, mean_q: -4.863863
  164417/1500000: episode: 723, duration: 36.154s, episode steps: 1000, steps per second: 28, episode reward: -138.584, mean reward: -0.139 [-4.808, 3.657], mean action: 1.944 [0.000, 3.000], mean observation: 0.092 [-0.241, 0.933], loss: 12.630592, mean_absolute_error: 8.679885, mean_q: -4.562843
  165417/1500000: episode: 724, duration: 36.625s, episode steps: 1000, steps per second: 27, episode reward: -98.556, mean reward: -0.099 [-4.094, 4.889], mean action: 1.961 [0.000, 3.000], mean observation: 0.094 [-0.597, 1.020], loss: 11.660873, mean_absolute_error: 8.394919, mean_q: -3.674518
  166417/1500000: episode: 725, duration: 36.775s, episode steps: 1000, steps per second: 27, episode reward: -109.787, mean reward: -0.110 [-3.442, 3.912], mean action: 1.969 [0.000, 3.000], mean observation: 0.104 [-0.401, 0.994], loss: 12.225463, mean_absolute_error: 8.534517, mean_q: -3.666993
  167417/1500000: episode: 726, duration: 39.466s, episode steps: 1000, steps per second: 25, episode reward: -88.507, mean reward: -0.089 [-4.686, 4.412], mean action: 1.945 [0.000, 3.000], mean observation: 0.093 [-0.645, 0.982], loss: 15.814972, mean_absolute_error: 8.456776, mean_q: -3.140618
  168417/1500000: episode: 727, duration: 34.920s, episode steps: 1000, steps per second: 29, episode reward: -118.110, mean reward: -0.118 [-4.149, 4.556], mean action: 1.766 [0.000, 3.000], mean observation: 0.080 [-0.463, 0.926], loss: 13.368052, mean_absolute_error: 8.497336, mean_q: -2.813684
  169417/1500000: episode: 728, duration: 33.101s, episode steps: 1000, steps per second: 30, episode reward: -90.331, mean reward: -0.090 [-4.079, 4.172], mean action: 1.808 [0.000, 3.000], mean observation: 0.100 [-0.221, 0.952], loss: 11.918657, mean_absolute_error: 8.437502, mean_q: -2.478870
  170417/1500000: episode: 729, duration: 32.957s, episode steps: 1000, steps per second: 30, episode reward: -88.229, mean reward: -0.088 [-4.787, 4.433], mean action: 1.954 [0.000, 3.000], mean observation: 0.098 [-0.440, 0.927], loss: 12.180131, mean_absolute_error: 8.332503, mean_q: -1.972120
  171417/1500000: episode: 730, duration: 36.199s, episode steps: 1000, steps per second: 28, episode reward: -64.401, mean reward: -0.064 [-4.872, 6.683], mean action: 1.987 [0.000, 3.000], mean observation: 0.094 [-0.784, 0.976], loss: 13.002375, mean_absolute_error: 8.398170, mean_q: -2.191847
  172417/1500000: episode: 731, duration: 38.616s, episode steps: 1000, steps per second: 26, episode reward: -120.855, mean reward: -0.121 [-3.471, 4.455], mean action: 1.869 [0.000, 3.000], mean observation: 0.101 [-0.136, 0.950], loss: 13.416806, mean_absolute_error: 8.322864, mean_q: -2.012437
  173417/1500000: episode: 732, duration: 34.782s, episode steps: 1000, steps per second: 29, episode reward: -151.758, mean reward: -0.152 [-4.601, 3.945], mean action: 1.750 [0.000, 3.000], mean observation: 0.089 [-0.207, 0.954], loss: 16.093924, mean_absolute_error: 8.435682, mean_q: -1.710853
  174417/1500000: episode: 733, duration: 40.370s, episode steps: 1000, steps per second: 25, episode reward: -134.335, mean reward: -0.134 [-4.227, 4.614], mean action: 1.677 [0.000, 3.000], mean observation: 0.082 [-0.305, 0.941], loss: 12.315213, mean_absolute_error: 8.347471, mean_q: -1.277651
  175417/1500000: episode: 734, duration: 35.521s, episode steps: 1000, steps per second: 28, episode reward: -144.857, mean reward: -0.145 [-4.076, 4.042], mean action: 1.636 [0.000, 3.000], mean observation: 0.074 [-0.345, 0.943], loss: 12.020663, mean_absolute_error: 8.327972, mean_q: -0.999835
  176417/1500000: episode: 735, duration: 41.226s, episode steps: 1000, steps per second: 24, episode reward: -69.477, mean reward: -0.069 [-4.126, 4.885], mean action: 1.960 [0.000, 3.000], mean observation: 0.099 [-0.752, 0.963], loss: 10.427990, mean_absolute_error: 8.312125, mean_q: -0.997757
  177417/1500000: episode: 736, duration: 26.665s, episode steps: 1000, steps per second: 38, episode reward: -134.900, mean reward: -0.135 [-3.352, 3.756], mean action: 1.638 [0.000, 3.000], mean observation: 0.079 [-0.237, 0.957], loss: 11.953639, mean_absolute_error: 8.307466, mean_q: -1.094248
  178417/1500000: episode: 737, duration: 25.655s, episode steps: 1000, steps per second: 39, episode reward: -143.547, mean reward: -0.144 [-3.882, 4.105], mean action: 1.814 [0.000, 3.000], mean observation: 0.088 [-0.303, 0.942], loss: 12.275676, mean_absolute_error: 8.499648, mean_q: -1.320793
  179417/1500000: episode: 738, duration: 30.826s, episode steps: 1000, steps per second: 32, episode reward: -144.007, mean reward: -0.144 [-4.167, 4.112], mean action: 1.919 [0.000, 3.000], mean observation: 0.073 [-0.281, 0.934], loss: 13.047915, mean_absolute_error: 8.559610, mean_q: -1.353701
  180417/1500000: episode: 739, duration: 38.782s, episode steps: 1000, steps per second: 26, episode reward: -110.570, mean reward: -0.111 [-4.789, 5.556], mean action: 1.945 [0.000, 3.000], mean observation: 0.073 [-0.431, 0.927], loss: 11.461506, mean_absolute_error: 8.489209, mean_q: -0.937337
  181417/1500000: episode: 740, duration: 24.771s, episode steps: 1000, steps per second: 40, episode reward: -89.333, mean reward: -0.089 [-4.518, 5.008], mean action: 1.824 [0.000, 3.000], mean observation: 0.086 [-0.247, 0.996], loss: 10.896073, mean_absolute_error: 8.702754, mean_q: -0.663495
  182417/1500000: episode: 741, duration: 13.259s, episode steps: 1000, steps per second: 75, episode reward: -111.853, mean reward: -0.112 [-4.749, 4.460], mean action: 1.946 [0.000, 3.000], mean observation: 0.075 [-0.219, 0.939], loss: 17.674807, mean_absolute_error: 8.856895, mean_q: -0.674043
  183417/1500000: episode: 742, duration: 13.832s, episode steps: 1000, steps per second: 72, episode reward: -107.830, mean reward: -0.108 [-4.429, 5.049], mean action: 1.931 [0.000, 3.000], mean observation: 0.066 [-0.668, 0.945], loss: 11.392807, mean_absolute_error: 8.875607, mean_q: 0.073409
  184417/1500000: episode: 743, duration: 17.223s, episode steps: 1000, steps per second: 58, episode reward: -96.275, mean reward: -0.096 [-3.617, 6.706], mean action: 1.917 [0.000, 3.000], mean observation: 0.064 [-0.664, 0.951], loss: 15.253776, mean_absolute_error: 8.833373, mean_q: 0.516294
  185417/1500000: episode: 744, duration: 14.477s, episode steps: 1000, steps per second: 69, episode reward: -79.741, mean reward: -0.080 [-4.771, 5.062], mean action: 1.938 [0.000, 3.000], mean observation: 0.079 [-0.520, 0.925], loss: 11.249982, mean_absolute_error: 8.896130, mean_q: 0.732656
  186417/1500000: episode: 745, duration: 17.557s, episode steps: 1000, steps per second: 57, episode reward: -120.590, mean reward: -0.121 [-4.508, 3.692], mean action: 1.902 [0.000, 3.000], mean observation: 0.085 [-0.194, 0.935], loss: 9.569637, mean_absolute_error: 8.974690, mean_q: 0.600914
  187417/1500000: episode: 746, duration: 18.829s, episode steps: 1000, steps per second: 53, episode reward: -136.526, mean reward: -0.137 [-3.518, 4.678], mean action: 1.875 [0.000, 3.000], mean observation: 0.082 [-0.311, 0.932], loss: 12.525118, mean_absolute_error: 9.101649, mean_q: 0.502978
  188417/1500000: episode: 747, duration: 21.030s, episode steps: 1000, steps per second: 48, episode reward: -101.089, mean reward: -0.101 [-4.280, 5.255], mean action: 1.938 [0.000, 3.000], mean observation: 0.072 [-0.675, 0.927], loss: 10.863960, mean_absolute_error: 9.153572, mean_q: 0.505957
  189417/1500000: episode: 748, duration: 16.673s, episode steps: 1000, steps per second: 60, episode reward: -148.694, mean reward: -0.149 [-3.557, 4.086], mean action: 1.941 [0.000, 3.000], mean observation: 0.078 [-0.163, 0.941], loss: 12.910486, mean_absolute_error: 9.261994, mean_q: 0.787783
  190417/1500000: episode: 749, duration: 16.374s, episode steps: 1000, steps per second: 61, episode reward: -118.430, mean reward: -0.118 [-4.349, 3.848], mean action: 1.919 [0.000, 3.000], mean observation: 0.067 [-0.383, 0.929], loss: 11.323008, mean_absolute_error: 9.108395, mean_q: 1.249397
  191417/1500000: episode: 750, duration: 17.050s, episode steps: 1000, steps per second: 59, episode reward: -71.235, mean reward: -0.071 [-3.674, 6.180], mean action: 1.847 [0.000, 3.000], mean observation: 0.075 [-0.655, 1.007], loss: 9.135716, mean_absolute_error: 9.127256, mean_q: 1.339241
  192417/1500000: episode: 751, duration: 17.332s, episode steps: 1000, steps per second: 58, episode reward: -143.034, mean reward: -0.143 [-4.865, 4.299], mean action: 1.841 [0.000, 3.000], mean observation: 0.070 [-0.174, 0.941], loss: 15.242073, mean_absolute_error: 9.310355, mean_q: 1.443792
  193417/1500000: episode: 752, duration: 17.748s, episode steps: 1000, steps per second: 56, episode reward: -133.225, mean reward: -0.133 [-3.141, 4.292], mean action: 1.855 [0.000, 3.000], mean observation: 0.062 [-0.224, 0.939], loss: 9.629920, mean_absolute_error: 9.383759, mean_q: 1.498326
  194417/1500000: episode: 753, duration: 17.186s, episode steps: 1000, steps per second: 58, episode reward: -106.921, mean reward: -0.107 [-4.097, 5.353], mean action: 1.855 [0.000, 3.000], mean observation: 0.064 [-0.429, 0.927], loss: 9.542006, mean_absolute_error: 9.421064, mean_q: 1.854879
  195417/1500000: episode: 754, duration: 18.902s, episode steps: 1000, steps per second: 53, episode reward: -88.898, mean reward: -0.089 [-3.401, 4.050], mean action: 1.944 [0.000, 3.000], mean observation: 0.111 [-0.439, 0.927], loss: 12.256912, mean_absolute_error: 9.515074, mean_q: 1.623893
  196417/1500000: episode: 755, duration: 15.318s, episode steps: 1000, steps per second: 65, episode reward: -103.421, mean reward: -0.103 [-3.949, 4.216], mean action: 1.757 [0.000, 3.000], mean observation: 0.062 [-0.363, 0.980], loss: 12.591539, mean_absolute_error: 9.534558, mean_q: 1.771349
  197417/1500000: episode: 756, duration: 17.686s, episode steps: 1000, steps per second: 57, episode reward: -122.781, mean reward: -0.123 [-3.237, 3.857], mean action: 1.837 [0.000, 3.000], mean observation: 0.049 [-0.355, 0.930], loss: 8.830495, mean_absolute_error: 9.504465, mean_q: 2.118583
  198417/1500000: episode: 757, duration: 17.568s, episode steps: 1000, steps per second: 57, episode reward: -79.444, mean reward: -0.079 [-3.321, 4.126], mean action: 1.929 [0.000, 3.000], mean observation: 0.086 [-0.210, 0.938], loss: 12.326022, mean_absolute_error: 9.590692, mean_q: 2.285571
  199417/1500000: episode: 758, duration: 16.388s, episode steps: 1000, steps per second: 61, episode reward: -79.118, mean reward: -0.079 [-3.802, 4.281], mean action: 1.960 [0.000, 3.000], mean observation: 0.103 [-0.290, 0.932], loss: 8.524949, mean_absolute_error: 9.664570, mean_q: 2.372736
  200417/1500000: episode: 759, duration: 15.889s, episode steps: 1000, steps per second: 63, episode reward: -91.856, mean reward: -0.092 [-4.246, 4.276], mean action: 1.962 [0.000, 3.000], mean observation: 0.097 [-0.258, 0.935], loss: 11.933879, mean_absolute_error: 9.747454, mean_q: 2.461926
  201417/1500000: episode: 760, duration: 18.293s, episode steps: 1000, steps per second: 55, episode reward: -81.338, mean reward: -0.081 [-4.576, 4.532], mean action: 1.944 [0.000, 3.000], mean observation: 0.095 [-0.227, 0.987], loss: 10.996448, mean_absolute_error: 9.721674, mean_q: 2.448950
  202417/1500000: episode: 761, duration: 25.292s, episode steps: 1000, steps per second: 40, episode reward: -108.960, mean reward: -0.109 [-3.625, 4.096], mean action: 1.943 [0.000, 3.000], mean observation: 0.091 [-0.165, 0.944], loss: 13.027310, mean_absolute_error: 9.859116, mean_q: 2.329501
  203417/1500000: episode: 762, duration: 18.440s, episode steps: 1000, steps per second: 54, episode reward: -131.310, mean reward: -0.131 [-3.492, 4.716], mean action: 1.916 [0.000, 3.000], mean observation: 0.071 [-0.185, 0.937], loss: 10.521435, mean_absolute_error: 9.880627, mean_q: 2.814619
  204417/1500000: episode: 763, duration: 17.501s, episode steps: 1000, steps per second: 57, episode reward: -113.699, mean reward: -0.114 [-3.249, 3.960], mean action: 1.909 [0.000, 3.000], mean observation: 0.066 [-0.220, 0.948], loss: 9.756387, mean_absolute_error: 9.914290, mean_q: 3.476824
  205417/1500000: episode: 764, duration: 19.271s, episode steps: 1000, steps per second: 52, episode reward: -113.438, mean reward: -0.113 [-4.083, 3.799], mean action: 1.935 [0.000, 3.000], mean observation: 0.154 [-0.534, 0.925], loss: 9.126472, mean_absolute_error: 10.078433, mean_q: 3.706613
  206417/1500000: episode: 765, duration: 18.051s, episode steps: 1000, steps per second: 55, episode reward: -130.352, mean reward: -0.130 [-3.048, 4.209], mean action: 1.819 [0.000, 3.000], mean observation: 0.046 [-0.252, 0.941], loss: 10.390209, mean_absolute_error: 10.088996, mean_q: 3.879490
  207417/1500000: episode: 766, duration: 17.064s, episode steps: 1000, steps per second: 59, episode reward: -80.584, mean reward: -0.081 [-3.783, 4.033], mean action: 1.833 [0.000, 3.000], mean observation: 0.029 [-0.668, 0.931], loss: 10.770008, mean_absolute_error: 10.056714, mean_q: 4.223899
  208417/1500000: episode: 767, duration: 16.485s, episode steps: 1000, steps per second: 61, episode reward: -93.481, mean reward: -0.093 [-3.116, 3.974], mean action: 1.871 [0.000, 3.000], mean observation: 0.027 [-0.582, 0.924], loss: 11.702886, mean_absolute_error: 10.092405, mean_q: 4.415989
  209417/1500000: episode: 768, duration: 16.642s, episode steps: 1000, steps per second: 60, episode reward: -108.606, mean reward: -0.109 [-3.940, 3.833], mean action: 1.950 [0.000, 3.000], mean observation: 0.011 [-0.499, 0.929], loss: 9.920902, mean_absolute_error: 10.242340, mean_q: 4.606486
  210417/1500000: episode: 769, duration: 17.002s, episode steps: 1000, steps per second: 59, episode reward: -81.096, mean reward: -0.081 [-3.245, 5.303], mean action: 1.910 [0.000, 3.000], mean observation: 0.077 [-0.232, 0.942], loss: 9.961578, mean_absolute_error: 10.277942, mean_q: 4.960396
  211417/1500000: episode: 770, duration: 17.137s, episode steps: 1000, steps per second: 58, episode reward: -114.906, mean reward: -0.115 [-4.381, 4.423], mean action: 1.848 [0.000, 3.000], mean observation: 0.054 [-0.344, 0.936], loss: 9.902689, mean_absolute_error: 10.533384, mean_q: 4.962350
  212417/1500000: episode: 771, duration: 16.540s, episode steps: 1000, steps per second: 60, episode reward: -112.634, mean reward: -0.113 [-3.636, 4.200], mean action: 1.815 [0.000, 3.000], mean observation: 0.044 [-0.306, 0.935], loss: 9.737209, mean_absolute_error: 10.577522, mean_q: 5.260791
  213417/1500000: episode: 772, duration: 16.338s, episode steps: 1000, steps per second: 61, episode reward: -92.195, mean reward: -0.092 [-3.461, 4.552], mean action: 1.856 [0.000, 3.000], mean observation: 0.033 [-0.402, 0.930], loss: 10.924006, mean_absolute_error: 10.685837, mean_q: 5.450162
  214417/1500000: episode: 773, duration: 17.483s, episode steps: 1000, steps per second: 57, episode reward: -103.821, mean reward: -0.104 [-3.339, 4.149], mean action: 1.891 [0.000, 3.000], mean observation: 0.052 [-0.437, 0.952], loss: 12.033623, mean_absolute_error: 10.669580, mean_q: 5.851034
  215417/1500000: episode: 774, duration: 16.028s, episode steps: 1000, steps per second: 62, episode reward: -66.575, mean reward: -0.067 [-4.206, 5.848], mean action: 1.909 [0.000, 3.000], mean observation: 0.021 [-0.752, 0.930], loss: 10.710856, mean_absolute_error: 10.766391, mean_q: 6.074317
  216417/1500000: episode: 775, duration: 16.973s, episode steps: 1000, steps per second: 59, episode reward: -110.925, mean reward: -0.111 [-4.337, 4.215], mean action: 1.857 [0.000, 3.000], mean observation: 0.037 [-0.358, 0.943], loss: 10.262565, mean_absolute_error: 10.764641, mean_q: 6.665580
  217417/1500000: episode: 776, duration: 17.470s, episode steps: 1000, steps per second: 57, episode reward: -82.344, mean reward: -0.082 [-3.671, 5.337], mean action: 1.875 [0.000, 3.000], mean observation: 0.044 [-0.511, 0.988], loss: 9.394485, mean_absolute_error: 10.783159, mean_q: 6.826583
  218417/1500000: episode: 777, duration: 16.788s, episode steps: 1000, steps per second: 60, episode reward: -105.817, mean reward: -0.106 [-3.400, 4.908], mean action: 1.856 [0.000, 3.000], mean observation: 0.060 [-0.302, 0.941], loss: 11.270092, mean_absolute_error: 10.909543, mean_q: 6.835094
  219417/1500000: episode: 778, duration: 15.659s, episode steps: 1000, steps per second: 64, episode reward: -105.361, mean reward: -0.105 [-3.760, 4.793], mean action: 1.844 [0.000, 3.000], mean observation: 0.022 [-0.403, 0.942], loss: 8.023242, mean_absolute_error: 10.905449, mean_q: 7.056569
  220417/1500000: episode: 779, duration: 16.584s, episode steps: 1000, steps per second: 60, episode reward: -95.257, mean reward: -0.095 [-3.428, 4.026], mean action: 1.826 [0.000, 3.000], mean observation: 0.043 [-0.613, 0.997], loss: 9.755656, mean_absolute_error: 10.794767, mean_q: 6.957487
  221417/1500000: episode: 780, duration: 16.096s, episode steps: 1000, steps per second: 62, episode reward: -85.022, mean reward: -0.085 [-3.776, 6.196], mean action: 1.887 [0.000, 3.000], mean observation: 0.022 [-0.576, 0.934], loss: 9.529334, mean_absolute_error: 10.936029, mean_q: 6.821621
  222417/1500000: episode: 781, duration: 18.962s, episode steps: 1000, steps per second: 53, episode reward: -96.247, mean reward: -0.096 [-3.711, 4.617], mean action: 1.880 [0.000, 3.000], mean observation: 0.037 [-0.333, 0.975], loss: 9.826135, mean_absolute_error: 11.144914, mean_q: 7.165529
  223417/1500000: episode: 782, duration: 21.324s, episode steps: 1000, steps per second: 47, episode reward: -96.643, mean reward: -0.097 [-3.571, 4.381], mean action: 1.832 [0.000, 3.000], mean observation: 0.053 [-0.224, 0.961], loss: 11.459789, mean_absolute_error: 11.347818, mean_q: 7.449630
  224417/1500000: episode: 783, duration: 18.429s, episode steps: 1000, steps per second: 54, episode reward: -92.282, mean reward: -0.092 [-3.778, 4.525], mean action: 1.878 [0.000, 3.000], mean observation: 0.055 [-0.514, 0.925], loss: 13.211236, mean_absolute_error: 11.207510, mean_q: 7.476239
  225417/1500000: episode: 784, duration: 16.321s, episode steps: 1000, steps per second: 61, episode reward: -108.806, mean reward: -0.109 [-3.429, 4.041], mean action: 1.806 [0.000, 3.000], mean observation: 0.053 [-0.312, 0.931], loss: 13.983505, mean_absolute_error: 11.370641, mean_q: 8.101688
  226417/1500000: episode: 785, duration: 17.248s, episode steps: 1000, steps per second: 58, episode reward: -83.979, mean reward: -0.084 [-3.487, 4.154], mean action: 1.954 [0.000, 3.000], mean observation: 0.131 [-0.402, 0.929], loss: 11.705973, mean_absolute_error: 11.486289, mean_q: 8.187634
  227417/1500000: episode: 786, duration: 17.180s, episode steps: 1000, steps per second: 58, episode reward: -106.231, mean reward: -0.106 [-3.258, 4.241], mean action: 1.927 [0.000, 3.000], mean observation: 0.093 [-0.240, 0.933], loss: 10.139998, mean_absolute_error: 11.420320, mean_q: 7.821197
  228417/1500000: episode: 787, duration: 16.949s, episode steps: 1000, steps per second: 59, episode reward: -100.662, mean reward: -0.101 [-3.836, 4.594], mean action: 1.948 [0.000, 3.000], mean observation: 0.088 [-0.174, 0.936], loss: 11.770907, mean_absolute_error: 11.290315, mean_q: 7.549920
  229417/1500000: episode: 788, duration: 16.740s, episode steps: 1000, steps per second: 60, episode reward: -114.344, mean reward: -0.114 [-3.256, 4.529], mean action: 1.950 [0.000, 3.000], mean observation: 0.021 [-0.388, 0.929], loss: 11.880837, mean_absolute_error: 11.487542, mean_q: 7.624610
  230417/1500000: episode: 789, duration: 15.091s, episode steps: 1000, steps per second: 66, episode reward: -66.810, mean reward: -0.067 [-4.275, 5.746], mean action: 1.913 [0.000, 3.000], mean observation: 0.007 [-0.585, 0.926], loss: 9.943414, mean_absolute_error: 11.436229, mean_q: 7.586511
  231417/1500000: episode: 790, duration: 15.718s, episode steps: 1000, steps per second: 64, episode reward: -99.805, mean reward: -0.100 [-4.716, 4.238], mean action: 1.940 [0.000, 3.000], mean observation: 0.022 [-0.372, 0.930], loss: 9.998164, mean_absolute_error: 11.338337, mean_q: 7.559629
  232417/1500000: episode: 791, duration: 15.702s, episode steps: 1000, steps per second: 64, episode reward: -112.318, mean reward: -0.112 [-4.051, 4.643], mean action: 1.850 [0.000, 3.000], mean observation: 0.028 [-0.332, 0.969], loss: 13.145512, mean_absolute_error: 11.461088, mean_q: 8.107697
  233417/1500000: episode: 792, duration: 16.311s, episode steps: 1000, steps per second: 61, episode reward: -71.363, mean reward: -0.071 [-3.536, 4.421], mean action: 1.957 [0.000, 3.000], mean observation: 0.144 [-0.138, 0.991], loss: 10.134266, mean_absolute_error: 11.531030, mean_q: 7.927546
  234417/1500000: episode: 793, duration: 16.063s, episode steps: 1000, steps per second: 62, episode reward: -48.311, mean reward: -0.048 [-3.738, 5.709], mean action: 1.700 [0.000, 3.000], mean observation: 0.018 [-0.780, 0.926], loss: 9.211270, mean_absolute_error: 11.510576, mean_q: 7.832323
  235417/1500000: episode: 794, duration: 18.766s, episode steps: 1000, steps per second: 53, episode reward: -84.871, mean reward: -0.085 [-4.375, 3.986], mean action: 1.948 [0.000, 3.000], mean observation: 0.130 [-0.142, 0.953], loss: 10.672876, mean_absolute_error: 11.634243, mean_q: 8.177255
  236417/1500000: episode: 795, duration: 24.863s, episode steps: 1000, steps per second: 40, episode reward: -115.787, mean reward: -0.116 [-3.214, 4.071], mean action: 1.740 [0.000, 3.000], mean observation: 0.038 [-0.252, 0.950], loss: 7.472726, mean_absolute_error: 11.606261, mean_q: 8.716539
  237417/1500000: episode: 796, duration: 23.193s, episode steps: 1000, steps per second: 43, episode reward: -93.556, mean reward: -0.094 [-3.290, 4.139], mean action: 1.881 [0.000, 3.000], mean observation: 0.019 [-0.391, 0.975], loss: 9.046646, mean_absolute_error: 11.686086, mean_q: 8.700181
  238417/1500000: episode: 797, duration: 21.703s, episode steps: 1000, steps per second: 46, episode reward: -92.439, mean reward: -0.092 [-4.658, 4.661], mean action: 1.890 [0.000, 3.000], mean observation: 0.097 [-0.402, 0.978], loss: 9.634052, mean_absolute_error: 11.707457, mean_q: 8.905674
  239417/1500000: episode: 798, duration: 19.613s, episode steps: 1000, steps per second: 51, episode reward: -94.611, mean reward: -0.095 [-4.347, 4.957], mean action: 1.897 [0.000, 3.000], mean observation: 0.028 [-0.418, 0.997], loss: 10.853000, mean_absolute_error: 11.726209, mean_q: 8.683179
  240417/1500000: episode: 799, duration: 23.828s, episode steps: 1000, steps per second: 42, episode reward: -105.195, mean reward: -0.105 [-3.564, 4.743], mean action: 1.835 [0.000, 3.000], mean observation: 0.041 [-0.343, 0.940], loss: 9.160522, mean_absolute_error: 11.966575, mean_q: 8.842266
  241417/1500000: episode: 800, duration: 20.792s, episode steps: 1000, steps per second: 48, episode reward: -45.477, mean reward: -0.045 [-3.909, 6.111], mean action: 1.875 [0.000, 3.000], mean observation: 0.021 [-0.797, 0.929], loss: 8.782457, mean_absolute_error: 11.944299, mean_q: 9.259668
  242417/1500000: episode: 801, duration: 23.756s, episode steps: 1000, steps per second: 42, episode reward: -113.050, mean reward: -0.113 [-3.337, 4.563], mean action: 1.934 [0.000, 3.000], mean observation: 0.141 [-0.321, 0.931], loss: 8.490036, mean_absolute_error: 12.092516, mean_q: 9.622112
  243417/1500000: episode: 802, duration: 21.408s, episode steps: 1000, steps per second: 47, episode reward: -63.287, mean reward: -0.063 [-3.893, 4.707], mean action: 1.867 [0.000, 3.000], mean observation: 0.055 [-0.264, 0.998], loss: 8.556766, mean_absolute_error: 12.126949, mean_q: 10.018078
  243865/1500000: episode: 803, duration: 11.631s, episode steps: 448, steps per second: 39, episode reward: -146.077, mean reward: -0.326 [-100.000, 4.070], mean action: 1.752 [0.000, 3.000], mean observation: 0.164 [-0.451, 1.000], loss: 8.122444, mean_absolute_error: 12.234567, mean_q: 9.882731
  244865/1500000: episode: 804, duration: 21.894s, episode steps: 1000, steps per second: 46, episode reward: -73.580, mean reward: -0.074 [-3.728, 3.959], mean action: 1.940 [0.000, 3.000], mean observation: 0.111 [-0.257, 0.936], loss: 8.299695, mean_absolute_error: 12.039467, mean_q: 9.851906
  245865/1500000: episode: 805, duration: 25.172s, episode steps: 1000, steps per second: 40, episode reward: -98.948, mean reward: -0.099 [-4.657, 4.266], mean action: 1.925 [0.000, 3.000], mean observation: 0.139 [-0.329, 0.933], loss: 8.017748, mean_absolute_error: 12.161466, mean_q: 9.436961
  246865/1500000: episode: 806, duration: 22.890s, episode steps: 1000, steps per second: 44, episode reward: -101.442, mean reward: -0.101 [-4.356, 5.132], mean action: 1.925 [0.000, 3.000], mean observation: 0.031 [-0.327, 0.944], loss: 7.356392, mean_absolute_error: 11.981646, mean_q: 9.557945
  247865/1500000: episode: 807, duration: 26.231s, episode steps: 1000, steps per second: 38, episode reward: -60.368, mean reward: -0.060 [-3.718, 5.240], mean action: 1.917 [0.000, 3.000], mean observation: 0.034 [-0.568, 0.997], loss: 10.341750, mean_absolute_error: 12.017856, mean_q: 9.639341
  248865/1500000: episode: 808, duration: 26.054s, episode steps: 1000, steps per second: 38, episode reward: -83.849, mean reward: -0.084 [-4.148, 4.340], mean action: 1.954 [0.000, 3.000], mean observation: 0.019 [-0.414, 0.999], loss: 7.547627, mean_absolute_error: 12.169398, mean_q: 9.785473
  249865/1500000: episode: 809, duration: 27.591s, episode steps: 1000, steps per second: 36, episode reward: -103.751, mean reward: -0.104 [-3.495, 4.440], mean action: 1.923 [0.000, 3.000], mean observation: 0.041 [-0.224, 0.939], loss: 7.908039, mean_absolute_error: 12.210139, mean_q: 9.957005
  250865/1500000: episode: 810, duration: 24.458s, episode steps: 1000, steps per second: 41, episode reward: -87.273, mean reward: -0.087 [-3.394, 3.920], mean action: 1.903 [0.000, 3.000], mean observation: 0.066 [-0.180, 0.939], loss: 7.710337, mean_absolute_error: 12.402195, mean_q: 10.113748
  251865/1500000: episode: 811, duration: 37.979s, episode steps: 1000, steps per second: 26, episode reward: -76.915, mean reward: -0.077 [-3.248, 4.287], mean action: 1.786 [0.000, 3.000], mean observation: 0.029 [-0.333, 1.000], loss: 7.216864, mean_absolute_error: 12.346482, mean_q: 10.012245
  252865/1500000: episode: 812, duration: 38.778s, episode steps: 1000, steps per second: 26, episode reward: -90.934, mean reward: -0.091 [-3.186, 4.714], mean action: 1.761 [0.000, 3.000], mean observation: 0.022 [-0.460, 0.927], loss: 11.235113, mean_absolute_error: 12.378403, mean_q: 9.690105
  253865/1500000: episode: 813, duration: 32.753s, episode steps: 1000, steps per second: 31, episode reward: -78.622, mean reward: -0.079 [-3.929, 4.209], mean action: 1.916 [0.000, 3.000], mean observation: 0.037 [-0.509, 0.925], loss: 7.469398, mean_absolute_error: 12.345490, mean_q: 9.937148
  254865/1500000: episode: 814, duration: 37.996s, episode steps: 1000, steps per second: 26, episode reward: -96.813, mean reward: -0.097 [-3.087, 4.307], mean action: 1.943 [0.000, 3.000], mean observation: 0.118 [-0.152, 0.966], loss: 7.646923, mean_absolute_error: 12.364755, mean_q: 9.704463
  255608/1500000: episode: 815, duration: 27.928s, episode steps: 743, steps per second: 27, episode reward: -212.201, mean reward: -0.286 [-100.000, 4.189], mean action: 1.946 [0.000, 3.000], mean observation: 0.153 [-0.379, 1.001], loss: 7.486857, mean_absolute_error: 12.516085, mean_q: 9.576150
  256608/1500000: episode: 816, duration: 44.569s, episode steps: 1000, steps per second: 22, episode reward: -85.458, mean reward: -0.085 [-4.535, 4.293], mean action: 1.895 [0.000, 3.000], mean observation: 0.042 [-0.266, 0.932], loss: 6.992455, mean_absolute_error: 12.666959, mean_q: 9.748796
  257608/1500000: episode: 817, duration: 38.485s, episode steps: 1000, steps per second: 26, episode reward: -97.562, mean reward: -0.098 [-4.280, 5.184], mean action: 1.949 [0.000, 3.000], mean observation: 0.113 [-0.310, 0.936], loss: 8.022318, mean_absolute_error: 12.753390, mean_q: 10.176782
  258608/1500000: episode: 818, duration: 37.109s, episode steps: 1000, steps per second: 27, episode reward: -93.533, mean reward: -0.094 [-3.565, 5.151], mean action: 1.851 [0.000, 3.000], mean observation: 0.028 [-0.388, 0.991], loss: 7.741432, mean_absolute_error: 12.928519, mean_q: 10.469172
  259608/1500000: episode: 819, duration: 36.712s, episode steps: 1000, steps per second: 27, episode reward: -62.894, mean reward: -0.063 [-3.656, 4.890], mean action: 1.922 [0.000, 3.000], mean observation: 0.004 [-0.576, 0.942], loss: 7.341715, mean_absolute_error: 13.051377, mean_q: 10.847894
  260608/1500000: episode: 820, duration: 40.922s, episode steps: 1000, steps per second: 24, episode reward: -57.504, mean reward: -0.058 [-3.130, 4.311], mean action: 1.825 [0.000, 3.000], mean observation: 0.057 [-0.267, 0.953], loss: 10.142763, mean_absolute_error: 13.358643, mean_q: 11.094893
  261608/1500000: episode: 821, duration: 38.683s, episode steps: 1000, steps per second: 26, episode reward: -103.397, mean reward: -0.103 [-3.471, 5.288], mean action: 1.900 [0.000, 3.000], mean observation: 0.031 [-0.328, 0.956], loss: 7.658849, mean_absolute_error: 13.362556, mean_q: 11.206070
  262608/1500000: episode: 822, duration: 34.805s, episode steps: 1000, steps per second: 29, episode reward: -90.684, mean reward: -0.091 [-3.285, 4.455], mean action: 1.883 [0.000, 3.000], mean observation: 0.012 [-0.385, 0.942], loss: 8.441907, mean_absolute_error: 13.280724, mean_q: 11.242733
  263608/1500000: episode: 823, duration: 35.314s, episode steps: 1000, steps per second: 28, episode reward: -103.456, mean reward: -0.103 [-3.261, 4.160], mean action: 1.956 [0.000, 3.000], mean observation: 0.017 [-0.362, 0.935], loss: 6.257569, mean_absolute_error: 13.459275, mean_q: 11.583233
  264608/1500000: episode: 824, duration: 35.565s, episode steps: 1000, steps per second: 28, episode reward: -61.338, mean reward: -0.061 [-3.229, 4.539], mean action: 1.911 [0.000, 3.000], mean observation: 0.016 [-0.734, 0.930], loss: 6.301624, mean_absolute_error: 13.656503, mean_q: 11.932606
  265608/1500000: episode: 825, duration: 37.612s, episode steps: 1000, steps per second: 27, episode reward: -72.107, mean reward: -0.072 [-3.431, 4.341], mean action: 1.591 [0.000, 3.000], mean observation: 0.013 [-0.367, 1.007], loss: 7.755329, mean_absolute_error: 13.939448, mean_q: 12.171135
  266608/1500000: episode: 826, duration: 36.405s, episode steps: 1000, steps per second: 27, episode reward: -25.006, mean reward: -0.025 [-4.521, 4.635], mean action: 1.809 [0.000, 3.000], mean observation: -0.001 [-0.797, 0.930], loss: 8.016707, mean_absolute_error: 14.109764, mean_q: 12.576498
  267608/1500000: episode: 827, duration: 44.634s, episode steps: 1000, steps per second: 22, episode reward: -90.938, mean reward: -0.091 [-3.147, 4.109], mean action: 1.917 [0.000, 3.000], mean observation: 0.010 [-0.458, 0.937], loss: 9.254547, mean_absolute_error: 14.014932, mean_q: 12.369194
  268608/1500000: episode: 828, duration: 45.655s, episode steps: 1000, steps per second: 22, episode reward: -77.921, mean reward: -0.078 [-3.223, 4.368], mean action: 1.952 [0.000, 3.000], mean observation: 0.022 [-0.415, 0.928], loss: 6.838339, mean_absolute_error: 13.805014, mean_q: 12.078224
  269608/1500000: episode: 829, duration: 42.711s, episode steps: 1000, steps per second: 23, episode reward: -36.914, mean reward: -0.037 [-3.785, 6.069], mean action: 1.978 [0.000, 3.000], mean observation: 0.036 [-0.736, 0.987], loss: 6.950703, mean_absolute_error: 13.925764, mean_q: 12.429472
  270608/1500000: episode: 830, duration: 41.818s, episode steps: 1000, steps per second: 24, episode reward: -78.272, mean reward: -0.078 [-3.307, 4.260], mean action: 1.915 [0.000, 3.000], mean observation: 0.019 [-0.337, 0.980], loss: 5.669560, mean_absolute_error: 14.057731, mean_q: 12.805281
  271608/1500000: episode: 831, duration: 39.251s, episode steps: 1000, steps per second: 25, episode reward: -71.697, mean reward: -0.072 [-3.205, 4.358], mean action: 1.928 [0.000, 3.000], mean observation: 0.054 [-0.177, 0.948], loss: 8.481981, mean_absolute_error: 14.277609, mean_q: 13.100089
  272608/1500000: episode: 832, duration: 38.408s, episode steps: 1000, steps per second: 26, episode reward: -109.350, mean reward: -0.109 [-3.279, 4.454], mean action: 1.712 [0.000, 3.000], mean observation: 0.012 [-0.370, 0.942], loss: 7.023993, mean_absolute_error: 14.506201, mean_q: 13.090999
  273608/1500000: episode: 833, duration: 39.072s, episode steps: 1000, steps per second: 26, episode reward: -75.959, mean reward: -0.076 [-3.293, 4.222], mean action: 1.938 [0.000, 3.000], mean observation: -0.000 [-0.475, 0.966], loss: 7.710470, mean_absolute_error: 14.615863, mean_q: 13.433755
  274608/1500000: episode: 834, duration: 41.465s, episode steps: 1000, steps per second: 24, episode reward: -58.496, mean reward: -0.058 [-3.158, 4.793], mean action: 1.903 [0.000, 3.000], mean observation: 0.015 [-0.494, 0.932], loss: 7.277522, mean_absolute_error: 14.689427, mean_q: 13.531813
  275608/1500000: episode: 835, duration: 39.736s, episode steps: 1000, steps per second: 25, episode reward: -72.690, mean reward: -0.073 [-3.217, 4.351], mean action: 1.621 [0.000, 3.000], mean observation: 0.017 [-0.394, 0.970], loss: 8.669346, mean_absolute_error: 14.738598, mean_q: 13.546880
  276608/1500000: episode: 836, duration: 39.460s, episode steps: 1000, steps per second: 25, episode reward: -52.841, mean reward: -0.053 [-3.076, 4.480], mean action: 1.545 [0.000, 3.000], mean observation: 0.034 [-0.339, 1.015], loss: 7.175988, mean_absolute_error: 14.668653, mean_q: 13.681474
  277608/1500000: episode: 837, duration: 38.277s, episode steps: 1000, steps per second: 26, episode reward: -75.257, mean reward: -0.075 [-3.207, 4.097], mean action: 1.844 [0.000, 3.000], mean observation: 0.022 [-0.364, 1.002], loss: 6.646010, mean_absolute_error: 14.590552, mean_q: 13.773028
  278608/1500000: episode: 838, duration: 37.642s, episode steps: 1000, steps per second: 27, episode reward: -68.304, mean reward: -0.068 [-3.247, 4.494], mean action: 1.972 [0.000, 3.000], mean observation: 0.021 [-0.484, 0.926], loss: 5.637358, mean_absolute_error: 14.491646, mean_q: 13.242762
  279608/1500000: episode: 839, duration: 37.105s, episode steps: 1000, steps per second: 27, episode reward: -76.657, mean reward: -0.077 [-3.639, 4.235], mean action: 1.931 [0.000, 3.000], mean observation: 0.015 [-0.379, 0.942], loss: 6.005748, mean_absolute_error: 14.555529, mean_q: 13.350943
  280608/1500000: episode: 840, duration: 39.208s, episode steps: 1000, steps per second: 26, episode reward: -52.100, mean reward: -0.052 [-3.282, 4.238], mean action: 1.956 [0.000, 3.000], mean observation: 0.042 [-0.301, 1.010], loss: 8.765655, mean_absolute_error: 14.584120, mean_q: 13.627348
  281608/1500000: episode: 841, duration: 40.443s, episode steps: 1000, steps per second: 25, episode reward: -84.285, mean reward: -0.084 [-3.202, 4.379], mean action: 1.914 [0.000, 3.000], mean observation: 0.016 [-0.412, 0.928], loss: 5.979410, mean_absolute_error: 14.596471, mean_q: 13.904787
  282608/1500000: episode: 842, duration: 42.337s, episode steps: 1000, steps per second: 24, episode reward: -69.380, mean reward: -0.069 [-3.164, 4.942], mean action: 1.908 [0.000, 3.000], mean observation: 0.024 [-0.406, 0.997], loss: 8.092416, mean_absolute_error: 14.620460, mean_q: 13.956974
  283608/1500000: episode: 843, duration: 38.138s, episode steps: 1000, steps per second: 26, episode reward: -58.545, mean reward: -0.059 [-4.424, 4.365], mean action: 1.843 [0.000, 3.000], mean observation: 0.003 [-0.703, 0.944], loss: 7.196483, mean_absolute_error: 14.623891, mean_q: 13.935705
  284608/1500000: episode: 844, duration: 38.361s, episode steps: 1000, steps per second: 26, episode reward: -61.384, mean reward: -0.061 [-3.278, 4.621], mean action: 1.692 [0.000, 3.000], mean observation: 0.003 [-0.423, 0.928], loss: 7.089494, mean_absolute_error: 14.735546, mean_q: 13.877680
  284962/1500000: episode: 845, duration: 12.498s, episode steps: 354, steps per second: 28, episode reward: -130.888, mean reward: -0.370 [-100.000, 3.834], mean action: 1.980 [0.000, 3.000], mean observation: 0.188 [-0.307, 1.001], loss: 4.952354, mean_absolute_error: 14.769166, mean_q: 14.346437
  285962/1500000: episode: 846, duration: 40.367s, episode steps: 1000, steps per second: 25, episode reward: -85.772, mean reward: -0.086 [-3.310, 4.150], mean action: 1.665 [0.000, 3.000], mean observation: 0.008 [-0.387, 0.929], loss: 7.406256, mean_absolute_error: 14.856616, mean_q: 14.050602
  286962/1500000: episode: 847, duration: 38.969s, episode steps: 1000, steps per second: 26, episode reward: -88.869, mean reward: -0.089 [-3.152, 5.084], mean action: 1.754 [0.000, 3.000], mean observation: -0.008 [-0.482, 0.962], loss: 6.645809, mean_absolute_error: 14.865787, mean_q: 13.996034
  287962/1500000: episode: 848, duration: 29.135s, episode steps: 1000, steps per second: 34, episode reward: -84.998, mean reward: -0.085 [-13.570, 11.914], mean action: 1.786 [0.000, 3.000], mean observation: 0.009 [-0.476, 1.000], loss: 5.963038, mean_absolute_error: 15.141743, mean_q: 14.342436
  288962/1500000: episode: 849, duration: 31.019s, episode steps: 1000, steps per second: 32, episode reward: -82.641, mean reward: -0.083 [-3.208, 4.446], mean action: 1.882 [0.000, 3.000], mean observation: -0.012 [-0.487, 0.926], loss: 7.778516, mean_absolute_error: 15.293495, mean_q: 15.056432
  289962/1500000: episode: 850, duration: 29.803s, episode steps: 1000, steps per second: 34, episode reward: -104.307, mean reward: -0.104 [-3.188, 5.022], mean action: 1.757 [0.000, 3.000], mean observation: 0.013 [-0.429, 0.949], loss: 8.992053, mean_absolute_error: 15.339582, mean_q: 15.145049
  290962/1500000: episode: 851, duration: 30.877s, episode steps: 1000, steps per second: 32, episode reward: -59.943, mean reward: -0.060 [-3.030, 3.995], mean action: 1.600 [0.000, 3.000], mean observation: 0.054 [-0.365, 0.932], loss: 8.468537, mean_absolute_error: 15.099599, mean_q: 14.692612
  291962/1500000: episode: 852, duration: 28.753s, episode steps: 1000, steps per second: 35, episode reward: -69.417, mean reward: -0.069 [-3.065, 4.240], mean action: 1.922 [0.000, 3.000], mean observation: -0.018 [-0.627, 0.931], loss: 7.397151, mean_absolute_error: 15.143650, mean_q: 14.701008
  292962/1500000: episode: 853, duration: 28.302s, episode steps: 1000, steps per second: 35, episode reward: -91.373, mean reward: -0.091 [-3.168, 5.326], mean action: 1.859 [0.000, 3.000], mean observation: 0.015 [-0.423, 0.935], loss: 6.866605, mean_absolute_error: 15.026694, mean_q: 14.522346
  293232/1500000: episode: 854, duration: 7.036s, episode steps: 270, steps per second: 38, episode reward: -166.288, mean reward: -0.616 [-100.000, 4.362], mean action: 1.548 [0.000, 3.000], mean observation: 0.168 [-0.776, 1.001], loss: 7.711164, mean_absolute_error: 14.909178, mean_q: 14.005113
  294232/1500000: episode: 855, duration: 27.303s, episode steps: 1000, steps per second: 37, episode reward: -84.047, mean reward: -0.084 [-3.125, 4.147], mean action: 1.720 [0.000, 3.000], mean observation: 0.005 [-0.496, 0.979], loss: 6.407623, mean_absolute_error: 14.862956, mean_q: 13.930396
  295232/1500000: episode: 856, duration: 27.370s, episode steps: 1000, steps per second: 37, episode reward: -49.059, mean reward: -0.049 [-3.318, 5.089], mean action: 1.886 [0.000, 3.000], mean observation: 0.003 [-0.525, 0.926], loss: 5.361483, mean_absolute_error: 14.681004, mean_q: 13.999043
  296232/1500000: episode: 857, duration: 31.765s, episode steps: 1000, steps per second: 31, episode reward: -44.353, mean reward: -0.044 [-3.180, 3.833], mean action: 1.933 [0.000, 3.000], mean observation: -0.012 [-0.837, 0.924], loss: 6.392363, mean_absolute_error: 14.640405, mean_q: 13.814285
  297232/1500000: episode: 858, duration: 30.412s, episode steps: 1000, steps per second: 33, episode reward: -97.981, mean reward: -0.098 [-2.969, 4.377], mean action: 1.747 [0.000, 3.000], mean observation: 0.018 [-0.539, 0.946], loss: 4.917293, mean_absolute_error: 14.388480, mean_q: 14.061388
  298232/1500000: episode: 859, duration: 29.881s, episode steps: 1000, steps per second: 33, episode reward: -30.941, mean reward: -0.031 [-3.172, 4.402], mean action: 1.541 [0.000, 3.000], mean observation: 0.058 [-0.246, 1.027], loss: 7.026731, mean_absolute_error: 14.600966, mean_q: 14.067681
  299232/1500000: episode: 860, duration: 27.880s, episode steps: 1000, steps per second: 36, episode reward: -51.803, mean reward: -0.052 [-3.028, 4.281], mean action: 1.649 [0.000, 3.000], mean observation: 0.000 [-0.539, 1.020], loss: 6.166386, mean_absolute_error: 14.489722, mean_q: 13.952970
  300232/1500000: episode: 861, duration: 34.502s, episode steps: 1000, steps per second: 29, episode reward: -58.765, mean reward: -0.059 [-2.982, 4.171], mean action: 1.921 [0.000, 3.000], mean observation: -0.017 [-0.689, 0.926], loss: 4.953374, mean_absolute_error: 14.730385, mean_q: 14.161325
  301232/1500000: episode: 862, duration: 30.682s, episode steps: 1000, steps per second: 33, episode reward: -89.565, mean reward: -0.090 [-3.401, 4.465], mean action: 1.646 [0.000, 3.000], mean observation: 0.006 [-0.485, 0.946], loss: 5.428748, mean_absolute_error: 14.688663, mean_q: 14.392058
  302232/1500000: episode: 863, duration: 30.437s, episode steps: 1000, steps per second: 33, episode reward: -37.795, mean reward: -0.038 [-3.401, 5.304], mean action: 1.890 [0.000, 3.000], mean observation: 0.030 [-0.683, 0.993], loss: 5.289472, mean_absolute_error: 14.581649, mean_q: 14.613106
  303232/1500000: episode: 864, duration: 27.280s, episode steps: 1000, steps per second: 37, episode reward: -26.369, mean reward: -0.026 [-3.699, 5.336], mean action: 1.909 [0.000, 3.000], mean observation: 0.076 [-0.842, 1.005], loss: 4.747642, mean_absolute_error: 14.440488, mean_q: 14.298643
  304232/1500000: episode: 865, duration: 28.245s, episode steps: 1000, steps per second: 35, episode reward: -49.690, mean reward: -0.050 [-3.015, 4.382], mean action: 1.775 [0.000, 3.000], mean observation: 0.090 [-0.243, 0.938], loss: 6.151147, mean_absolute_error: 14.427764, mean_q: 14.169396
  305232/1500000: episode: 866, duration: 30.606s, episode steps: 1000, steps per second: 33, episode reward: -91.519, mean reward: -0.092 [-2.976, 4.918], mean action: 1.936 [0.000, 3.000], mean observation: 0.026 [-0.443, 0.941], loss: 6.700462, mean_absolute_error: 14.371943, mean_q: 14.291617
  306232/1500000: episode: 867, duration: 29.490s, episode steps: 1000, steps per second: 34, episode reward: -43.544, mean reward: -0.044 [-3.140, 4.652], mean action: 1.857 [0.000, 3.000], mean observation: 0.067 [-0.202, 0.941], loss: 5.446584, mean_absolute_error: 14.412209, mean_q: 14.205441
  307077/1500000: episode: 868, duration: 22.904s, episode steps: 845, steps per second: 37, episode reward: -244.561, mean reward: -0.289 [-100.000, 6.467], mean action: 1.806 [0.000, 3.000], mean observation: 0.117 [-3.068, 1.000], loss: 5.630455, mean_absolute_error: 14.288249, mean_q: 14.391437
  308077/1500000: episode: 869, duration: 31.829s, episode steps: 1000, steps per second: 31, episode reward: -78.441, mean reward: -0.078 [-3.379, 4.077], mean action: 1.942 [0.000, 3.000], mean observation: 0.009 [-0.444, 0.931], loss: 3.949416, mean_absolute_error: 14.477534, mean_q: 14.198365
  309014/1500000: episode: 870, duration: 17.381s, episode steps: 937, steps per second: 54, episode reward: -255.496, mean reward: -0.273 [-100.000, 10.949], mean action: 1.935 [0.000, 3.000], mean observation: 0.114 [-0.289, 1.369], loss: 6.661417, mean_absolute_error: 14.477037, mean_q: 13.724282
  310014/1500000: episode: 871, duration: 13.996s, episode steps: 1000, steps per second: 71, episode reward: -63.960, mean reward: -0.064 [-3.118, 4.290], mean action: 1.954 [0.000, 3.000], mean observation: 0.004 [-0.671, 0.930], loss: 5.842532, mean_absolute_error: 14.534307, mean_q: 13.775706
  311014/1500000: episode: 872, duration: 13.111s, episode steps: 1000, steps per second: 76, episode reward: -75.154, mean reward: -0.075 [-3.307, 4.729], mean action: 1.779 [0.000, 3.000], mean observation: 0.040 [-0.212, 0.939], loss: 4.563468, mean_absolute_error: 14.595148, mean_q: 14.294885
  312014/1500000: episode: 873, duration: 13.186s, episode steps: 1000, steps per second: 76, episode reward: -74.144, mean reward: -0.074 [-3.068, 4.203], mean action: 1.728 [0.000, 3.000], mean observation: 0.031 [-0.335, 0.963], loss: 4.716058, mean_absolute_error: 14.677950, mean_q: 14.539515
  313014/1500000: episode: 874, duration: 15.268s, episode steps: 1000, steps per second: 65, episode reward: -75.451, mean reward: -0.075 [-3.012, 4.608], mean action: 1.834 [0.000, 3.000], mean observation: 0.011 [-0.595, 1.029], loss: 4.272380, mean_absolute_error: 14.878705, mean_q: 14.767399
  314014/1500000: episode: 875, duration: 12.371s, episode steps: 1000, steps per second: 81, episode reward: -69.149, mean reward: -0.069 [-3.383, 4.304], mean action: 1.873 [0.000, 3.000], mean observation: 0.003 [-0.659, 0.932], loss: 4.892116, mean_absolute_error: 14.883127, mean_q: 14.965631
  315014/1500000: episode: 876, duration: 15.585s, episode steps: 1000, steps per second: 64, episode reward: -77.233, mean reward: -0.077 [-3.009, 4.619], mean action: 1.887 [0.000, 3.000], mean observation: -0.011 [-0.659, 0.929], loss: 4.882652, mean_absolute_error: 14.898397, mean_q: 15.105899
  316014/1500000: episode: 877, duration: 16.081s, episode steps: 1000, steps per second: 62, episode reward: -95.187, mean reward: -0.095 [-3.156, 4.124], mean action: 1.853 [0.000, 3.000], mean observation: 0.009 [-0.504, 0.955], loss: 9.542853, mean_absolute_error: 14.956478, mean_q: 15.393674
  317014/1500000: episode: 878, duration: 15.389s, episode steps: 1000, steps per second: 65, episode reward: -48.434, mean reward: -0.048 [-3.271, 4.217], mean action: 1.489 [0.000, 3.000], mean observation: 0.078 [-0.160, 0.942], loss: 6.635833, mean_absolute_error: 14.967240, mean_q: 15.562499
  318014/1500000: episode: 879, duration: 16.979s, episode steps: 1000, steps per second: 59, episode reward: -75.993, mean reward: -0.076 [-3.157, 4.683], mean action: 1.828 [0.000, 3.000], mean observation: -0.001 [-0.572, 0.925], loss: 4.408501, mean_absolute_error: 14.749669, mean_q: 15.203268
  319014/1500000: episode: 880, duration: 16.749s, episode steps: 1000, steps per second: 60, episode reward: -79.031, mean reward: -0.079 [-3.702, 4.052], mean action: 1.935 [0.000, 3.000], mean observation: 0.003 [-0.645, 0.932], loss: 5.071246, mean_absolute_error: 14.526313, mean_q: 15.007242
  320014/1500000: episode: 881, duration: 21.171s, episode steps: 1000, steps per second: 47, episode reward: -106.798, mean reward: -0.107 [-3.182, 4.081], mean action: 1.853 [0.000, 3.000], mean observation: 0.015 [-0.542, 0.934], loss: 4.164978, mean_absolute_error: 14.346541, mean_q: 14.914006
  321014/1500000: episode: 882, duration: 15.894s, episode steps: 1000, steps per second: 63, episode reward: -86.595, mean reward: -0.087 [-3.163, 4.531], mean action: 1.406 [0.000, 3.000], mean observation: 0.034 [-0.298, 0.938], loss: 3.780759, mean_absolute_error: 14.248444, mean_q: 15.092060
  321630/1500000: episode: 883, duration: 10.729s, episode steps: 616, steps per second: 57, episode reward: -168.068, mean reward: -0.273 [-100.000, 7.374], mean action: 1.909 [0.000, 3.000], mean observation: 0.132 [-0.538, 1.000], loss: 6.479120, mean_absolute_error: 14.259074, mean_q: 14.893651
  322630/1500000: episode: 884, duration: 18.250s, episode steps: 1000, steps per second: 55, episode reward: -48.724, mean reward: -0.049 [-3.194, 4.055], mean action: 1.714 [0.000, 3.000], mean observation: 0.059 [-0.327, 0.992], loss: 3.834034, mean_absolute_error: 14.247996, mean_q: 14.897826
  323630/1500000: episode: 885, duration: 15.563s, episode steps: 1000, steps per second: 64, episode reward: -103.707, mean reward: -0.104 [-4.336, 4.928], mean action: 1.467 [0.000, 3.000], mean observation: 0.028 [-0.673, 0.935], loss: 4.631368, mean_absolute_error: 14.220985, mean_q: 14.711599
  323837/1500000: episode: 886, duration: 2.417s, episode steps: 207, steps per second: 86, episode reward: -90.017, mean reward: -0.435 [-100.000, 3.809], mean action: 1.531 [0.000, 3.000], mean observation: 0.188 [-0.607, 1.001], loss: 3.315152, mean_absolute_error: 14.219138, mean_q: 14.489717
  324837/1500000: episode: 887, duration: 16.111s, episode steps: 1000, steps per second: 62, episode reward: -70.251, mean reward: -0.070 [-3.041, 4.013], mean action: 1.485 [0.000, 3.000], mean observation: 0.036 [-0.401, 0.936], loss: 3.237966, mean_absolute_error: 14.126164, mean_q: 14.745634
  325837/1500000: episode: 888, duration: 16.412s, episode steps: 1000, steps per second: 61, episode reward: -69.389, mean reward: -0.069 [-3.221, 4.326], mean action: 1.647 [0.000, 3.000], mean observation: 0.039 [-0.606, 0.958], loss: 5.677742, mean_absolute_error: 14.122493, mean_q: 14.459210
  326837/1500000: episode: 889, duration: 17.961s, episode steps: 1000, steps per second: 56, episode reward: -128.292, mean reward: -0.128 [-4.348, 4.048], mean action: 1.604 [0.000, 3.000], mean observation: 0.037 [-0.412, 0.948], loss: 4.532717, mean_absolute_error: 13.948030, mean_q: 14.372779
  327837/1500000: episode: 890, duration: 19.790s, episode steps: 1000, steps per second: 51, episode reward: -87.255, mean reward: -0.087 [-3.803, 4.587], mean action: 1.662 [0.000, 3.000], mean observation: 0.043 [-0.397, 0.953], loss: 6.987953, mean_absolute_error: 13.973886, mean_q: 14.575521
  328837/1500000: episode: 891, duration: 18.364s, episode steps: 1000, steps per second: 54, episode reward: -31.348, mean reward: -0.031 [-4.340, 4.111], mean action: 1.549 [0.000, 3.000], mean observation: 0.079 [-0.179, 0.954], loss: 4.400260, mean_absolute_error: 13.794574, mean_q: 14.133466
  329837/1500000: episode: 892, duration: 16.479s, episode steps: 1000, steps per second: 61, episode reward: -68.385, mean reward: -0.068 [-4.278, 4.573], mean action: 1.480 [0.000, 3.000], mean observation: 0.025 [-0.613, 1.012], loss: 4.034427, mean_absolute_error: 13.636897, mean_q: 14.201888
  330837/1500000: episode: 893, duration: 17.413s, episode steps: 1000, steps per second: 57, episode reward: -132.728, mean reward: -0.133 [-3.462, 4.641], mean action: 1.676 [0.000, 3.000], mean observation: 0.027 [-0.385, 0.950], loss: 4.703300, mean_absolute_error: 13.801881, mean_q: 13.898504
  331837/1500000: episode: 894, duration: 17.857s, episode steps: 1000, steps per second: 56, episode reward: -130.166, mean reward: -0.130 [-3.628, 4.227], mean action: 1.496 [0.000, 3.000], mean observation: 0.031 [-0.373, 0.937], loss: 4.476037, mean_absolute_error: 13.544185, mean_q: 13.952903
  332837/1500000: episode: 895, duration: 16.593s, episode steps: 1000, steps per second: 60, episode reward: -83.130, mean reward: -0.083 [-4.296, 4.202], mean action: 1.614 [0.000, 3.000], mean observation: 0.015 [-0.533, 0.925], loss: 3.846803, mean_absolute_error: 13.468884, mean_q: 13.786170
  333837/1500000: episode: 896, duration: 16.501s, episode steps: 1000, steps per second: 61, episode reward: -120.369, mean reward: -0.120 [-4.347, 4.405], mean action: 1.726 [0.000, 3.000], mean observation: 0.024 [-0.414, 0.932], loss: 5.447647, mean_absolute_error: 13.326960, mean_q: 13.759965
  334837/1500000: episode: 897, duration: 16.755s, episode steps: 1000, steps per second: 60, episode reward: -99.289, mean reward: -0.099 [-4.705, 4.302], mean action: 1.709 [0.000, 3.000], mean observation: 0.025 [-0.419, 0.928], loss: 5.757371, mean_absolute_error: 13.197549, mean_q: 13.730275
  335837/1500000: episode: 898, duration: 17.312s, episode steps: 1000, steps per second: 58, episode reward: -96.322, mean reward: -0.096 [-4.426, 3.493], mean action: 1.945 [0.000, 3.000], mean observation: 0.007 [-0.741, 0.924], loss: 3.267612, mean_absolute_error: 13.206087, mean_q: 13.639218
  336837/1500000: episode: 899, duration: 18.607s, episode steps: 1000, steps per second: 54, episode reward: -64.060, mean reward: -0.064 [-4.874, 5.004], mean action: 1.400 [0.000, 3.000], mean observation: 0.045 [-0.393, 0.966], loss: 3.984381, mean_absolute_error: 13.163895, mean_q: 13.688618
  337837/1500000: episode: 900, duration: 19.057s, episode steps: 1000, steps per second: 52, episode reward: -126.699, mean reward: -0.127 [-4.502, 4.205], mean action: 1.683 [0.000, 3.000], mean observation: 0.029 [-0.541, 0.937], loss: 3.699575, mean_absolute_error: 13.112043, mean_q: 13.878586
  338837/1500000: episode: 901, duration: 17.234s, episode steps: 1000, steps per second: 58, episode reward: -108.647, mean reward: -0.109 [-3.157, 3.983], mean action: 1.452 [0.000, 3.000], mean observation: 0.023 [-0.422, 0.931], loss: 3.386715, mean_absolute_error: 13.083269, mean_q: 13.917594
  339837/1500000: episode: 902, duration: 16.953s, episode steps: 1000, steps per second: 59, episode reward: -131.894, mean reward: -0.132 [-3.701, 4.077], mean action: 1.490 [0.000, 3.000], mean observation: 0.030 [-0.443, 0.946], loss: 3.404917, mean_absolute_error: 12.982335, mean_q: 13.871299
  340837/1500000: episode: 903, duration: 17.106s, episode steps: 1000, steps per second: 58, episode reward: -114.512, mean reward: -0.115 [-3.669, 4.246], mean action: 1.354 [0.000, 3.000], mean observation: 0.027 [-0.414, 0.956], loss: 3.584257, mean_absolute_error: 12.724578, mean_q: 13.421070
  341837/1500000: episode: 904, duration: 17.463s, episode steps: 1000, steps per second: 57, episode reward: -38.540, mean reward: -0.039 [-4.138, 4.478], mean action: 1.934 [0.000, 3.000], mean observation: 0.070 [-0.184, 1.027], loss: 4.027569, mean_absolute_error: 12.546264, mean_q: 13.103743
  342837/1500000: episode: 905, duration: 16.960s, episode steps: 1000, steps per second: 59, episode reward: -99.090, mean reward: -0.099 [-4.057, 4.203], mean action: 1.806 [0.000, 3.000], mean observation: 0.034 [-0.393, 0.960], loss: 5.693726, mean_absolute_error: 12.479593, mean_q: 12.853654
  343837/1500000: episode: 906, duration: 17.438s, episode steps: 1000, steps per second: 57, episode reward: -85.822, mean reward: -0.086 [-3.536, 4.548], mean action: 1.760 [0.000, 3.000], mean observation: 0.028 [-0.557, 0.979], loss: 3.068159, mean_absolute_error: 12.422691, mean_q: 13.163287
  344837/1500000: episode: 907, duration: 19.546s, episode steps: 1000, steps per second: 51, episode reward: -67.804, mean reward: -0.068 [-3.387, 4.352], mean action: 1.461 [0.000, 3.000], mean observation: 0.035 [-0.302, 0.934], loss: 3.178287, mean_absolute_error: 12.396658, mean_q: 13.300664
  345837/1500000: episode: 908, duration: 16.813s, episode steps: 1000, steps per second: 59, episode reward: -77.496, mean reward: -0.077 [-2.932, 4.148], mean action: 1.489 [0.000, 3.000], mean observation: 0.043 [-0.188, 0.944], loss: 4.625941, mean_absolute_error: 12.331241, mean_q: 13.130466
  346837/1500000: episode: 909, duration: 16.240s, episode steps: 1000, steps per second: 62, episode reward: -54.887, mean reward: -0.055 [-3.277, 6.449], mean action: 1.431 [0.000, 3.000], mean observation: 0.026 [-0.722, 0.967], loss: 2.536023, mean_absolute_error: 12.188160, mean_q: 12.845653
  347837/1500000: episode: 910, duration: 16.773s, episode steps: 1000, steps per second: 60, episode reward: -125.786, mean reward: -0.126 [-4.222, 3.995], mean action: 1.592 [0.000, 3.000], mean observation: 0.027 [-0.486, 0.957], loss: 2.550207, mean_absolute_error: 12.096338, mean_q: 12.724284
  348837/1500000: episode: 911, duration: 16.396s, episode steps: 1000, steps per second: 61, episode reward: -37.303, mean reward: -0.037 [-3.242, 4.564], mean action: 1.474 [0.000, 3.000], mean observation: 0.061 [-0.214, 1.010], loss: 2.867086, mean_absolute_error: 12.022532, mean_q: 12.380340
  349837/1500000: episode: 912, duration: 16.800s, episode steps: 1000, steps per second: 60, episode reward: -61.367, mean reward: -0.061 [-3.054, 4.450], mean action: 1.393 [0.000, 3.000], mean observation: 0.047 [-0.167, 0.954], loss: 2.602149, mean_absolute_error: 11.958617, mean_q: 12.494407
  350837/1500000: episode: 913, duration: 17.235s, episode steps: 1000, steps per second: 58, episode reward: -45.814, mean reward: -0.046 [-3.352, 5.624], mean action: 1.402 [0.000, 3.000], mean observation: 0.028 [-0.684, 1.009], loss: 3.173484, mean_absolute_error: 11.676793, mean_q: 12.195844
  351837/1500000: episode: 914, duration: 16.380s, episode steps: 1000, steps per second: 61, episode reward: -49.685, mean reward: -0.050 [-3.261, 4.395], mean action: 1.612 [0.000, 3.000], mean observation: 0.123 [-0.172, 0.960], loss: 3.382945, mean_absolute_error: 11.480864, mean_q: 11.885434
  352837/1500000: episode: 915, duration: 20.714s, episode steps: 1000, steps per second: 48, episode reward: -77.793, mean reward: -0.078 [-3.047, 4.216], mean action: 1.805 [0.000, 3.000], mean observation: 0.042 [-0.318, 1.004], loss: 2.464657, mean_absolute_error: 11.358851, mean_q: 11.703219
  353837/1500000: episode: 916, duration: 27.784s, episode steps: 1000, steps per second: 36, episode reward: -35.864, mean reward: -0.036 [-3.143, 4.216], mean action: 1.581 [0.000, 3.000], mean observation: 0.011 [-0.697, 0.925], loss: 3.594460, mean_absolute_error: 11.149670, mean_q: 11.642591
  354837/1500000: episode: 917, duration: 31.339s, episode steps: 1000, steps per second: 32, episode reward: -81.581, mean reward: -0.082 [-3.245, 4.694], mean action: 1.892 [0.000, 3.000], mean observation: 0.087 [-0.149, 0.961], loss: 2.097798, mean_absolute_error: 10.940221, mean_q: 11.284170
  355837/1500000: episode: 918, duration: 30.256s, episode steps: 1000, steps per second: 33, episode reward: -85.628, mean reward: -0.086 [-3.331, 4.381], mean action: 1.912 [0.000, 3.000], mean observation: 0.036 [-0.336, 0.930], loss: 2.518252, mean_absolute_error: 10.815051, mean_q: 11.056032
  356837/1500000: episode: 919, duration: 31.161s, episode steps: 1000, steps per second: 32, episode reward: -82.853, mean reward: -0.083 [-3.226, 4.174], mean action: 1.823 [0.000, 3.000], mean observation: 0.054 [-0.176, 0.965], loss: 1.939392, mean_absolute_error: 10.596767, mean_q: 10.985394
  357837/1500000: episode: 920, duration: 27.594s, episode steps: 1000, steps per second: 36, episode reward: -92.562, mean reward: -0.093 [-3.175, 3.813], mean action: 1.734 [0.000, 3.000], mean observation: 0.122 [-0.402, 0.929], loss: 2.409418, mean_absolute_error: 10.598467, mean_q: 10.850195
  358837/1500000: episode: 921, duration: 31.023s, episode steps: 1000, steps per second: 32, episode reward: -52.152, mean reward: -0.052 [-2.999, 3.986], mean action: 1.694 [0.000, 3.000], mean observation: 0.103 [-0.194, 0.944], loss: 2.778158, mean_absolute_error: 10.412125, mean_q: 10.670288
  359837/1500000: episode: 922, duration: 21.811s, episode steps: 1000, steps per second: 46, episode reward: -102.074, mean reward: -0.102 [-3.681, 4.292], mean action: 1.529 [0.000, 3.000], mean observation: 0.071 [-0.185, 0.939], loss: 2.681593, mean_absolute_error: 10.126917, mean_q: 10.171494
  360837/1500000: episode: 923, duration: 13.184s, episode steps: 1000, steps per second: 76, episode reward: -46.898, mean reward: -0.047 [-4.028, 4.341], mean action: 1.733 [0.000, 3.000], mean observation: 0.095 [-0.244, 1.024], loss: 2.217743, mean_absolute_error: 9.854829, mean_q: 10.055153
  361837/1500000: episode: 924, duration: 13.923s, episode steps: 1000, steps per second: 72, episode reward: -75.209, mean reward: -0.075 [-3.022, 4.604], mean action: 1.674 [0.000, 3.000], mean observation: 0.070 [-0.136, 0.954], loss: 2.122420, mean_absolute_error: 9.766631, mean_q: 10.013762
  362837/1500000: episode: 925, duration: 13.043s, episode steps: 1000, steps per second: 77, episode reward: -81.527, mean reward: -0.082 [-3.076, 4.518], mean action: 1.418 [0.000, 3.000], mean observation: 0.044 [-0.232, 0.933], loss: 1.949323, mean_absolute_error: 9.661127, mean_q: 9.590996
  363837/1500000: episode: 926, duration: 12.942s, episode steps: 1000, steps per second: 77, episode reward: -132.326, mean reward: -0.132 [-4.908, 3.448], mean action: 1.948 [0.000, 3.000], mean observation: 0.180 [-0.451, 0.930], loss: 2.000192, mean_absolute_error: 9.493127, mean_q: 9.299342
  364837/1500000: episode: 927, duration: 15.486s, episode steps: 1000, steps per second: 65, episode reward: -68.721, mean reward: -0.069 [-3.270, 4.290], mean action: 1.482 [0.000, 3.000], mean observation: 0.098 [-0.148, 0.948], loss: 1.931924, mean_absolute_error: 9.212908, mean_q: 8.987270
  365837/1500000: episode: 928, duration: 14.120s, episode steps: 1000, steps per second: 71, episode reward: -85.684, mean reward: -0.086 [-3.743, 3.756], mean action: 1.617 [0.000, 3.000], mean observation: 0.099 [-0.353, 0.930], loss: 2.201922, mean_absolute_error: 8.822667, mean_q: 8.513396
  366837/1500000: episode: 929, duration: 16.219s, episode steps: 1000, steps per second: 62, episode reward: -89.248, mean reward: -0.089 [-3.092, 4.373], mean action: 1.725 [0.000, 3.000], mean observation: 0.058 [-0.265, 0.933], loss: 1.488540, mean_absolute_error: 8.722700, mean_q: 8.104795
  367837/1500000: episode: 930, duration: 15.976s, episode steps: 1000, steps per second: 63, episode reward: -89.858, mean reward: -0.090 [-3.186, 4.110], mean action: 1.761 [0.000, 3.000], mean observation: 0.078 [-0.216, 0.969], loss: 1.838377, mean_absolute_error: 8.409066, mean_q: 7.670143
  368837/1500000: episode: 931, duration: 17.077s, episode steps: 1000, steps per second: 59, episode reward: -45.042, mean reward: -0.045 [-3.820, 3.835], mean action: 1.569 [0.000, 3.000], mean observation: 0.039 [-0.693, 0.928], loss: 5.264859, mean_absolute_error: 8.007636, mean_q: 7.067922
  369837/1500000: episode: 932, duration: 17.219s, episode steps: 1000, steps per second: 58, episode reward: -112.049, mean reward: -0.112 [-3.079, 4.125], mean action: 1.804 [0.000, 3.000], mean observation: 0.067 [-0.198, 0.939], loss: 4.132906, mean_absolute_error: 7.761926, mean_q: 6.700834
  370837/1500000: episode: 933, duration: 17.021s, episode steps: 1000, steps per second: 59, episode reward: -94.938, mean reward: -0.095 [-3.232, 4.349], mean action: 1.723 [0.000, 3.000], mean observation: 0.090 [-0.143, 0.967], loss: 1.590804, mean_absolute_error: 7.545227, mean_q: 6.289410
  371837/1500000: episode: 934, duration: 16.386s, episode steps: 1000, steps per second: 61, episode reward: -61.157, mean reward: -0.061 [-3.135, 3.730], mean action: 1.899 [0.000, 3.000], mean observation: 0.031 [-0.714, 0.926], loss: 1.660199, mean_absolute_error: 7.276913, mean_q: 5.972896
  372837/1500000: episode: 935, duration: 17.185s, episode steps: 1000, steps per second: 58, episode reward: -112.473, mean reward: -0.112 [-3.165, 4.046], mean action: 1.759 [0.000, 3.000], mean observation: 0.090 [-0.207, 0.954], loss: 1.766784, mean_absolute_error: 7.081060, mean_q: 5.696705
  373837/1500000: episode: 936, duration: 17.357s, episode steps: 1000, steps per second: 58, episode reward: -101.403, mean reward: -0.101 [-3.396, 4.317], mean action: 1.956 [0.000, 3.000], mean observation: 0.110 [-0.598, 0.951], loss: 1.706666, mean_absolute_error: 6.958187, mean_q: 5.366879
  374837/1500000: episode: 937, duration: 16.250s, episode steps: 1000, steps per second: 62, episode reward: -110.989, mean reward: -0.111 [-3.078, 4.139], mean action: 1.958 [0.000, 3.000], mean observation: 0.159 [-0.489, 0.925], loss: 1.660151, mean_absolute_error: 6.731110, mean_q: 5.513624
  375837/1500000: episode: 938, duration: 29.755s, episode steps: 1000, steps per second: 34, episode reward: -136.815, mean reward: -0.137 [-3.150, 3.864], mean action: 1.742 [0.000, 3.000], mean observation: 0.112 [-0.125, 0.950], loss: 1.384895, mean_absolute_error: 6.565849, mean_q: 5.049833
  376837/1500000: episode: 939, duration: 27.406s, episode steps: 1000, steps per second: 36, episode reward: -98.247, mean reward: -0.098 [-4.044, 4.421], mean action: 1.679 [0.000, 3.000], mean observation: 0.160 [-0.163, 0.965], loss: 2.714095, mean_absolute_error: 6.581074, mean_q: 5.195557
  377837/1500000: episode: 940, duration: 43.089s, episode steps: 1000, steps per second: 23, episode reward: -113.049, mean reward: -0.113 [-3.482, 4.096], mean action: 1.952 [0.000, 3.000], mean observation: 0.170 [-0.238, 0.942], loss: 1.827572, mean_absolute_error: 6.475564, mean_q: 5.160933
  378837/1500000: episode: 941, duration: 27.841s, episode steps: 1000, steps per second: 36, episode reward: -95.816, mean reward: -0.096 [-3.286, 4.192], mean action: 1.932 [0.000, 3.000], mean observation: 0.133 [-0.392, 0.929], loss: 2.062867, mean_absolute_error: 6.339942, mean_q: 5.158873
  379837/1500000: episode: 942, duration: 46.753s, episode steps: 1000, steps per second: 21, episode reward: -113.415, mean reward: -0.113 [-3.217, 4.096], mean action: 1.874 [0.000, 3.000], mean observation: 0.100 [-0.212, 0.977], loss: 4.977469, mean_absolute_error: 6.333014, mean_q: 5.119030
  380837/1500000: episode: 943, duration: 41.693s, episode steps: 1000, steps per second: 24, episode reward: -67.939, mean reward: -0.068 [-3.034, 5.121], mean action: 1.589 [0.000, 3.000], mean observation: 0.060 [-0.495, 0.926], loss: 5.306953, mean_absolute_error: 6.314811, mean_q: 5.025977
  381837/1500000: episode: 944, duration: 36.772s, episode steps: 1000, steps per second: 27, episode reward: -118.442, mean reward: -0.118 [-3.073, 4.120], mean action: 1.856 [0.000, 3.000], mean observation: 0.121 [-0.117, 0.939], loss: 2.418994, mean_absolute_error: 6.207884, mean_q: 4.897369
  382837/1500000: episode: 945, duration: 18.342s, episode steps: 1000, steps per second: 55, episode reward: -107.863, mean reward: -0.108 [-3.189, 4.006], mean action: 1.533 [0.000, 3.000], mean observation: 0.088 [-0.149, 0.967], loss: 1.677675, mean_absolute_error: 6.086707, mean_q: 4.837577
  383837/1500000: episode: 946, duration: 17.133s, episode steps: 1000, steps per second: 58, episode reward: -70.381, mean reward: -0.070 [-3.186, 4.567], mean action: 1.516 [0.000, 3.000], mean observation: 0.059 [-0.560, 0.938], loss: 2.511842, mean_absolute_error: 5.970143, mean_q: 4.590063
  384837/1500000: episode: 947, duration: 18.153s, episode steps: 1000, steps per second: 55, episode reward: -97.055, mean reward: -0.097 [-4.236, 4.333], mean action: 1.665 [0.000, 3.000], mean observation: 0.088 [-0.122, 1.001], loss: 1.673094, mean_absolute_error: 5.825681, mean_q: 4.537066
  385837/1500000: episode: 948, duration: 18.326s, episode steps: 1000, steps per second: 55, episode reward: -83.937, mean reward: -0.084 [-3.516, 4.505], mean action: 1.449 [0.000, 3.000], mean observation: 0.053 [-0.272, 0.932], loss: 2.727687, mean_absolute_error: 5.722595, mean_q: 4.478961
  386837/1500000: episode: 949, duration: 18.876s, episode steps: 1000, steps per second: 53, episode reward: -61.395, mean reward: -0.061 [-3.843, 6.021], mean action: 1.753 [0.000, 3.000], mean observation: 0.098 [-0.443, 0.927], loss: 4.155795, mean_absolute_error: 5.602483, mean_q: 4.405261
  387837/1500000: episode: 950, duration: 17.729s, episode steps: 1000, steps per second: 56, episode reward: -96.015, mean reward: -0.096 [-3.797, 4.441], mean action: 1.771 [0.000, 3.000], mean observation: 0.066 [-0.387, 0.930], loss: 1.430327, mean_absolute_error: 5.508240, mean_q: 4.279843
  388837/1500000: episode: 951, duration: 19.524s, episode steps: 1000, steps per second: 51, episode reward: -87.057, mean reward: -0.087 [-3.081, 4.138], mean action: 1.962 [0.000, 3.000], mean observation: 0.132 [-0.367, 0.936], loss: 1.969510, mean_absolute_error: 5.483132, mean_q: 4.018102
  389837/1500000: episode: 952, duration: 23.487s, episode steps: 1000, steps per second: 43, episode reward: -63.387, mean reward: -0.063 [-3.248, 4.219], mean action: 1.933 [0.000, 3.000], mean observation: 0.124 [-0.490, 0.925], loss: 3.102304, mean_absolute_error: 5.466377, mean_q: 3.842513
  390837/1500000: episode: 953, duration: 21.769s, episode steps: 1000, steps per second: 46, episode reward: -94.061, mean reward: -0.094 [-3.328, 4.181], mean action: 1.825 [0.000, 3.000], mean observation: 0.136 [-0.159, 1.003], loss: 1.151378, mean_absolute_error: 5.369450, mean_q: 3.792339
  391837/1500000: episode: 954, duration: 27.865s, episode steps: 1000, steps per second: 36, episode reward: -100.910, mean reward: -0.101 [-3.889, 4.291], mean action: 1.687 [0.000, 3.000], mean observation: 0.109 [-0.145, 0.965], loss: 1.861328, mean_absolute_error: 5.351263, mean_q: 3.666693
  392837/1500000: episode: 955, duration: 20.321s, episode steps: 1000, steps per second: 49, episode reward: -90.466, mean reward: -0.090 [-3.070, 4.242], mean action: 1.841 [0.000, 3.000], mean observation: 0.101 [-0.502, 0.925], loss: 2.587582, mean_absolute_error: 5.287459, mean_q: 3.752939
  393837/1500000: episode: 956, duration: 23.728s, episode steps: 1000, steps per second: 42, episode reward: -86.381, mean reward: -0.086 [-3.027, 4.545], mean action: 1.526 [0.000, 3.000], mean observation: 0.051 [-0.432, 0.930], loss: 2.455580, mean_absolute_error: 5.237205, mean_q: 3.844304
  394837/1500000: episode: 957, duration: 29.608s, episode steps: 1000, steps per second: 34, episode reward: -79.391, mean reward: -0.079 [-3.055, 4.378], mean action: 1.747 [0.000, 3.000], mean observation: 0.075 [-0.497, 0.926], loss: 1.152180, mean_absolute_error: 5.224491, mean_q: 3.804222
  395837/1500000: episode: 958, duration: 24.274s, episode steps: 1000, steps per second: 41, episode reward: -100.421, mean reward: -0.100 [-3.242, 4.256], mean action: 1.715 [0.000, 3.000], mean observation: 0.068 [-0.247, 0.984], loss: 4.681525, mean_absolute_error: 5.112259, mean_q: 3.570611
  396837/1500000: episode: 959, duration: 24.357s, episode steps: 1000, steps per second: 41, episode reward: -86.038, mean reward: -0.086 [-2.973, 4.682], mean action: 1.740 [0.000, 3.000], mean observation: 0.099 [-0.261, 0.956], loss: 0.983793, mean_absolute_error: 5.072354, mean_q: 3.668505
  397837/1500000: episode: 960, duration: 26.101s, episode steps: 1000, steps per second: 38, episode reward: -82.925, mean reward: -0.083 [-3.502, 4.240], mean action: 1.556 [0.000, 3.000], mean observation: 0.094 [-0.301, 0.932], loss: 4.817605, mean_absolute_error: 5.033028, mean_q: 3.575820
  398837/1500000: episode: 961, duration: 32.635s, episode steps: 1000, steps per second: 31, episode reward: -47.232, mean reward: -0.047 [-3.853, 5.576], mean action: 1.698 [0.000, 3.000], mean observation: 0.080 [-0.674, 1.011], loss: 2.586688, mean_absolute_error: 4.973164, mean_q: 3.698838
  399837/1500000: episode: 962, duration: 39.166s, episode steps: 1000, steps per second: 26, episode reward: -133.867, mean reward: -0.134 [-3.034, 4.211], mean action: 1.533 [0.000, 3.000], mean observation: 0.072 [-0.178, 0.939], loss: 1.398735, mean_absolute_error: 4.814184, mean_q: 3.727383
  400837/1500000: episode: 963, duration: 36.940s, episode steps: 1000, steps per second: 27, episode reward: -62.356, mean reward: -0.062 [-3.354, 6.394], mean action: 1.688 [0.000, 3.000], mean observation: 0.067 [-0.714, 0.967], loss: 4.195228, mean_absolute_error: 4.812675, mean_q: 3.652896
  401837/1500000: episode: 964, duration: 31.577s, episode steps: 1000, steps per second: 32, episode reward: -111.712, mean reward: -0.112 [-3.102, 4.612], mean action: 1.564 [0.000, 3.000], mean observation: 0.077 [-0.246, 0.935], loss: 1.387960, mean_absolute_error: 4.648144, mean_q: 3.596635
  402837/1500000: episode: 965, duration: 29.934s, episode steps: 1000, steps per second: 33, episode reward: -109.024, mean reward: -0.109 [-3.261, 4.248], mean action: 1.783 [0.000, 3.000], mean observation: 0.048 [-0.498, 0.931], loss: 1.922004, mean_absolute_error: 4.591676, mean_q: 3.323973
  403837/1500000: episode: 966, duration: 34.884s, episode steps: 1000, steps per second: 29, episode reward: -114.280, mean reward: -0.114 [-4.637, 4.164], mean action: 1.856 [0.000, 3.000], mean observation: 0.056 [-0.525, 0.938], loss: 0.836551, mean_absolute_error: 4.460423, mean_q: 3.153728
  404837/1500000: episode: 967, duration: 36.639s, episode steps: 1000, steps per second: 27, episode reward: -77.431, mean reward: -0.077 [-3.193, 4.205], mean action: 1.654 [0.000, 3.000], mean observation: 0.068 [-0.472, 0.931], loss: 1.121403, mean_absolute_error: 4.438819, mean_q: 3.079482
  405837/1500000: episode: 968, duration: 18.565s, episode steps: 1000, steps per second: 54, episode reward: -112.616, mean reward: -0.113 [-2.963, 4.016], mean action: 1.682 [0.000, 3.000], mean observation: 0.108 [-0.162, 0.941], loss: 0.751774, mean_absolute_error: 4.293885, mean_q: 3.187896
  406837/1500000: episode: 969, duration: 19.186s, episode steps: 1000, steps per second: 52, episode reward: -57.647, mean reward: -0.058 [-4.337, 4.499], mean action: 1.726 [0.000, 3.000], mean observation: 0.050 [-0.775, 0.934], loss: 0.787985, mean_absolute_error: 4.279377, mean_q: 3.186104
  407837/1500000: episode: 970, duration: 18.921s, episode steps: 1000, steps per second: 53, episode reward: -98.356, mean reward: -0.098 [-3.366, 4.560], mean action: 1.838 [0.000, 3.000], mean observation: 0.134 [-0.146, 0.968], loss: 0.746913, mean_absolute_error: 4.236971, mean_q: 3.141435
  408837/1500000: episode: 971, duration: 19.639s, episode steps: 1000, steps per second: 51, episode reward: -46.728, mean reward: -0.047 [-3.499, 4.839], mean action: 1.568 [0.000, 3.000], mean observation: 0.031 [-0.789, 0.931], loss: 0.828603, mean_absolute_error: 4.175053, mean_q: 3.165225
  409837/1500000: episode: 972, duration: 19.439s, episode steps: 1000, steps per second: 51, episode reward: -127.547, mean reward: -0.128 [-3.007, 3.951], mean action: 1.684 [0.000, 3.000], mean observation: 0.085 [-0.149, 0.949], loss: 0.632123, mean_absolute_error: 4.087211, mean_q: 3.093938
  410837/1500000: episode: 973, duration: 18.899s, episode steps: 1000, steps per second: 53, episode reward: -113.896, mean reward: -0.114 [-3.296, 4.278], mean action: 1.677 [0.000, 3.000], mean observation: 0.093 [-0.167, 0.950], loss: 0.791481, mean_absolute_error: 4.122172, mean_q: 3.175678
  411837/1500000: episode: 974, duration: 17.556s, episode steps: 1000, steps per second: 57, episode reward: -87.904, mean reward: -0.088 [-3.071, 4.334], mean action: 1.597 [0.000, 3.000], mean observation: 0.094 [-0.338, 0.931], loss: 0.864676, mean_absolute_error: 4.106440, mean_q: 3.138690
  412837/1500000: episode: 975, duration: 14.824s, episode steps: 1000, steps per second: 67, episode reward: -78.131, mean reward: -0.078 [-3.209, 4.361], mean action: 1.791 [0.000, 3.000], mean observation: 0.122 [-0.198, 1.016], loss: 0.999143, mean_absolute_error: 4.069674, mean_q: 3.210509
  413837/1500000: episode: 976, duration: 19.895s, episode steps: 1000, steps per second: 50, episode reward: -90.512, mean reward: -0.091 [-3.549, 4.095], mean action: 1.645 [0.000, 3.000], mean observation: 0.081 [-0.456, 0.974], loss: 0.747905, mean_absolute_error: 4.089315, mean_q: 3.290544
  414837/1500000: episode: 977, duration: 18.267s, episode steps: 1000, steps per second: 55, episode reward: -77.901, mean reward: -0.078 [-4.388, 5.313], mean action: 1.836 [0.000, 3.000], mean observation: 0.109 [-0.268, 1.011], loss: 0.600406, mean_absolute_error: 4.102425, mean_q: 3.325107
  415837/1500000: episode: 978, duration: 18.031s, episode steps: 1000, steps per second: 55, episode reward: -83.279, mean reward: -0.083 [-3.702, 5.063], mean action: 1.686 [0.000, 3.000], mean observation: 0.078 [-0.513, 0.966], loss: 0.522071, mean_absolute_error: 4.060104, mean_q: 3.305057
  416837/1500000: episode: 979, duration: 16.719s, episode steps: 1000, steps per second: 60, episode reward: -139.979, mean reward: -0.140 [-3.214, 3.876], mean action: 1.612 [0.000, 3.000], mean observation: 0.083 [-0.227, 0.937], loss: 0.633485, mean_absolute_error: 4.056122, mean_q: 3.094503
  417797/1500000: episode: 980, duration: 17.454s, episode steps: 960, steps per second: 55, episode reward: -250.594, mean reward: -0.261 [-100.000, 3.713], mean action: 1.910 [0.000, 3.000], mean observation: -0.000 [-1.000, 0.939], loss: 1.020235, mean_absolute_error: 4.191034, mean_q: 3.258992
  418768/1500000: episode: 981, duration: 14.503s, episode steps: 971, steps per second: 67, episode reward: -278.223, mean reward: -0.287 [-100.000, 3.684], mean action: 1.958 [0.000, 3.000], mean observation: 0.016 [-1.001, 0.950], loss: 0.689019, mean_absolute_error: 4.113153, mean_q: 3.275108
  419768/1500000: episode: 982, duration: 18.460s, episode steps: 1000, steps per second: 54, episode reward: -167.195, mean reward: -0.167 [-3.199, 4.005], mean action: 1.834 [0.000, 3.000], mean observation: 0.028 [-0.807, 0.950], loss: 0.580014, mean_absolute_error: 4.106357, mean_q: 3.181490
  420768/1500000: episode: 983, duration: 18.311s, episode steps: 1000, steps per second: 55, episode reward: -105.221, mean reward: -0.105 [-3.349, 5.850], mean action: 1.826 [0.000, 3.000], mean observation: 0.043 [-0.702, 0.947], loss: 0.600695, mean_absolute_error: 4.089563, mean_q: 3.124164
  421768/1500000: episode: 984, duration: 25.914s, episode steps: 1000, steps per second: 39, episode reward: -111.473, mean reward: -0.111 [-3.964, 4.511], mean action: 1.889 [0.000, 3.000], mean observation: 0.017 [-0.630, 0.937], loss: 0.484348, mean_absolute_error: 4.070156, mean_q: 3.080376
  422768/1500000: episode: 985, duration: 18.505s, episode steps: 1000, steps per second: 54, episode reward: -113.692, mean reward: -0.114 [-3.061, 4.427], mean action: 1.582 [0.000, 3.000], mean observation: 0.083 [-0.216, 0.943], loss: 0.483357, mean_absolute_error: 4.081535, mean_q: 3.017613
  423768/1500000: episode: 986, duration: 19.508s, episode steps: 1000, steps per second: 51, episode reward: -161.612, mean reward: -0.162 [-3.514, 3.809], mean action: 1.685 [0.000, 3.000], mean observation: 0.059 [-0.471, 0.936], loss: 0.488864, mean_absolute_error: 4.090719, mean_q: 3.032054
  424768/1500000: episode: 987, duration: 20.574s, episode steps: 1000, steps per second: 49, episode reward: -105.163, mean reward: -0.105 [-3.150, 4.817], mean action: 1.601 [0.000, 3.000], mean observation: 0.081 [-0.327, 0.983], loss: 0.455472, mean_absolute_error: 4.056736, mean_q: 2.863138
  425768/1500000: episode: 988, duration: 21.449s, episode steps: 1000, steps per second: 47, episode reward: -57.860, mean reward: -0.058 [-3.053, 4.099], mean action: 1.697 [0.000, 3.000], mean observation: 0.095 [-0.388, 0.929], loss: 0.502650, mean_absolute_error: 4.077572, mean_q: 2.746985
  426768/1500000: episode: 989, duration: 19.597s, episode steps: 1000, steps per second: 51, episode reward: -141.055, mean reward: -0.141 [-3.372, 4.231], mean action: 1.682 [0.000, 3.000], mean observation: 0.064 [-0.358, 0.947], loss: 0.568797, mean_absolute_error: 4.048436, mean_q: 2.516671
  427768/1500000: episode: 990, duration: 24.886s, episode steps: 1000, steps per second: 40, episode reward: -124.853, mean reward: -0.125 [-3.101, 3.721], mean action: 1.575 [0.000, 3.000], mean observation: 0.061 [-0.330, 0.931], loss: 0.559229, mean_absolute_error: 3.968911, mean_q: 2.264539
  428768/1500000: episode: 991, duration: 19.275s, episode steps: 1000, steps per second: 52, episode reward: -59.291, mean reward: -0.059 [-3.291, 4.363], mean action: 1.636 [0.000, 3.000], mean observation: 0.116 [-0.286, 0.955], loss: 0.451711, mean_absolute_error: 4.029747, mean_q: 2.293628
  429768/1500000: episode: 992, duration: 25.969s, episode steps: 1000, steps per second: 39, episode reward: -68.714, mean reward: -0.069 [-3.189, 4.510], mean action: 1.915 [0.000, 3.000], mean observation: 0.101 [-0.332, 0.931], loss: 0.482717, mean_absolute_error: 3.993095, mean_q: 2.158533
  430768/1500000: episode: 993, duration: 20.509s, episode steps: 1000, steps per second: 49, episode reward: -123.060, mean reward: -0.123 [-4.335, 4.419], mean action: 1.672 [0.000, 3.000], mean observation: 0.055 [-0.441, 0.948], loss: 0.516329, mean_absolute_error: 4.006999, mean_q: 2.097539
  431768/1500000: episode: 994, duration: 30.480s, episode steps: 1000, steps per second: 33, episode reward: -70.006, mean reward: -0.070 [-3.303, 5.375], mean action: 1.635 [0.000, 3.000], mean observation: 0.063 [-0.618, 0.940], loss: 0.575731, mean_absolute_error: 4.032523, mean_q: 1.917491
  432768/1500000: episode: 995, duration: 23.562s, episode steps: 1000, steps per second: 42, episode reward: -56.578, mean reward: -0.057 [-3.668, 6.299], mean action: 1.619 [0.000, 3.000], mean observation: 0.098 [-0.424, 0.955], loss: 0.410807, mean_absolute_error: 4.099561, mean_q: 1.849374
  433768/1500000: episode: 996, duration: 28.231s, episode steps: 1000, steps per second: 35, episode reward: -60.133, mean reward: -0.060 [-3.211, 4.542], mean action: 1.806 [0.000, 3.000], mean observation: 0.051 [-0.543, 0.924], loss: 0.535101, mean_absolute_error: 4.066491, mean_q: 1.702552
  434768/1500000: episode: 997, duration: 30.899s, episode steps: 1000, steps per second: 32, episode reward: -97.740, mean reward: -0.098 [-3.234, 4.246], mean action: 1.571 [0.000, 3.000], mean observation: 0.045 [-0.479, 0.926], loss: 0.411534, mean_absolute_error: 4.116734, mean_q: 1.685329
  435768/1500000: episode: 998, duration: 25.313s, episode steps: 1000, steps per second: 40, episode reward: -32.407, mean reward: -0.032 [-13.433, 12.841], mean action: 1.905 [0.000, 3.000], mean observation: 0.088 [-0.555, 1.000], loss: 0.559824, mean_absolute_error: 4.118253, mean_q: 1.568676
  436768/1500000: episode: 999, duration: 30.112s, episode steps: 1000, steps per second: 33, episode reward: -80.391, mean reward: -0.080 [-2.923, 3.868], mean action: 1.686 [0.000, 3.000], mean observation: 0.088 [-0.184, 0.938], loss: 0.495378, mean_absolute_error: 4.156442, mean_q: 1.545360
  437768/1500000: episode: 1000, duration: 31.963s, episode steps: 1000, steps per second: 31, episode reward: -70.157, mean reward: -0.070 [-3.702, 5.598], mean action: 1.706 [0.000, 3.000], mean observation: 0.069 [-0.704, 0.985], loss: 0.381523, mean_absolute_error: 4.164749, mean_q: 1.458448
  438768/1500000: episode: 1001, duration: 35.036s, episode steps: 1000, steps per second: 29, episode reward: -65.773, mean reward: -0.066 [-3.436, 5.541], mean action: 1.619 [0.000, 3.000], mean observation: 0.061 [-0.672, 0.971], loss: 0.423203, mean_absolute_error: 4.239115, mean_q: 1.624497
  439768/1500000: episode: 1002, duration: 34.852s, episode steps: 1000, steps per second: 29, episode reward: -133.866, mean reward: -0.134 [-3.092, 3.955], mean action: 1.491 [0.000, 3.000], mean observation: 0.062 [-0.277, 0.937], loss: 0.499450, mean_absolute_error: 4.284624, mean_q: 1.706229
  440768/1500000: episode: 1003, duration: 41.227s, episode steps: 1000, steps per second: 24, episode reward: -106.695, mean reward: -0.107 [-3.432, 4.629], mean action: 1.630 [0.000, 3.000], mean observation: 0.073 [-0.316, 0.991], loss: 0.378821, mean_absolute_error: 4.220271, mean_q: 1.726408
  441768/1500000: episode: 1004, duration: 37.943s, episode steps: 1000, steps per second: 26, episode reward: -56.951, mean reward: -0.057 [-3.782, 4.001], mean action: 1.799 [0.000, 3.000], mean observation: 0.011 [-0.771, 0.932], loss: 0.366716, mean_absolute_error: 4.278098, mean_q: 1.850893
  442768/1500000: episode: 1005, duration: 37.778s, episode steps: 1000, steps per second: 26, episode reward: -94.016, mean reward: -0.094 [-3.138, 5.228], mean action: 1.594 [0.000, 3.000], mean observation: 0.075 [-0.361, 0.983], loss: 0.395798, mean_absolute_error: 4.296817, mean_q: 1.951072
  443768/1500000: episode: 1006, duration: 42.029s, episode steps: 1000, steps per second: 24, episode reward: -157.239, mean reward: -0.157 [-3.968, 3.994], mean action: 1.539 [0.000, 3.000], mean observation: 0.066 [-0.325, 0.939], loss: 0.348837, mean_absolute_error: 4.390327, mean_q: 2.108198
  444768/1500000: episode: 1007, duration: 37.780s, episode steps: 1000, steps per second: 26, episode reward: -99.902, mean reward: -0.100 [-4.221, 4.407], mean action: 1.640 [0.000, 3.000], mean observation: 0.064 [-0.419, 0.935], loss: 0.349493, mean_absolute_error: 4.518189, mean_q: 2.298125
  445768/1500000: episode: 1008, duration: 36.446s, episode steps: 1000, steps per second: 27, episode reward: -101.230, mean reward: -0.101 [-3.515, 4.512], mean action: 1.563 [0.000, 3.000], mean observation: 0.055 [-0.482, 0.926], loss: 0.474430, mean_absolute_error: 4.572580, mean_q: 2.378831
  446768/1500000: episode: 1009, duration: 33.660s, episode steps: 1000, steps per second: 30, episode reward: -108.765, mean reward: -0.109 [-3.209, 6.010], mean action: 1.579 [0.000, 3.000], mean observation: 0.068 [-0.313, 0.996], loss: 0.506652, mean_absolute_error: 4.575193, mean_q: 2.437428
  447768/1500000: episode: 1010, duration: 35.872s, episode steps: 1000, steps per second: 28, episode reward: -150.465, mean reward: -0.150 [-3.031, 3.318], mean action: 1.617 [0.000, 3.000], mean observation: 0.065 [-0.329, 0.940], loss: 0.343372, mean_absolute_error: 4.741510, mean_q: 2.691572
  448768/1500000: episode: 1011, duration: 33.083s, episode steps: 1000, steps per second: 30, episode reward: -117.212, mean reward: -0.117 [-3.188, 4.636], mean action: 1.689 [0.000, 3.000], mean observation: 0.067 [-0.442, 0.965], loss: 0.878579, mean_absolute_error: 4.631560, mean_q: 2.580458
  448933/1500000: episode: 1012, duration: 5.258s, episode steps: 165, steps per second: 31, episode reward: -270.675, mean reward: -1.640 [-100.000, 71.515], mean action: 1.661 [0.000, 3.000], mean observation: 0.183 [-1.200, 2.173], loss: 1.195196, mean_absolute_error: 4.725898, mean_q: 2.623443
  449933/1500000: episode: 1013, duration: 35.958s, episode steps: 1000, steps per second: 28, episode reward: -127.126, mean reward: -0.127 [-3.414, 3.690], mean action: 1.666 [0.000, 3.000], mean observation: 0.076 [-0.458, 0.956], loss: 0.336716, mean_absolute_error: 4.686798, mean_q: 2.700016
  450933/1500000: episode: 1014, duration: 35.234s, episode steps: 1000, steps per second: 28, episode reward: -145.103, mean reward: -0.145 [-3.417, 3.952], mean action: 1.627 [0.000, 3.000], mean observation: 0.080 [-0.227, 0.944], loss: 0.326557, mean_absolute_error: 4.809501, mean_q: 2.878595
  451933/1500000: episode: 1015, duration: 38.375s, episode steps: 1000, steps per second: 26, episode reward: -97.905, mean reward: -0.098 [-3.537, 5.751], mean action: 1.755 [0.000, 3.000], mean observation: 0.071 [-0.625, 0.953], loss: 0.726798, mean_absolute_error: 4.816432, mean_q: 2.943009
  452059/1500000: episode: 1016, duration: 3.856s, episode steps: 126, steps per second: 33, episode reward: -126.107, mean reward: -1.001 [-100.000, 12.363], mean action: 1.270 [0.000, 3.000], mean observation: 0.136 [-4.445, 1.000], loss: 0.580071, mean_absolute_error: 4.663265, mean_q: 2.772141
  453059/1500000: episode: 1017, duration: 34.851s, episode steps: 1000, steps per second: 29, episode reward: -118.538, mean reward: -0.119 [-4.092, 3.907], mean action: 1.539 [0.000, 3.000], mean observation: 0.085 [-0.224, 0.975], loss: 0.654749, mean_absolute_error: 4.851138, mean_q: 3.011685
  454059/1500000: episode: 1018, duration: 38.982s, episode steps: 1000, steps per second: 26, episode reward: -112.737, mean reward: -0.113 [-2.981, 3.758], mean action: 1.519 [0.000, 3.000], mean observation: 0.082 [-0.253, 1.009], loss: 0.315132, mean_absolute_error: 4.871887, mean_q: 3.064152
  455059/1500000: episode: 1019, duration: 36.937s, episode steps: 1000, steps per second: 27, episode reward: -121.065, mean reward: -0.121 [-3.676, 4.309], mean action: 1.629 [0.000, 3.000], mean observation: 0.079 [-0.206, 0.935], loss: 0.322052, mean_absolute_error: 4.777068, mean_q: 2.919259
  455190/1500000: episode: 1020, duration: 3.948s, episode steps: 131, steps per second: 33, episode reward: -90.353, mean reward: -0.690 [-100.000, 59.100], mean action: 1.336 [0.000, 3.000], mean observation: 0.147 [-2.275, 1.000], loss: 0.288345, mean_absolute_error: 5.019427, mean_q: 3.285673
  456190/1500000: episode: 1021, duration: 33.826s, episode steps: 1000, steps per second: 30, episode reward: -81.147, mean reward: -0.081 [-3.078, 5.305], mean action: 1.801 [0.000, 3.000], mean observation: 0.064 [-0.646, 0.940], loss: 0.628891, mean_absolute_error: 4.877590, mean_q: 3.095481
  457190/1500000: episode: 1022, duration: 35.935s, episode steps: 1000, steps per second: 28, episode reward: -85.669, mean reward: -0.086 [-3.895, 5.019], mean action: 1.665 [0.000, 3.000], mean observation: 0.074 [-0.366, 0.953], loss: 0.448746, mean_absolute_error: 4.886508, mean_q: 3.130477
  458190/1500000: episode: 1023, duration: 32.935s, episode steps: 1000, steps per second: 30, episode reward: -74.240, mean reward: -0.074 [-3.679, 4.451], mean action: 1.601 [0.000, 3.000], mean observation: 0.033 [-0.747, 0.935], loss: 0.327190, mean_absolute_error: 5.039224, mean_q: 3.313054
  459190/1500000: episode: 1024, duration: 34.093s, episode steps: 1000, steps per second: 29, episode reward: -38.741, mean reward: -0.039 [-4.408, 5.612], mean action: 1.734 [0.000, 3.000], mean observation: 0.073 [-0.494, 0.981], loss: 0.589202, mean_absolute_error: 5.017865, mean_q: 3.311720
  460190/1500000: episode: 1025, duration: 41.890s, episode steps: 1000, steps per second: 24, episode reward: -76.127, mean reward: -0.076 [-4.200, 5.800], mean action: 1.729 [0.000, 3.000], mean observation: 0.061 [-0.519, 0.938], loss: 0.313879, mean_absolute_error: 5.193250, mean_q: 3.584805
  461190/1500000: episode: 1026, duration: 37.331s, episode steps: 1000, steps per second: 27, episode reward: -104.078, mean reward: -0.104 [-3.549, 4.568], mean action: 1.619 [0.000, 3.000], mean observation: 0.079 [-0.249, 0.985], loss: 0.348435, mean_absolute_error: 5.296557, mean_q: 3.618227
  462190/1500000: episode: 1027, duration: 35.729s, episode steps: 1000, steps per second: 28, episode reward: -99.354, mean reward: -0.099 [-3.431, 4.748], mean action: 1.600 [0.000, 3.000], mean observation: 0.058 [-0.334, 0.973], loss: 0.540402, mean_absolute_error: 5.422724, mean_q: 3.804699
  463190/1500000: episode: 1028, duration: 34.840s, episode steps: 1000, steps per second: 29, episode reward: -109.309, mean reward: -0.109 [-3.946, 4.357], mean action: 1.597 [0.000, 3.000], mean observation: 0.039 [-0.377, 0.934], loss: 0.350047, mean_absolute_error: 5.490636, mean_q: 3.871199
  464190/1500000: episode: 1029, duration: 36.033s, episode steps: 1000, steps per second: 28, episode reward: -95.182, mean reward: -0.095 [-4.400, 3.936], mean action: 1.689 [0.000, 3.000], mean observation: 0.043 [-0.477, 0.926], loss: 0.483337, mean_absolute_error: 5.472970, mean_q: 3.753578
  465190/1500000: episode: 1030, duration: 36.476s, episode steps: 1000, steps per second: 27, episode reward: -105.236, mean reward: -0.105 [-5.168, 4.834], mean action: 1.679 [0.000, 3.000], mean observation: 0.071 [-0.316, 0.998], loss: 0.450922, mean_absolute_error: 5.615726, mean_q: 3.934582
  466190/1500000: episode: 1031, duration: 39.760s, episode steps: 1000, steps per second: 25, episode reward: -106.827, mean reward: -0.107 [-3.825, 4.465], mean action: 1.706 [0.000, 3.000], mean observation: 0.051 [-0.454, 0.947], loss: 0.297094, mean_absolute_error: 5.588960, mean_q: 3.799712
  467190/1500000: episode: 1032, duration: 35.707s, episode steps: 1000, steps per second: 28, episode reward: -70.486, mean reward: -0.070 [-4.152, 5.802], mean action: 1.810 [0.000, 3.000], mean observation: 0.048 [-0.761, 0.940], loss: 0.475147, mean_absolute_error: 5.694127, mean_q: 3.885428
  467382/1500000: episode: 1033, duration: 5.842s, episode steps: 192, steps per second: 33, episode reward: -266.503, mean reward: -1.388 [-100.000, 5.188], mean action: 1.729 [0.000, 3.000], mean observation: 0.053 [-3.075, 1.000], loss: 0.982235, mean_absolute_error: 5.845951, mean_q: 4.093786
  468382/1500000: episode: 1034, duration: 39.892s, episode steps: 1000, steps per second: 25, episode reward: -100.632, mean reward: -0.101 [-3.558, 4.819], mean action: 1.560 [0.000, 3.000], mean observation: 0.067 [-0.345, 0.970], loss: 0.324760, mean_absolute_error: 5.840832, mean_q: 3.931846
  469382/1500000: episode: 1035, duration: 37.080s, episode steps: 1000, steps per second: 27, episode reward: -101.243, mean reward: -0.101 [-3.208, 4.469], mean action: 1.584 [0.000, 3.000], mean observation: 0.054 [-0.268, 0.938], loss: 0.488014, mean_absolute_error: 5.755786, mean_q: 3.914980
  470382/1500000: episode: 1036, duration: 38.357s, episode steps: 1000, steps per second: 26, episode reward: -20.469, mean reward: -0.020 [-20.662, 20.585], mean action: 1.692 [0.000, 3.000], mean observation: 0.123 [-0.491, 1.000], loss: 0.656397, mean_absolute_error: 5.895654, mean_q: 4.080572
  470927/1500000: episode: 1037, duration: 18.396s, episode steps: 545, steps per second: 30, episode reward: 126.209, mean reward: 0.232 [-13.280, 100.000], mean action: 1.958 [0.000, 3.000], mean observation: 0.109 [-0.972, 1.017], loss: 0.555238, mean_absolute_error: 5.934059, mean_q: 4.037703
  471927/1500000: episode: 1038, duration: 39.458s, episode steps: 1000, steps per second: 25, episode reward: -116.377, mean reward: -0.116 [-3.437, 4.468], mean action: 1.640 [0.000, 3.000], mean observation: 0.065 [-0.223, 0.946], loss: 0.518079, mean_absolute_error: 5.870314, mean_q: 3.887058
  472805/1500000: episode: 1039, duration: 31.333s, episode steps: 878, steps per second: 28, episode reward: 61.270, mean reward: 0.070 [-12.584, 100.000], mean action: 1.961 [0.000, 3.000], mean observation: 0.117 [-0.708, 1.013], loss: 0.832119, mean_absolute_error: 5.876428, mean_q: 3.889516
  473805/1500000: episode: 1040, duration: 39.126s, episode steps: 1000, steps per second: 26, episode reward: -96.717, mean reward: -0.097 [-4.302, 5.031], mean action: 1.738 [0.000, 3.000], mean observation: 0.079 [-0.310, 0.980], loss: 0.350163, mean_absolute_error: 5.925557, mean_q: 3.955440
  474805/1500000: episode: 1041, duration: 36.362s, episode steps: 1000, steps per second: 28, episode reward: -2.923, mean reward: -0.003 [-17.722, 22.061], mean action: 2.373 [0.000, 3.000], mean observation: 0.224 [-0.676, 1.453], loss: 0.481635, mean_absolute_error: 5.937537, mean_q: 4.138503
  475805/1500000: episode: 1042, duration: 35.432s, episode steps: 1000, steps per second: 28, episode reward: -118.357, mean reward: -0.118 [-3.709, 4.998], mean action: 1.730 [0.000, 3.000], mean observation: 0.070 [-0.290, 0.936], loss: 0.427690, mean_absolute_error: 5.880000, mean_q: 4.208040
  476077/1500000: episode: 1043, duration: 9.152s, episode steps: 272, steps per second: 30, episode reward: -243.592, mean reward: -0.896 [-100.000, 15.823], mean action: 1.835 [0.000, 3.000], mean observation: 0.001 [-2.071, 1.011], loss: 0.327699, mean_absolute_error: 5.774043, mean_q: 4.153224
  476547/1500000: episode: 1044, duration: 14.922s, episode steps: 470, steps per second: 31, episode reward: -264.199, mean reward: -0.562 [-100.000, 6.207], mean action: 1.879 [0.000, 3.000], mean observation: 0.130 [-0.817, 1.377], loss: 0.357115, mean_absolute_error: 5.764905, mean_q: 4.076338
  477547/1500000: episode: 1045, duration: 38.973s, episode steps: 1000, steps per second: 26, episode reward: -69.767, mean reward: -0.070 [-3.618, 4.571], mean action: 1.835 [0.000, 3.000], mean observation: 0.045 [-0.464, 0.927], loss: 0.472157, mean_absolute_error: 5.848865, mean_q: 4.144814
  478547/1500000: episode: 1046, duration: 40.917s, episode steps: 1000, steps per second: 24, episode reward: -104.093, mean reward: -0.104 [-4.221, 4.681], mean action: 1.852 [0.000, 3.000], mean observation: 0.057 [-0.273, 0.935], loss: 0.467114, mean_absolute_error: 5.827584, mean_q: 4.109190
  479547/1500000: episode: 1047, duration: 35.108s, episode steps: 1000, steps per second: 28, episode reward: -88.547, mean reward: -0.089 [-4.784, 5.480], mean action: 1.811 [0.000, 3.000], mean observation: 0.075 [-0.376, 0.938], loss: 0.486656, mean_absolute_error: 5.792072, mean_q: 4.094496
  480039/1500000: episode: 1048, duration: 14.106s, episode steps: 492, steps per second: 35, episode reward: -309.322, mean reward: -0.629 [-100.000, 27.038], mean action: 1.921 [0.000, 3.000], mean observation: 0.092 [-1.651, 1.147], loss: 0.703120, mean_absolute_error: 5.806230, mean_q: 4.001963
  481039/1500000: episode: 1049, duration: 38.435s, episode steps: 1000, steps per second: 26, episode reward: -74.752, mean reward: -0.075 [-3.917, 5.881], mean action: 1.830 [0.000, 3.000], mean observation: 0.050 [-0.407, 0.957], loss: 0.497537, mean_absolute_error: 5.876128, mean_q: 3.950101
  481686/1500000: episode: 1050, duration: 19.846s, episode steps: 647, steps per second: 33, episode reward: -344.589, mean reward: -0.533 [-100.000, 23.916], mean action: 1.869 [0.000, 3.000], mean observation: 0.077 [-1.588, 1.016], loss: 1.013353, mean_absolute_error: 5.823335, mean_q: 3.831077
  482686/1500000: episode: 1051, duration: 25.839s, episode steps: 1000, steps per second: 39, episode reward: -113.084, mean reward: -0.113 [-4.424, 4.776], mean action: 1.725 [0.000, 3.000], mean observation: 0.032 [-0.350, 0.936], loss: 0.345154, mean_absolute_error: 5.836701, mean_q: 3.660838
  483686/1500000: episode: 1052, duration: 34.741s, episode steps: 1000, steps per second: 29, episode reward: -180.542, mean reward: -0.181 [-4.402, 4.994], mean action: 1.832 [0.000, 3.000], mean observation: 0.055 [-0.593, 0.942], loss: 0.982414, mean_absolute_error: 5.971964, mean_q: 3.924307
  484686/1500000: episode: 1053, duration: 42.154s, episode steps: 1000, steps per second: 24, episode reward: -86.177, mean reward: -0.086 [-4.394, 5.795], mean action: 1.705 [0.000, 3.000], mean observation: 0.060 [-0.440, 0.932], loss: 0.914058, mean_absolute_error: 5.870803, mean_q: 3.584860
  485686/1500000: episode: 1054, duration: 39.042s, episode steps: 1000, steps per second: 26, episode reward: -122.380, mean reward: -0.122 [-3.848, 4.826], mean action: 1.687 [0.000, 3.000], mean observation: 0.056 [-0.249, 0.934], loss: 0.556768, mean_absolute_error: 5.982578, mean_q: 3.719869
  486686/1500000: episode: 1055, duration: 36.591s, episode steps: 1000, steps per second: 27, episode reward: -59.846, mean reward: -0.060 [-3.946, 5.562], mean action: 1.770 [0.000, 3.000], mean observation: 0.018 [-0.467, 0.927], loss: 0.476172, mean_absolute_error: 5.936402, mean_q: 3.623960
  487686/1500000: episode: 1056, duration: 36.229s, episode steps: 1000, steps per second: 28, episode reward: -101.240, mean reward: -0.101 [-3.470, 4.479], mean action: 1.733 [0.000, 3.000], mean observation: 0.040 [-0.323, 0.941], loss: 0.581654, mean_absolute_error: 5.998334, mean_q: 3.542364
  488686/1500000: episode: 1057, duration: 38.453s, episode steps: 1000, steps per second: 26, episode reward: 14.670, mean reward: 0.015 [-12.328, 22.596], mean action: 1.755 [0.000, 3.000], mean observation: 0.067 [-0.406, 1.000], loss: 0.998360, mean_absolute_error: 5.899289, mean_q: 3.390454
  489686/1500000: episode: 1058, duration: 38.397s, episode steps: 1000, steps per second: 26, episode reward: -98.236, mean reward: -0.098 [-3.809, 4.778], mean action: 1.772 [0.000, 3.000], mean observation: 0.058 [-0.306, 0.936], loss: 0.613147, mean_absolute_error: 6.014301, mean_q: 3.389858
  490686/1500000: episode: 1059, duration: 37.828s, episode steps: 1000, steps per second: 26, episode reward: -87.195, mean reward: -0.087 [-3.476, 4.294], mean action: 1.763 [0.000, 3.000], mean observation: 0.013 [-0.447, 0.932], loss: 0.614351, mean_absolute_error: 5.992701, mean_q: 3.358707
  491370/1500000: episode: 1060, duration: 24.679s, episode steps: 684, steps per second: 28, episode reward: 146.467, mean reward: 0.214 [-18.846, 100.000], mean action: 1.958 [0.000, 3.000], mean observation: 0.155 [-0.774, 1.000], loss: 0.764731, mean_absolute_error: 5.980377, mean_q: 3.367849
  492370/1500000: episode: 1061, duration: 35.099s, episode steps: 1000, steps per second: 28, episode reward: -99.892, mean reward: -0.100 [-4.320, 5.201], mean action: 1.720 [0.000, 3.000], mean observation: 0.050 [-0.364, 0.936], loss: 0.797178, mean_absolute_error: 6.033854, mean_q: 3.348788
  493370/1500000: episode: 1062, duration: 50.038s, episode steps: 1000, steps per second: 20, episode reward: -119.478, mean reward: -0.119 [-3.422, 4.798], mean action: 1.788 [0.000, 3.000], mean observation: 0.057 [-0.260, 0.938], loss: 0.673307, mean_absolute_error: 5.997708, mean_q: 3.261208
  494370/1500000: episode: 1063, duration: 47.496s, episode steps: 1000, steps per second: 21, episode reward: -176.811, mean reward: -0.177 [-3.980, 5.308], mean action: 1.667 [0.000, 3.000], mean observation: 0.065 [-0.363, 0.940], loss: 0.832482, mean_absolute_error: 5.940154, mean_q: 3.215269
  495370/1500000: episode: 1064, duration: 34.330s, episode steps: 1000, steps per second: 29, episode reward: -77.689, mean reward: -0.078 [-3.630, 4.790], mean action: 1.777 [0.000, 3.000], mean observation: 0.056 [-0.597, 0.947], loss: 0.542891, mean_absolute_error: 5.871182, mean_q: 3.061973
  496370/1500000: episode: 1065, duration: 36.033s, episode steps: 1000, steps per second: 28, episode reward: -121.109, mean reward: -0.121 [-3.633, 4.123], mean action: 1.837 [0.000, 3.000], mean observation: 0.070 [-0.201, 0.949], loss: 1.042771, mean_absolute_error: 5.936790, mean_q: 3.348294
  497370/1500000: episode: 1066, duration: 38.426s, episode steps: 1000, steps per second: 26, episode reward: -86.207, mean reward: -0.086 [-3.485, 5.946], mean action: 1.736 [0.000, 3.000], mean observation: 0.074 [-0.331, 0.992], loss: 0.582859, mean_absolute_error: 5.870053, mean_q: 3.400424
  498370/1500000: episode: 1067, duration: 36.301s, episode steps: 1000, steps per second: 28, episode reward: -88.183, mean reward: -0.088 [-4.107, 4.637], mean action: 1.850 [0.000, 3.000], mean observation: 0.065 [-0.408, 0.959], loss: 0.704074, mean_absolute_error: 5.861032, mean_q: 3.335486
  499370/1500000: episode: 1068, duration: 35.842s, episode steps: 1000, steps per second: 28, episode reward: -92.826, mean reward: -0.093 [-4.037, 4.698], mean action: 1.569 [0.000, 3.000], mean observation: 0.023 [-0.351, 0.942], loss: 0.909218, mean_absolute_error: 5.970429, mean_q: 3.394876
  500370/1500000: episode: 1069, duration: 34.644s, episode steps: 1000, steps per second: 29, episode reward: -134.730, mean reward: -0.135 [-3.827, 4.535], mean action: 1.807 [0.000, 3.000], mean observation: 0.046 [-0.281, 0.941], loss: 0.674676, mean_absolute_error: 5.916817, mean_q: 3.188504
  501370/1500000: episode: 1070, duration: 33.810s, episode steps: 1000, steps per second: 30, episode reward: -48.665, mean reward: -0.049 [-14.111, 11.638], mean action: 1.819 [0.000, 3.000], mean observation: 0.051 [-0.470, 1.000], loss: 0.880600, mean_absolute_error: 6.030416, mean_q: 3.298165
  502370/1500000: episode: 1071, duration: 34.506s, episode steps: 1000, steps per second: 29, episode reward: -74.975, mean reward: -0.075 [-3.331, 4.293], mean action: 1.817 [0.000, 3.000], mean observation: -0.025 [-0.703, 0.965], loss: 0.670416, mean_absolute_error: 6.032435, mean_q: 3.148023
  503370/1500000: episode: 1072, duration: 34.690s, episode steps: 1000, steps per second: 29, episode reward: -61.434, mean reward: -0.061 [-4.016, 4.650], mean action: 1.664 [0.000, 3.000], mean observation: 0.051 [-0.283, 0.972], loss: 0.891633, mean_absolute_error: 6.010265, mean_q: 2.990763
  504370/1500000: episode: 1073, duration: 34.035s, episode steps: 1000, steps per second: 29, episode reward: -102.196, mean reward: -0.102 [-4.344, 3.807], mean action: 1.824 [0.000, 3.000], mean observation: -0.024 [-0.628, 0.933], loss: 0.374606, mean_absolute_error: 6.038758, mean_q: 3.150944
  505370/1500000: episode: 1074, duration: 39.244s, episode steps: 1000, steps per second: 25, episode reward: -37.677, mean reward: -0.038 [-4.484, 5.563], mean action: 1.711 [0.000, 3.000], mean observation: 0.067 [-0.340, 0.982], loss: 0.419422, mean_absolute_error: 6.014146, mean_q: 2.864071
  506264/1500000: episode: 1075, duration: 39.997s, episode steps: 894, steps per second: 22, episode reward: 134.729, mean reward: 0.151 [-14.568, 100.000], mean action: 1.655 [0.000, 3.000], mean observation: 0.045 [-0.473, 1.000], loss: 0.526266, mean_absolute_error: 6.031937, mean_q: 2.730551
  506942/1500000: episode: 1076, duration: 29.439s, episode steps: 678, steps per second: 23, episode reward: -178.903, mean reward: -0.264 [-100.000, 3.874], mean action: 1.811 [0.000, 3.000], mean observation: -0.077 [-1.000, 0.954], loss: 0.382784, mean_absolute_error: 5.951511, mean_q: 2.753489
  507942/1500000: episode: 1077, duration: 48.728s, episode steps: 1000, steps per second: 21, episode reward: -101.566, mean reward: -0.102 [-3.338, 4.425], mean action: 1.905 [0.000, 3.000], mean observation: 0.009 [-0.506, 0.939], loss: 0.708069, mean_absolute_error: 6.030280, mean_q: 2.842131
  508415/1500000: episode: 1078, duration: 20.547s, episode steps: 473, steps per second: 23, episode reward: -147.624, mean reward: -0.312 [-100.000, 3.772], mean action: 1.839 [0.000, 3.000], mean observation: -0.055 [-1.002, 0.979], loss: 0.401926, mean_absolute_error: 5.989069, mean_q: 2.739842
  509415/1500000: episode: 1079, duration: 44.735s, episode steps: 1000, steps per second: 22, episode reward: -92.181, mean reward: -0.092 [-3.348, 5.206], mean action: 1.840 [0.000, 3.000], mean observation: 0.076 [-0.372, 0.985], loss: 0.750278, mean_absolute_error: 5.979764, mean_q: 2.723974
  510415/1500000: episode: 1080, duration: 36.838s, episode steps: 1000, steps per second: 27, episode reward: -97.388, mean reward: -0.097 [-3.175, 3.998], mean action: 1.784 [0.000, 3.000], mean observation: 0.056 [-0.276, 0.938], loss: 0.402696, mean_absolute_error: 5.984128, mean_q: 2.637121
  511415/1500000: episode: 1081, duration: 39.476s, episode steps: 1000, steps per second: 25, episode reward: -91.329, mean reward: -0.091 [-3.235, 3.985], mean action: 1.803 [0.000, 3.000], mean observation: 0.056 [-0.208, 0.953], loss: 0.538960, mean_absolute_error: 6.013463, mean_q: 2.661951
  511994/1500000: episode: 1082, duration: 20.529s, episode steps: 579, steps per second: 28, episode reward: 131.304, mean reward: 0.227 [-17.235, 100.000], mean action: 1.430 [0.000, 3.000], mean observation: 0.084 [-0.719, 1.000], loss: 0.785561, mean_absolute_error: 6.190138, mean_q: 2.881095
  512785/1500000: episode: 1083, duration: 31.586s, episode steps: 791, steps per second: 25, episode reward: -268.628, mean reward: -0.340 [-100.000, 3.352], mean action: 1.779 [0.000, 3.000], mean observation: 0.006 [-1.001, 0.937], loss: 0.633051, mean_absolute_error: 6.043941, mean_q: 2.873097
  512951/1500000: episode: 1084, duration: 5.541s, episode steps: 166, steps per second: 30, episode reward: -95.404, mean reward: -0.575 [-100.000, 3.375], mean action: 1.536 [0.000, 3.000], mean observation: -0.147 [-1.001, 1.000], loss: 0.366796, mean_absolute_error: 5.989507, mean_q: 2.927217
  513152/1500000: episode: 1085, duration: 6.927s, episode steps: 201, steps per second: 29, episode reward: -123.331, mean reward: -0.614 [-100.000, 3.081], mean action: 1.517 [0.000, 3.000], mean observation: -0.113 [-1.002, 0.935], loss: 0.506707, mean_absolute_error: 6.183265, mean_q: 3.176155
  514136/1500000: episode: 1086, duration: 35.945s, episode steps: 984, steps per second: 27, episode reward: -269.753, mean reward: -0.274 [-100.000, 3.926], mean action: 1.788 [0.000, 3.000], mean observation: -0.030 [-1.000, 0.932], loss: 0.411637, mean_absolute_error: 6.279411, mean_q: 3.161838
  515136/1500000: episode: 1087, duration: 40.026s, episode steps: 1000, steps per second: 25, episode reward: -97.729, mean reward: -0.098 [-3.243, 3.737], mean action: 1.877 [0.000, 3.000], mean observation: 0.056 [-0.226, 0.937], loss: 0.637933, mean_absolute_error: 6.228433, mean_q: 3.242374
  516136/1500000: episode: 1088, duration: 35.122s, episode steps: 1000, steps per second: 28, episode reward: -99.115, mean reward: -0.099 [-4.018, 4.830], mean action: 1.666 [0.000, 3.000], mean observation: 0.035 [-0.577, 0.941], loss: 1.011434, mean_absolute_error: 6.283827, mean_q: 3.253516
  517136/1500000: episode: 1089, duration: 36.640s, episode steps: 1000, steps per second: 27, episode reward: -103.659, mean reward: -0.104 [-4.602, 5.520], mean action: 1.887 [0.000, 3.000], mean observation: -0.033 [-0.793, 0.926], loss: 0.552009, mean_absolute_error: 6.354272, mean_q: 3.166003
  518136/1500000: episode: 1090, duration: 30.684s, episode steps: 1000, steps per second: 33, episode reward: -87.419, mean reward: -0.087 [-3.408, 3.630], mean action: 1.723 [0.000, 3.000], mean observation: 0.017 [-0.739, 0.976], loss: 0.836414, mean_absolute_error: 6.343884, mean_q: 3.132001
  519136/1500000: episode: 1091, duration: 22.978s, episode steps: 1000, steps per second: 44, episode reward: -135.555, mean reward: -0.136 [-3.355, 4.596], mean action: 1.716 [0.000, 3.000], mean observation: 0.063 [-0.331, 0.936], loss: 1.554402, mean_absolute_error: 6.343513, mean_q: 3.043291
  520136/1500000: episode: 1092, duration: 26.503s, episode steps: 1000, steps per second: 38, episode reward: -116.655, mean reward: -0.117 [-3.951, 3.573], mean action: 1.803 [0.000, 3.000], mean observation: 0.018 [-0.635, 0.934], loss: 0.631960, mean_absolute_error: 6.314690, mean_q: 2.866147
  521136/1500000: episode: 1093, duration: 25.482s, episode steps: 1000, steps per second: 39, episode reward: -142.023, mean reward: -0.142 [-4.121, 3.236], mean action: 1.936 [0.000, 3.000], mean observation: -0.039 [-0.842, 0.932], loss: 0.522061, mean_absolute_error: 6.342001, mean_q: 2.822988
  521396/1500000: episode: 1094, duration: 6.013s, episode steps: 260, steps per second: 43, episode reward: -147.243, mean reward: -0.566 [-100.000, 2.903], mean action: 1.781 [0.000, 3.000], mean observation: -0.060 [-1.000, 0.930], loss: 0.356171, mean_absolute_error: 6.130783, mean_q: 2.570669
  522396/1500000: episode: 1095, duration: 26.186s, episode steps: 1000, steps per second: 38, episode reward: -140.653, mean reward: -0.141 [-4.007, 3.461], mean action: 1.864 [0.000, 3.000], mean observation: 0.041 [-0.571, 0.957], loss: 0.744691, mean_absolute_error: 6.296653, mean_q: 2.854895
  523396/1500000: episode: 1096, duration: 23.691s, episode steps: 1000, steps per second: 42, episode reward: -85.325, mean reward: -0.085 [-3.264, 4.103], mean action: 1.539 [0.000, 3.000], mean observation: 0.078 [-0.530, 0.924], loss: 1.208421, mean_absolute_error: 6.227451, mean_q: 2.613413
  524396/1500000: episode: 1097, duration: 29.121s, episode steps: 1000, steps per second: 34, episode reward: -88.562, mean reward: -0.089 [-3.313, 4.013], mean action: 1.655 [0.000, 3.000], mean observation: 0.068 [-0.272, 0.932], loss: 0.983354, mean_absolute_error: 6.290058, mean_q: 2.720603
  525396/1500000: episode: 1098, duration: 19.951s, episode steps: 1000, steps per second: 50, episode reward: -89.947, mean reward: -0.090 [-3.417, 3.903], mean action: 1.734 [0.000, 3.000], mean observation: 0.019 [-0.916, 0.942], loss: 0.468103, mean_absolute_error: 6.210161, mean_q: 2.550051
  526396/1500000: episode: 1099, duration: 26.701s, episode steps: 1000, steps per second: 37, episode reward: -112.556, mean reward: -0.113 [-3.201, 4.028], mean action: 1.580 [0.000, 3.000], mean observation: 0.033 [-0.510, 0.933], loss: 1.134060, mean_absolute_error: 6.185968, mean_q: 2.495352
  527396/1500000: episode: 1100, duration: 32.089s, episode steps: 1000, steps per second: 31, episode reward: -108.377, mean reward: -0.108 [-3.399, 3.691], mean action: 1.818 [0.000, 3.000], mean observation: 0.046 [-0.476, 0.933], loss: 0.702766, mean_absolute_error: 6.164454, mean_q: 2.378417
  528396/1500000: episode: 1101, duration: 27.166s, episode steps: 1000, steps per second: 37, episode reward: -72.489, mean reward: -0.072 [-4.138, 3.972], mean action: 1.603 [0.000, 3.000], mean observation: 0.067 [-0.215, 1.022], loss: 0.636397, mean_absolute_error: 6.270153, mean_q: 2.409040
  529396/1500000: episode: 1102, duration: 24.165s, episode steps: 1000, steps per second: 41, episode reward: -110.089, mean reward: -0.110 [-3.281, 3.800], mean action: 1.707 [0.000, 3.000], mean observation: 0.038 [-0.400, 0.931], loss: 0.456722, mean_absolute_error: 6.170859, mean_q: 2.295944
  529605/1500000: episode: 1103, duration: 6.586s, episode steps: 209, steps per second: 32, episode reward: -134.266, mean reward: -0.642 [-100.000, 2.406], mean action: 1.713 [0.000, 3.000], mean observation: -0.071 [-1.000, 0.939], loss: 0.974991, mean_absolute_error: 6.148702, mean_q: 2.468640
  530605/1500000: episode: 1104, duration: 28.857s, episode steps: 1000, steps per second: 35, episode reward: -98.534, mean reward: -0.099 [-4.266, 4.082], mean action: 1.794 [0.000, 3.000], mean observation: 0.037 [-0.455, 0.927], loss: 0.678037, mean_absolute_error: 6.150813, mean_q: 2.321199
  531605/1500000: episode: 1105, duration: 25.740s, episode steps: 1000, steps per second: 39, episode reward: -92.216, mean reward: -0.092 [-3.094, 3.425], mean action: 1.416 [0.000, 3.000], mean observation: 0.039 [-0.460, 0.938], loss: 0.841987, mean_absolute_error: 6.200898, mean_q: 2.360156
  532605/1500000: episode: 1106, duration: 33.201s, episode steps: 1000, steps per second: 30, episode reward: -125.294, mean reward: -0.125 [-3.298, 3.777], mean action: 1.463 [0.000, 3.000], mean observation: 0.055 [-0.290, 0.941], loss: 0.538015, mean_absolute_error: 6.064675, mean_q: 2.219316
  533605/1500000: episode: 1107, duration: 35.656s, episode steps: 1000, steps per second: 28, episode reward: -65.280, mean reward: -0.065 [-3.746, 3.929], mean action: 1.723 [0.000, 3.000], mean observation: 0.023 [-0.580, 0.939], loss: 0.587057, mean_absolute_error: 6.081447, mean_q: 2.182680
  534605/1500000: episode: 1108, duration: 29.025s, episode steps: 1000, steps per second: 34, episode reward: -90.577, mean reward: -0.091 [-3.829, 3.632], mean action: 1.608 [0.000, 3.000], mean observation: 0.052 [-0.526, 0.977], loss: 0.684656, mean_absolute_error: 5.953363, mean_q: 2.130435
  535605/1500000: episode: 1109, duration: 26.927s, episode steps: 1000, steps per second: 37, episode reward: -103.197, mean reward: -0.103 [-4.678, 3.608], mean action: 1.650 [0.000, 3.000], mean observation: 0.043 [-0.464, 0.946], loss: 0.784827, mean_absolute_error: 6.026137, mean_q: 2.069693
  536605/1500000: episode: 1110, duration: 40.374s, episode steps: 1000, steps per second: 25, episode reward: -96.461, mean reward: -0.096 [-3.097, 3.918], mean action: 1.829 [0.000, 3.000], mean observation: 0.039 [-0.473, 0.948], loss: 0.556622, mean_absolute_error: 6.013733, mean_q: 2.083462
  537605/1500000: episode: 1111, duration: 49.775s, episode steps: 1000, steps per second: 20, episode reward: -77.152, mean reward: -0.077 [-4.033, 3.842], mean action: 1.715 [0.000, 3.000], mean observation: 0.026 [-0.833, 0.998], loss: 0.381238, mean_absolute_error: 6.064680, mean_q: 2.304546
  538605/1500000: episode: 1112, duration: 40.448s, episode steps: 1000, steps per second: 25, episode reward: -113.669, mean reward: -0.114 [-3.020, 3.459], mean action: 1.571 [0.000, 3.000], mean observation: 0.051 [-0.420, 0.944], loss: 0.637712, mean_absolute_error: 6.013399, mean_q: 2.139301
  539605/1500000: episode: 1113, duration: 24.479s, episode steps: 1000, steps per second: 41, episode reward: -120.119, mean reward: -0.120 [-3.016, 4.313], mean action: 1.722 [0.000, 3.000], mean observation: 0.051 [-0.261, 0.938], loss: 0.524553, mean_absolute_error: 5.939206, mean_q: 1.940966
  540605/1500000: episode: 1114, duration: 32.972s, episode steps: 1000, steps per second: 30, episode reward: -94.936, mean reward: -0.095 [-3.717, 3.509], mean action: 1.638 [0.000, 3.000], mean observation: 0.046 [-0.523, 1.003], loss: 0.525171, mean_absolute_error: 5.887900, mean_q: 1.892131
  541605/1500000: episode: 1115, duration: 36.105s, episode steps: 1000, steps per second: 28, episode reward: -123.987, mean reward: -0.124 [-3.149, 4.169], mean action: 1.613 [0.000, 3.000], mean observation: 0.025 [-0.512, 0.938], loss: 1.213022, mean_absolute_error: 5.886999, mean_q: 1.809597
  542605/1500000: episode: 1116, duration: 41.520s, episode steps: 1000, steps per second: 24, episode reward: -69.010, mean reward: -0.069 [-3.350, 3.758], mean action: 1.566 [0.000, 3.000], mean observation: 0.056 [-0.430, 0.973], loss: 0.955028, mean_absolute_error: 5.876215, mean_q: 1.921861
  543605/1500000: episode: 1117, duration: 30.591s, episode steps: 1000, steps per second: 33, episode reward: -133.627, mean reward: -0.134 [-2.859, 4.156], mean action: 1.436 [0.000, 3.000], mean observation: 0.043 [-0.443, 0.941], loss: 0.698146, mean_absolute_error: 5.731172, mean_q: 1.740958
  544605/1500000: episode: 1118, duration: 33.309s, episode steps: 1000, steps per second: 30, episode reward: -121.871, mean reward: -0.122 [-3.164, 4.399], mean action: 1.407 [0.000, 3.000], mean observation: 0.074 [-0.311, 0.945], loss: 1.007820, mean_absolute_error: 5.699498, mean_q: 1.665140
  545605/1500000: episode: 1119, duration: 35.962s, episode steps: 1000, steps per second: 28, episode reward: -67.146, mean reward: -0.067 [-2.899, 4.085], mean action: 1.690 [0.000, 3.000], mean observation: 0.067 [-0.455, 0.927], loss: 1.602049, mean_absolute_error: 5.564634, mean_q: 1.418463
  546605/1500000: episode: 1120, duration: 36.518s, episode steps: 1000, steps per second: 27, episode reward: -112.067, mean reward: -0.112 [-4.593, 3.774], mean action: 1.501 [0.000, 3.000], mean observation: 0.083 [-0.278, 0.937], loss: 0.663164, mean_absolute_error: 5.561594, mean_q: 1.441574
  547605/1500000: episode: 1121, duration: 37.324s, episode steps: 1000, steps per second: 27, episode reward: -103.907, mean reward: -0.104 [-3.164, 4.131], mean action: 1.644 [0.000, 3.000], mean observation: 0.023 [-0.514, 0.937], loss: 0.804766, mean_absolute_error: 5.468701, mean_q: 1.048212
  548605/1500000: episode: 1122, duration: 39.537s, episode steps: 1000, steps per second: 25, episode reward: -98.210, mean reward: -0.098 [-4.172, 4.151], mean action: 1.718 [0.000, 3.000], mean observation: 0.035 [-0.528, 0.948], loss: 0.319623, mean_absolute_error: 5.523515, mean_q: 1.234697
  549605/1500000: episode: 1123, duration: 41.017s, episode steps: 1000, steps per second: 24, episode reward: -122.172, mean reward: -0.122 [-3.073, 3.336], mean action: 1.496 [0.000, 3.000], mean observation: 0.076 [-0.334, 0.945], loss: 1.461443, mean_absolute_error: 5.457069, mean_q: 1.038405
  550605/1500000: episode: 1124, duration: 34.574s, episode steps: 1000, steps per second: 29, episode reward: -85.203, mean reward: -0.085 [-3.254, 4.170], mean action: 1.394 [0.000, 3.000], mean observation: 0.058 [-0.375, 1.017], loss: 1.029899, mean_absolute_error: 5.448885, mean_q: 0.865712
  551605/1500000: episode: 1125, duration: 36.940s, episode steps: 1000, steps per second: 27, episode reward: -71.783, mean reward: -0.072 [-3.149, 4.174], mean action: 1.478 [0.000, 3.000], mean observation: 0.052 [-0.340, 1.010], loss: 0.343218, mean_absolute_error: 5.470775, mean_q: 0.910773
  552605/1500000: episode: 1126, duration: 32.442s, episode steps: 1000, steps per second: 31, episode reward: -75.727, mean reward: -0.076 [-3.042, 4.477], mean action: 1.583 [0.000, 3.000], mean observation: 0.038 [-0.512, 0.943], loss: 0.820362, mean_absolute_error: 5.438530, mean_q: 0.878314
  553605/1500000: episode: 1127, duration: 33.557s, episode steps: 1000, steps per second: 30, episode reward: -125.136, mean reward: -0.125 [-3.200, 3.653], mean action: 1.746 [0.000, 3.000], mean observation: 0.019 [-0.436, 0.941], loss: 0.594429, mean_absolute_error: 5.435843, mean_q: 1.019839
  554605/1500000: episode: 1128, duration: 33.766s, episode steps: 1000, steps per second: 30, episode reward: -91.674, mean reward: -0.092 [-3.108, 4.366], mean action: 1.627 [0.000, 3.000], mean observation: 0.013 [-0.442, 0.927], loss: 0.751442, mean_absolute_error: 5.454637, mean_q: 1.075718
  555605/1500000: episode: 1129, duration: 39.507s, episode steps: 1000, steps per second: 25, episode reward: -72.402, mean reward: -0.072 [-3.098, 3.893], mean action: 1.601 [0.000, 3.000], mean observation: 0.040 [-0.610, 0.974], loss: 0.529391, mean_absolute_error: 5.401957, mean_q: 0.897964
  556605/1500000: episode: 1130, duration: 39.344s, episode steps: 1000, steps per second: 25, episode reward: -76.839, mean reward: -0.077 [-3.693, 4.878], mean action: 1.671 [0.000, 3.000], mean observation: 0.035 [-0.612, 0.985], loss: 0.771395, mean_absolute_error: 5.374957, mean_q: 1.093410
  557605/1500000: episode: 1131, duration: 32.589s, episode steps: 1000, steps per second: 31, episode reward: -103.838, mean reward: -0.104 [-3.132, 3.749], mean action: 1.707 [0.000, 3.000], mean observation: 0.011 [-0.505, 0.929], loss: 0.787511, mean_absolute_error: 5.259188, mean_q: 1.055391
  558605/1500000: episode: 1132, duration: 33.168s, episode steps: 1000, steps per second: 30, episode reward: -85.221, mean reward: -0.085 [-3.272, 3.849], mean action: 1.530 [0.000, 3.000], mean observation: 0.026 [-0.593, 0.935], loss: 0.580110, mean_absolute_error: 5.226772, mean_q: 1.131071
  559605/1500000: episode: 1133, duration: 34.674s, episode steps: 1000, steps per second: 29, episode reward: -118.697, mean reward: -0.119 [-3.829, 4.165], mean action: 1.626 [0.000, 3.000], mean observation: 0.040 [-0.519, 0.933], loss: 0.606463, mean_absolute_error: 5.320906, mean_q: 1.407135
  560605/1500000: episode: 1134, duration: 34.835s, episode steps: 1000, steps per second: 29, episode reward: -79.599, mean reward: -0.080 [-3.350, 4.171], mean action: 1.659 [0.000, 3.000], mean observation: 0.036 [-0.609, 0.963], loss: 0.351178, mean_absolute_error: 5.292451, mean_q: 1.222524
  561605/1500000: episode: 1135, duration: 32.694s, episode steps: 1000, steps per second: 31, episode reward: -64.699, mean reward: -0.065 [-4.458, 4.152], mean action: 1.854 [0.000, 3.000], mean observation: 0.037 [-0.735, 0.966], loss: 0.532784, mean_absolute_error: 5.200711, mean_q: 1.045454
  562605/1500000: episode: 1136, duration: 35.624s, episode steps: 1000, steps per second: 28, episode reward: -119.414, mean reward: -0.119 [-4.286, 4.058], mean action: 1.567 [0.000, 3.000], mean observation: 0.110 [-0.146, 0.954], loss: 0.974605, mean_absolute_error: 5.158872, mean_q: 1.015483
  563605/1500000: episode: 1137, duration: 35.193s, episode steps: 1000, steps per second: 28, episode reward: -62.161, mean reward: -0.062 [-3.123, 4.123], mean action: 1.745 [0.000, 3.000], mean observation: 0.034 [-0.716, 0.945], loss: 0.415574, mean_absolute_error: 5.254063, mean_q: 1.124097
  564605/1500000: episode: 1138, duration: 37.118s, episode steps: 1000, steps per second: 27, episode reward: -125.463, mean reward: -0.125 [-4.081, 4.323], mean action: 1.742 [0.000, 3.000], mean observation: 0.022 [-0.491, 0.943], loss: 0.507613, mean_absolute_error: 5.206891, mean_q: 1.179994
  565605/1500000: episode: 1139, duration: 37.631s, episode steps: 1000, steps per second: 27, episode reward: -98.196, mean reward: -0.098 [-3.209, 4.451], mean action: 1.747 [0.000, 3.000], mean observation: 0.047 [-0.465, 0.944], loss: 0.379948, mean_absolute_error: 5.131630, mean_q: 1.030029
  566605/1500000: episode: 1140, duration: 35.118s, episode steps: 1000, steps per second: 28, episode reward: -86.010, mean reward: -0.086 [-3.274, 4.088], mean action: 1.421 [0.000, 3.000], mean observation: 0.057 [-0.402, 0.970], loss: 1.375170, mean_absolute_error: 5.127627, mean_q: 1.077789
  567605/1500000: episode: 1141, duration: 35.103s, episode steps: 1000, steps per second: 28, episode reward: -86.311, mean reward: -0.086 [-3.024, 4.044], mean action: 1.443 [0.000, 3.000], mean observation: 0.030 [-0.507, 0.925], loss: 0.780232, mean_absolute_error: 5.197597, mean_q: 0.849026
  568605/1500000: episode: 1142, duration: 34.765s, episode steps: 1000, steps per second: 29, episode reward: -95.882, mean reward: -0.096 [-3.269, 4.770], mean action: 1.664 [0.000, 3.000], mean observation: 0.002 [-0.473, 0.927], loss: 0.587639, mean_absolute_error: 5.069654, mean_q: 0.649060
  569605/1500000: episode: 1143, duration: 33.862s, episode steps: 1000, steps per second: 30, episode reward: -79.018, mean reward: -0.079 [-3.856, 3.336], mean action: 1.939 [0.000, 3.000], mean observation: -0.009 [-0.781, 0.927], loss: 0.583639, mean_absolute_error: 5.142056, mean_q: 0.845113
  570605/1500000: episode: 1144, duration: 32.938s, episode steps: 1000, steps per second: 30, episode reward: -64.184, mean reward: -0.064 [-3.188, 3.763], mean action: 1.668 [0.000, 3.000], mean observation: 0.042 [-0.677, 1.026], loss: 0.664289, mean_absolute_error: 5.148930, mean_q: 0.714975
  571605/1500000: episode: 1145, duration: 43.395s, episode steps: 1000, steps per second: 23, episode reward: -81.960, mean reward: -0.082 [-4.224, 4.126], mean action: 1.576 [0.000, 3.000], mean observation: 0.040 [-0.424, 1.013], loss: 0.548732, mean_absolute_error: 5.196641, mean_q: 0.749301
  572605/1500000: episode: 1146, duration: 54.846s, episode steps: 1000, steps per second: 18, episode reward: -138.813, mean reward: -0.139 [-3.148, 3.915], mean action: 1.615 [0.000, 3.000], mean observation: 0.033 [-0.544, 0.938], loss: 0.713781, mean_absolute_error: 5.127576, mean_q: 0.680888
  573605/1500000: episode: 1147, duration: 57.415s, episode steps: 1000, steps per second: 17, episode reward: -73.216, mean reward: -0.073 [-3.805, 4.721], mean action: 1.719 [0.000, 3.000], mean observation: 0.114 [-0.165, 0.995], loss: 0.596345, mean_absolute_error: 5.134310, mean_q: 0.478218
  574605/1500000: episode: 1148, duration: 54.163s, episode steps: 1000, steps per second: 18, episode reward: -140.709, mean reward: -0.141 [-3.466, 4.400], mean action: 1.813 [0.000, 3.000], mean observation: 0.011 [-0.545, 0.935], loss: 0.734969, mean_absolute_error: 5.065147, mean_q: 0.501482
  575605/1500000: episode: 1149, duration: 45.123s, episode steps: 1000, steps per second: 22, episode reward: -113.494, mean reward: -0.113 [-3.324, 4.119], mean action: 1.706 [0.000, 3.000], mean observation: 0.018 [-0.425, 0.933], loss: 0.528402, mean_absolute_error: 5.122888, mean_q: 0.495680
  576605/1500000: episode: 1150, duration: 53.669s, episode steps: 1000, steps per second: 19, episode reward: -70.949, mean reward: -0.071 [-3.787, 4.116], mean action: 1.544 [0.000, 3.000], mean observation: 0.089 [-0.273, 0.976], loss: 0.595662, mean_absolute_error: 5.065357, mean_q: 0.385580
  577605/1500000: episode: 1151, duration: 37.242s, episode steps: 1000, steps per second: 27, episode reward: -84.443, mean reward: -0.084 [-3.341, 4.209], mean action: 1.571 [0.000, 3.000], mean observation: 0.058 [-0.348, 0.937], loss: 1.355360, mean_absolute_error: 5.113011, mean_q: 0.339580
  578605/1500000: episode: 1152, duration: 38.588s, episode steps: 1000, steps per second: 26, episode reward: -96.892, mean reward: -0.097 [-4.185, 4.044], mean action: 1.645 [0.000, 3.000], mean observation: 0.051 [-0.341, 0.934], loss: 0.529699, mean_absolute_error: 5.107922, mean_q: 0.277200
  579605/1500000: episode: 1153, duration: 30.299s, episode steps: 1000, steps per second: 33, episode reward: -117.700, mean reward: -0.118 [-4.268, 3.937], mean action: 1.529 [0.000, 3.000], mean observation: 0.041 [-0.427, 0.934], loss: 0.871848, mean_absolute_error: 5.161056, mean_q: 0.341307
  580605/1500000: episode: 1154, duration: 29.219s, episode steps: 1000, steps per second: 34, episode reward: -101.413, mean reward: -0.101 [-3.709, 4.922], mean action: 1.612 [0.000, 3.000], mean observation: 0.006 [-0.582, 0.927], loss: 0.930041, mean_absolute_error: 5.139995, mean_q: 0.284845
  581605/1500000: episode: 1155, duration: 30.108s, episode steps: 1000, steps per second: 33, episode reward: -94.195, mean reward: -0.094 [-3.054, 4.199], mean action: 1.495 [0.000, 3.000], mean observation: 0.039 [-0.547, 0.959], loss: 0.665569, mean_absolute_error: 5.134689, mean_q: 0.083239
  582605/1500000: episode: 1156, duration: 40.178s, episode steps: 1000, steps per second: 25, episode reward: -68.273, mean reward: -0.068 [-3.224, 3.673], mean action: 1.494 [0.000, 3.000], mean observation: 0.084 [-0.300, 0.970], loss: 0.727198, mean_absolute_error: 5.259539, mean_q: 0.026562
  583605/1500000: episode: 1157, duration: 43.228s, episode steps: 1000, steps per second: 23, episode reward: -74.721, mean reward: -0.075 [-3.465, 4.001], mean action: 1.904 [0.000, 3.000], mean observation: 0.030 [-0.790, 0.935], loss: 0.759867, mean_absolute_error: 5.160662, mean_q: -0.000454
  584605/1500000: episode: 1158, duration: 46.855s, episode steps: 1000, steps per second: 21, episode reward: -68.434, mean reward: -0.068 [-3.557, 4.001], mean action: 1.671 [0.000, 3.000], mean observation: 0.039 [-0.531, 0.925], loss: 1.151944, mean_absolute_error: 5.210186, mean_q: 0.149456
  585605/1500000: episode: 1159, duration: 38.350s, episode steps: 1000, steps per second: 26, episode reward: -112.564, mean reward: -0.113 [-3.767, 3.985], mean action: 1.601 [0.000, 3.000], mean observation: 0.046 [-0.371, 0.966], loss: 0.517805, mean_absolute_error: 5.239535, mean_q: 0.139793
  586605/1500000: episode: 1160, duration: 40.289s, episode steps: 1000, steps per second: 25, episode reward: -109.818, mean reward: -0.110 [-3.194, 3.605], mean action: 1.430 [0.000, 3.000], mean observation: 0.059 [-0.324, 0.945], loss: 0.546336, mean_absolute_error: 5.155631, mean_q: 0.020448
  587605/1500000: episode: 1161, duration: 37.709s, episode steps: 1000, steps per second: 27, episode reward: -131.457, mean reward: -0.131 [-3.041, 4.027], mean action: 1.647 [0.000, 3.000], mean observation: 0.016 [-0.478, 0.948], loss: 0.483260, mean_absolute_error: 5.173879, mean_q: -0.037846
  588605/1500000: episode: 1162, duration: 40.537s, episode steps: 1000, steps per second: 25, episode reward: -154.494, mean reward: -0.154 [-2.927, 4.125], mean action: 1.694 [0.000, 3.000], mean observation: 0.009 [-0.533, 0.942], loss: 0.517877, mean_absolute_error: 5.188703, mean_q: -0.155216
  589605/1500000: episode: 1163, duration: 30.437s, episode steps: 1000, steps per second: 33, episode reward: -68.685, mean reward: -0.069 [-3.179, 4.195], mean action: 1.754 [0.000, 3.000], mean observation: 0.016 [-0.752, 0.927], loss: 0.404757, mean_absolute_error: 5.222583, mean_q: -0.166369
  590605/1500000: episode: 1164, duration: 15.503s, episode steps: 1000, steps per second: 65, episode reward: -88.989, mean reward: -0.089 [-4.297, 3.640], mean action: 1.712 [0.000, 3.000], mean observation: 0.015 [-0.620, 0.951], loss: 0.499575, mean_absolute_error: 5.281512, mean_q: -0.138812
  591605/1500000: episode: 1165, duration: 16.004s, episode steps: 1000, steps per second: 62, episode reward: -112.781, mean reward: -0.113 [-3.391, 4.467], mean action: 1.492 [0.000, 3.000], mean observation: 0.039 [-0.394, 0.934], loss: 1.793710, mean_absolute_error: 5.331215, mean_q: -0.209924
  592605/1500000: episode: 1166, duration: 18.829s, episode steps: 1000, steps per second: 53, episode reward: -85.654, mean reward: -0.086 [-3.804, 4.102], mean action: 1.700 [0.000, 3.000], mean observation: 0.013 [-0.553, 0.980], loss: 0.709679, mean_absolute_error: 5.390861, mean_q: -0.196403
  593605/1500000: episode: 1167, duration: 23.824s, episode steps: 1000, steps per second: 42, episode reward: -49.940, mean reward: -0.050 [-2.994, 3.935], mean action: 1.483 [0.000, 3.000], mean observation: 0.076 [-0.276, 0.944], loss: 0.736799, mean_absolute_error: 5.394196, mean_q: -0.254006
  594605/1500000: episode: 1168, duration: 21.327s, episode steps: 1000, steps per second: 47, episode reward: -47.069, mean reward: -0.047 [-3.595, 6.319], mean action: 1.535 [0.000, 3.000], mean observation: 0.044 [-0.386, 0.939], loss: 0.426902, mean_absolute_error: 5.393507, mean_q: -0.298090
  595605/1500000: episode: 1169, duration: 31.290s, episode steps: 1000, steps per second: 32, episode reward: -105.113, mean reward: -0.105 [-4.491, 4.231], mean action: 1.681 [0.000, 3.000], mean observation: 0.018 [-0.446, 0.984], loss: 0.548069, mean_absolute_error: 5.365711, mean_q: -0.475665
  596605/1500000: episode: 1170, duration: 38.912s, episode steps: 1000, steps per second: 26, episode reward: -114.862, mean reward: -0.115 [-4.308, 3.848], mean action: 1.912 [0.000, 3.000], mean observation: -0.006 [-0.604, 0.932], loss: 0.857720, mean_absolute_error: 5.416116, mean_q: -0.418241
  597605/1500000: episode: 1171, duration: 35.792s, episode steps: 1000, steps per second: 28, episode reward: -56.090, mean reward: -0.056 [-3.129, 4.307], mean action: 1.501 [0.000, 3.000], mean observation: 0.125 [-0.310, 0.966], loss: 1.040638, mean_absolute_error: 5.349602, mean_q: -0.585674
  598605/1500000: episode: 1172, duration: 37.717s, episode steps: 1000, steps per second: 27, episode reward: -112.208, mean reward: -0.112 [-3.994, 4.043], mean action: 1.582 [0.000, 3.000], mean observation: 0.028 [-0.507, 0.931], loss: 0.729964, mean_absolute_error: 5.481490, mean_q: -0.703746
  599605/1500000: episode: 1173, duration: 40.059s, episode steps: 1000, steps per second: 25, episode reward: -87.484, mean reward: -0.087 [-3.186, 4.018], mean action: 1.715 [0.000, 3.000], mean observation: 0.008 [-0.515, 0.926], loss: 1.091152, mean_absolute_error: 5.511809, mean_q: -0.735202
  600605/1500000: episode: 1174, duration: 36.093s, episode steps: 1000, steps per second: 28, episode reward: -98.417, mean reward: -0.098 [-3.018, 4.096], mean action: 1.426 [0.000, 3.000], mean observation: 0.023 [-0.424, 0.938], loss: 1.917993, mean_absolute_error: 5.565904, mean_q: -0.979005
  601605/1500000: episode: 1175, duration: 36.386s, episode steps: 1000, steps per second: 27, episode reward: -105.786, mean reward: -0.106 [-3.340, 4.597], mean action: 1.476 [0.000, 3.000], mean observation: 0.029 [-0.405, 0.935], loss: 1.744145, mean_absolute_error: 5.574202, mean_q: -1.110673
  602605/1500000: episode: 1176, duration: 36.993s, episode steps: 1000, steps per second: 27, episode reward: -100.674, mean reward: -0.101 [-2.968, 3.924], mean action: 1.408 [0.000, 3.000], mean observation: 0.059 [-0.350, 0.936], loss: 0.416919, mean_absolute_error: 5.490115, mean_q: -1.092893
  603605/1500000: episode: 1177, duration: 35.901s, episode steps: 1000, steps per second: 28, episode reward: -60.867, mean reward: -0.061 [-3.777, 3.588], mean action: 1.736 [0.000, 3.000], mean observation: 0.049 [-0.781, 0.955], loss: 1.371548, mean_absolute_error: 5.515953, mean_q: -1.113493
  604605/1500000: episode: 1178, duration: 36.282s, episode steps: 1000, steps per second: 28, episode reward: -31.655, mean reward: -0.032 [-3.056, 3.804], mean action: 1.522 [0.000, 3.000], mean observation: 0.077 [-0.493, 0.930], loss: 0.648449, mean_absolute_error: 5.566512, mean_q: -1.141195
  605605/1500000: episode: 1179, duration: 42.510s, episode steps: 1000, steps per second: 24, episode reward: -90.354, mean reward: -0.090 [-3.496, 3.498], mean action: 1.704 [0.000, 3.000], mean observation: 0.027 [-0.669, 0.929], loss: 1.368425, mean_absolute_error: 5.671013, mean_q: -1.359364
  606605/1500000: episode: 1180, duration: 27.676s, episode steps: 1000, steps per second: 36, episode reward: -92.432, mean reward: -0.092 [-4.131, 3.779], mean action: 1.558 [0.000, 3.000], mean observation: 0.028 [-0.430, 0.928], loss: 0.638164, mean_absolute_error: 5.582520, mean_q: -1.595336
  607605/1500000: episode: 1181, duration: 33.413s, episode steps: 1000, steps per second: 30, episode reward: -93.949, mean reward: -0.094 [-3.256, 3.386], mean action: 1.813 [0.000, 3.000], mean observation: 0.023 [-0.429, 0.973], loss: 0.641929, mean_absolute_error: 5.679835, mean_q: -1.703971
  608605/1500000: episode: 1182, duration: 33.517s, episode steps: 1000, steps per second: 30, episode reward: -47.936, mean reward: -0.048 [-3.304, 4.285], mean action: 1.568 [0.000, 3.000], mean observation: 0.063 [-0.363, 0.938], loss: 1.387281, mean_absolute_error: 5.685445, mean_q: -1.627963
  609605/1500000: episode: 1183, duration: 39.434s, episode steps: 1000, steps per second: 25, episode reward: -116.209, mean reward: -0.116 [-3.208, 4.121], mean action: 1.633 [0.000, 3.000], mean observation: 0.029 [-0.343, 0.943], loss: 0.528331, mean_absolute_error: 5.568170, mean_q: -1.700387
  610605/1500000: episode: 1184, duration: 29.343s, episode steps: 1000, steps per second: 34, episode reward: -99.816, mean reward: -0.100 [-3.402, 4.145], mean action: 1.622 [0.000, 3.000], mean observation: 0.040 [-0.632, 0.940], loss: 0.563971, mean_absolute_error: 5.667849, mean_q: -2.018479
  611605/1500000: episode: 1185, duration: 27.348s, episode steps: 1000, steps per second: 37, episode reward: -62.931, mean reward: -0.063 [-3.457, 4.133], mean action: 1.502 [0.000, 3.000], mean observation: 0.066 [-0.418, 1.003], loss: 0.981100, mean_absolute_error: 5.743623, mean_q: -1.931831
  612605/1500000: episode: 1186, duration: 35.876s, episode steps: 1000, steps per second: 28, episode reward: -71.073, mean reward: -0.071 [-3.084, 3.988], mean action: 1.469 [0.000, 3.000], mean observation: 0.061 [-0.260, 0.938], loss: 1.152484, mean_absolute_error: 5.746556, mean_q: -2.031424
  613605/1500000: episode: 1187, duration: 39.244s, episode steps: 1000, steps per second: 25, episode reward: -21.547, mean reward: -0.022 [-3.879, 3.938], mean action: 1.365 [0.000, 3.000], mean observation: 0.125 [-0.254, 1.056], loss: 0.993133, mean_absolute_error: 5.833907, mean_q: -2.154686
  614605/1500000: episode: 1188, duration: 39.654s, episode steps: 1000, steps per second: 25, episode reward: -115.956, mean reward: -0.116 [-3.326, 4.352], mean action: 1.462 [0.000, 3.000], mean observation: 0.043 [-0.363, 0.938], loss: 0.858646, mean_absolute_error: 5.785074, mean_q: -2.262108
  615605/1500000: episode: 1189, duration: 37.796s, episode steps: 1000, steps per second: 26, episode reward: -104.688, mean reward: -0.105 [-3.771, 4.200], mean action: 1.646 [0.000, 3.000], mean observation: 0.016 [-0.469, 0.926], loss: 0.757309, mean_absolute_error: 5.761606, mean_q: -2.392438
  616605/1500000: episode: 1190, duration: 37.105s, episode steps: 1000, steps per second: 27, episode reward: -130.156, mean reward: -0.130 [-3.522, 3.932], mean action: 1.808 [0.000, 3.000], mean observation: 0.039 [-0.464, 0.937], loss: 0.462316, mean_absolute_error: 5.820607, mean_q: -2.434803
  617605/1500000: episode: 1191, duration: 41.882s, episode steps: 1000, steps per second: 24, episode reward: -76.017, mean reward: -0.076 [-3.138, 3.859], mean action: 1.442 [0.000, 3.000], mean observation: 0.048 [-0.224, 0.952], loss: 1.462343, mean_absolute_error: 5.956995, mean_q: -2.634301
  618605/1500000: episode: 1192, duration: 36.650s, episode steps: 1000, steps per second: 27, episode reward: -81.905, mean reward: -0.082 [-3.087, 4.172], mean action: 1.620 [0.000, 3.000], mean observation: 0.036 [-0.427, 0.939], loss: 0.559406, mean_absolute_error: 5.895113, mean_q: -2.564273
  619605/1500000: episode: 1193, duration: 44.330s, episode steps: 1000, steps per second: 23, episode reward: -127.727, mean reward: -0.128 [-3.644, 4.516], mean action: 1.662 [0.000, 3.000], mean observation: 0.033 [-0.461, 0.993], loss: 0.681793, mean_absolute_error: 5.938927, mean_q: -2.429332
  620605/1500000: episode: 1194, duration: 34.896s, episode steps: 1000, steps per second: 29, episode reward: -135.944, mean reward: -0.136 [-3.340, 3.964], mean action: 1.728 [0.000, 3.000], mean observation: 0.019 [-0.517, 0.950], loss: 0.598126, mean_absolute_error: 5.945795, mean_q: -2.599091
  621605/1500000: episode: 1195, duration: 39.639s, episode steps: 1000, steps per second: 25, episode reward: -134.367, mean reward: -0.134 [-2.949, 4.253], mean action: 1.575 [0.000, 3.000], mean observation: 0.024 [-0.470, 0.946], loss: 0.629364, mean_absolute_error: 6.032972, mean_q: -2.595195
  622605/1500000: episode: 1196, duration: 41.407s, episode steps: 1000, steps per second: 24, episode reward: -72.057, mean reward: -0.072 [-3.167, 3.841], mean action: 1.438 [0.000, 3.000], mean observation: 0.063 [-0.449, 1.003], loss: 1.082476, mean_absolute_error: 5.939887, mean_q: -2.648542
  623605/1500000: episode: 1197, duration: 40.076s, episode steps: 1000, steps per second: 25, episode reward: -88.243, mean reward: -0.088 [-3.436, 3.880], mean action: 1.492 [0.000, 3.000], mean observation: 0.047 [-0.470, 0.974], loss: 0.702757, mean_absolute_error: 6.018682, mean_q: -2.555683
  624605/1500000: episode: 1198, duration: 42.861s, episode steps: 1000, steps per second: 23, episode reward: -134.128, mean reward: -0.134 [-4.354, 4.399], mean action: 1.872 [0.000, 3.000], mean observation: 0.025 [-0.489, 0.951], loss: 0.709145, mean_absolute_error: 5.983308, mean_q: -2.643620
  625605/1500000: episode: 1199, duration: 38.670s, episode steps: 1000, steps per second: 26, episode reward: -65.745, mean reward: -0.066 [-4.043, 4.579], mean action: 1.506 [0.000, 3.000], mean observation: 0.044 [-0.511, 0.925], loss: 1.152055, mean_absolute_error: 5.957035, mean_q: -2.683117
  626605/1500000: episode: 1200, duration: 42.575s, episode steps: 1000, steps per second: 23, episode reward: -35.689, mean reward: -0.036 [-12.553, 12.471], mean action: 1.740 [0.000, 3.000], mean observation: 0.089 [-0.691, 1.000], loss: 0.594567, mean_absolute_error: 6.039687, mean_q: -2.409996
  627605/1500000: episode: 1201, duration: 38.953s, episode steps: 1000, steps per second: 26, episode reward: -105.930, mean reward: -0.106 [-3.236, 3.974], mean action: 1.542 [0.000, 3.000], mean observation: 0.056 [-0.556, 0.941], loss: 0.443955, mean_absolute_error: 5.918789, mean_q: -2.470561
  628605/1500000: episode: 1202, duration: 40.805s, episode steps: 1000, steps per second: 25, episode reward: -79.341, mean reward: -0.079 [-3.086, 3.909], mean action: 1.509 [0.000, 3.000], mean observation: 0.041 [-0.315, 0.997], loss: 0.624505, mean_absolute_error: 5.985227, mean_q: -2.438016
  629605/1500000: episode: 1203, duration: 39.057s, episode steps: 1000, steps per second: 26, episode reward: -95.584, mean reward: -0.096 [-3.304, 4.325], mean action: 1.485 [0.000, 3.000], mean observation: 0.081 [-0.185, 0.940], loss: 0.996728, mean_absolute_error: 5.978621, mean_q: -2.473579
  630605/1500000: episode: 1204, duration: 40.789s, episode steps: 1000, steps per second: 25, episode reward: -89.157, mean reward: -0.089 [-3.144, 4.239], mean action: 1.465 [0.000, 3.000], mean observation: 0.032 [-0.315, 0.932], loss: 1.598164, mean_absolute_error: 6.025702, mean_q: -2.526531
  631605/1500000: episode: 1205, duration: 41.745s, episode steps: 1000, steps per second: 24, episode reward: -58.014, mean reward: -0.058 [-3.023, 4.032], mean action: 1.428 [0.000, 3.000], mean observation: 0.068 [-0.381, 0.929], loss: 0.541532, mean_absolute_error: 6.033524, mean_q: -2.377182
  632605/1500000: episode: 1206, duration: 38.013s, episode steps: 1000, steps per second: 26, episode reward: -49.493, mean reward: -0.049 [-2.982, 3.762], mean action: 1.477 [0.000, 3.000], mean observation: 0.083 [-0.530, 0.926], loss: 0.597240, mean_absolute_error: 6.032259, mean_q: -2.398368
  633605/1500000: episode: 1207, duration: 43.650s, episode steps: 1000, steps per second: 23, episode reward: -112.256, mean reward: -0.112 [-3.012, 4.268], mean action: 1.494 [0.000, 3.000], mean observation: 0.037 [-0.349, 0.951], loss: 0.996129, mean_absolute_error: 5.982464, mean_q: -2.420885
  634605/1500000: episode: 1208, duration: 37.061s, episode steps: 1000, steps per second: 27, episode reward: -92.624, mean reward: -0.093 [-4.116, 4.082], mean action: 1.620 [0.000, 3.000], mean observation: 0.050 [-0.693, 0.939], loss: 1.609682, mean_absolute_error: 5.982411, mean_q: -2.505888
  635605/1500000: episode: 1209, duration: 45.064s, episode steps: 1000, steps per second: 22, episode reward: -104.490, mean reward: -0.104 [-2.971, 4.155], mean action: 1.430 [0.000, 3.000], mean observation: 0.020 [-0.409, 0.932], loss: 0.522394, mean_absolute_error: 6.026481, mean_q: -2.554916
  636605/1500000: episode: 1210, duration: 27.037s, episode steps: 1000, steps per second: 37, episode reward: -65.457, mean reward: -0.065 [-2.982, 4.038], mean action: 1.521 [0.000, 3.000], mean observation: 0.050 [-0.225, 0.959], loss: 0.903877, mean_absolute_error: 6.101414, mean_q: -2.450191
  637605/1500000: episode: 1211, duration: 22.364s, episode steps: 1000, steps per second: 45, episode reward: -80.728, mean reward: -0.081 [-3.351, 4.256], mean action: 1.551 [0.000, 3.000], mean observation: 0.025 [-0.462, 1.004], loss: 0.670088, mean_absolute_error: 6.024042, mean_q: -2.506337
  638605/1500000: episode: 1212, duration: 26.807s, episode steps: 1000, steps per second: 37, episode reward: -54.816, mean reward: -0.055 [-3.077, 4.063], mean action: 1.443 [0.000, 3.000], mean observation: 0.045 [-0.378, 0.953], loss: 0.678401, mean_absolute_error: 6.155533, mean_q: -2.491644
  639605/1500000: episode: 1213, duration: 20.911s, episode steps: 1000, steps per second: 48, episode reward: -63.445, mean reward: -0.063 [-3.259, 3.817], mean action: 1.522 [0.000, 3.000], mean observation: 0.033 [-0.532, 0.926], loss: 0.541802, mean_absolute_error: 6.155670, mean_q: -2.633920
  640605/1500000: episode: 1214, duration: 26.033s, episode steps: 1000, steps per second: 38, episode reward: -43.672, mean reward: -0.044 [-3.019, 3.897], mean action: 1.406 [0.000, 3.000], mean observation: 0.040 [-0.545, 0.926], loss: 0.672770, mean_absolute_error: 6.196028, mean_q: -2.512339
  641605/1500000: episode: 1215, duration: 26.644s, episode steps: 1000, steps per second: 38, episode reward: -69.679, mean reward: -0.070 [-3.907, 4.206], mean action: 1.602 [0.000, 3.000], mean observation: 0.047 [-0.302, 0.952], loss: 0.422338, mean_absolute_error: 6.119519, mean_q: -2.586570
  642605/1500000: episode: 1216, duration: 27.398s, episode steps: 1000, steps per second: 36, episode reward: -91.647, mean reward: -0.092 [-4.075, 4.242], mean action: 1.587 [0.000, 3.000], mean observation: 0.037 [-0.406, 0.999], loss: 0.503449, mean_absolute_error: 6.124538, mean_q: -2.516246
  643605/1500000: episode: 1217, duration: 45.828s, episode steps: 1000, steps per second: 22, episode reward: -122.814, mean reward: -0.123 [-4.121, 4.003], mean action: 1.786 [0.000, 3.000], mean observation: 0.027 [-0.430, 0.941], loss: 1.109246, mean_absolute_error: 6.101168, mean_q: -2.583050
  644605/1500000: episode: 1218, duration: 30.079s, episode steps: 1000, steps per second: 33, episode reward: -121.299, mean reward: -0.121 [-3.977, 3.926], mean action: 1.715 [0.000, 3.000], mean observation: 0.035 [-0.559, 0.942], loss: 0.934857, mean_absolute_error: 6.172656, mean_q: -2.462927
  645605/1500000: episode: 1219, duration: 27.543s, episode steps: 1000, steps per second: 36, episode reward: -84.278, mean reward: -0.084 [-3.230, 4.454], mean action: 1.475 [0.000, 3.000], mean observation: 0.040 [-0.315, 0.985], loss: 0.447092, mean_absolute_error: 6.155264, mean_q: -2.533741
  646605/1500000: episode: 1220, duration: 23.444s, episode steps: 1000, steps per second: 43, episode reward: -82.297, mean reward: -0.082 [-3.141, 4.702], mean action: 1.523 [0.000, 3.000], mean observation: 0.028 [-0.527, 0.924], loss: 0.705319, mean_absolute_error: 6.197524, mean_q: -2.598542
  647605/1500000: episode: 1221, duration: 27.141s, episode steps: 1000, steps per second: 37, episode reward: -103.404, mean reward: -0.103 [-3.152, 4.417], mean action: 1.500 [0.000, 3.000], mean observation: 0.082 [-0.217, 0.937], loss: 0.944561, mean_absolute_error: 6.235594, mean_q: -2.704853
  648605/1500000: episode: 1222, duration: 22.975s, episode steps: 1000, steps per second: 44, episode reward: -95.406, mean reward: -0.095 [-3.436, 4.296], mean action: 1.783 [0.000, 3.000], mean observation: 0.034 [-0.427, 0.928], loss: 0.538946, mean_absolute_error: 6.218101, mean_q: -2.622547
  649497/1500000: episode: 1223, duration: 19.901s, episode steps: 892, steps per second: 45, episode reward: 145.903, mean reward: 0.164 [-5.680, 100.000], mean action: 1.570 [0.000, 3.000], mean observation: 0.105 [-0.611, 1.064], loss: 1.138711, mean_absolute_error: 6.343849, mean_q: -2.624636
  650497/1500000: episode: 1224, duration: 27.088s, episode steps: 1000, steps per second: 37, episode reward: -130.231, mean reward: -0.130 [-3.559, 4.318], mean action: 1.660 [0.000, 3.000], mean observation: 0.029 [-0.435, 0.946], loss: 0.353238, mean_absolute_error: 6.279070, mean_q: -2.646003
  651497/1500000: episode: 1225, duration: 21.099s, episode steps: 1000, steps per second: 47, episode reward: -69.030, mean reward: -0.069 [-4.459, 3.776], mean action: 1.792 [0.000, 3.000], mean observation: 0.026 [-0.660, 0.924], loss: 0.794900, mean_absolute_error: 6.297152, mean_q: -2.695197
  652497/1500000: episode: 1226, duration: 27.372s, episode steps: 1000, steps per second: 37, episode reward: -17.332, mean reward: -0.017 [-3.140, 4.179], mean action: 1.429 [0.000, 3.000], mean observation: 0.066 [-0.607, 0.925], loss: 0.491565, mean_absolute_error: 6.294518, mean_q: -2.700315
  653497/1500000: episode: 1227, duration: 22.875s, episode steps: 1000, steps per second: 44, episode reward: -19.725, mean reward: -0.020 [-6.391, 4.858], mean action: 1.499 [0.000, 3.000], mean observation: 0.053 [-0.423, 0.968], loss: 0.824593, mean_absolute_error: 6.351492, mean_q: -2.612107
  654497/1500000: episode: 1228, duration: 28.212s, episode steps: 1000, steps per second: 35, episode reward: -84.376, mean reward: -0.084 [-4.352, 4.118], mean action: 1.633 [0.000, 3.000], mean observation: 0.039 [-0.398, 0.998], loss: 0.807598, mean_absolute_error: 6.332310, mean_q: -2.815226
  655497/1500000: episode: 1229, duration: 21.022s, episode steps: 1000, steps per second: 48, episode reward: -111.297, mean reward: -0.111 [-4.822, 4.091], mean action: 1.572 [0.000, 3.000], mean observation: 0.016 [-0.424, 0.932], loss: 0.668869, mean_absolute_error: 6.358473, mean_q: -2.719699
  656497/1500000: episode: 1230, duration: 24.546s, episode steps: 1000, steps per second: 41, episode reward: -8.625, mean reward: -0.009 [-3.100, 3.925], mean action: 1.439 [0.000, 3.000], mean observation: 0.064 [-0.693, 0.923], loss: 0.710957, mean_absolute_error: 6.454979, mean_q: -2.691710
  657497/1500000: episode: 1231, duration: 22.672s, episode steps: 1000, steps per second: 44, episode reward: -111.554, mean reward: -0.112 [-3.567, 4.254], mean action: 1.826 [0.000, 3.000], mean observation: 0.046 [-0.568, 0.951], loss: 0.918300, mean_absolute_error: 6.416805, mean_q: -2.769368
  658497/1500000: episode: 1232, duration: 24.394s, episode steps: 1000, steps per second: 41, episode reward: -92.182, mean reward: -0.092 [-4.432, 3.698], mean action: 1.899 [0.000, 3.000], mean observation: 0.029 [-0.756, 0.929], loss: 0.388585, mean_absolute_error: 6.415718, mean_q: -2.744823
  659497/1500000: episode: 1233, duration: 29.205s, episode steps: 1000, steps per second: 34, episode reward: -16.426, mean reward: -0.016 [-5.894, 4.647], mean action: 1.534 [0.000, 3.000], mean observation: 0.055 [-0.553, 0.956], loss: 0.501432, mean_absolute_error: 6.567440, mean_q: -2.610021
  660497/1500000: episode: 1234, duration: 24.173s, episode steps: 1000, steps per second: 41, episode reward: -92.918, mean reward: -0.093 [-3.804, 4.457], mean action: 1.601 [0.000, 3.000], mean observation: 0.048 [-0.381, 0.979], loss: 0.513701, mean_absolute_error: 6.450602, mean_q: -2.730247
  661497/1500000: episode: 1235, duration: 26.920s, episode steps: 1000, steps per second: 37, episode reward: -85.744, mean reward: -0.086 [-4.262, 4.410], mean action: 1.523 [0.000, 3.000], mean observation: 0.032 [-0.346, 0.945], loss: 0.833220, mean_absolute_error: 6.456287, mean_q: -2.689157
  662497/1500000: episode: 1236, duration: 25.402s, episode steps: 1000, steps per second: 39, episode reward: -109.980, mean reward: -0.110 [-3.104, 4.049], mean action: 1.506 [0.000, 3.000], mean observation: 0.036 [-0.215, 0.939], loss: 0.777698, mean_absolute_error: 6.469451, mean_q: -2.617222
  663497/1500000: episode: 1237, duration: 39.127s, episode steps: 1000, steps per second: 26, episode reward: -105.062, mean reward: -0.105 [-3.123, 4.111], mean action: 1.711 [0.000, 3.000], mean observation: 0.027 [-0.461, 0.942], loss: 0.375839, mean_absolute_error: 6.467533, mean_q: -2.509696
  664497/1500000: episode: 1238, duration: 29.418s, episode steps: 1000, steps per second: 34, episode reward: -129.926, mean reward: -0.130 [-3.397, 4.178], mean action: 1.649 [0.000, 3.000], mean observation: 0.011 [-0.464, 0.934], loss: 0.583159, mean_absolute_error: 6.448627, mean_q: -2.506386
  665497/1500000: episode: 1239, duration: 39.198s, episode steps: 1000, steps per second: 26, episode reward: -76.485, mean reward: -0.076 [-3.127, 4.430], mean action: 1.547 [0.000, 3.000], mean observation: 0.039 [-0.273, 0.941], loss: 0.747220, mean_absolute_error: 6.447019, mean_q: -2.501081
  666497/1500000: episode: 1240, duration: 31.568s, episode steps: 1000, steps per second: 32, episode reward: -98.969, mean reward: -0.099 [-4.039, 4.040], mean action: 1.653 [0.000, 3.000], mean observation: 0.018 [-0.417, 0.930], loss: 0.637193, mean_absolute_error: 6.381093, mean_q: -2.387707
  667497/1500000: episode: 1241, duration: 32.849s, episode steps: 1000, steps per second: 30, episode reward: -89.729, mean reward: -0.090 [-3.030, 4.177], mean action: 1.788 [0.000, 3.000], mean observation: 0.031 [-0.492, 0.938], loss: 0.343495, mean_absolute_error: 6.360061, mean_q: -2.227929
  668497/1500000: episode: 1242, duration: 29.994s, episode steps: 1000, steps per second: 33, episode reward: -90.688, mean reward: -0.091 [-4.216, 3.764], mean action: 1.642 [0.000, 3.000], mean observation: 0.004 [-0.701, 0.928], loss: 1.266969, mean_absolute_error: 6.330980, mean_q: -2.196081
  669497/1500000: episode: 1243, duration: 31.769s, episode steps: 1000, steps per second: 31, episode reward: -69.299, mean reward: -0.069 [-3.270, 4.471], mean action: 1.521 [0.000, 3.000], mean observation: 0.028 [-0.314, 0.935], loss: 0.393199, mean_absolute_error: 6.336709, mean_q: -2.240856
  670497/1500000: episode: 1244, duration: 22.011s, episode steps: 1000, steps per second: 45, episode reward: -47.914, mean reward: -0.048 [-3.823, 4.498], mean action: 1.502 [0.000, 3.000], mean observation: 0.108 [-0.324, 0.943], loss: 0.715877, mean_absolute_error: 6.423987, mean_q: -2.057610
  671497/1500000: episode: 1245, duration: 29.838s, episode steps: 1000, steps per second: 34, episode reward: -89.898, mean reward: -0.090 [-3.352, 4.332], mean action: 1.445 [0.000, 3.000], mean observation: 0.039 [-0.277, 0.940], loss: 0.547731, mean_absolute_error: 6.336016, mean_q: -2.047351
  672497/1500000: episode: 1246, duration: 23.257s, episode steps: 1000, steps per second: 43, episode reward: -47.862, mean reward: -0.048 [-3.291, 4.291], mean action: 1.517 [0.000, 3.000], mean observation: 0.046 [-0.335, 0.939], loss: 0.632591, mean_absolute_error: 6.386337, mean_q: -2.056857
  673497/1500000: episode: 1247, duration: 27.559s, episode steps: 1000, steps per second: 36, episode reward: -38.441, mean reward: -0.038 [-3.074, 5.318], mean action: 1.417 [0.000, 3.000], mean observation: 0.057 [-0.511, 0.925], loss: 1.238683, mean_absolute_error: 6.369454, mean_q: -1.930547
  674497/1500000: episode: 1248, duration: 29.473s, episode steps: 1000, steps per second: 34, episode reward: -76.569, mean reward: -0.077 [-3.923, 4.212], mean action: 1.604 [0.000, 3.000], mean observation: 0.043 [-0.741, 0.939], loss: 1.181059, mean_absolute_error: 6.364436, mean_q: -2.000328
  675497/1500000: episode: 1249, duration: 25.338s, episode steps: 1000, steps per second: 39, episode reward: -77.682, mean reward: -0.078 [-2.980, 4.027], mean action: 1.511 [0.000, 3.000], mean observation: 0.042 [-0.251, 0.933], loss: 0.704012, mean_absolute_error: 6.352769, mean_q: -1.741919
  676497/1500000: episode: 1250, duration: 19.998s, episode steps: 1000, steps per second: 50, episode reward: -71.443, mean reward: -0.071 [-3.193, 4.188], mean action: 1.596 [0.000, 3.000], mean observation: 0.025 [-0.622, 0.972], loss: 0.640833, mean_absolute_error: 6.407439, mean_q: -1.707494
  677497/1500000: episode: 1251, duration: 19.899s, episode steps: 1000, steps per second: 50, episode reward: -66.776, mean reward: -0.067 [-2.926, 4.412], mean action: 1.412 [0.000, 3.000], mean observation: 0.029 [-0.292, 1.013], loss: 1.016390, mean_absolute_error: 6.330045, mean_q: -1.500183
  678497/1500000: episode: 1252, duration: 19.073s, episode steps: 1000, steps per second: 52, episode reward: -56.121, mean reward: -0.056 [-3.888, 4.197], mean action: 1.450 [0.000, 3.000], mean observation: 0.025 [-0.293, 0.939], loss: 1.112666, mean_absolute_error: 6.295437, mean_q: -1.533952
  679497/1500000: episode: 1253, duration: 21.371s, episode steps: 1000, steps per second: 47, episode reward: -30.456, mean reward: -0.030 [-3.135, 4.555], mean action: 1.677 [0.000, 3.000], mean observation: 0.017 [-0.728, 0.924], loss: 1.597019, mean_absolute_error: 6.305949, mean_q: -1.388610
  680497/1500000: episode: 1254, duration: 21.908s, episode steps: 1000, steps per second: 46, episode reward: -24.070, mean reward: -0.024 [-3.593, 4.123], mean action: 1.456 [0.000, 3.000], mean observation: 0.077 [-0.654, 0.925], loss: 0.990491, mean_absolute_error: 6.338434, mean_q: -1.117200
  681497/1500000: episode: 1255, duration: 27.398s, episode steps: 1000, steps per second: 36, episode reward: -94.252, mean reward: -0.094 [-3.529, 4.096], mean action: 1.550 [0.000, 3.000], mean observation: 0.009 [-0.311, 0.937], loss: 1.375260, mean_absolute_error: 6.337826, mean_q: -0.986158
  682497/1500000: episode: 1256, duration: 20.073s, episode steps: 1000, steps per second: 50, episode reward: -112.403, mean reward: -0.112 [-3.921, 3.981], mean action: 1.862 [0.000, 3.000], mean observation: 0.061 [-0.746, 1.089], loss: 0.484140, mean_absolute_error: 6.283210, mean_q: -1.028770
  683497/1500000: episode: 1257, duration: 25.786s, episode steps: 1000, steps per second: 39, episode reward: -71.011, mean reward: -0.071 [-3.370, 4.454], mean action: 1.503 [0.000, 3.000], mean observation: 0.056 [-0.290, 0.932], loss: 0.907631, mean_absolute_error: 6.314019, mean_q: -1.180115
  684497/1500000: episode: 1258, duration: 20.941s, episode steps: 1000, steps per second: 48, episode reward: -138.031, mean reward: -0.138 [-3.532, 3.982], mean action: 1.542 [0.000, 3.000], mean observation: 0.022 [-0.457, 0.942], loss: 1.040223, mean_absolute_error: 6.233732, mean_q: -1.192432
  685497/1500000: episode: 1259, duration: 24.917s, episode steps: 1000, steps per second: 40, episode reward: -75.420, mean reward: -0.075 [-4.488, 4.110], mean action: 1.633 [0.000, 3.000], mean observation: 0.037 [-0.714, 0.940], loss: 0.979162, mean_absolute_error: 6.244509, mean_q: -1.061676
  686497/1500000: episode: 1260, duration: 22.490s, episode steps: 1000, steps per second: 44, episode reward: -56.147, mean reward: -0.056 [-3.813, 4.278], mean action: 1.536 [0.000, 3.000], mean observation: 0.034 [-0.253, 0.950], loss: 0.731459, mean_absolute_error: 6.178973, mean_q: -1.154799
  687497/1500000: episode: 1261, duration: 24.881s, episode steps: 1000, steps per second: 40, episode reward: -91.516, mean reward: -0.092 [-3.324, 4.229], mean action: 1.501 [0.000, 3.000], mean observation: 0.031 [-0.273, 0.936], loss: 0.825068, mean_absolute_error: 6.214648, mean_q: -1.089587
  688497/1500000: episode: 1262, duration: 25.666s, episode steps: 1000, steps per second: 39, episode reward: -133.751, mean reward: -0.134 [-3.543, 3.685], mean action: 1.596 [0.000, 3.000], mean observation: 0.030 [-0.559, 0.948], loss: 0.581587, mean_absolute_error: 6.230117, mean_q: -1.031967
  689497/1500000: episode: 1263, duration: 19.854s, episode steps: 1000, steps per second: 50, episode reward: -91.257, mean reward: -0.091 [-3.563, 4.231], mean action: 1.631 [0.000, 3.000], mean observation: -0.004 [-0.533, 0.925], loss: 0.509493, mean_absolute_error: 6.152775, mean_q: -1.087667
  690497/1500000: episode: 1264, duration: 23.851s, episode steps: 1000, steps per second: 42, episode reward: -108.699, mean reward: -0.109 [-3.406, 3.785], mean action: 1.799 [0.000, 3.000], mean observation: 0.015 [-0.580, 0.936], loss: 0.305102, mean_absolute_error: 6.171463, mean_q: -1.154272
  691497/1500000: episode: 1265, duration: 21.547s, episode steps: 1000, steps per second: 46, episode reward: -60.833, mean reward: -0.061 [-4.810, 4.103], mean action: 1.521 [0.000, 3.000], mean observation: 0.031 [-0.312, 0.931], loss: 0.846715, mean_absolute_error: 6.118505, mean_q: -1.168764
  692497/1500000: episode: 1266, duration: 24.821s, episode steps: 1000, steps per second: 40, episode reward: -99.501, mean reward: -0.100 [-3.347, 4.612], mean action: 1.720 [0.000, 3.000], mean observation: 0.022 [-0.494, 1.001], loss: 0.473363, mean_absolute_error: 6.235873, mean_q: -1.125366
  693495/1500000: episode: 1267, duration: 20.703s, episode steps: 998, steps per second: 48, episode reward: 138.829, mean reward: 0.139 [-17.822, 100.000], mean action: 1.331 [0.000, 3.000], mean observation: 0.083 [-0.640, 1.000], loss: 0.696067, mean_absolute_error: 6.222730, mean_q: -1.162170
  694495/1500000: episode: 1268, duration: 23.356s, episode steps: 1000, steps per second: 43, episode reward: -85.068, mean reward: -0.085 [-4.110, 3.693], mean action: 1.717 [0.000, 3.000], mean observation: 0.013 [-0.582, 0.925], loss: 0.588155, mean_absolute_error: 6.214872, mean_q: -0.994791
  695495/1500000: episode: 1269, duration: 19.779s, episode steps: 1000, steps per second: 51, episode reward: -131.809, mean reward: -0.132 [-3.387, 4.066], mean action: 1.604 [0.000, 3.000], mean observation: 0.015 [-0.476, 0.938], loss: 1.106914, mean_absolute_error: 6.275191, mean_q: -0.798564
  696495/1500000: episode: 1270, duration: 23.047s, episode steps: 1000, steps per second: 43, episode reward: -57.692, mean reward: -0.058 [-3.930, 4.290], mean action: 1.533 [0.000, 3.000], mean observation: 0.048 [-0.325, 0.931], loss: 0.763462, mean_absolute_error: 6.296899, mean_q: -0.885293
  697495/1500000: episode: 1271, duration: 21.930s, episode steps: 1000, steps per second: 46, episode reward: -33.423, mean reward: -0.033 [-3.471, 4.324], mean action: 1.490 [0.000, 3.000], mean observation: 0.031 [-0.280, 1.000], loss: 0.886909, mean_absolute_error: 6.266881, mean_q: -0.943999
  698340/1500000: episode: 1272, duration: 22.241s, episode steps: 845, steps per second: 38, episode reward: 157.545, mean reward: 0.186 [-3.675, 100.000], mean action: 1.444 [0.000, 3.000], mean observation: 0.099 [-0.626, 1.000], loss: 0.698336, mean_absolute_error: 6.239750, mean_q: -0.877949
  699340/1500000: episode: 1273, duration: 19.662s, episode steps: 1000, steps per second: 51, episode reward: -68.838, mean reward: -0.069 [-3.858, 4.256], mean action: 1.721 [0.000, 3.000], mean observation: 0.027 [-0.696, 0.929], loss: 1.602150, mean_absolute_error: 6.264425, mean_q: -0.765816
  700340/1500000: episode: 1274, duration: 23.956s, episode steps: 1000, steps per second: 42, episode reward: -108.328, mean reward: -0.108 [-3.860, 3.897], mean action: 1.595 [0.000, 3.000], mean observation: 0.040 [-0.449, 0.939], loss: 0.692100, mean_absolute_error: 6.335728, mean_q: -0.853567
  701340/1500000: episode: 1275, duration: 21.940s, episode steps: 1000, steps per second: 46, episode reward: -102.997, mean reward: -0.103 [-3.658, 4.445], mean action: 1.552 [0.000, 3.000], mean observation: 0.019 [-0.380, 0.942], loss: 0.820042, mean_absolute_error: 6.345491, mean_q: -1.179106
  702340/1500000: episode: 1276, duration: 27.404s, episode steps: 1000, steps per second: 36, episode reward: -112.260, mean reward: -0.112 [-3.262, 4.554], mean action: 1.671 [0.000, 3.000], mean observation: 0.040 [-0.301, 0.952], loss: 0.607654, mean_absolute_error: 6.385292, mean_q: -1.273699
  703340/1500000: episode: 1277, duration: 20.243s, episode steps: 1000, steps per second: 49, episode reward: -79.108, mean reward: -0.079 [-4.091, 4.004], mean action: 1.779 [0.000, 3.000], mean observation: 0.021 [-0.758, 0.929], loss: 0.812360, mean_absolute_error: 6.392801, mean_q: -1.344049
  704340/1500000: episode: 1278, duration: 22.698s, episode steps: 1000, steps per second: 44, episode reward: -37.795, mean reward: -0.038 [-3.897, 4.509], mean action: 1.683 [0.000, 3.000], mean observation: 0.069 [-0.431, 0.941], loss: 0.530564, mean_absolute_error: 6.369264, mean_q: -1.570132
  705340/1500000: episode: 1279, duration: 20.838s, episode steps: 1000, steps per second: 48, episode reward: -23.787, mean reward: -0.024 [-3.832, 4.560], mean action: 1.531 [0.000, 3.000], mean observation: 0.049 [-0.581, 0.924], loss: 1.601432, mean_absolute_error: 6.463133, mean_q: -1.455665
  706340/1500000: episode: 1280, duration: 21.965s, episode steps: 1000, steps per second: 46, episode reward: -92.924, mean reward: -0.093 [-3.294, 4.245], mean action: 1.746 [0.000, 3.000], mean observation: 0.016 [-0.588, 0.926], loss: 0.548953, mean_absolute_error: 6.431581, mean_q: -1.478633
  707340/1500000: episode: 1281, duration: 20.433s, episode steps: 1000, steps per second: 49, episode reward: -104.675, mean reward: -0.105 [-3.556, 3.956], mean action: 1.622 [0.000, 3.000], mean observation: 0.021 [-0.316, 0.933], loss: 0.596345, mean_absolute_error: 6.455382, mean_q: -1.662105
  708340/1500000: episode: 1282, duration: 18.647s, episode steps: 1000, steps per second: 54, episode reward: -110.182, mean reward: -0.110 [-3.304, 4.332], mean action: 1.649 [0.000, 3.000], mean observation: 0.005 [-0.484, 0.939], loss: 0.578870, mean_absolute_error: 6.423891, mean_q: -1.734059
  709340/1500000: episode: 1283, duration: 23.894s, episode steps: 1000, steps per second: 42, episode reward: -107.796, mean reward: -0.108 [-3.517, 4.600], mean action: 1.630 [0.000, 3.000], mean observation: 0.028 [-0.336, 0.937], loss: 0.904039, mean_absolute_error: 6.443079, mean_q: -1.758266
  710340/1500000: episode: 1284, duration: 18.611s, episode steps: 1000, steps per second: 54, episode reward: -10.670, mean reward: -0.011 [-3.338, 4.383], mean action: 1.487 [0.000, 3.000], mean observation: 0.048 [-0.628, 0.924], loss: 0.563292, mean_absolute_error: 6.415833, mean_q: -1.755692
  711340/1500000: episode: 1285, duration: 19.628s, episode steps: 1000, steps per second: 51, episode reward: -71.689, mean reward: -0.072 [-3.242, 4.381], mean action: 1.442 [0.000, 3.000], mean observation: 0.045 [-0.251, 0.962], loss: 0.703510, mean_absolute_error: 6.491424, mean_q: -1.689106
  712340/1500000: episode: 1286, duration: 20.145s, episode steps: 1000, steps per second: 50, episode reward: -109.326, mean reward: -0.109 [-3.500, 4.352], mean action: 1.788 [0.000, 3.000], mean observation: 0.019 [-0.365, 0.954], loss: 0.666356, mean_absolute_error: 6.491765, mean_q: -1.892879
  713340/1500000: episode: 1287, duration: 18.245s, episode steps: 1000, steps per second: 55, episode reward: -121.257, mean reward: -0.121 [-4.089, 4.772], mean action: 1.728 [0.000, 3.000], mean observation: -0.001 [-0.519, 0.934], loss: 0.838750, mean_absolute_error: 6.525288, mean_q: -1.721628
  714340/1500000: episode: 1288, duration: 18.794s, episode steps: 1000, steps per second: 53, episode reward: -94.787, mean reward: -0.095 [-3.763, 3.882], mean action: 1.882 [0.000, 3.000], mean observation: 0.017 [-0.709, 0.927], loss: 0.700031, mean_absolute_error: 6.502920, mean_q: -2.032472
  715340/1500000: episode: 1289, duration: 21.830s, episode steps: 1000, steps per second: 46, episode reward: -74.423, mean reward: -0.074 [-3.380, 4.309], mean action: 1.445 [0.000, 3.000], mean observation: 0.020 [-0.390, 0.953], loss: 1.146274, mean_absolute_error: 6.534606, mean_q: -2.081740
  716340/1500000: episode: 1290, duration: 18.743s, episode steps: 1000, steps per second: 53, episode reward: -9.572, mean reward: -0.010 [-3.509, 3.894], mean action: 1.584 [0.000, 3.000], mean observation: 0.051 [-0.595, 0.925], loss: 0.865003, mean_absolute_error: 6.554653, mean_q: -2.070584
  717340/1500000: episode: 1291, duration: 18.145s, episode steps: 1000, steps per second: 55, episode reward: -91.286, mean reward: -0.091 [-3.513, 4.368], mean action: 1.758 [0.000, 3.000], mean observation: 0.004 [-0.586, 0.927], loss: 1.216603, mean_absolute_error: 6.612533, mean_q: -2.113579
  718340/1500000: episode: 1292, duration: 21.140s, episode steps: 1000, steps per second: 47, episode reward: -100.108, mean reward: -0.100 [-3.960, 3.911], mean action: 1.752 [0.000, 3.000], mean observation: 0.020 [-0.486, 0.930], loss: 0.611892, mean_absolute_error: 6.671698, mean_q: -2.036004
  719340/1500000: episode: 1293, duration: 19.237s, episode steps: 1000, steps per second: 52, episode reward: -100.405, mean reward: -0.100 [-3.187, 4.166], mean action: 1.735 [0.000, 3.000], mean observation: 0.025 [-0.487, 0.931], loss: 0.848402, mean_absolute_error: 6.574707, mean_q: -2.123375
  720340/1500000: episode: 1294, duration: 18.826s, episode steps: 1000, steps per second: 53, episode reward: -121.234, mean reward: -0.121 [-3.193, 4.188], mean action: 1.476 [0.000, 3.000], mean observation: 0.009 [-0.417, 0.941], loss: 1.403843, mean_absolute_error: 6.628287, mean_q: -2.132674
  721340/1500000: episode: 1295, duration: 23.342s, episode steps: 1000, steps per second: 43, episode reward: -110.278, mean reward: -0.110 [-3.269, 4.521], mean action: 1.770 [0.000, 3.000], mean observation: 0.041 [-0.464, 0.974], loss: 0.688276, mean_absolute_error: 6.574210, mean_q: -2.163259
  722340/1500000: episode: 1296, duration: 19.719s, episode steps: 1000, steps per second: 51, episode reward: -67.006, mean reward: -0.067 [-3.180, 4.228], mean action: 1.639 [0.000, 3.000], mean observation: 0.063 [-0.323, 0.938], loss: 1.165058, mean_absolute_error: 6.640727, mean_q: -2.186039
  723340/1500000: episode: 1297, duration: 21.784s, episode steps: 1000, steps per second: 46, episode reward: -75.726, mean reward: -0.076 [-3.037, 4.204], mean action: 1.617 [0.000, 3.000], mean observation: 0.016 [-0.349, 0.930], loss: 0.704449, mean_absolute_error: 6.643953, mean_q: -2.095429
  724340/1500000: episode: 1298, duration: 19.566s, episode steps: 1000, steps per second: 51, episode reward: -59.357, mean reward: -0.059 [-3.053, 4.490], mean action: 1.551 [0.000, 3.000], mean observation: 0.055 [-0.288, 0.934], loss: 1.770136, mean_absolute_error: 6.729790, mean_q: -2.313798
  725340/1500000: episode: 1299, duration: 18.374s, episode steps: 1000, steps per second: 54, episode reward: -93.840, mean reward: -0.094 [-3.481, 4.148], mean action: 1.535 [0.000, 3.000], mean observation: 0.014 [-0.406, 0.980], loss: 0.663551, mean_absolute_error: 6.693067, mean_q: -2.332548
  726340/1500000: episode: 1300, duration: 20.599s, episode steps: 1000, steps per second: 49, episode reward: -142.653, mean reward: -0.143 [-3.093, 4.385], mean action: 1.612 [0.000, 3.000], mean observation: 0.010 [-0.527, 0.938], loss: 0.514802, mean_absolute_error: 6.681359, mean_q: -2.378907
  727340/1500000: episode: 1301, duration: 20.165s, episode steps: 1000, steps per second: 50, episode reward: -5.822, mean reward: -0.006 [-3.892, 4.017], mean action: 1.552 [0.000, 3.000], mean observation: 0.085 [-0.666, 0.925], loss: 1.532982, mean_absolute_error: 6.701122, mean_q: -2.464232
  728340/1500000: episode: 1302, duration: 19.278s, episode steps: 1000, steps per second: 52, episode reward: -42.650, mean reward: -0.043 [-3.657, 3.965], mean action: 1.464 [0.000, 3.000], mean observation: 0.038 [-0.225, 0.944], loss: 1.101733, mean_absolute_error: 6.684902, mean_q: -2.435841
  729340/1500000: episode: 1303, duration: 21.400s, episode steps: 1000, steps per second: 47, episode reward: -97.367, mean reward: -0.097 [-3.927, 4.241], mean action: 1.675 [0.000, 3.000], mean observation: 0.035 [-0.276, 0.932], loss: 0.609389, mean_absolute_error: 6.746543, mean_q: -2.345202
  730340/1500000: episode: 1304, duration: 20.164s, episode steps: 1000, steps per second: 50, episode reward: -102.327, mean reward: -0.102 [-3.388, 3.813], mean action: 1.616 [0.000, 3.000], mean observation: 0.013 [-0.495, 0.928], loss: 0.857914, mean_absolute_error: 6.789974, mean_q: -2.278287
  731340/1500000: episode: 1305, duration: 18.875s, episode steps: 1000, steps per second: 53, episode reward: -104.062, mean reward: -0.104 [-3.852, 4.541], mean action: 1.514 [0.000, 3.000], mean observation: 0.009 [-0.533, 0.941], loss: 0.783234, mean_absolute_error: 6.824401, mean_q: -2.308954
  732340/1500000: episode: 1306, duration: 20.532s, episode steps: 1000, steps per second: 49, episode reward: -65.885, mean reward: -0.066 [-4.735, 4.088], mean action: 1.592 [0.000, 3.000], mean observation: 0.030 [-0.300, 1.026], loss: 0.714372, mean_absolute_error: 6.724293, mean_q: -2.403121
  733340/1500000: episode: 1307, duration: 21.701s, episode steps: 1000, steps per second: 46, episode reward: -35.379, mean reward: -0.035 [-3.442, 4.104], mean action: 1.741 [0.000, 3.000], mean observation: 0.063 [-0.386, 0.940], loss: 0.583697, mean_absolute_error: 6.695591, mean_q: -2.405582
  734340/1500000: episode: 1308, duration: 17.720s, episode steps: 1000, steps per second: 56, episode reward: -78.089, mean reward: -0.078 [-3.191, 4.079], mean action: 1.703 [0.000, 3.000], mean observation: 0.021 [-0.534, 0.928], loss: 0.540511, mean_absolute_error: 6.728322, mean_q: -2.186121
  735340/1500000: episode: 1309, duration: 21.499s, episode steps: 1000, steps per second: 47, episode reward: -54.558, mean reward: -0.055 [-3.300, 4.270], mean action: 1.616 [0.000, 3.000], mean observation: 0.034 [-0.235, 0.985], loss: 0.827376, mean_absolute_error: 6.754152, mean_q: -2.279078
  736340/1500000: episode: 1310, duration: 19.839s, episode steps: 1000, steps per second: 50, episode reward: -128.580, mean reward: -0.129 [-3.870, 4.319], mean action: 1.749 [0.000, 3.000], mean observation: -0.004 [-0.536, 0.956], loss: 0.771007, mean_absolute_error: 6.720747, mean_q: -2.292606
  737340/1500000: episode: 1311, duration: 20.356s, episode steps: 1000, steps per second: 49, episode reward: -96.776, mean reward: -0.097 [-4.024, 4.076], mean action: 1.645 [0.000, 3.000], mean observation: 0.008 [-0.352, 0.932], loss: 0.596327, mean_absolute_error: 6.646255, mean_q: -2.384067
  738340/1500000: episode: 1312, duration: 22.077s, episode steps: 1000, steps per second: 45, episode reward: -101.700, mean reward: -0.102 [-4.288, 4.058], mean action: 1.868 [0.000, 3.000], mean observation: 0.030 [-0.561, 0.983], loss: 0.783719, mean_absolute_error: 6.627553, mean_q: -2.275649
  739340/1500000: episode: 1313, duration: 19.120s, episode steps: 1000, steps per second: 52, episode reward: -33.329, mean reward: -0.033 [-9.771, 14.968], mean action: 1.539 [0.000, 3.000], mean observation: 0.053 [-0.518, 1.000], loss: 0.684435, mean_absolute_error: 6.615469, mean_q: -2.411255
  740340/1500000: episode: 1314, duration: 19.901s, episode steps: 1000, steps per second: 50, episode reward: -106.432, mean reward: -0.106 [-3.007, 3.855], mean action: 1.663 [0.000, 3.000], mean observation: 0.012 [-0.431, 0.959], loss: 0.521141, mean_absolute_error: 6.622198, mean_q: -2.368954
  741340/1500000: episode: 1315, duration: 21.705s, episode steps: 1000, steps per second: 46, episode reward: -96.781, mean reward: -0.097 [-3.272, 4.087], mean action: 1.770 [0.000, 3.000], mean observation: 0.037 [-0.334, 0.972], loss: 0.676142, mean_absolute_error: 6.593837, mean_q: -2.335645
  742340/1500000: episode: 1316, duration: 18.293s, episode steps: 1000, steps per second: 55, episode reward: -119.885, mean reward: -0.120 [-4.574, 4.285], mean action: 1.670 [0.000, 3.000], mean observation: -0.009 [-0.456, 0.948], loss: 0.502119, mean_absolute_error: 6.496216, mean_q: -2.329526
  743340/1500000: episode: 1317, duration: 21.992s, episode steps: 1000, steps per second: 45, episode reward: -70.495, mean reward: -0.070 [-14.179, 13.384], mean action: 1.658 [0.000, 3.000], mean observation: 0.020 [-0.415, 1.000], loss: 0.964941, mean_absolute_error: 6.584405, mean_q: -2.204915
  744340/1500000: episode: 1318, duration: 19.763s, episode steps: 1000, steps per second: 51, episode reward: -82.334, mean reward: -0.082 [-3.593, 4.444], mean action: 1.833 [0.000, 3.000], mean observation: 0.026 [-0.366, 1.001], loss: 0.672616, mean_absolute_error: 6.594074, mean_q: -2.113569
  745340/1500000: episode: 1319, duration: 19.565s, episode steps: 1000, steps per second: 51, episode reward: -128.029, mean reward: -0.128 [-4.675, 4.016], mean action: 1.689 [0.000, 3.000], mean observation: 0.002 [-0.560, 0.936], loss: 0.622045, mean_absolute_error: 6.494799, mean_q: -2.116705
  746340/1500000: episode: 1320, duration: 21.363s, episode steps: 1000, steps per second: 47, episode reward: -108.297, mean reward: -0.108 [-3.573, 4.427], mean action: 1.719 [0.000, 3.000], mean observation: 0.016 [-0.412, 0.958], loss: 0.902092, mean_absolute_error: 6.597820, mean_q: -2.170070
  747340/1500000: episode: 1321, duration: 20.122s, episode steps: 1000, steps per second: 50, episode reward: -78.030, mean reward: -0.078 [-3.931, 3.882], mean action: 1.618 [0.000, 3.000], mean observation: -0.008 [-0.486, 0.935], loss: 0.903637, mean_absolute_error: 6.512125, mean_q: -2.229351
  748340/1500000: episode: 1322, duration: 18.454s, episode steps: 1000, steps per second: 54, episode reward: -91.219, mean reward: -0.091 [-3.305, 4.019], mean action: 1.821 [0.000, 3.000], mean observation: 0.010 [-0.315, 0.949], loss: 0.837771, mean_absolute_error: 6.530850, mean_q: -2.097709
  749340/1500000: episode: 1323, duration: 20.042s, episode steps: 1000, steps per second: 50, episode reward: -101.259, mean reward: -0.101 [-4.625, 3.854], mean action: 1.793 [0.000, 3.000], mean observation: 0.030 [-0.395, 0.948], loss: 0.672976, mean_absolute_error: 6.447070, mean_q: -2.302725
  750340/1500000: episode: 1324, duration: 22.245s, episode steps: 1000, steps per second: 45, episode reward: -90.958, mean reward: -0.091 [-3.056, 4.256], mean action: 1.787 [0.000, 3.000], mean observation: 0.020 [-0.319, 0.967], loss: 0.517632, mean_absolute_error: 6.416889, mean_q: -2.069774
  751340/1500000: episode: 1325, duration: 19.919s, episode steps: 1000, steps per second: 50, episode reward: -49.127, mean reward: -0.049 [-13.975, 12.584], mean action: 1.643 [0.000, 3.000], mean observation: 0.001 [-0.422, 1.000], loss: 1.343126, mean_absolute_error: 6.459917, mean_q: -2.095339
  752340/1500000: episode: 1326, duration: 22.992s, episode steps: 1000, steps per second: 43, episode reward: -104.470, mean reward: -0.104 [-3.615, 4.022], mean action: 1.808 [0.000, 3.000], mean observation: 0.022 [-0.385, 0.934], loss: 1.007149, mean_absolute_error: 6.504097, mean_q: -2.304048
  753340/1500000: episode: 1327, duration: 18.497s, episode steps: 1000, steps per second: 54, episode reward: -80.641, mean reward: -0.081 [-3.315, 4.168], mean action: 1.638 [0.000, 3.000], mean observation: 0.039 [-0.245, 0.936], loss: 0.998306, mean_absolute_error: 6.511716, mean_q: -2.263166
  754340/1500000: episode: 1328, duration: 18.353s, episode steps: 1000, steps per second: 54, episode reward: -69.257, mean reward: -0.069 [-4.120, 4.170], mean action: 1.838 [0.000, 3.000], mean observation: 0.032 [-0.745, 0.947], loss: 0.456415, mean_absolute_error: 6.490079, mean_q: -2.279225
  755340/1500000: episode: 1329, duration: 23.955s, episode steps: 1000, steps per second: 42, episode reward: -103.840, mean reward: -0.104 [-7.768, 13.777], mean action: 1.680 [0.000, 3.000], mean observation: 0.020 [-0.823, 1.000], loss: 0.619053, mean_absolute_error: 6.495772, mean_q: -2.309135
  756340/1500000: episode: 1330, duration: 18.358s, episode steps: 1000, steps per second: 54, episode reward: -120.454, mean reward: -0.120 [-4.303, 3.831], mean action: 1.712 [0.000, 3.000], mean observation: 0.024 [-0.465, 0.940], loss: 0.857822, mean_absolute_error: 6.511065, mean_q: -2.198426
  757340/1500000: episode: 1331, duration: 20.207s, episode steps: 1000, steps per second: 49, episode reward: -79.443, mean reward: -0.079 [-3.245, 4.029], mean action: 1.651 [0.000, 3.000], mean observation: 0.048 [-0.503, 0.974], loss: 0.767197, mean_absolute_error: 6.496709, mean_q: -2.242774
  758340/1500000: episode: 1332, duration: 23.073s, episode steps: 1000, steps per second: 43, episode reward: -110.768, mean reward: -0.111 [-3.634, 3.524], mean action: 1.871 [0.000, 3.000], mean observation: 0.062 [-0.788, 1.034], loss: 0.537736, mean_absolute_error: 6.457000, mean_q: -2.485584
  759040/1500000: episode: 1333, duration: 13.282s, episode steps: 700, steps per second: 53, episode reward: -161.372, mean reward: -0.231 [-100.000, 8.127], mean action: 1.657 [0.000, 3.000], mean observation: 0.018 [-0.480, 1.000], loss: 0.316093, mean_absolute_error: 6.515920, mean_q: -2.478116
  760040/1500000: episode: 1334, duration: 20.805s, episode steps: 1000, steps per second: 48, episode reward: -98.158, mean reward: -0.098 [-3.481, 4.179], mean action: 1.638 [0.000, 3.000], mean observation: 0.014 [-0.331, 0.938], loss: 0.753961, mean_absolute_error: 6.449070, mean_q: -2.503459
  761040/1500000: episode: 1335, duration: 19.360s, episode steps: 1000, steps per second: 52, episode reward: -61.792, mean reward: -0.062 [-3.289, 4.087], mean action: 1.739 [0.000, 3.000], mean observation: -0.005 [-0.712, 0.924], loss: 0.709858, mean_absolute_error: 6.510240, mean_q: -2.407751
  762040/1500000: episode: 1336, duration: 26.065s, episode steps: 1000, steps per second: 38, episode reward: -66.061, mean reward: -0.066 [-4.083, 4.180], mean action: 1.651 [0.000, 3.000], mean observation: 0.019 [-0.331, 0.930], loss: 1.494306, mean_absolute_error: 6.504088, mean_q: -2.527030
  763040/1500000: episode: 1337, duration: 17.981s, episode steps: 1000, steps per second: 56, episode reward: -76.989, mean reward: -0.077 [-3.241, 4.084], mean action: 1.472 [0.000, 3.000], mean observation: 0.003 [-0.508, 0.925], loss: 0.667615, mean_absolute_error: 6.455980, mean_q: -2.404095
  764040/1500000: episode: 1338, duration: 18.861s, episode steps: 1000, steps per second: 53, episode reward: -75.227, mean reward: -0.075 [-3.501, 4.189], mean action: 1.696 [0.000, 3.000], mean observation: -0.001 [-0.573, 0.938], loss: 0.613110, mean_absolute_error: 6.449990, mean_q: -2.318744
  765040/1500000: episode: 1339, duration: 24.932s, episode steps: 1000, steps per second: 40, episode reward: -75.524, mean reward: -0.076 [-3.175, 4.653], mean action: 1.674 [0.000, 3.000], mean observation: 0.024 [-0.334, 0.930], loss: 0.859309, mean_absolute_error: 6.480117, mean_q: -2.276118
  766037/1500000: episode: 1340, duration: 19.167s, episode steps: 997, steps per second: 52, episode reward: 101.020, mean reward: 0.101 [-9.853, 100.000], mean action: 1.546 [0.000, 3.000], mean observation: 0.040 [-0.327, 1.000], loss: 0.523979, mean_absolute_error: 6.468689, mean_q: -2.227179
  767037/1500000: episode: 1341, duration: 21.437s, episode steps: 1000, steps per second: 47, episode reward: -92.849, mean reward: -0.093 [-3.545, 4.211], mean action: 1.663 [0.000, 3.000], mean observation: 0.033 [-0.418, 0.939], loss: 1.220817, mean_absolute_error: 6.508520, mean_q: -2.187668
  768037/1500000: episode: 1342, duration: 22.512s, episode steps: 1000, steps per second: 44, episode reward: -94.270, mean reward: -0.094 [-3.213, 4.486], mean action: 1.838 [0.000, 3.000], mean observation: -0.006 [-0.551, 0.924], loss: 0.499473, mean_absolute_error: 6.486224, mean_q: -2.296232
  769037/1500000: episode: 1343, duration: 18.450s, episode steps: 1000, steps per second: 54, episode reward: -80.363, mean reward: -0.080 [-3.055, 4.380], mean action: 1.677 [0.000, 3.000], mean observation: 0.029 [-0.356, 0.984], loss: 0.693168, mean_absolute_error: 6.440079, mean_q: -2.099696
  769729/1500000: episode: 1344, duration: 14.489s, episode steps: 692, steps per second: 48, episode reward: 168.472, mean reward: 0.243 [-19.683, 100.000], mean action: 1.260 [0.000, 3.000], mean observation: 0.070 [-0.537, 1.000], loss: 0.759421, mean_absolute_error: 6.455276, mean_q: -2.237756
  770729/1500000: episode: 1345, duration: 18.231s, episode steps: 1000, steps per second: 55, episode reward: -97.111, mean reward: -0.097 [-4.416, 4.116], mean action: 1.670 [0.000, 3.000], mean observation: 0.013 [-0.527, 1.005], loss: 0.875240, mean_absolute_error: 6.462241, mean_q: -2.221891
  771729/1500000: episode: 1346, duration: 23.206s, episode steps: 1000, steps per second: 43, episode reward: -71.081, mean reward: -0.071 [-3.650, 3.957], mean action: 1.758 [0.000, 3.000], mean observation: 0.017 [-0.518, 0.929], loss: 0.491254, mean_absolute_error: 6.396486, mean_q: -2.173389
  772729/1500000: episode: 1347, duration: 20.737s, episode steps: 1000, steps per second: 48, episode reward: -102.983, mean reward: -0.103 [-3.943, 4.212], mean action: 1.641 [0.000, 3.000], mean observation: 0.018 [-0.264, 0.950], loss: 0.597410, mean_absolute_error: 6.354172, mean_q: -2.086936
  773729/1500000: episode: 1348, duration: 20.373s, episode steps: 1000, steps per second: 49, episode reward: -97.476, mean reward: -0.097 [-4.336, 4.265], mean action: 1.782 [0.000, 3.000], mean observation: 0.017 [-0.337, 0.950], loss: 0.564630, mean_absolute_error: 6.384124, mean_q: -2.059525
  774729/1500000: episode: 1349, duration: 23.832s, episode steps: 1000, steps per second: 42, episode reward: -86.649, mean reward: -0.087 [-3.543, 4.283], mean action: 1.846 [0.000, 3.000], mean observation: -0.004 [-0.459, 0.926], loss: 0.485340, mean_absolute_error: 6.344623, mean_q: -2.291641
  775729/1500000: episode: 1350, duration: 22.642s, episode steps: 1000, steps per second: 44, episode reward: -54.203, mean reward: -0.054 [-14.839, 13.584], mean action: 1.677 [0.000, 3.000], mean observation: 0.011 [-0.637, 1.000], loss: 0.634090, mean_absolute_error: 6.307930, mean_q: -2.247766
  776572/1500000: episode: 1351, duration: 22.147s, episode steps: 843, steps per second: 38, episode reward: 135.169, mean reward: 0.160 [-19.560, 100.000], mean action: 1.269 [0.000, 3.000], mean observation: 0.045 [-0.678, 1.000], loss: 0.492818, mean_absolute_error: 6.334166, mean_q: -2.071982
  777572/1500000: episode: 1352, duration: 18.002s, episode steps: 1000, steps per second: 56, episode reward: -74.743, mean reward: -0.075 [-3.458, 3.997], mean action: 1.728 [0.000, 3.000], mean observation: 0.008 [-0.334, 0.987], loss: 0.432936, mean_absolute_error: 6.315545, mean_q: -1.972944
  778572/1500000: episode: 1353, duration: 19.996s, episode steps: 1000, steps per second: 50, episode reward: -107.171, mean reward: -0.107 [-3.114, 4.085], mean action: 1.688 [0.000, 3.000], mean observation: 0.029 [-0.437, 0.963], loss: 0.696395, mean_absolute_error: 6.289335, mean_q: -1.995945
  779572/1500000: episode: 1354, duration: 25.366s, episode steps: 1000, steps per second: 39, episode reward: -78.002, mean reward: -0.078 [-3.160, 4.376], mean action: 1.570 [0.000, 3.000], mean observation: 0.001 [-0.371, 0.955], loss: 1.071073, mean_absolute_error: 6.384037, mean_q: -1.981228
  780572/1500000: episode: 1355, duration: 20.244s, episode steps: 1000, steps per second: 49, episode reward: -58.501, mean reward: -0.059 [-3.072, 4.370], mean action: 1.869 [0.000, 3.000], mean observation: 0.049 [-0.444, 0.931], loss: 0.440053, mean_absolute_error: 6.275095, mean_q: -1.882343
  781572/1500000: episode: 1356, duration: 24.534s, episode steps: 1000, steps per second: 41, episode reward: -84.066, mean reward: -0.084 [-3.771, 4.273], mean action: 1.765 [0.000, 3.000], mean observation: -0.014 [-0.472, 0.926], loss: 0.441039, mean_absolute_error: 6.247551, mean_q: -1.834070
  782572/1500000: episode: 1357, duration: 19.352s, episode steps: 1000, steps per second: 52, episode reward: -55.266, mean reward: -0.055 [-13.422, 13.487], mean action: 1.738 [0.000, 3.000], mean observation: 0.024 [-0.741, 1.000], loss: 0.627834, mean_absolute_error: 6.269735, mean_q: -1.639402
  783572/1500000: episode: 1358, duration: 19.237s, episode steps: 1000, steps per second: 52, episode reward: -66.216, mean reward: -0.066 [-3.268, 4.316], mean action: 1.763 [0.000, 3.000], mean observation: 0.050 [-0.798, 0.941], loss: 0.709951, mean_absolute_error: 6.316966, mean_q: -1.796601
  784572/1500000: episode: 1359, duration: 22.289s, episode steps: 1000, steps per second: 45, episode reward: -78.130, mean reward: -0.078 [-3.332, 4.249], mean action: 1.709 [0.000, 3.000], mean observation: 0.011 [-0.389, 0.987], loss: 0.572280, mean_absolute_error: 6.311290, mean_q: -1.703881
  785572/1500000: episode: 1360, duration: 21.166s, episode steps: 1000, steps per second: 47, episode reward: -95.785, mean reward: -0.096 [-3.491, 4.566], mean action: 1.713 [0.000, 3.000], mean observation: 0.020 [-0.367, 0.935], loss: 0.357956, mean_absolute_error: 6.280545, mean_q: -1.786056
  785947/1500000: episode: 1361, duration: 4.710s, episode steps: 375, steps per second: 80, episode reward: 225.932, mean reward: 0.602 [-19.851, 100.000], mean action: 1.128 [0.000, 3.000], mean observation: 0.109 [-0.608, 1.000], loss: 0.406343, mean_absolute_error: 6.248656, mean_q: -1.761203
  786947/1500000: episode: 1362, duration: 25.329s, episode steps: 1000, steps per second: 39, episode reward: -89.262, mean reward: -0.089 [-3.857, 4.445], mean action: 1.790 [0.000, 3.000], mean observation: 0.031 [-0.374, 0.972], loss: 0.408620, mean_absolute_error: 6.380122, mean_q: -1.603279
  787947/1500000: episode: 1363, duration: 19.668s, episode steps: 1000, steps per second: 51, episode reward: -113.879, mean reward: -0.114 [-4.758, 4.264], mean action: 1.661 [0.000, 3.000], mean observation: 0.001 [-0.371, 0.965], loss: 0.665962, mean_absolute_error: 6.412996, mean_q: -1.769127
  788947/1500000: episode: 1364, duration: 21.296s, episode steps: 1000, steps per second: 47, episode reward: 40.183, mean reward: 0.040 [-19.767, 23.508], mean action: 1.334 [0.000, 3.000], mean observation: 0.077 [-0.396, 1.000], loss: 0.561483, mean_absolute_error: 6.353978, mean_q: -1.808561
  789586/1500000: episode: 1365, duration: 11.990s, episode steps: 639, steps per second: 53, episode reward: 174.677, mean reward: 0.273 [-3.674, 100.000], mean action: 1.609 [0.000, 3.000], mean observation: 0.068 [-0.634, 1.000], loss: 0.443573, mean_absolute_error: 6.492682, mean_q: -1.674300
  790382/1500000: episode: 1366, duration: 16.462s, episode steps: 796, steps per second: 48, episode reward: 97.292, mean reward: 0.122 [-10.336, 100.000], mean action: 1.518 [0.000, 3.000], mean observation: 0.045 [-0.274, 1.000], loss: 0.546301, mean_absolute_error: 6.463444, mean_q: -1.713103
  791382/1500000: episode: 1367, duration: 21.330s, episode steps: 1000, steps per second: 47, episode reward: -76.966, mean reward: -0.077 [-3.051, 4.718], mean action: 1.706 [0.000, 3.000], mean observation: 0.012 [-0.317, 1.001], loss: 0.586870, mean_absolute_error: 6.457308, mean_q: -1.684087
  792382/1500000: episode: 1368, duration: 23.345s, episode steps: 1000, steps per second: 43, episode reward: -99.035, mean reward: -0.099 [-3.489, 3.908], mean action: 1.674 [0.000, 3.000], mean observation: 0.022 [-0.377, 0.930], loss: 0.711339, mean_absolute_error: 6.429493, mean_q: -1.956974
  793200/1500000: episode: 1369, duration: 16.422s, episode steps: 818, steps per second: 50, episode reward: 129.608, mean reward: 0.158 [-20.779, 100.000], mean action: 1.346 [0.000, 3.000], mean observation: 0.074 [-0.577, 1.000], loss: 0.616617, mean_absolute_error: 6.498165, mean_q: -1.822270
  794063/1500000: episode: 1370, duration: 17.824s, episode steps: 863, steps per second: 48, episode reward: 101.823, mean reward: 0.118 [-4.713, 100.000], mean action: 1.705 [0.000, 3.000], mean observation: 0.049 [-0.369, 1.000], loss: 0.650657, mean_absolute_error: 6.483273, mean_q: -1.842646
  794820/1500000: episode: 1371, duration: 17.626s, episode steps: 757, steps per second: 43, episode reward: 118.789, mean reward: 0.157 [-14.662, 100.000], mean action: 1.513 [0.000, 3.000], mean observation: 0.065 [-0.348, 1.000], loss: 0.731065, mean_absolute_error: 6.403059, mean_q: -1.799327
  795820/1500000: episode: 1372, duration: 22.356s, episode steps: 1000, steps per second: 45, episode reward: -80.276, mean reward: -0.080 [-3.093, 4.297], mean action: 1.649 [0.000, 3.000], mean observation: 0.036 [-0.292, 0.976], loss: 0.728531, mean_absolute_error: 6.419436, mean_q: -1.963953
  796820/1500000: episode: 1373, duration: 24.306s, episode steps: 1000, steps per second: 41, episode reward: -102.542, mean reward: -0.103 [-3.400, 4.214], mean action: 1.649 [0.000, 3.000], mean observation: 0.026 [-0.354, 0.967], loss: 0.495921, mean_absolute_error: 6.384131, mean_q: -1.939528
  797642/1500000: episode: 1374, duration: 16.839s, episode steps: 822, steps per second: 49, episode reward: 37.939, mean reward: 0.046 [-17.555, 100.000], mean action: 1.529 [0.000, 3.000], mean observation: 0.024 [-0.698, 1.000], loss: 0.907091, mean_absolute_error: 6.364705, mean_q: -2.045011
  798642/1500000: episode: 1375, duration: 19.263s, episode steps: 1000, steps per second: 52, episode reward: -55.866, mean reward: -0.056 [-3.179, 4.091], mean action: 1.725 [0.000, 3.000], mean observation: 0.061 [-0.791, 0.952], loss: 0.633637, mean_absolute_error: 6.413476, mean_q: -1.989695
  799642/1500000: episode: 1376, duration: 22.355s, episode steps: 1000, steps per second: 45, episode reward: -87.924, mean reward: -0.088 [-3.286, 4.222], mean action: 1.737 [0.000, 3.000], mean observation: 0.013 [-0.305, 0.934], loss: 0.838506, mean_absolute_error: 6.479427, mean_q: -1.801651
  800642/1500000: episode: 1377, duration: 23.153s, episode steps: 1000, steps per second: 43, episode reward: -38.020, mean reward: -0.038 [-20.299, 14.915], mean action: 1.692 [0.000, 3.000], mean observation: 0.080 [-0.358, 1.000], loss: 0.485211, mean_absolute_error: 6.487329, mean_q: -2.036372
  801642/1500000: episode: 1378, duration: 19.562s, episode steps: 1000, steps per second: 51, episode reward: -84.282, mean reward: -0.084 [-4.440, 4.155], mean action: 1.596 [0.000, 3.000], mean observation: 0.008 [-0.400, 0.973], loss: 1.182524, mean_absolute_error: 6.575761, mean_q: -1.883488
  802642/1500000: episode: 1379, duration: 20.028s, episode steps: 1000, steps per second: 50, episode reward: -90.431, mean reward: -0.090 [-3.914, 4.349], mean action: 1.819 [0.000, 3.000], mean observation: 0.031 [-0.581, 0.933], loss: 0.874461, mean_absolute_error: 6.534497, mean_q: -1.876333
  803642/1500000: episode: 1380, duration: 26.144s, episode steps: 1000, steps per second: 38, episode reward: -98.720, mean reward: -0.099 [-3.251, 4.100], mean action: 1.773 [0.000, 3.000], mean observation: 0.018 [-0.479, 0.961], loss: 0.609817, mean_absolute_error: 6.411651, mean_q: -1.886154
  804211/1500000: episode: 1381, duration: 10.083s, episode steps: 569, steps per second: 56, episode reward: 159.161, mean reward: 0.280 [-17.776, 100.000], mean action: 1.445 [0.000, 3.000], mean observation: 0.065 [-0.742, 1.013], loss: 1.014210, mean_absolute_error: 6.476947, mean_q: -1.833839
  805211/1500000: episode: 1382, duration: 22.503s, episode steps: 1000, steps per second: 44, episode reward: -123.016, mean reward: -0.123 [-3.661, 4.160], mean action: 1.801 [0.000, 3.000], mean observation: 0.005 [-0.563, 0.937], loss: 0.492428, mean_absolute_error: 6.470756, mean_q: -1.819396
  806206/1500000: episode: 1383, duration: 20.663s, episode steps: 995, steps per second: 48, episode reward: 106.568, mean reward: 0.107 [-10.074, 100.000], mean action: 1.389 [0.000, 3.000], mean observation: 0.035 [-0.544, 1.000], loss: 1.057654, mean_absolute_error: 6.476295, mean_q: -1.840704
  807066/1500000: episode: 1384, duration: 17.318s, episode steps: 860, steps per second: 50, episode reward: 151.384, mean reward: 0.176 [-19.939, 100.000], mean action: 1.152 [0.000, 3.000], mean observation: 0.085 [-0.498, 1.000], loss: 0.773313, mean_absolute_error: 6.468832, mean_q: -1.852581
  808066/1500000: episode: 1385, duration: 21.705s, episode steps: 1000, steps per second: 46, episode reward: -63.113, mean reward: -0.063 [-10.329, 12.835], mean action: 1.880 [0.000, 3.000], mean observation: 0.026 [-0.255, 1.000], loss: 0.937419, mean_absolute_error: 6.426848, mean_q: -1.811766
  809066/1500000: episode: 1386, duration: 25.049s, episode steps: 1000, steps per second: 40, episode reward: -16.362, mean reward: -0.016 [-11.028, 21.294], mean action: 1.767 [0.000, 3.000], mean observation: 0.040 [-0.490, 1.000], loss: 0.855277, mean_absolute_error: 6.412129, mean_q: -1.790637
  809623/1500000: episode: 1387, duration: 10.369s, episode steps: 557, steps per second: 54, episode reward: 161.134, mean reward: 0.289 [-9.423, 100.000], mean action: 1.578 [0.000, 3.000], mean observation: 0.045 [-0.345, 1.000], loss: 1.093597, mean_absolute_error: 6.423286, mean_q: -1.716701
  810035/1500000: episode: 1388, duration: 8.271s, episode steps: 412, steps per second: 50, episode reward: 195.942, mean reward: 0.476 [-10.487, 100.000], mean action: 1.820 [0.000, 3.000], mean observation: 0.072 [-0.645, 1.000], loss: 1.046490, mean_absolute_error: 6.408702, mean_q: -1.779917
  811020/1500000: episode: 1389, duration: 20.673s, episode steps: 985, steps per second: 48, episode reward: 79.604, mean reward: 0.081 [-20.758, 100.000], mean action: 1.885 [0.000, 3.000], mean observation: 0.054 [-0.311, 1.000], loss: 0.718934, mean_absolute_error: 6.440322, mean_q: -1.710208
  811729/1500000: episode: 1390, duration: 15.107s, episode steps: 709, steps per second: 47, episode reward: 64.025, mean reward: 0.090 [-6.928, 100.000], mean action: 1.748 [0.000, 3.000], mean observation: 0.009 [-0.687, 1.000], loss: 0.911874, mean_absolute_error: 6.436272, mean_q: -1.832098
  812525/1500000: episode: 1391, duration: 17.274s, episode steps: 796, steps per second: 46, episode reward: -104.383, mean reward: -0.131 [-100.000, 13.225], mean action: 1.528 [0.000, 3.000], mean observation: 0.097 [-0.516, 1.000], loss: 0.904876, mean_absolute_error: 6.452216, mean_q: -1.623909
  813261/1500000: episode: 1392, duration: 15.752s, episode steps: 736, steps per second: 47, episode reward: 82.127, mean reward: 0.112 [-15.027, 100.000], mean action: 2.058 [0.000, 3.000], mean observation: 0.027 [-0.672, 1.000], loss: 1.002745, mean_absolute_error: 6.491312, mean_q: -1.595892
  813859/1500000: episode: 1393, duration: 13.922s, episode steps: 598, steps per second: 43, episode reward: 178.107, mean reward: 0.298 [-6.725, 100.000], mean action: 1.548 [0.000, 3.000], mean observation: 0.054 [-0.749, 1.000], loss: 1.009385, mean_absolute_error: 6.495335, mean_q: -1.679943
  814859/1500000: episode: 1394, duration: 20.858s, episode steps: 1000, steps per second: 48, episode reward: -93.181, mean reward: -0.093 [-3.234, 4.166], mean action: 1.788 [0.000, 3.000], mean observation: 0.004 [-0.370, 0.949], loss: 0.594091, mean_absolute_error: 6.452621, mean_q: -2.001125
  815859/1500000: episode: 1395, duration: 28.322s, episode steps: 1000, steps per second: 35, episode reward: -87.710, mean reward: -0.088 [-11.953, 17.040], mean action: 1.675 [0.000, 3.000], mean observation: 0.004 [-0.604, 1.000], loss: 1.220482, mean_absolute_error: 6.445768, mean_q: -1.909396
  816849/1500000: episode: 1396, duration: 20.361s, episode steps: 990, steps per second: 49, episode reward: 79.331, mean reward: 0.080 [-20.260, 100.000], mean action: 1.859 [0.000, 3.000], mean observation: 0.054 [-0.384, 1.000], loss: 0.638297, mean_absolute_error: 6.354961, mean_q: -2.084406
  817849/1500000: episode: 1397, duration: 21.002s, episode steps: 1000, steps per second: 48, episode reward: -65.113, mean reward: -0.065 [-3.735, 4.449], mean action: 1.679 [0.000, 3.000], mean observation: 0.020 [-0.317, 1.011], loss: 0.428879, mean_absolute_error: 6.369340, mean_q: -2.235465
  818849/1500000: episode: 1398, duration: 24.015s, episode steps: 1000, steps per second: 42, episode reward: -86.874, mean reward: -0.087 [-3.536, 4.096], mean action: 1.816 [0.000, 3.000], mean observation: 0.004 [-0.602, 0.925], loss: 0.675690, mean_absolute_error: 6.362016, mean_q: -2.126651
  819526/1500000: episode: 1399, duration: 12.882s, episode steps: 677, steps per second: 53, episode reward: 134.560, mean reward: 0.199 [-15.639, 100.000], mean action: 1.421 [0.000, 3.000], mean observation: 0.103 [-0.669, 1.000], loss: 0.840222, mean_absolute_error: 6.375870, mean_q: -2.348695
  820526/1500000: episode: 1400, duration: 23.016s, episode steps: 1000, steps per second: 43, episode reward: -0.719, mean reward: -0.001 [-17.393, 12.168], mean action: 1.991 [0.000, 3.000], mean observation: 0.056 [-0.331, 1.000], loss: 1.344471, mean_absolute_error: 6.420681, mean_q: -2.496650
  821526/1500000: episode: 1401, duration: 20.294s, episode steps: 1000, steps per second: 49, episode reward: -118.839, mean reward: -0.119 [-5.173, 4.419], mean action: 1.620 [0.000, 3.000], mean observation: 0.142 [-0.590, 0.927], loss: 0.606068, mean_absolute_error: 6.402300, mean_q: -2.456894
  822526/1500000: episode: 1402, duration: 24.661s, episode steps: 1000, steps per second: 41, episode reward: -76.556, mean reward: -0.077 [-3.459, 3.788], mean action: 1.733 [0.000, 3.000], mean observation: 0.020 [-0.485, 0.928], loss: 0.811781, mean_absolute_error: 6.362227, mean_q: -2.388619
  823526/1500000: episode: 1403, duration: 22.496s, episode steps: 1000, steps per second: 44, episode reward: -93.533, mean reward: -0.094 [-3.267, 4.308], mean action: 1.770 [0.000, 3.000], mean observation: 0.015 [-0.558, 0.976], loss: 0.875460, mean_absolute_error: 6.388284, mean_q: -2.523722
  824312/1500000: episode: 1404, duration: 17.960s, episode steps: 786, steps per second: 44, episode reward: 135.789, mean reward: 0.173 [-13.793, 100.000], mean action: 1.427 [0.000, 3.000], mean observation: 0.088 [-0.572, 1.000], loss: 1.260482, mean_absolute_error: 6.497367, mean_q: -2.300551
  825301/1500000: episode: 1405, duration: 19.718s, episode steps: 989, steps per second: 50, episode reward: 70.427, mean reward: 0.071 [-10.421, 100.000], mean action: 1.770 [0.000, 3.000], mean observation: 0.064 [-0.303, 1.000], loss: 1.119169, mean_absolute_error: 6.433263, mean_q: -2.297779
  826301/1500000: episode: 1406, duration: 23.391s, episode steps: 1000, steps per second: 43, episode reward: -91.490, mean reward: -0.091 [-3.286, 3.964], mean action: 1.859 [0.000, 3.000], mean observation: 0.027 [-0.538, 0.941], loss: 1.138442, mean_absolute_error: 6.372859, mean_q: -2.063375
  827301/1500000: episode: 1407, duration: 22.712s, episode steps: 1000, steps per second: 44, episode reward: 24.641, mean reward: 0.025 [-20.175, 13.754], mean action: 1.267 [0.000, 3.000], mean observation: 0.105 [-0.491, 1.000], loss: 0.970310, mean_absolute_error: 6.310812, mean_q: -1.932648
  828301/1500000: episode: 1408, duration: 19.827s, episode steps: 1000, steps per second: 50, episode reward: -86.601, mean reward: -0.087 [-3.992, 4.167], mean action: 1.647 [0.000, 3.000], mean observation: 0.055 [-0.481, 0.974], loss: 0.962689, mean_absolute_error: 6.308888, mean_q: -1.771012
  829301/1500000: episode: 1409, duration: 25.662s, episode steps: 1000, steps per second: 39, episode reward: -64.374, mean reward: -0.064 [-11.135, 16.514], mean action: 1.975 [0.000, 3.000], mean observation: 0.022 [-0.369, 1.000], loss: 0.930008, mean_absolute_error: 6.274373, mean_q: -1.628169
  830198/1500000: episode: 1410, duration: 19.169s, episode steps: 897, steps per second: 47, episode reward: -147.429, mean reward: -0.164 [-100.000, 12.984], mean action: 1.812 [0.000, 3.000], mean observation: 0.024 [-0.679, 1.024], loss: 0.987309, mean_absolute_error: 6.193768, mean_q: -1.442323
  831198/1500000: episode: 1411, duration: 20.921s, episode steps: 1000, steps per second: 48, episode reward: -65.143, mean reward: -0.065 [-3.294, 4.238], mean action: 1.811 [0.000, 3.000], mean observation: 0.006 [-0.674, 0.936], loss: 1.158717, mean_absolute_error: 6.215567, mean_q: -1.380307
  832198/1500000: episode: 1412, duration: 20.232s, episode steps: 1000, steps per second: 49, episode reward: -114.552, mean reward: -0.115 [-4.371, 4.255], mean action: 1.763 [0.000, 3.000], mean observation: 0.019 [-0.510, 0.971], loss: 0.771400, mean_absolute_error: 6.101057, mean_q: -1.366946
  832792/1500000: episode: 1413, duration: 13.013s, episode steps: 594, steps per second: 46, episode reward: 187.736, mean reward: 0.316 [-10.423, 100.000], mean action: 1.667 [0.000, 3.000], mean observation: 0.099 [-0.385, 1.000], loss: 1.102014, mean_absolute_error: 6.201810, mean_q: -0.989856
  833341/1500000: episode: 1414, duration: 12.860s, episode steps: 549, steps per second: 43, episode reward: 141.290, mean reward: 0.257 [-10.623, 100.000], mean action: 1.701 [0.000, 3.000], mean observation: 0.087 [-0.363, 1.000], loss: 1.304995, mean_absolute_error: 6.187737, mean_q: -1.165400
  833899/1500000: episode: 1415, duration: 9.550s, episode steps: 558, steps per second: 58, episode reward: 89.144, mean reward: 0.160 [-10.228, 100.000], mean action: 2.002 [0.000, 3.000], mean observation: 0.032 [-0.326, 1.000], loss: 0.721523, mean_absolute_error: 6.142724, mean_q: -0.844609
  834899/1500000: episode: 1416, duration: 24.688s, episode steps: 1000, steps per second: 41, episode reward: -80.466, mean reward: -0.080 [-3.539, 4.233], mean action: 1.753 [0.000, 3.000], mean observation: -0.002 [-0.502, 0.928], loss: 0.776980, mean_absolute_error: 6.251879, mean_q: -0.723125
  835899/1500000: episode: 1417, duration: 21.356s, episode steps: 1000, steps per second: 47, episode reward: -39.638, mean reward: -0.040 [-17.591, 15.169], mean action: 1.667 [0.000, 3.000], mean observation: 0.053 [-0.508, 1.000], loss: 1.116278, mean_absolute_error: 6.151474, mean_q: -0.845845
  836899/1500000: episode: 1418, duration: 20.606s, episode steps: 1000, steps per second: 49, episode reward: -76.574, mean reward: -0.077 [-4.670, 3.908], mean action: 1.780 [0.000, 3.000], mean observation: -0.020 [-0.490, 0.939], loss: 1.379379, mean_absolute_error: 6.102226, mean_q: -0.704348
  837899/1500000: episode: 1419, duration: 24.503s, episode steps: 1000, steps per second: 41, episode reward: -55.653, mean reward: -0.056 [-10.809, 11.231], mean action: 1.665 [0.000, 3.000], mean observation: 0.025 [-0.347, 1.000], loss: 1.406225, mean_absolute_error: 6.107044, mean_q: -0.716631
  838653/1500000: episode: 1420, duration: 14.153s, episode steps: 754, steps per second: 53, episode reward: -252.768, mean reward: -0.335 [-100.000, 3.586], mean action: 1.562 [0.000, 3.000], mean observation: 0.237 [-0.317, 1.359], loss: 1.005945, mean_absolute_error: 6.134376, mean_q: -0.664657
  839653/1500000: episode: 1421, duration: 22.024s, episode steps: 1000, steps per second: 45, episode reward: -195.883, mean reward: -0.196 [-5.046, 3.109], mean action: 1.418 [0.000, 3.000], mean observation: 0.220 [-0.269, 1.342], loss: 1.040132, mean_absolute_error: 6.103752, mean_q: -0.437749
  840653/1500000: episode: 1422, duration: 21.589s, episode steps: 1000, steps per second: 46, episode reward: -30.981, mean reward: -0.031 [-21.195, 19.367], mean action: 1.668 [0.000, 3.000], mean observation: 0.117 [-0.279, 1.000], loss: 0.704037, mean_absolute_error: 5.985167, mean_q: -0.327438
  841653/1500000: episode: 1423, duration: 20.830s, episode steps: 1000, steps per second: 48, episode reward: -81.610, mean reward: -0.082 [-11.494, 11.692], mean action: 1.855 [0.000, 3.000], mean observation: 0.008 [-0.470, 1.014], loss: 1.022351, mean_absolute_error: 5.936629, mean_q: -0.242433
  842385/1500000: episode: 1424, duration: 17.054s, episode steps: 732, steps per second: 43, episode reward: 105.980, mean reward: 0.145 [-20.479, 100.000], mean action: 1.471 [0.000, 3.000], mean observation: 0.102 [-0.343, 1.000], loss: 0.931715, mean_absolute_error: 6.007698, mean_q: -0.187683
  843385/1500000: episode: 1425, duration: 24.661s, episode steps: 1000, steps per second: 41, episode reward: -26.500, mean reward: -0.027 [-17.713, 13.279], mean action: 1.884 [0.000, 3.000], mean observation: 0.078 [-0.294, 1.000], loss: 0.921977, mean_absolute_error: 5.920796, mean_q: -0.157093
  844047/1500000: episode: 1426, duration: 11.939s, episode steps: 662, steps per second: 55, episode reward: 152.765, mean reward: 0.231 [-11.873, 100.000], mean action: 1.527 [0.000, 3.000], mean observation: 0.096 [-0.590, 1.000], loss: 1.540138, mean_absolute_error: 6.025925, mean_q: 0.041014
  844956/1500000: episode: 1427, duration: 21.161s, episode steps: 909, steps per second: 43, episode reward: 58.921, mean reward: 0.065 [-21.278, 100.000], mean action: 2.063 [0.000, 3.000], mean observation: 0.047 [-0.532, 1.000], loss: 0.900028, mean_absolute_error: 6.082084, mean_q: 0.239118
  845829/1500000: episode: 1428, duration: 19.552s, episode steps: 873, steps per second: 45, episode reward: 70.277, mean reward: 0.081 [-10.958, 100.000], mean action: 1.959 [0.000, 3.000], mean observation: 0.040 [-0.265, 1.000], loss: 0.754653, mean_absolute_error: 6.088291, mean_q: 0.279363
  846807/1500000: episode: 1429, duration: 22.456s, episode steps: 978, steps per second: 44, episode reward: 84.201, mean reward: 0.086 [-18.256, 100.000], mean action: 1.894 [0.000, 3.000], mean observation: 0.080 [-0.330, 1.000], loss: 0.739255, mean_absolute_error: 6.065053, mean_q: 0.415611
  847734/1500000: episode: 1430, duration: 19.088s, episode steps: 927, steps per second: 49, episode reward: 123.353, mean reward: 0.133 [-18.926, 100.000], mean action: 1.518 [0.000, 3.000], mean observation: 0.056 [-0.572, 1.000], loss: 1.405520, mean_absolute_error: 6.138122, mean_q: 0.564324
  848734/1500000: episode: 1431, duration: 21.386s, episode steps: 1000, steps per second: 47, episode reward: -75.983, mean reward: -0.076 [-3.349, 4.471], mean action: 1.875 [0.000, 3.000], mean observation: -0.018 [-0.409, 0.928], loss: 0.744292, mean_absolute_error: 6.147739, mean_q: 0.676654
  849734/1500000: episode: 1432, duration: 24.990s, episode steps: 1000, steps per second: 40, episode reward: -123.763, mean reward: -0.124 [-10.659, 14.532], mean action: 1.866 [0.000, 3.000], mean observation: -0.012 [-0.504, 1.000], loss: 1.114873, mean_absolute_error: 6.181949, mean_q: 0.683125
  850734/1500000: episode: 1433, duration: 23.138s, episode steps: 1000, steps per second: 43, episode reward: -17.278, mean reward: -0.017 [-13.761, 13.314], mean action: 1.645 [0.000, 3.000], mean observation: 0.019 [-0.416, 1.000], loss: 1.141600, mean_absolute_error: 6.266764, mean_q: 0.876618
  851734/1500000: episode: 1434, duration: 21.400s, episode steps: 1000, steps per second: 47, episode reward: -107.181, mean reward: -0.107 [-4.801, 4.179], mean action: 1.727 [0.000, 3.000], mean observation: 0.040 [-0.505, 0.995], loss: 0.898286, mean_absolute_error: 6.146301, mean_q: 0.728516
  852734/1500000: episode: 1435, duration: 24.825s, episode steps: 1000, steps per second: 40, episode reward: -29.743, mean reward: -0.030 [-9.435, 11.905], mean action: 1.795 [0.000, 3.000], mean observation: 0.040 [-0.312, 1.000], loss: 0.720296, mean_absolute_error: 6.135059, mean_q: 0.754103
  853519/1500000: episode: 1436, duration: 16.265s, episode steps: 785, steps per second: 48, episode reward: 143.283, mean reward: 0.183 [-11.753, 100.000], mean action: 1.954 [0.000, 3.000], mean observation: 0.037 [-0.656, 1.000], loss: 0.958755, mean_absolute_error: 6.113499, mean_q: 1.056944
  854519/1500000: episode: 1437, duration: 20.749s, episode steps: 1000, steps per second: 48, episode reward: -6.081, mean reward: -0.006 [-10.162, 13.306], mean action: 1.870 [0.000, 3.000], mean observation: 0.063 [-0.412, 1.000], loss: 0.566397, mean_absolute_error: 5.958770, mean_q: 1.322371
  855427/1500000: episode: 1438, duration: 19.240s, episode steps: 908, steps per second: 47, episode reward: 121.981, mean reward: 0.134 [-11.484, 100.000], mean action: 1.728 [0.000, 3.000], mean observation: 0.039 [-0.762, 1.000], loss: 1.147777, mean_absolute_error: 5.949220, mean_q: 1.389078
  856427/1500000: episode: 1439, duration: 20.183s, episode steps: 1000, steps per second: 50, episode reward: -92.480, mean reward: -0.092 [-4.344, 5.167], mean action: 1.796 [0.000, 3.000], mean observation: 0.016 [-0.407, 1.010], loss: 0.816094, mean_absolute_error: 5.961217, mean_q: 1.347669
  857361/1500000: episode: 1440, duration: 19.732s, episode steps: 934, steps per second: 47, episode reward: 137.202, mean reward: 0.147 [-18.048, 100.000], mean action: 1.727 [0.000, 3.000], mean observation: 0.062 [-0.690, 1.000], loss: 1.401279, mean_absolute_error: 6.036178, mean_q: 1.719657
  858361/1500000: episode: 1441, duration: 22.624s, episode steps: 1000, steps per second: 44, episode reward: -93.852, mean reward: -0.094 [-4.143, 4.723], mean action: 1.842 [0.000, 3.000], mean observation: 0.009 [-0.559, 1.000], loss: 1.045671, mean_absolute_error: 6.083363, mean_q: 1.802248
  859361/1500000: episode: 1442, duration: 20.048s, episode steps: 1000, steps per second: 50, episode reward: -78.919, mean reward: -0.079 [-3.997, 4.119], mean action: 1.844 [0.000, 3.000], mean observation: 0.011 [-0.425, 0.979], loss: 1.089637, mean_absolute_error: 6.032976, mean_q: 1.911037
  860361/1500000: episode: 1443, duration: 23.961s, episode steps: 1000, steps per second: 42, episode reward: -21.918, mean reward: -0.022 [-12.949, 14.026], mean action: 1.795 [0.000, 3.000], mean observation: 0.019 [-0.619, 1.000], loss: 0.869827, mean_absolute_error: 6.044319, mean_q: 2.118841
  861361/1500000: episode: 1444, duration: 23.621s, episode steps: 1000, steps per second: 42, episode reward: -81.244, mean reward: -0.081 [-6.777, 15.973], mean action: 1.882 [0.000, 3.000], mean observation: -0.002 [-0.562, 1.032], loss: 0.850229, mean_absolute_error: 6.109221, mean_q: 2.497285
  861714/1500000: episode: 1445, duration: 5.240s, episode steps: 353, steps per second: 67, episode reward: 226.031, mean reward: 0.640 [-3.161, 100.000], mean action: 1.513 [0.000, 3.000], mean observation: 0.093 [-0.595, 1.000], loss: 1.072004, mean_absolute_error: 6.095990, mean_q: 2.450818
  862714/1500000: episode: 1446, duration: 21.604s, episode steps: 1000, steps per second: 46, episode reward: -67.482, mean reward: -0.067 [-5.239, 4.375], mean action: 1.857 [0.000, 3.000], mean observation: -0.001 [-0.592, 0.942], loss: 1.163836, mean_absolute_error: 6.153472, mean_q: 2.653748
  863714/1500000: episode: 1447, duration: 24.507s, episode steps: 1000, steps per second: 41, episode reward: 31.922, mean reward: 0.032 [-20.245, 23.905], mean action: 1.371 [0.000, 3.000], mean observation: 0.099 [-0.347, 1.000], loss: 1.001268, mean_absolute_error: 6.216297, mean_q: 2.776575
  864714/1500000: episode: 1448, duration: 21.689s, episode steps: 1000, steps per second: 46, episode reward: -98.942, mean reward: -0.099 [-5.079, 4.119], mean action: 1.780 [0.000, 3.000], mean observation: 0.005 [-0.557, 0.931], loss: 1.278962, mean_absolute_error: 6.254320, mean_q: 2.830441
  865389/1500000: episode: 1449, duration: 15.057s, episode steps: 675, steps per second: 45, episode reward: 152.027, mean reward: 0.225 [-18.679, 100.000], mean action: 1.696 [0.000, 3.000], mean observation: 0.085 [-0.412, 1.000], loss: 1.115291, mean_absolute_error: 6.243054, mean_q: 2.827451
  866327/1500000: episode: 1450, duration: 19.188s, episode steps: 938, steps per second: 49, episode reward: 91.293, mean reward: 0.097 [-14.095, 100.000], mean action: 1.606 [0.000, 3.000], mean observation: 0.032 [-0.469, 1.001], loss: 1.027921, mean_absolute_error: 6.191485, mean_q: 2.594820
  866900/1500000: episode: 1451, duration: 14.322s, episode steps: 573, steps per second: 40, episode reward: 183.913, mean reward: 0.321 [-17.353, 100.000], mean action: 1.305 [0.000, 3.000], mean observation: 0.116 [-0.584, 1.000], loss: 1.046883, mean_absolute_error: 6.192361, mean_q: 2.527756
  867574/1500000: episode: 1452, duration: 13.664s, episode steps: 674, steps per second: 49, episode reward: 128.425, mean reward: 0.191 [-12.018, 100.000], mean action: 1.629 [0.000, 3.000], mean observation: 0.076 [-0.325, 1.000], loss: 1.193303, mean_absolute_error: 6.179014, mean_q: 2.486177
  868328/1500000: episode: 1453, duration: 15.611s, episode steps: 754, steps per second: 48, episode reward: 88.069, mean reward: 0.117 [-10.961, 100.000], mean action: 1.675 [0.000, 3.000], mean observation: 0.041 [-0.288, 1.000], loss: 0.906933, mean_absolute_error: 6.190466, mean_q: 2.545199
  869066/1500000: episode: 1454, duration: 14.336s, episode steps: 738, steps per second: 51, episode reward: 136.427, mean reward: 0.185 [-4.422, 100.000], mean action: 1.771 [0.000, 3.000], mean observation: 0.076 [-0.250, 1.000], loss: 1.015964, mean_absolute_error: 6.272366, mean_q: 2.739903
  869381/1500000: episode: 1455, duration: 6.924s, episode steps: 315, steps per second: 45, episode reward: -86.430, mean reward: -0.274 [-100.000, 15.123], mean action: 1.902 [0.000, 3.000], mean observation: 0.059 [-1.010, 1.919], loss: 0.749692, mean_absolute_error: 6.254434, mean_q: 2.791190
  870277/1500000: episode: 1456, duration: 28.759s, episode steps: 896, steps per second: 31, episode reward: 166.489, mean reward: 0.186 [-21.056, 100.000], mean action: 1.356 [0.000, 3.000], mean observation: 0.119 [-0.455, 1.000], loss: 0.805242, mean_absolute_error: 6.317142, mean_q: 2.888366
  870816/1500000: episode: 1457, duration: 16.094s, episode steps: 539, steps per second: 33, episode reward: 182.591, mean reward: 0.339 [-17.982, 100.000], mean action: 1.638 [0.000, 3.000], mean observation: 0.075 [-0.571, 1.000], loss: 0.755313, mean_absolute_error: 6.303473, mean_q: 2.918214
  871450/1500000: episode: 1458, duration: 18.544s, episode steps: 634, steps per second: 34, episode reward: 177.401, mean reward: 0.280 [-4.251, 100.000], mean action: 1.763 [0.000, 3.000], mean observation: 0.097 [-0.727, 1.000], loss: 1.513434, mean_absolute_error: 6.381611, mean_q: 3.417923
  871871/1500000: episode: 1459, duration: 11.712s, episode steps: 421, steps per second: 36, episode reward: 218.487, mean reward: 0.519 [-10.144, 100.000], mean action: 1.722 [0.000, 3.000], mean observation: 0.058 [-0.675, 1.000], loss: 1.582860, mean_absolute_error: 6.454480, mean_q: 3.687327
  872656/1500000: episode: 1460, duration: 22.775s, episode steps: 785, steps per second: 34, episode reward: 98.750, mean reward: 0.126 [-19.549, 100.000], mean action: 1.855 [0.000, 3.000], mean observation: 0.090 [-0.314, 1.000], loss: 1.255252, mean_absolute_error: 6.459359, mean_q: 3.788817
  873411/1500000: episode: 1461, duration: 22.078s, episode steps: 755, steps per second: 34, episode reward: 177.406, mean reward: 0.235 [-17.426, 100.000], mean action: 1.266 [0.000, 3.000], mean observation: 0.098 [-0.421, 1.000], loss: 1.030429, mean_absolute_error: 6.558146, mean_q: 4.028300
  874411/1500000: episode: 1462, duration: 30.270s, episode steps: 1000, steps per second: 33, episode reward: -122.812, mean reward: -0.123 [-4.681, 4.239], mean action: 1.832 [0.000, 3.000], mean observation: -0.011 [-0.534, 0.957], loss: 1.656736, mean_absolute_error: 6.563753, mean_q: 4.199798
  875411/1500000: episode: 1463, duration: 33.520s, episode steps: 1000, steps per second: 30, episode reward: -124.149, mean reward: -0.124 [-4.519, 4.275], mean action: 1.744 [0.000, 3.000], mean observation: 0.011 [-0.595, 0.967], loss: 1.737821, mean_absolute_error: 6.519999, mean_q: 4.113699
  876411/1500000: episode: 1464, duration: 31.634s, episode steps: 1000, steps per second: 32, episode reward: -71.914, mean reward: -0.072 [-22.366, 21.403], mean action: 1.802 [0.000, 3.000], mean observation: 0.048 [-0.486, 1.000], loss: 1.065920, mean_absolute_error: 6.576525, mean_q: 4.246197
  877411/1500000: episode: 1465, duration: 30.724s, episode steps: 1000, steps per second: 33, episode reward: -79.173, mean reward: -0.079 [-5.132, 4.774], mean action: 1.801 [0.000, 3.000], mean observation: 0.015 [-0.451, 1.007], loss: 1.108761, mean_absolute_error: 6.540707, mean_q: 3.997530
  878411/1500000: episode: 1466, duration: 31.671s, episode steps: 1000, steps per second: 32, episode reward: -78.999, mean reward: -0.079 [-4.399, 4.920], mean action: 1.808 [0.000, 3.000], mean observation: 0.025 [-0.360, 0.963], loss: 0.959254, mean_absolute_error: 6.500048, mean_q: 3.814019
  879183/1500000: episode: 1467, duration: 23.843s, episode steps: 772, steps per second: 32, episode reward: 104.279, mean reward: 0.135 [-10.591, 100.000], mean action: 1.754 [0.000, 3.000], mean observation: 0.048 [-0.292, 1.000], loss: 1.093095, mean_absolute_error: 6.529722, mean_q: 3.830222
  879823/1500000: episode: 1468, duration: 18.968s, episode steps: 640, steps per second: 34, episode reward: 139.022, mean reward: 0.217 [-12.203, 100.000], mean action: 1.391 [0.000, 3.000], mean observation: 0.062 [-0.310, 1.000], loss: 1.358064, mean_absolute_error: 6.540502, mean_q: 3.676491
  880492/1500000: episode: 1469, duration: 18.596s, episode steps: 669, steps per second: 36, episode reward: 165.531, mean reward: 0.247 [-4.059, 100.000], mean action: 1.622 [0.000, 3.000], mean observation: 0.109 [-0.387, 1.000], loss: 1.352708, mean_absolute_error: 6.691044, mean_q: 3.765008
  881003/1500000: episode: 1470, duration: 14.153s, episode steps: 511, steps per second: 36, episode reward: 190.898, mean reward: 0.374 [-9.604, 100.000], mean action: 1.751 [0.000, 3.000], mean observation: 0.054 [-1.296, 1.000], loss: 1.399716, mean_absolute_error: 6.680816, mean_q: 3.821659
  882003/1500000: episode: 1471, duration: 29.212s, episode steps: 1000, steps per second: 34, episode reward: -13.878, mean reward: -0.014 [-18.033, 13.188], mean action: 1.427 [0.000, 3.000], mean observation: 0.076 [-0.372, 1.000], loss: 1.270027, mean_absolute_error: 6.725673, mean_q: 3.785197
  882638/1500000: episode: 1472, duration: 18.021s, episode steps: 635, steps per second: 35, episode reward: 128.023, mean reward: 0.202 [-18.713, 100.000], mean action: 1.630 [0.000, 3.000], mean observation: 0.031 [-0.580, 1.000], loss: 1.650365, mean_absolute_error: 6.844561, mean_q: 3.898017
  883200/1500000: episode: 1473, duration: 15.888s, episode steps: 562, steps per second: 35, episode reward: 118.653, mean reward: 0.211 [-18.653, 100.000], mean action: 1.349 [0.000, 3.000], mean observation: 0.055 [-0.288, 1.000], loss: 1.278169, mean_absolute_error: 6.811323, mean_q: 4.000995
  883674/1500000: episode: 1474, duration: 13.768s, episode steps: 474, steps per second: 34, episode reward: 189.027, mean reward: 0.399 [-10.738, 100.000], mean action: 1.536 [0.000, 3.000], mean observation: 0.093 [-0.487, 1.000], loss: 1.294479, mean_absolute_error: 6.939068, mean_q: 4.217825
  884277/1500000: episode: 1475, duration: 17.717s, episode steps: 603, steps per second: 34, episode reward: 155.344, mean reward: 0.258 [-13.226, 100.000], mean action: 1.992 [0.000, 3.000], mean observation: 0.004 [-0.678, 1.000], loss: 1.207032, mean_absolute_error: 6.893225, mean_q: 4.206898
  885277/1500000: episode: 1476, duration: 31.245s, episode steps: 1000, steps per second: 32, episode reward: -23.274, mean reward: -0.023 [-18.868, 21.887], mean action: 2.088 [0.000, 3.000], mean observation: 0.078 [-0.331, 1.000], loss: 1.106144, mean_absolute_error: 6.938346, mean_q: 4.311274
  885849/1500000: episode: 1477, duration: 17.172s, episode steps: 572, steps per second: 33, episode reward: 189.703, mean reward: 0.332 [-18.967, 100.000], mean action: 1.678 [0.000, 3.000], mean observation: 0.054 [-0.693, 1.000], loss: 0.983601, mean_absolute_error: 6.862975, mean_q: 4.222617
  886394/1500000: episode: 1478, duration: 15.368s, episode steps: 545, steps per second: 35, episode reward: 168.489, mean reward: 0.309 [-18.893, 100.000], mean action: 1.532 [0.000, 3.000], mean observation: 0.049 [-0.472, 1.000], loss: 1.727405, mean_absolute_error: 7.095404, mean_q: 4.397342
  887333/1500000: episode: 1479, duration: 29.103s, episode steps: 939, steps per second: 32, episode reward: 92.340, mean reward: 0.098 [-18.169, 100.000], mean action: 1.544 [0.000, 3.000], mean observation: 0.047 [-0.438, 1.000], loss: 0.998647, mean_absolute_error: 6.970589, mean_q: 4.373433
  887909/1500000: episode: 1480, duration: 16.079s, episode steps: 576, steps per second: 36, episode reward: 129.259, mean reward: 0.224 [-12.602, 100.000], mean action: 1.427 [0.000, 3.000], mean observation: 0.041 [-0.390, 1.000], loss: 1.148643, mean_absolute_error: 7.023878, mean_q: 4.483667
  888909/1500000: episode: 1481, duration: 29.753s, episode steps: 1000, steps per second: 34, episode reward: -92.062, mean reward: -0.092 [-3.651, 4.474], mean action: 1.885 [0.000, 3.000], mean observation: -0.017 [-0.482, 0.961], loss: 1.523343, mean_absolute_error: 7.101079, mean_q: 4.747933
  889567/1500000: episode: 1482, duration: 18.082s, episode steps: 658, steps per second: 36, episode reward: 191.256, mean reward: 0.291 [-14.520, 100.000], mean action: 1.486 [0.000, 3.000], mean observation: 0.117 [-0.412, 1.000], loss: 1.224994, mean_absolute_error: 7.198280, mean_q: 4.935372
  890335/1500000: episode: 1483, duration: 23.884s, episode steps: 768, steps per second: 32, episode reward: 115.366, mean reward: 0.150 [-18.190, 100.000], mean action: 1.474 [0.000, 3.000], mean observation: 0.084 [-0.285, 1.000], loss: 1.754951, mean_absolute_error: 7.226440, mean_q: 5.001152
  890985/1500000: episode: 1484, duration: 18.789s, episode steps: 650, steps per second: 35, episode reward: 178.776, mean reward: 0.275 [-12.643, 100.000], mean action: 1.706 [0.000, 3.000], mean observation: 0.033 [-0.675, 1.000], loss: 1.130516, mean_absolute_error: 7.192533, mean_q: 4.887723
  891666/1500000: episode: 1485, duration: 20.664s, episode steps: 681, steps per second: 33, episode reward: 114.613, mean reward: 0.168 [-12.788, 100.000], mean action: 1.680 [0.000, 3.000], mean observation: 0.004 [-0.466, 1.000], loss: 1.759687, mean_absolute_error: 7.276209, mean_q: 5.000862
  892335/1500000: episode: 1486, duration: 19.442s, episode steps: 669, steps per second: 34, episode reward: 120.343, mean reward: 0.180 [-12.658, 100.000], mean action: 1.667 [0.000, 3.000], mean observation: 0.062 [-0.441, 1.000], loss: 1.337094, mean_absolute_error: 7.247436, mean_q: 4.948354
  892906/1500000: episode: 1487, duration: 16.144s, episode steps: 571, steps per second: 35, episode reward: 133.206, mean reward: 0.233 [-17.129, 100.000], mean action: 2.047 [0.000, 3.000], mean observation: 0.056 [-0.477, 1.000], loss: 1.679344, mean_absolute_error: 7.333622, mean_q: 5.042089
  893547/1500000: episode: 1488, duration: 19.593s, episode steps: 641, steps per second: 33, episode reward: 190.064, mean reward: 0.297 [-9.842, 100.000], mean action: 1.484 [0.000, 3.000], mean observation: 0.116 [-0.387, 1.000], loss: 1.278308, mean_absolute_error: 7.341881, mean_q: 5.182261
  894294/1500000: episode: 1489, duration: 22.612s, episode steps: 747, steps per second: 33, episode reward: 98.279, mean reward: 0.132 [-10.487, 100.000], mean action: 1.770 [0.000, 3.000], mean observation: 0.026 [-0.346, 1.000], loss: 1.424756, mean_absolute_error: 7.467939, mean_q: 5.214973
  894720/1500000: episode: 1490, duration: 12.659s, episode steps: 426, steps per second: 34, episode reward: 199.198, mean reward: 0.468 [-9.052, 100.000], mean action: 1.437 [0.000, 3.000], mean observation: 0.071 [-0.478, 1.000], loss: 1.115495, mean_absolute_error: 7.495311, mean_q: 5.370730
  895681/1500000: episode: 1491, duration: 26.992s, episode steps: 961, steps per second: 36, episode reward: 159.785, mean reward: 0.166 [-22.212, 100.000], mean action: 1.265 [0.000, 3.000], mean observation: 0.151 [-0.427, 1.010], loss: 2.214059, mean_absolute_error: 7.573673, mean_q: 5.538342
  896133/1500000: episode: 1492, duration: 12.728s, episode steps: 452, steps per second: 36, episode reward: 198.680, mean reward: 0.440 [-18.381, 100.000], mean action: 1.511 [0.000, 3.000], mean observation: 0.078 [-0.493, 1.000], loss: 1.125011, mean_absolute_error: 7.762480, mean_q: 6.095477
  896727/1500000: episode: 1493, duration: 16.828s, episode steps: 594, steps per second: 35, episode reward: 179.622, mean reward: 0.302 [-17.247, 100.000], mean action: 2.056 [0.000, 3.000], mean observation: 0.128 [-0.550, 1.000], loss: 1.480685, mean_absolute_error: 7.895154, mean_q: 6.390590
  897273/1500000: episode: 1494, duration: 15.989s, episode steps: 546, steps per second: 34, episode reward: 174.571, mean reward: 0.320 [-17.392, 100.000], mean action: 1.071 [0.000, 3.000], mean observation: 0.117 [-0.321, 1.000], loss: 1.273525, mean_absolute_error: 7.955761, mean_q: 6.677903
  897814/1500000: episode: 1495, duration: 7.410s, episode steps: 541, steps per second: 73, episode reward: 181.736, mean reward: 0.336 [-17.540, 100.000], mean action: 1.553 [0.000, 3.000], mean observation: 0.084 [-1.318, 1.000], loss: 1.785690, mean_absolute_error: 7.982126, mean_q: 6.773863
  898814/1500000: episode: 1496, duration: 12.918s, episode steps: 1000, steps per second: 77, episode reward: 3.465, mean reward: 0.003 [-18.004, 22.980], mean action: 1.467 [0.000, 3.000], mean observation: 0.030 [-0.474, 1.000], loss: 1.879351, mean_absolute_error: 8.081513, mean_q: 7.046856
  899226/1500000: episode: 1497, duration: 5.314s, episode steps: 412, steps per second: 78, episode reward: 144.263, mean reward: 0.350 [-13.252, 100.000], mean action: 2.126 [0.000, 3.000], mean observation: 0.077 [-0.325, 1.000], loss: 2.234905, mean_absolute_error: 8.321583, mean_q: 7.462413
  899919/1500000: episode: 1498, duration: 9.362s, episode steps: 693, steps per second: 74, episode reward: 136.730, mean reward: 0.197 [-13.599, 100.000], mean action: 2.075 [0.000, 3.000], mean observation: 0.113 [-0.311, 1.000], loss: 1.301247, mean_absolute_error: 8.293274, mean_q: 7.641044
  900371/1500000: episode: 1499, duration: 8.479s, episode steps: 452, steps per second: 53, episode reward: 212.643, mean reward: 0.470 [-17.588, 100.000], mean action: 1.487 [0.000, 3.000], mean observation: 0.094 [-0.656, 1.000], loss: 1.321524, mean_absolute_error: 8.451377, mean_q: 7.887787
  901276/1500000: episode: 1500, duration: 15.544s, episode steps: 905, steps per second: 58, episode reward: 108.383, mean reward: 0.120 [-21.364, 100.000], mean action: 1.266 [0.000, 3.000], mean observation: 0.067 [-0.409, 1.000], loss: 1.246767, mean_absolute_error: 8.411853, mean_q: 7.872286
  901840/1500000: episode: 1501, duration: 9.325s, episode steps: 564, steps per second: 60, episode reward: 184.406, mean reward: 0.327 [-15.606, 100.000], mean action: 1.399 [0.000, 3.000], mean observation: 0.062 [-1.021, 1.000], loss: 1.668726, mean_absolute_error: 8.431602, mean_q: 7.977714
  902324/1500000: episode: 1502, duration: 7.567s, episode steps: 484, steps per second: 64, episode reward: 115.818, mean reward: 0.239 [-17.602, 100.000], mean action: 1.876 [0.000, 3.000], mean observation: 0.135 [-0.495, 1.000], loss: 1.770590, mean_absolute_error: 8.595287, mean_q: 8.357078
  903166/1500000: episode: 1503, duration: 14.925s, episode steps: 842, steps per second: 56, episode reward: 124.139, mean reward: 0.147 [-20.667, 100.000], mean action: 2.163 [0.000, 3.000], mean observation: 0.160 [-0.328, 1.000], loss: 1.989554, mean_absolute_error: 8.682867, mean_q: 8.453328
  903791/1500000: episode: 1504, duration: 12.123s, episode steps: 625, steps per second: 52, episode reward: 128.579, mean reward: 0.206 [-11.998, 100.000], mean action: 1.568 [0.000, 3.000], mean observation: 0.111 [-0.348, 1.000], loss: 1.363440, mean_absolute_error: 8.792081, mean_q: 8.718713
  904262/1500000: episode: 1505, duration: 8.729s, episode steps: 471, steps per second: 54, episode reward: 205.973, mean reward: 0.437 [-10.211, 100.000], mean action: 1.425 [0.000, 3.000], mean observation: 0.070 [-0.768, 1.000], loss: 1.262034, mean_absolute_error: 8.959505, mean_q: 8.968024
  904918/1500000: episode: 1506, duration: 10.522s, episode steps: 656, steps per second: 62, episode reward: 185.750, mean reward: 0.283 [-18.027, 100.000], mean action: 1.287 [0.000, 3.000], mean observation: 0.095 [-0.610, 1.000], loss: 1.832818, mean_absolute_error: 9.103242, mean_q: 9.287027
  905449/1500000: episode: 1507, duration: 11.199s, episode steps: 531, steps per second: 47, episode reward: 218.182, mean reward: 0.411 [-19.068, 100.000], mean action: 1.215 [0.000, 3.000], mean observation: 0.151 [-0.753, 1.000], loss: 1.893393, mean_absolute_error: 9.173499, mean_q: 9.383366
  906091/1500000: episode: 1508, duration: 12.599s, episode steps: 642, steps per second: 51, episode reward: 193.057, mean reward: 0.301 [-9.803, 100.000], mean action: 1.419 [0.000, 3.000], mean observation: 0.124 [-0.461, 1.000], loss: 1.873628, mean_absolute_error: 9.363624, mean_q: 9.673790
  906500/1500000: episode: 1509, duration: 5.191s, episode steps: 409, steps per second: 79, episode reward: 187.567, mean reward: 0.459 [-12.805, 100.000], mean action: 1.621 [0.000, 3.000], mean observation: 0.036 [-0.602, 1.000], loss: 0.946686, mean_absolute_error: 9.315076, mean_q: 9.614437
  906994/1500000: episode: 1510, duration: 11.552s, episode steps: 494, steps per second: 43, episode reward: 171.055, mean reward: 0.346 [-13.668, 100.000], mean action: 1.164 [0.000, 3.000], mean observation: 0.066 [-0.612, 1.000], loss: 1.588884, mean_absolute_error: 9.484465, mean_q: 9.734698
  907603/1500000: episode: 1511, duration: 12.136s, episode steps: 609, steps per second: 50, episode reward: 166.606, mean reward: 0.274 [-18.041, 100.000], mean action: 1.320 [0.000, 3.000], mean observation: 0.065 [-0.580, 1.000], loss: 1.518824, mean_absolute_error: 9.428238, mean_q: 9.795647
  907865/1500000: episode: 1512, duration: 4.199s, episode steps: 262, steps per second: 62, episode reward: -25.149, mean reward: -0.096 [-100.000, 18.539], mean action: 1.821 [0.000, 3.000], mean observation: 0.065 [-1.500, 1.000], loss: 2.352917, mean_absolute_error: 9.588775, mean_q: 10.066549
  908245/1500000: episode: 1513, duration: 6.805s, episode steps: 380, steps per second: 56, episode reward: 225.973, mean reward: 0.595 [-3.106, 100.000], mean action: 1.192 [0.000, 3.000], mean observation: 0.112 [-0.515, 1.000], loss: 1.213309, mean_absolute_error: 9.680772, mean_q: 10.354919
  908797/1500000: episode: 1514, duration: 10.388s, episode steps: 552, steps per second: 53, episode reward: 155.620, mean reward: 0.282 [-18.904, 100.000], mean action: 1.391 [0.000, 3.000], mean observation: 0.054 [-0.547, 1.000], loss: 1.539788, mean_absolute_error: 9.706748, mean_q: 10.507537
  909208/1500000: episode: 1515, duration: 9.210s, episode steps: 411, steps per second: 45, episode reward: 198.572, mean reward: 0.483 [-3.906, 100.000], mean action: 1.440 [0.000, 3.000], mean observation: 0.074 [-0.801, 1.000], loss: 2.120394, mean_absolute_error: 9.564878, mean_q: 10.383757
  909574/1500000: episode: 1516, duration: 6.073s, episode steps: 366, steps per second: 60, episode reward: 231.078, mean reward: 0.631 [-10.626, 100.000], mean action: 1.107 [0.000, 3.000], mean observation: 0.124 [-0.639, 1.000], loss: 1.410983, mean_absolute_error: 9.853957, mean_q: 10.834757
  909948/1500000: episode: 1517, duration: 7.012s, episode steps: 374, steps per second: 53, episode reward: 139.943, mean reward: 0.374 [-17.814, 100.000], mean action: 1.471 [0.000, 3.000], mean observation: 0.025 [-0.328, 1.000], loss: 2.202030, mean_absolute_error: 9.720905, mean_q: 10.675791
  910637/1500000: episode: 1518, duration: 13.845s, episode steps: 689, steps per second: 50, episode reward: 182.204, mean reward: 0.264 [-4.337, 100.000], mean action: 1.511 [0.000, 3.000], mean observation: 0.085 [-0.676, 1.000], loss: 1.263731, mean_absolute_error: 9.938351, mean_q: 11.088882
  911127/1500000: episode: 1519, duration: 11.272s, episode steps: 490, steps per second: 43, episode reward: 187.670, mean reward: 0.383 [-11.264, 100.000], mean action: 1.353 [0.000, 3.000], mean observation: 0.075 [-0.599, 1.000], loss: 1.764384, mean_absolute_error: 10.064489, mean_q: 11.310947
  911601/1500000: episode: 1520, duration: 9.390s, episode steps: 474, steps per second: 50, episode reward: 114.958, mean reward: 0.243 [-9.432, 100.000], mean action: 1.496 [0.000, 3.000], mean observation: -0.001 [-0.367, 1.000], loss: 1.229850, mean_absolute_error: 9.926659, mean_q: 11.142313
  912145/1500000: episode: 1521, duration: 9.130s, episode steps: 544, steps per second: 60, episode reward: 160.328, mean reward: 0.295 [-20.648, 100.000], mean action: 1.176 [0.000, 3.000], mean observation: 0.087 [-0.315, 1.000], loss: 2.503723, mean_absolute_error: 10.058120, mean_q: 11.333852
  912700/1500000: episode: 1522, duration: 13.212s, episode steps: 555, steps per second: 42, episode reward: 182.036, mean reward: 0.328 [-10.083, 100.000], mean action: 1.341 [0.000, 3.000], mean observation: 0.108 [-0.320, 1.000], loss: 1.242640, mean_absolute_error: 10.028149, mean_q: 11.386077
  913123/1500000: episode: 1523, duration: 6.320s, episode steps: 423, steps per second: 67, episode reward: 220.926, mean reward: 0.522 [-17.873, 100.000], mean action: 1.421 [0.000, 3.000], mean observation: 0.081 [-1.400, 1.000], loss: 1.762466, mean_absolute_error: 10.114149, mean_q: 11.504383
  913523/1500000: episode: 1524, duration: 10.096s, episode steps: 400, steps per second: 40, episode reward: 165.455, mean reward: 0.414 [-3.730, 100.000], mean action: 1.425 [0.000, 3.000], mean observation: 0.049 [-0.383, 1.000], loss: 2.192929, mean_absolute_error: 10.167660, mean_q: 11.556026
  914190/1500000: episode: 1525, duration: 14.543s, episode steps: 667, steps per second: 46, episode reward: 92.734, mean reward: 0.139 [-18.528, 100.000], mean action: 1.843 [0.000, 3.000], mean observation: 0.038 [-0.442, 1.000], loss: 2.374301, mean_absolute_error: 10.309401, mean_q: 11.772831
  914611/1500000: episode: 1526, duration: 6.666s, episode steps: 421, steps per second: 63, episode reward: 148.108, mean reward: 0.352 [-13.433, 100.000], mean action: 1.606 [0.000, 3.000], mean observation: 0.018 [-0.453, 1.000], loss: 1.650574, mean_absolute_error: 10.452229, mean_q: 12.008573
  914923/1500000: episode: 1527, duration: 7.036s, episode steps: 312, steps per second: 44, episode reward: 180.376, mean reward: 0.578 [-19.559, 100.000], mean action: 1.179 [0.000, 3.000], mean observation: 0.054 [-0.404, 1.000], loss: 2.186951, mean_absolute_error: 10.491270, mean_q: 12.012383
  915466/1500000: episode: 1528, duration: 10.285s, episode steps: 543, steps per second: 53, episode reward: 139.610, mean reward: 0.257 [-18.894, 100.000], mean action: 1.884 [0.000, 3.000], mean observation: 0.104 [-0.313, 1.000], loss: 1.894420, mean_absolute_error: 10.527858, mean_q: 12.146559
  916466/1500000: episode: 1529, duration: 21.482s, episode steps: 1000, steps per second: 47, episode reward: 5.890, mean reward: 0.006 [-17.414, 12.928], mean action: 2.246 [0.000, 3.000], mean observation: 0.160 [-0.339, 1.000], loss: 2.199784, mean_absolute_error: 10.599527, mean_q: 12.257077
  917105/1500000: episode: 1530, duration: 13.281s, episode steps: 639, steps per second: 48, episode reward: 191.100, mean reward: 0.299 [-23.827, 100.000], mean action: 1.203 [0.000, 3.000], mean observation: 0.131 [-0.487, 1.000], loss: 1.827567, mean_absolute_error: 10.766361, mean_q: 12.484054
  917549/1500000: episode: 1531, duration: 10.617s, episode steps: 444, steps per second: 42, episode reward: 213.356, mean reward: 0.481 [-11.497, 100.000], mean action: 1.133 [0.000, 3.000], mean observation: 0.123 [-0.530, 1.000], loss: 2.094125, mean_absolute_error: 10.802298, mean_q: 12.506419
  918336/1500000: episode: 1532, duration: 18.465s, episode steps: 787, steps per second: 43, episode reward: 137.169, mean reward: 0.174 [-18.233, 100.000], mean action: 1.003 [0.000, 3.000], mean observation: 0.123 [-0.690, 1.000], loss: 1.574874, mean_absolute_error: 10.734112, mean_q: 12.449184
  918966/1500000: episode: 1533, duration: 12.014s, episode steps: 630, steps per second: 52, episode reward: 136.215, mean reward: 0.216 [-13.038, 100.000], mean action: 2.113 [0.000, 3.000], mean observation: 0.085 [-0.823, 1.019], loss: 1.866308, mean_absolute_error: 11.024077, mean_q: 12.900115
  919549/1500000: episode: 1534, duration: 12.543s, episode steps: 583, steps per second: 46, episode reward: 155.714, mean reward: 0.267 [-19.426, 100.000], mean action: 1.244 [0.000, 3.000], mean observation: 0.078 [-0.443, 1.000], loss: 1.962891, mean_absolute_error: 11.263429, mean_q: 13.328168
  919974/1500000: episode: 1535, duration: 9.185s, episode steps: 425, steps per second: 46, episode reward: 196.837, mean reward: 0.463 [-17.393, 100.000], mean action: 1.235 [0.000, 3.000], mean observation: 0.099 [-0.449, 1.000], loss: 1.611659, mean_absolute_error: 11.294531, mean_q: 13.467352
  920527/1500000: episode: 1536, duration: 10.543s, episode steps: 553, steps per second: 52, episode reward: 186.535, mean reward: 0.337 [-8.145, 100.000], mean action: 1.297 [0.000, 3.000], mean observation: 0.089 [-0.803, 1.000], loss: 1.871441, mean_absolute_error: 11.401409, mean_q: 13.719513
  921033/1500000: episode: 1537, duration: 12.319s, episode steps: 506, steps per second: 41, episode reward: 191.390, mean reward: 0.378 [-3.006, 100.000], mean action: 1.235 [0.000, 3.000], mean observation: 0.125 [-0.402, 1.000], loss: 2.331923, mean_absolute_error: 11.287973, mean_q: 13.618402
  921673/1500000: episode: 1538, duration: 11.480s, episode steps: 640, steps per second: 56, episode reward: 178.218, mean reward: 0.278 [-12.446, 100.000], mean action: 1.122 [0.000, 3.000], mean observation: 0.070 [-0.459, 1.000], loss: 1.768762, mean_absolute_error: 11.392946, mean_q: 13.750262
  922274/1500000: episode: 1539, duration: 13.125s, episode steps: 601, steps per second: 46, episode reward: 166.845, mean reward: 0.278 [-17.888, 100.000], mean action: 1.704 [0.000, 3.000], mean observation: 0.082 [-0.572, 1.000], loss: 2.845208, mean_absolute_error: 11.434663, mean_q: 13.826848
  922767/1500000: episode: 1540, duration: 11.409s, episode steps: 493, steps per second: 43, episode reward: 172.525, mean reward: 0.350 [-17.762, 100.000], mean action: 1.047 [0.000, 3.000], mean observation: 0.092 [-0.406, 1.000], loss: 3.707335, mean_absolute_error: 11.793695, mean_q: 14.440730
  923169/1500000: episode: 1541, duration: 7.499s, episode steps: 402, steps per second: 54, episode reward: 156.164, mean reward: 0.388 [-13.996, 100.000], mean action: 1.410 [0.000, 3.000], mean observation: 0.061 [-0.369, 1.000], loss: 1.866774, mean_absolute_error: 12.058394, mean_q: 14.914798
  923570/1500000: episode: 1542, duration: 8.608s, episode steps: 401, steps per second: 47, episode reward: 146.285, mean reward: 0.365 [-9.888, 100.000], mean action: 1.536 [0.000, 3.000], mean observation: 0.045 [-0.347, 1.000], loss: 1.638589, mean_absolute_error: 11.957004, mean_q: 14.832879
  923960/1500000: episode: 1543, duration: 8.885s, episode steps: 390, steps per second: 44, episode reward: 180.577, mean reward: 0.463 [-13.248, 100.000], mean action: 1.592 [0.000, 3.000], mean observation: 0.050 [-0.418, 1.000], loss: 1.666506, mean_absolute_error: 12.040500, mean_q: 14.948014
  924485/1500000: episode: 1544, duration: 10.923s, episode steps: 525, steps per second: 48, episode reward: -118.786, mean reward: -0.226 [-100.000, 17.153], mean action: 1.806 [0.000, 3.000], mean observation: 0.016 [-0.349, 1.000], loss: 2.296237, mean_absolute_error: 12.090765, mean_q: 15.056428
  925048/1500000: episode: 1545, duration: 16.857s, episode steps: 563, steps per second: 33, episode reward: 176.521, mean reward: 0.314 [-18.700, 100.000], mean action: 1.169 [0.000, 3.000], mean observation: 0.119 [-0.308, 1.000], loss: 2.099646, mean_absolute_error: 12.206502, mean_q: 15.146783
  925471/1500000: episode: 1546, duration: 11.927s, episode steps: 423, steps per second: 35, episode reward: 171.007, mean reward: 0.404 [-17.413, 100.000], mean action: 1.520 [0.000, 3.000], mean observation: 0.068 [-0.405, 1.000], loss: 1.669098, mean_absolute_error: 12.178581, mean_q: 15.132860
  926430/1500000: episode: 1547, duration: 27.033s, episode steps: 959, steps per second: 35, episode reward: 137.415, mean reward: 0.143 [-19.323, 100.000], mean action: 1.199 [0.000, 3.000], mean observation: 0.110 [-0.433, 1.027], loss: 1.910581, mean_absolute_error: 12.264974, mean_q: 15.261886
  926724/1500000: episode: 1548, duration: 8.145s, episode steps: 294, steps per second: 36, episode reward: 214.991, mean reward: 0.731 [-2.781, 100.000], mean action: 1.673 [0.000, 3.000], mean observation: 0.075 [-0.511, 1.000], loss: 1.806548, mean_absolute_error: 12.505554, mean_q: 15.588789
  927408/1500000: episode: 1549, duration: 18.453s, episode steps: 684, steps per second: 37, episode reward: 214.900, mean reward: 0.314 [-18.207, 100.000], mean action: 1.085 [0.000, 3.000], mean observation: 0.152 [-0.548, 1.032], loss: 1.748251, mean_absolute_error: 12.503164, mean_q: 15.664896
  927820/1500000: episode: 1550, duration: 11.205s, episode steps: 412, steps per second: 37, episode reward: 227.654, mean reward: 0.553 [-18.565, 100.000], mean action: 1.490 [0.000, 3.000], mean observation: 0.113 [-0.500, 1.000], loss: 2.337780, mean_absolute_error: 12.628032, mean_q: 15.821391
  928450/1500000: episode: 1551, duration: 17.665s, episode steps: 630, steps per second: 36, episode reward: 175.373, mean reward: 0.278 [-17.406, 100.000], mean action: 1.157 [0.000, 3.000], mean observation: 0.098 [-0.610, 1.000], loss: 1.710948, mean_absolute_error: 12.804189, mean_q: 16.122375
  928879/1500000: episode: 1552, duration: 11.518s, episode steps: 429, steps per second: 37, episode reward: 204.338, mean reward: 0.476 [-17.385, 100.000], mean action: 1.256 [0.000, 3.000], mean observation: 0.119 [-0.752, 1.000], loss: 1.894741, mean_absolute_error: 12.834616, mean_q: 16.100664
  929269/1500000: episode: 1553, duration: 10.217s, episode steps: 390, steps per second: 38, episode reward: 221.218, mean reward: 0.567 [-17.607, 100.000], mean action: 1.141 [0.000, 3.000], mean observation: 0.116 [-0.970, 1.000], loss: 3.019025, mean_absolute_error: 12.873530, mean_q: 16.225821
  929681/1500000: episode: 1554, duration: 11.334s, episode steps: 412, steps per second: 36, episode reward: 224.682, mean reward: 0.545 [-18.026, 100.000], mean action: 0.951 [0.000, 3.000], mean observation: 0.140 [-0.832, 1.000], loss: 1.798617, mean_absolute_error: 12.862680, mean_q: 16.149979
  930045/1500000: episode: 1555, duration: 10.105s, episode steps: 364, steps per second: 36, episode reward: 199.550, mean reward: 0.548 [-8.618, 100.000], mean action: 1.190 [0.000, 3.000], mean observation: 0.086 [-0.503, 1.000], loss: 1.973413, mean_absolute_error: 13.251144, mean_q: 16.779263
  930627/1500000: episode: 1556, duration: 15.969s, episode steps: 582, steps per second: 36, episode reward: 211.454, mean reward: 0.363 [-13.019, 100.000], mean action: 1.009 [0.000, 3.000], mean observation: 0.136 [-0.501, 1.011], loss: 2.273989, mean_absolute_error: 13.280190, mean_q: 16.732302
  930896/1500000: episode: 1557, duration: 6.984s, episode steps: 269, steps per second: 39, episode reward: 216.191, mean reward: 0.804 [-3.172, 100.000], mean action: 1.576 [0.000, 3.000], mean observation: 0.052 [-0.507, 1.000], loss: 2.085419, mean_absolute_error: 13.192515, mean_q: 16.756763
  931229/1500000: episode: 1558, duration: 9.112s, episode steps: 333, steps per second: 37, episode reward: 240.062, mean reward: 0.721 [-11.212, 100.000], mean action: 1.249 [0.000, 3.000], mean observation: 0.112 [-0.737, 1.000], loss: 2.729736, mean_absolute_error: 13.284439, mean_q: 16.794209
  931655/1500000: episode: 1559, duration: 12.152s, episode steps: 426, steps per second: 35, episode reward: 103.423, mean reward: 0.243 [-13.885, 100.000], mean action: 2.019 [0.000, 3.000], mean observation: 0.073 [-0.321, 1.000], loss: 1.514780, mean_absolute_error: 13.414010, mean_q: 16.980230
  932371/1500000: episode: 1560, duration: 19.627s, episode steps: 716, steps per second: 36, episode reward: 200.759, mean reward: 0.280 [-19.508, 100.000], mean action: 0.919 [0.000, 3.000], mean observation: 0.182 [-0.738, 1.000], loss: 2.345068, mean_absolute_error: 13.398164, mean_q: 17.021801
  932706/1500000: episode: 1561, duration: 9.233s, episode steps: 335, steps per second: 36, episode reward: 208.614, mean reward: 0.623 [-19.291, 100.000], mean action: 1.358 [0.000, 3.000], mean observation: 0.077 [-0.668, 1.000], loss: 2.456998, mean_absolute_error: 13.777979, mean_q: 17.570251
  933102/1500000: episode: 1562, duration: 10.853s, episode steps: 396, steps per second: 36, episode reward: 211.004, mean reward: 0.533 [-19.068, 100.000], mean action: 1.306 [0.000, 3.000], mean observation: 0.044 [-0.600, 1.000], loss: 2.641923, mean_absolute_error: 13.607522, mean_q: 17.362400
  933532/1500000: episode: 1563, duration: 12.090s, episode steps: 430, steps per second: 36, episode reward: 227.658, mean reward: 0.529 [-17.540, 100.000], mean action: 1.333 [0.000, 3.000], mean observation: 0.084 [-0.916, 1.000], loss: 2.048741, mean_absolute_error: 13.983081, mean_q: 17.916719
  934532/1500000: episode: 1564, duration: 32.830s, episode steps: 1000, steps per second: 30, episode reward: 37.144, mean reward: 0.037 [-20.017, 14.190], mean action: 1.796 [0.000, 3.000], mean observation: 0.179 [-0.398, 1.000], loss: 2.593033, mean_absolute_error: 14.054812, mean_q: 18.080891
  934906/1500000: episode: 1565, duration: 10.806s, episode steps: 374, steps per second: 35, episode reward: 239.510, mean reward: 0.640 [-2.805, 100.000], mean action: 1.225 [0.000, 3.000], mean observation: 0.117 [-0.563, 1.000], loss: 1.758567, mean_absolute_error: 14.303202, mean_q: 18.359938
  935388/1500000: episode: 1566, duration: 15.338s, episode steps: 482, steps per second: 31, episode reward: 156.957, mean reward: 0.326 [-17.373, 100.000], mean action: 1.411 [0.000, 3.000], mean observation: 0.072 [-0.439, 1.000], loss: 1.206398, mean_absolute_error: 14.317428, mean_q: 18.525297
  935686/1500000: episode: 1567, duration: 7.984s, episode steps: 298, steps per second: 37, episode reward: 178.623, mean reward: 0.599 [-2.977, 100.000], mean action: 1.664 [0.000, 3.000], mean observation: 0.060 [-0.437, 1.000], loss: 2.539075, mean_absolute_error: 14.401728, mean_q: 18.686531
  936021/1500000: episode: 1568, duration: 9.215s, episode steps: 335, steps per second: 36, episode reward: 195.187, mean reward: 0.583 [-9.703, 100.000], mean action: 1.582 [0.000, 3.000], mean observation: 0.042 [-0.501, 1.000], loss: 1.806384, mean_absolute_error: 14.536310, mean_q: 18.809809
  936320/1500000: episode: 1569, duration: 8.072s, episode steps: 299, steps per second: 37, episode reward: 217.392, mean reward: 0.727 [-10.557, 100.000], mean action: 1.425 [0.000, 3.000], mean observation: 0.075 [-0.665, 1.000], loss: 3.463063, mean_absolute_error: 14.719070, mean_q: 19.125582
  936705/1500000: episode: 1570, duration: 10.624s, episode steps: 385, steps per second: 36, episode reward: 151.361, mean reward: 0.393 [-15.319, 100.000], mean action: 1.592 [0.000, 3.000], mean observation: 0.113 [-0.977, 1.000], loss: 1.265310, mean_absolute_error: 14.484103, mean_q: 18.778172
  937081/1500000: episode: 1571, duration: 10.766s, episode steps: 376, steps per second: 35, episode reward: 229.080, mean reward: 0.609 [-10.415, 100.000], mean action: 1.545 [0.000, 3.000], mean observation: 0.086 [-0.579, 1.000], loss: 2.754308, mean_absolute_error: 14.770425, mean_q: 19.210899
  937758/1500000: episode: 1572, duration: 19.966s, episode steps: 677, steps per second: 34, episode reward: 129.081, mean reward: 0.191 [-18.847, 100.000], mean action: 2.121 [0.000, 3.000], mean observation: 0.139 [-0.400, 1.000], loss: 1.575386, mean_absolute_error: 14.655536, mean_q: 19.017572
  938209/1500000: episode: 1573, duration: 12.664s, episode steps: 451, steps per second: 36, episode reward: 205.099, mean reward: 0.455 [-9.370, 100.000], mean action: 1.468 [0.000, 3.000], mean observation: 0.073 [-0.435, 1.029], loss: 1.939955, mean_absolute_error: 14.809001, mean_q: 19.270935
  938628/1500000: episode: 1574, duration: 11.800s, episode steps: 419, steps per second: 36, episode reward: 197.057, mean reward: 0.470 [-22.437, 100.000], mean action: 1.411 [0.000, 3.000], mean observation: 0.090 [-0.422, 1.000], loss: 1.872290, mean_absolute_error: 14.716163, mean_q: 19.134640
  939383/1500000: episode: 1575, duration: 21.167s, episode steps: 755, steps per second: 36, episode reward: 164.865, mean reward: 0.218 [-19.247, 100.000], mean action: 1.241 [0.000, 3.000], mean observation: 0.144 [-0.427, 1.000], loss: 1.573842, mean_absolute_error: 14.946866, mean_q: 19.481276
  939778/1500000: episode: 1576, duration: 10.864s, episode steps: 395, steps per second: 36, episode reward: 185.389, mean reward: 0.469 [-11.184, 100.000], mean action: 1.253 [0.000, 3.000], mean observation: 0.036 [-0.704, 1.000], loss: 2.136674, mean_absolute_error: 14.876792, mean_q: 19.435947
  940147/1500000: episode: 1577, duration: 10.275s, episode steps: 369, steps per second: 36, episode reward: 187.029, mean reward: 0.507 [-10.632, 100.000], mean action: 1.556 [0.000, 3.000], mean observation: 0.023 [-0.486, 1.000], loss: 1.276677, mean_absolute_error: 14.960612, mean_q: 19.569147
  940779/1500000: episode: 1578, duration: 18.058s, episode steps: 632, steps per second: 35, episode reward: 190.504, mean reward: 0.301 [-18.599, 100.000], mean action: 0.930 [0.000, 3.000], mean observation: 0.163 [-0.725, 1.006], loss: 1.926933, mean_absolute_error: 15.000832, mean_q: 19.613066
  941100/1500000: episode: 1579, duration: 8.759s, episode steps: 321, steps per second: 37, episode reward: 205.943, mean reward: 0.642 [-2.926, 100.000], mean action: 1.218 [0.000, 3.000], mean observation: 0.068 [-0.412, 1.000], loss: 2.459857, mean_absolute_error: 15.203691, mean_q: 19.975582
  941419/1500000: episode: 1580, duration: 8.995s, episode steps: 319, steps per second: 35, episode reward: 202.163, mean reward: 0.634 [-8.428, 100.000], mean action: 1.596 [0.000, 3.000], mean observation: 0.070 [-0.470, 1.000], loss: 1.866287, mean_absolute_error: 15.190718, mean_q: 19.941021
  941730/1500000: episode: 1581, duration: 8.538s, episode steps: 311, steps per second: 36, episode reward: 206.075, mean reward: 0.663 [-9.324, 100.000], mean action: 1.495 [0.000, 3.000], mean observation: 0.072 [-0.723, 1.000], loss: 1.675328, mean_absolute_error: 15.265463, mean_q: 20.081385
  942201/1500000: episode: 1582, duration: 13.267s, episode steps: 471, steps per second: 36, episode reward: 185.328, mean reward: 0.393 [-20.242, 100.000], mean action: 0.994 [0.000, 3.000], mean observation: 0.144 [-0.420, 1.000], loss: 2.352524, mean_absolute_error: 15.471790, mean_q: 20.344831
  942333/1500000: episode: 1583, duration: 3.478s, episode steps: 132, steps per second: 38, episode reward: -22.213, mean reward: -0.168 [-100.000, 11.669], mean action: 1.788 [0.000, 3.000], mean observation: 0.093 [-1.638, 1.000], loss: 2.390295, mean_absolute_error: 15.677583, mean_q: 20.643402
  942819/1500000: episode: 1584, duration: 13.887s, episode steps: 486, steps per second: 35, episode reward: 112.936, mean reward: 0.232 [-17.617, 100.000], mean action: 1.969 [0.000, 3.000], mean observation: 0.052 [-0.403, 1.000], loss: 2.194865, mean_absolute_error: 15.418911, mean_q: 20.296661
  943291/1500000: episode: 1585, duration: 12.774s, episode steps: 472, steps per second: 37, episode reward: 220.099, mean reward: 0.466 [-10.271, 100.000], mean action: 1.146 [0.000, 3.000], mean observation: 0.132 [-0.732, 1.025], loss: 2.375625, mean_absolute_error: 15.614763, mean_q: 20.585951
  943636/1500000: episode: 1586, duration: 9.340s, episode steps: 345, steps per second: 37, episode reward: 206.668, mean reward: 0.599 [-18.954, 100.000], mean action: 1.522 [0.000, 3.000], mean observation: 0.022 [-0.703, 1.000], loss: 2.893907, mean_absolute_error: 15.523218, mean_q: 20.453831
  944021/1500000: episode: 1587, duration: 10.372s, episode steps: 385, steps per second: 37, episode reward: 227.973, mean reward: 0.592 [-11.984, 100.000], mean action: 1.130 [0.000, 3.000], mean observation: 0.086 [-0.633, 1.021], loss: 2.886707, mean_absolute_error: 15.813411, mean_q: 20.889534
  944193/1500000: episode: 1588, duration: 4.393s, episode steps: 172, steps per second: 39, episode reward: -97.306, mean reward: -0.566 [-100.000, 3.630], mean action: 1.523 [0.000, 3.000], mean observation: 0.171 [-0.875, 1.001], loss: 2.361697, mean_absolute_error: 15.671665, mean_q: 20.693596
  944570/1500000: episode: 1589, duration: 10.134s, episode steps: 377, steps per second: 37, episode reward: 210.668, mean reward: 0.559 [-10.318, 100.000], mean action: 1.350 [0.000, 3.000], mean observation: 0.127 [-1.007, 1.014], loss: 1.004312, mean_absolute_error: 15.798691, mean_q: 20.923321
  944901/1500000: episode: 1590, duration: 9.243s, episode steps: 331, steps per second: 36, episode reward: 184.052, mean reward: 0.556 [-2.971, 100.000], mean action: 1.218 [0.000, 3.000], mean observation: 0.083 [-0.401, 1.000], loss: 2.384731, mean_absolute_error: 15.810796, mean_q: 20.892838
  945262/1500000: episode: 1591, duration: 9.926s, episode steps: 361, steps per second: 36, episode reward: 189.590, mean reward: 0.525 [-9.778, 100.000], mean action: 1.418 [0.000, 3.000], mean observation: 0.090 [-0.780, 1.000], loss: 1.941143, mean_absolute_error: 15.911968, mean_q: 21.051895
  945583/1500000: episode: 1592, duration: 8.809s, episode steps: 321, steps per second: 36, episode reward: 197.850, mean reward: 0.616 [-12.815, 100.000], mean action: 1.299 [0.000, 3.000], mean observation: 0.094 [-0.847, 1.000], loss: 1.970640, mean_absolute_error: 15.824474, mean_q: 20.932632
  945944/1500000: episode: 1593, duration: 9.992s, episode steps: 361, steps per second: 36, episode reward: 212.564, mean reward: 0.589 [-18.151, 100.000], mean action: 1.086 [0.000, 3.000], mean observation: 0.114 [-0.680, 1.000], loss: 2.829704, mean_absolute_error: 15.721498, mean_q: 20.779039
  946420/1500000: episode: 1594, duration: 13.164s, episode steps: 476, steps per second: 36, episode reward: 201.867, mean reward: 0.424 [-11.117, 100.000], mean action: 1.006 [0.000, 3.000], mean observation: 0.123 [-0.315, 1.000], loss: 2.452599, mean_absolute_error: 15.766183, mean_q: 20.829784
  946969/1500000: episode: 1595, duration: 14.833s, episode steps: 549, steps per second: 37, episode reward: 155.759, mean reward: 0.284 [-17.389, 100.000], mean action: 1.821 [0.000, 3.000], mean observation: 0.081 [-0.473, 1.000], loss: 2.026235, mean_absolute_error: 15.808676, mean_q: 20.919287
  947315/1500000: episode: 1596, duration: 9.692s, episode steps: 346, steps per second: 36, episode reward: 186.563, mean reward: 0.539 [-19.241, 100.000], mean action: 1.379 [0.000, 3.000], mean observation: 0.106 [-0.397, 1.000], loss: 2.786746, mean_absolute_error: 16.095661, mean_q: 21.328596
  947661/1500000: episode: 1597, duration: 9.440s, episode steps: 346, steps per second: 37, episode reward: 237.662, mean reward: 0.687 [-17.572, 100.000], mean action: 1.393 [0.000, 3.000], mean observation: 0.077 [-0.743, 1.000], loss: 1.028077, mean_absolute_error: 15.866175, mean_q: 21.002508
  948097/1500000: episode: 1598, duration: 12.497s, episode steps: 436, steps per second: 35, episode reward: 171.854, mean reward: 0.394 [-11.893, 100.000], mean action: 1.126 [0.000, 3.000], mean observation: 0.126 [-0.637, 1.000], loss: 2.490869, mean_absolute_error: 15.947506, mean_q: 21.110497
  948698/1500000: episode: 1599, duration: 18.995s, episode steps: 601, steps per second: 32, episode reward: 161.852, mean reward: 0.269 [-17.235, 100.000], mean action: 1.764 [0.000, 3.000], mean observation: 0.107 [-0.498, 1.006], loss: 1.899198, mean_absolute_error: 15.957200, mean_q: 21.100235
  949379/1500000: episode: 1600, duration: 19.683s, episode steps: 681, steps per second: 35, episode reward: 205.740, mean reward: 0.302 [-18.606, 100.000], mean action: 0.988 [0.000, 3.000], mean observation: 0.163 [-0.715, 1.000], loss: 2.181211, mean_absolute_error: 15.807031, mean_q: 20.901186
  949646/1500000: episode: 1601, duration: 6.978s, episode steps: 267, steps per second: 38, episode reward: 238.343, mean reward: 0.893 [-10.438, 100.000], mean action: 1.412 [0.000, 3.000], mean observation: 0.053 [-1.436, 1.000], loss: 1.965602, mean_absolute_error: 15.567249, mean_q: 20.521366
  950032/1500000: episode: 1602, duration: 10.442s, episode steps: 386, steps per second: 37, episode reward: 193.759, mean reward: 0.502 [-10.547, 100.000], mean action: 1.373 [0.000, 3.000], mean observation: 0.076 [-0.482, 1.000], loss: 1.838442, mean_absolute_error: 15.741305, mean_q: 20.821194
  950365/1500000: episode: 1603, duration: 9.107s, episode steps: 333, steps per second: 37, episode reward: 239.113, mean reward: 0.718 [-17.935, 100.000], mean action: 1.282 [0.000, 3.000], mean observation: 0.088 [-0.692, 1.000], loss: 2.583809, mean_absolute_error: 15.858502, mean_q: 20.959393
  950840/1500000: episode: 1604, duration: 13.695s, episode steps: 475, steps per second: 35, episode reward: 206.019, mean reward: 0.434 [-18.409, 100.000], mean action: 1.091 [0.000, 3.000], mean observation: 0.100 [-0.546, 1.000], loss: 2.357984, mean_absolute_error: 15.846898, mean_q: 20.930618
  951255/1500000: episode: 1605, duration: 11.882s, episode steps: 415, steps per second: 35, episode reward: 159.014, mean reward: 0.383 [-20.404, 100.000], mean action: 1.270 [0.000, 3.000], mean observation: 0.038 [-0.636, 1.000], loss: 2.479930, mean_absolute_error: 15.690079, mean_q: 20.736610
  951665/1500000: episode: 1606, duration: 11.755s, episode steps: 410, steps per second: 35, episode reward: 181.648, mean reward: 0.443 [-15.578, 100.000], mean action: 1.290 [0.000, 3.000], mean observation: 0.071 [-0.920, 1.000], loss: 2.006507, mean_absolute_error: 15.790632, mean_q: 20.867922
  952014/1500000: episode: 1607, duration: 9.600s, episode steps: 349, steps per second: 36, episode reward: 168.157, mean reward: 0.482 [-3.295, 100.000], mean action: 1.476 [0.000, 3.000], mean observation: 0.083 [-0.608, 1.000], loss: 2.930099, mean_absolute_error: 15.933896, mean_q: 21.024580
  952340/1500000: episode: 1608, duration: 8.933s, episode steps: 326, steps per second: 36, episode reward: 199.146, mean reward: 0.611 [-10.698, 100.000], mean action: 1.479 [0.000, 3.000], mean observation: 0.103 [-0.457, 1.000], loss: 2.816660, mean_absolute_error: 15.791772, mean_q: 20.882402
  952893/1500000: episode: 1609, duration: 16.077s, episode steps: 553, steps per second: 34, episode reward: 189.145, mean reward: 0.342 [-18.231, 100.000], mean action: 0.913 [0.000, 3.000], mean observation: 0.106 [-0.656, 1.000], loss: 1.890748, mean_absolute_error: 15.518484, mean_q: 20.509659
  953325/1500000: episode: 1610, duration: 12.178s, episode steps: 432, steps per second: 35, episode reward: 169.565, mean reward: 0.393 [-14.484, 100.000], mean action: 1.213 [0.000, 3.000], mean observation: 0.044 [-0.635, 1.000], loss: 3.542652, mean_absolute_error: 15.717093, mean_q: 20.751429
  953655/1500000: episode: 1611, duration: 8.822s, episode steps: 330, steps per second: 37, episode reward: 186.393, mean reward: 0.565 [-10.972, 100.000], mean action: 1.282 [0.000, 3.000], mean observation: 0.042 [-0.492, 1.000], loss: 2.487513, mean_absolute_error: 15.920481, mean_q: 21.069429
  954104/1500000: episode: 1612, duration: 13.493s, episode steps: 449, steps per second: 33, episode reward: 225.012, mean reward: 0.501 [-19.509, 100.000], mean action: 0.955 [0.000, 3.000], mean observation: 0.132 [-0.750, 1.000], loss: 1.795743, mean_absolute_error: 15.823543, mean_q: 20.929104
  954432/1500000: episode: 1613, duration: 9.623s, episode steps: 328, steps per second: 34, episode reward: 188.268, mean reward: 0.574 [-18.580, 100.000], mean action: 1.247 [0.000, 3.000], mean observation: 0.069 [-0.343, 1.000], loss: 2.990980, mean_absolute_error: 16.176914, mean_q: 21.435259
  955216/1500000: episode: 1614, duration: 22.041s, episode steps: 784, steps per second: 36, episode reward: 200.223, mean reward: 0.255 [-20.441, 100.000], mean action: 0.959 [0.000, 3.000], mean observation: 0.179 [-0.760, 1.000], loss: 2.490317, mean_absolute_error: 16.237234, mean_q: 21.533772
  955602/1500000: episode: 1615, duration: 10.815s, episode steps: 386, steps per second: 36, episode reward: 176.618, mean reward: 0.458 [-13.130, 100.000], mean action: 2.313 [0.000, 3.000], mean observation: 0.180 [-0.760, 1.000], loss: 1.599369, mean_absolute_error: 16.587563, mean_q: 22.020363
  956028/1500000: episode: 1616, duration: 11.885s, episode steps: 426, steps per second: 36, episode reward: 191.423, mean reward: 0.449 [-17.821, 100.000], mean action: 1.249 [0.000, 3.000], mean observation: 0.063 [-0.515, 1.000], loss: 1.881557, mean_absolute_error: 16.394274, mean_q: 21.779072
  956384/1500000: episode: 1617, duration: 9.947s, episode steps: 356, steps per second: 36, episode reward: 176.988, mean reward: 0.497 [-2.866, 100.000], mean action: 1.087 [0.000, 3.000], mean observation: 0.075 [-0.718, 1.000], loss: 2.161330, mean_absolute_error: 16.573904, mean_q: 22.038187
  957384/1500000: episode: 1618, duration: 28.525s, episode steps: 1000, steps per second: 35, episode reward: 89.543, mean reward: 0.090 [-18.761, 24.776], mean action: 0.867 [0.000, 3.000], mean observation: 0.199 [-0.768, 1.000], loss: 2.646262, mean_absolute_error: 16.490734, mean_q: 21.947367
  957747/1500000: episode: 1619, duration: 9.904s, episode steps: 363, steps per second: 37, episode reward: 230.874, mean reward: 0.636 [-17.836, 100.000], mean action: 1.072 [0.000, 3.000], mean observation: 0.100 [-0.614, 1.000], loss: 3.216079, mean_absolute_error: 16.622576, mean_q: 22.143345
  958056/1500000: episode: 1620, duration: 8.403s, episode steps: 309, steps per second: 37, episode reward: 261.723, mean reward: 0.847 [-17.948, 100.000], mean action: 0.948 [0.000, 3.000], mean observation: 0.102 [-0.766, 1.000], loss: 1.953981, mean_absolute_error: 16.809811, mean_q: 22.391644
  958559/1500000: episode: 1621, duration: 14.250s, episode steps: 503, steps per second: 35, episode reward: 170.279, mean reward: 0.339 [-18.819, 100.000], mean action: 1.489 [0.000, 3.000], mean observation: 0.131 [-0.443, 1.000], loss: 2.266397, mean_absolute_error: 16.927628, mean_q: 22.557076
  958865/1500000: episode: 1622, duration: 8.249s, episode steps: 306, steps per second: 37, episode reward: 180.666, mean reward: 0.590 [-8.963, 100.000], mean action: 1.458 [0.000, 3.000], mean observation: 0.026 [-0.384, 1.000], loss: 1.947274, mean_absolute_error: 16.784319, mean_q: 22.361446
  959298/1500000: episode: 1623, duration: 12.196s, episode steps: 433, steps per second: 36, episode reward: 181.479, mean reward: 0.419 [-17.501, 100.000], mean action: 1.044 [0.000, 3.000], mean observation: 0.081 [-0.372, 1.000], loss: 3.383780, mean_absolute_error: 16.821138, mean_q: 22.390192
  959670/1500000: episode: 1624, duration: 10.447s, episode steps: 372, steps per second: 36, episode reward: 169.731, mean reward: 0.456 [-14.532, 100.000], mean action: 1.578 [0.000, 3.000], mean observation: 0.099 [-0.834, 1.000], loss: 2.993710, mean_absolute_error: 17.033184, mean_q: 22.682821
  960377/1500000: episode: 1625, duration: 21.456s, episode steps: 707, steps per second: 33, episode reward: 139.003, mean reward: 0.197 [-17.439, 100.000], mean action: 1.537 [0.000, 3.000], mean observation: 0.113 [-0.584, 1.000], loss: 1.706848, mean_absolute_error: 17.109524, mean_q: 22.792759
  960850/1500000: episode: 1626, duration: 12.819s, episode steps: 473, steps per second: 37, episode reward: 148.091, mean reward: 0.313 [-12.140, 100.000], mean action: 1.922 [0.000, 3.000], mean observation: 0.136 [-0.686, 1.000], loss: 3.375564, mean_absolute_error: 17.048454, mean_q: 22.686409
  961850/1500000: episode: 1627, duration: 28.733s, episode steps: 1000, steps per second: 35, episode reward: 58.755, mean reward: 0.059 [-17.423, 13.185], mean action: 2.478 [0.000, 3.000], mean observation: 0.220 [-0.700, 1.000], loss: 2.155128, mean_absolute_error: 17.380045, mean_q: 23.167171
  962173/1500000: episode: 1628, duration: 8.955s, episode steps: 323, steps per second: 36, episode reward: 176.725, mean reward: 0.547 [-17.844, 100.000], mean action: 1.641 [0.000, 3.000], mean observation: 0.102 [-0.600, 1.000], loss: 2.384748, mean_absolute_error: 17.165277, mean_q: 22.876612
  962552/1500000: episode: 1629, duration: 10.537s, episode steps: 379, steps per second: 36, episode reward: 181.979, mean reward: 0.480 [-18.594, 100.000], mean action: 1.470 [0.000, 3.000], mean observation: 0.114 [-0.594, 1.000], loss: 2.932206, mean_absolute_error: 17.406939, mean_q: 23.206009
  962837/1500000: episode: 1630, duration: 8.487s, episode steps: 285, steps per second: 34, episode reward: 221.368, mean reward: 0.777 [-8.177, 100.000], mean action: 1.674 [0.000, 3.000], mean observation: 0.040 [-0.559, 1.000], loss: 2.047117, mean_absolute_error: 17.369184, mean_q: 23.143227
  963114/1500000: episode: 1631, duration: 7.742s, episode steps: 277, steps per second: 36, episode reward: 191.756, mean reward: 0.692 [-2.914, 100.000], mean action: 1.213 [0.000, 3.000], mean observation: 0.088 [-0.416, 1.000], loss: 3.120729, mean_absolute_error: 17.421507, mean_q: 23.204838
  963419/1500000: episode: 1632, duration: 8.817s, episode steps: 305, steps per second: 35, episode reward: -24.978, mean reward: -0.082 [-100.000, 14.069], mean action: 1.548 [0.000, 3.000], mean observation: 0.079 [-0.767, 1.000], loss: 1.820519, mean_absolute_error: 17.524170, mean_q: 23.384691
  963746/1500000: episode: 1633, duration: 8.966s, episode steps: 327, steps per second: 36, episode reward: 202.711, mean reward: 0.620 [-11.801, 100.000], mean action: 1.324 [0.000, 3.000], mean observation: 0.059 [-0.523, 1.000], loss: 2.405012, mean_absolute_error: 17.578165, mean_q: 23.459942
  964442/1500000: episode: 1634, duration: 19.694s, episode steps: 696, steps per second: 35, episode reward: 172.714, mean reward: 0.248 [-18.640, 100.000], mean action: 1.168 [0.000, 3.000], mean observation: 0.152 [-0.550, 1.000], loss: 2.376618, mean_absolute_error: 17.453358, mean_q: 23.266815
  964762/1500000: episode: 1635, duration: 8.745s, episode steps: 320, steps per second: 37, episode reward: 196.386, mean reward: 0.614 [-18.031, 100.000], mean action: 1.519 [0.000, 3.000], mean observation: 0.088 [-0.513, 1.000], loss: 2.740681, mean_absolute_error: 17.279139, mean_q: 23.030432
  965052/1500000: episode: 1636, duration: 7.768s, episode steps: 290, steps per second: 37, episode reward: 192.357, mean reward: 0.663 [-2.934, 100.000], mean action: 1.541 [0.000, 3.000], mean observation: 0.075 [-0.699, 1.000], loss: 2.111629, mean_absolute_error: 17.520586, mean_q: 23.341314
  965411/1500000: episode: 1637, duration: 10.219s, episode steps: 359, steps per second: 35, episode reward: 258.327, mean reward: 0.720 [-18.456, 100.000], mean action: 1.326 [0.000, 3.000], mean observation: 0.075 [-0.814, 1.048], loss: 3.365942, mean_absolute_error: 17.308235, mean_q: 23.020023
  965740/1500000: episode: 1638, duration: 8.779s, episode steps: 329, steps per second: 37, episode reward: 208.329, mean reward: 0.633 [-2.952, 100.000], mean action: 1.143 [0.000, 3.000], mean observation: 0.078 [-0.438, 1.000], loss: 2.795874, mean_absolute_error: 17.475742, mean_q: 23.287445
  966119/1500000: episode: 1639, duration: 10.400s, episode steps: 379, steps per second: 36, episode reward: 169.030, mean reward: 0.446 [-8.458, 100.000], mean action: 1.404 [0.000, 3.000], mean observation: 0.036 [-0.431, 1.000], loss: 1.438222, mean_absolute_error: 17.359182, mean_q: 23.115065
  966436/1500000: episode: 1640, duration: 8.699s, episode steps: 317, steps per second: 36, episode reward: 207.953, mean reward: 0.656 [-4.458, 100.000], mean action: 1.435 [0.000, 3.000], mean observation: 0.068 [-0.493, 1.000], loss: 2.530570, mean_absolute_error: 17.578117, mean_q: 23.420933
  966841/1500000: episode: 1641, duration: 11.297s, episode steps: 405, steps per second: 36, episode reward: 207.306, mean reward: 0.512 [-20.102, 100.000], mean action: 1.472 [0.000, 3.000], mean observation: 0.098 [-0.462, 1.000], loss: 2.822150, mean_absolute_error: 17.533871, mean_q: 23.372290
  967210/1500000: episode: 1642, duration: 10.810s, episode steps: 369, steps per second: 34, episode reward: 162.141, mean reward: 0.439 [-17.335, 100.000], mean action: 1.238 [0.000, 3.000], mean observation: 0.104 [-0.406, 1.000], loss: 2.966233, mean_absolute_error: 17.681067, mean_q: 23.596998
  967719/1500000: episode: 1643, duration: 14.489s, episode steps: 509, steps per second: 35, episode reward: 199.604, mean reward: 0.392 [-17.894, 100.000], mean action: 1.234 [0.000, 3.000], mean observation: 0.152 [-0.765, 1.000], loss: 2.525903, mean_absolute_error: 17.746147, mean_q: 23.670321
  968271/1500000: episode: 1644, duration: 16.165s, episode steps: 552, steps per second: 34, episode reward: 195.794, mean reward: 0.355 [-10.990, 100.000], mean action: 1.018 [0.000, 3.000], mean observation: 0.139 [-0.395, 1.000], loss: 2.780561, mean_absolute_error: 17.994537, mean_q: 24.025892
  969271/1500000: episode: 1645, duration: 32.424s, episode steps: 1000, steps per second: 31, episode reward: 76.354, mean reward: 0.076 [-19.236, 21.950], mean action: 1.236 [0.000, 3.000], mean observation: 0.171 [-0.645, 1.022], loss: 2.256145, mean_absolute_error: 18.013453, mean_q: 24.059673
  969662/1500000: episode: 1646, duration: 10.544s, episode steps: 391, steps per second: 37, episode reward: 200.995, mean reward: 0.514 [-3.639, 100.000], mean action: 1.662 [0.000, 3.000], mean observation: 0.069 [-0.601, 1.000], loss: 2.437156, mean_absolute_error: 18.108635, mean_q: 24.183044
  969992/1500000: episode: 1647, duration: 8.905s, episode steps: 330, steps per second: 37, episode reward: 213.949, mean reward: 0.648 [-2.959, 100.000], mean action: 1.697 [0.000, 3.000], mean observation: 0.061 [-0.495, 1.000], loss: 2.012145, mean_absolute_error: 18.104097, mean_q: 24.161818
  970642/1500000: episode: 1648, duration: 18.404s, episode steps: 650, steps per second: 35, episode reward: 183.700, mean reward: 0.283 [-19.660, 100.000], mean action: 1.098 [0.000, 3.000], mean observation: 0.145 [-0.638, 1.000], loss: 2.720940, mean_absolute_error: 18.293594, mean_q: 24.449877
  971063/1500000: episode: 1649, duration: 12.140s, episode steps: 421, steps per second: 35, episode reward: 220.630, mean reward: 0.524 [-12.646, 100.000], mean action: 1.133 [0.000, 3.000], mean observation: 0.066 [-0.582, 1.000], loss: 2.386122, mean_absolute_error: 18.451719, mean_q: 24.679550
  971529/1500000: episode: 1650, duration: 13.164s, episode steps: 466, steps per second: 35, episode reward: 154.711, mean reward: 0.332 [-18.983, 100.000], mean action: 2.077 [0.000, 3.000], mean observation: 0.152 [-0.688, 1.000], loss: 2.388608, mean_absolute_error: 18.575916, mean_q: 24.854618
  971938/1500000: episode: 1651, duration: 11.522s, episode steps: 409, steps per second: 35, episode reward: 164.690, mean reward: 0.403 [-19.289, 100.000], mean action: 1.457 [0.000, 3.000], mean observation: 0.107 [-0.629, 1.000], loss: 2.151974, mean_absolute_error: 18.498957, mean_q: 24.743731
  972456/1500000: episode: 1652, duration: 13.894s, episode steps: 518, steps per second: 37, episode reward: 229.068, mean reward: 0.442 [-17.742, 100.000], mean action: 1.156 [0.000, 3.000], mean observation: 0.119 [-0.712, 1.000], loss: 3.149469, mean_absolute_error: 18.418262, mean_q: 24.629480
  973456/1500000: episode: 1653, duration: 29.627s, episode steps: 1000, steps per second: 34, episode reward: 63.233, mean reward: 0.063 [-17.622, 16.936], mean action: 2.513 [0.000, 3.000], mean observation: 0.266 [-0.916, 1.429], loss: 1.622480, mean_absolute_error: 18.414454, mean_q: 24.629391
  973977/1500000: episode: 1654, duration: 15.718s, episode steps: 521, steps per second: 33, episode reward: 203.038, mean reward: 0.390 [-18.893, 100.000], mean action: 1.250 [0.000, 3.000], mean observation: 0.133 [-0.733, 1.000], loss: 3.562414, mean_absolute_error: 18.674015, mean_q: 24.988579
  974331/1500000: episode: 1655, duration: 9.871s, episode steps: 354, steps per second: 36, episode reward: 198.293, mean reward: 0.560 [-23.697, 100.000], mean action: 1.353 [0.000, 3.000], mean observation: 0.078 [-0.632, 1.000], loss: 3.523047, mean_absolute_error: 18.622509, mean_q: 24.886503
  974669/1500000: episode: 1656, duration: 9.282s, episode steps: 338, steps per second: 36, episode reward: 198.912, mean reward: 0.588 [-2.710, 100.000], mean action: 1.450 [0.000, 3.000], mean observation: 0.096 [-0.783, 1.000], loss: 3.454658, mean_absolute_error: 18.428577, mean_q: 24.675440
  974776/1500000: episode: 1657, duration: 2.835s, episode steps: 107, steps per second: 38, episode reward: -29.512, mean reward: -0.276 [-100.000, 19.560], mean action: 1.673 [0.000, 3.000], mean observation: -0.068 [-0.850, 1.821], loss: 1.377904, mean_absolute_error: 18.733940, mean_q: 25.079254
  975487/1500000: episode: 1658, duration: 20.709s, episode steps: 711, steps per second: 34, episode reward: -302.671, mean reward: -0.426 [-100.000, 40.891], mean action: 1.757 [0.000, 3.000], mean observation: 0.110 [-0.886, 1.686], loss: 2.963216, mean_absolute_error: 18.469692, mean_q: 24.711899
  975841/1500000: episode: 1659, duration: 9.586s, episode steps: 354, steps per second: 37, episode reward: 183.187, mean reward: 0.517 [-20.297, 100.000], mean action: 1.407 [0.000, 3.000], mean observation: 0.104 [-0.442, 1.000], loss: 1.383247, mean_absolute_error: 18.668854, mean_q: 24.976946
  976327/1500000: episode: 1660, duration: 13.464s, episode steps: 486, steps per second: 36, episode reward: 222.520, mean reward: 0.458 [-19.197, 100.000], mean action: 1.198 [0.000, 3.000], mean observation: 0.119 [-0.582, 1.000], loss: 2.972797, mean_absolute_error: 19.018917, mean_q: 25.450352
  976680/1500000: episode: 1661, duration: 10.130s, episode steps: 353, steps per second: 35, episode reward: 189.620, mean reward: 0.537 [-20.748, 100.000], mean action: 1.269 [0.000, 3.000], mean observation: 0.099 [-0.469, 1.000], loss: 1.812384, mean_absolute_error: 18.803446, mean_q: 25.180540
  977106/1500000: episode: 1662, duration: 12.062s, episode steps: 426, steps per second: 35, episode reward: 181.790, mean reward: 0.427 [-19.017, 100.000], mean action: 1.188 [0.000, 3.000], mean observation: 0.126 [-0.464, 1.000], loss: 1.912126, mean_absolute_error: 18.751104, mean_q: 25.104330
  977276/1500000: episode: 1663, duration: 4.810s, episode steps: 170, steps per second: 35, episode reward: -39.183, mean reward: -0.230 [-100.000, 17.738], mean action: 1.653 [0.000, 3.000], mean observation: 0.001 [-0.961, 1.169], loss: 5.414408, mean_absolute_error: 18.863419, mean_q: 25.253969
  977604/1500000: episode: 1664, duration: 9.170s, episode steps: 328, steps per second: 36, episode reward: 212.774, mean reward: 0.649 [-10.610, 100.000], mean action: 1.183 [0.000, 3.000], mean observation: 0.144 [-0.697, 1.000], loss: 2.907013, mean_absolute_error: 18.842905, mean_q: 25.235256
  978095/1500000: episode: 1665, duration: 14.802s, episode steps: 491, steps per second: 33, episode reward: 195.363, mean reward: 0.398 [-19.033, 100.000], mean action: 1.312 [0.000, 3.000], mean observation: 0.110 [-0.669, 1.000], loss: 1.683892, mean_absolute_error: 18.711128, mean_q: 25.065771
  978631/1500000: episode: 1666, duration: 14.925s, episode steps: 536, steps per second: 36, episode reward: 206.625, mean reward: 0.385 [-19.693, 100.000], mean action: 0.985 [0.000, 3.000], mean observation: 0.130 [-0.409, 1.000], loss: 3.031332, mean_absolute_error: 18.766563, mean_q: 25.123028
  979046/1500000: episode: 1667, duration: 11.780s, episode steps: 415, steps per second: 35, episode reward: 188.246, mean reward: 0.454 [-17.470, 100.000], mean action: 1.487 [0.000, 3.000], mean observation: 0.156 [-0.860, 1.684], loss: 4.965813, mean_absolute_error: 19.070820, mean_q: 25.545835
  979336/1500000: episode: 1668, duration: 7.887s, episode steps: 290, steps per second: 37, episode reward: 185.053, mean reward: 0.638 [-17.555, 100.000], mean action: 1.455 [0.000, 3.000], mean observation: 0.079 [-0.572, 1.000], loss: 2.253521, mean_absolute_error: 18.906374, mean_q: 25.354004
  979618/1500000: episode: 1669, duration: 7.562s, episode steps: 282, steps per second: 37, episode reward: 250.891, mean reward: 0.890 [-17.545, 100.000], mean action: 1.383 [0.000, 3.000], mean observation: 0.071 [-0.726, 1.000], loss: 2.197957, mean_absolute_error: 18.735594, mean_q: 25.115871
  980618/1500000: episode: 1670, duration: 29.234s, episode steps: 1000, steps per second: 34, episode reward: 34.050, mean reward: 0.034 [-20.411, 15.008], mean action: 1.376 [0.000, 3.000], mean observation: 0.137 [-0.876, 1.000], loss: 2.406282, mean_absolute_error: 18.987289, mean_q: 25.436710
  981035/1500000: episode: 1671, duration: 11.594s, episode steps: 417, steps per second: 36, episode reward: 242.622, mean reward: 0.582 [-18.155, 100.000], mean action: 1.228 [0.000, 3.000], mean observation: 0.108 [-0.686, 1.000], loss: 2.112717, mean_absolute_error: 19.056870, mean_q: 25.534857
  982035/1500000: episode: 1672, duration: 32.938s, episode steps: 1000, steps per second: 30, episode reward: -26.101, mean reward: -0.026 [-22.448, 20.342], mean action: 1.393 [0.000, 3.000], mean observation: 0.148 [-0.756, 1.000], loss: 2.732739, mean_absolute_error: 18.926701, mean_q: 25.353121
  982326/1500000: episode: 1673, duration: 7.909s, episode steps: 291, steps per second: 37, episode reward: 164.056, mean reward: 0.564 [-10.391, 100.000], mean action: 1.378 [0.000, 3.000], mean observation: 0.066 [-0.409, 1.000], loss: 2.486172, mean_absolute_error: 19.103724, mean_q: 25.576097
  982757/1500000: episode: 1674, duration: 11.821s, episode steps: 431, steps per second: 36, episode reward: 151.295, mean reward: 0.351 [-19.526, 100.000], mean action: 1.244 [0.000, 3.000], mean observation: 0.043 [-0.626, 1.000], loss: 3.120927, mean_absolute_error: 19.112198, mean_q: 25.588158
  983235/1500000: episode: 1675, duration: 13.631s, episode steps: 478, steps per second: 35, episode reward: 196.548, mean reward: 0.411 [-17.355, 100.000], mean action: 1.172 [0.000, 3.000], mean observation: 0.103 [-0.580, 1.000], loss: 2.547411, mean_absolute_error: 18.866211, mean_q: 25.291817
  983574/1500000: episode: 1676, duration: 8.769s, episode steps: 339, steps per second: 39, episode reward: 221.711, mean reward: 0.654 [-17.338, 100.000], mean action: 1.386 [0.000, 3.000], mean observation: 0.070 [-0.606, 1.000], loss: 1.645572, mean_absolute_error: 19.090246, mean_q: 25.587122
  984090/1500000: episode: 1677, duration: 15.265s, episode steps: 516, steps per second: 34, episode reward: 183.082, mean reward: 0.355 [-11.178, 100.000], mean action: 1.370 [0.000, 3.000], mean observation: 0.114 [-0.621, 1.000], loss: 2.317169, mean_absolute_error: 18.862972, mean_q: 25.269911
  984664/1500000: episode: 1678, duration: 15.939s, episode steps: 574, steps per second: 36, episode reward: 180.837, mean reward: 0.315 [-10.997, 100.000], mean action: 1.303 [0.000, 3.000], mean observation: 0.131 [-0.664, 1.000], loss: 2.600871, mean_absolute_error: 18.960876, mean_q: 25.395123
  985283/1500000: episode: 1679, duration: 17.991s, episode steps: 619, steps per second: 34, episode reward: 135.446, mean reward: 0.219 [-18.923, 100.000], mean action: 1.415 [0.000, 3.000], mean observation: 0.122 [-0.364, 1.000], loss: 3.637060, mean_absolute_error: 18.788567, mean_q: 25.175169
  985600/1500000: episode: 1680, duration: 8.980s, episode steps: 317, steps per second: 35, episode reward: 226.517, mean reward: 0.715 [-17.330, 100.000], mean action: 1.574 [0.000, 3.000], mean observation: 0.051 [-0.625, 1.000], loss: 3.014592, mean_absolute_error: 18.892735, mean_q: 25.302105
  986237/1500000: episode: 1681, duration: 17.727s, episode steps: 637, steps per second: 36, episode reward: 151.899, mean reward: 0.238 [-19.858, 100.000], mean action: 1.224 [0.000, 3.000], mean observation: 0.130 [-0.517, 1.000], loss: 2.833063, mean_absolute_error: 18.807804, mean_q: 25.170574
  986728/1500000: episode: 1682, duration: 13.919s, episode steps: 491, steps per second: 35, episode reward: 158.396, mean reward: 0.323 [-12.365, 100.000], mean action: 1.580 [0.000, 3.000], mean observation: 0.098 [-0.409, 1.000], loss: 2.491040, mean_absolute_error: 18.709604, mean_q: 25.018322
  987368/1500000: episode: 1683, duration: 17.909s, episode steps: 640, steps per second: 36, episode reward: 159.915, mean reward: 0.250 [-19.965, 100.000], mean action: 1.427 [0.000, 3.000], mean observation: 0.124 [-0.583, 1.000], loss: 2.976436, mean_absolute_error: 18.841604, mean_q: 25.194439
  987817/1500000: episode: 1684, duration: 13.073s, episode steps: 449, steps per second: 34, episode reward: 245.815, mean reward: 0.547 [-18.885, 100.000], mean action: 1.102 [0.000, 3.000], mean observation: 0.116 [-0.683, 1.000], loss: 4.034617, mean_absolute_error: 18.852859, mean_q: 25.259466
  988372/1500000: episode: 1685, duration: 17.185s, episode steps: 555, steps per second: 32, episode reward: 142.579, mean reward: 0.257 [-17.261, 100.000], mean action: 1.341 [0.000, 3.000], mean observation: 0.055 [-0.495, 1.000], loss: 3.862540, mean_absolute_error: 19.085848, mean_q: 25.547844
  988746/1500000: episode: 1686, duration: 10.113s, episode steps: 374, steps per second: 37, episode reward: 157.455, mean reward: 0.421 [-11.942, 100.000], mean action: 2.040 [0.000, 3.000], mean observation: 0.102 [-0.627, 1.000], loss: 2.340111, mean_absolute_error: 19.105413, mean_q: 25.569479
  989129/1500000: episode: 1687, duration: 11.076s, episode steps: 383, steps per second: 35, episode reward: 179.206, mean reward: 0.468 [-18.065, 100.000], mean action: 1.298 [0.000, 3.000], mean observation: 0.064 [-0.379, 1.000], loss: 3.550879, mean_absolute_error: 19.145319, mean_q: 25.620058
  989545/1500000: episode: 1688, duration: 11.480s, episode steps: 416, steps per second: 36, episode reward: 216.948, mean reward: 0.522 [-17.414, 100.000], mean action: 1.341 [0.000, 3.000], mean observation: 0.099 [-0.532, 1.000], loss: 2.339413, mean_absolute_error: 18.868122, mean_q: 25.268200
  989820/1500000: episode: 1689, duration: 7.472s, episode steps: 275, steps per second: 37, episode reward: 187.182, mean reward: 0.681 [-3.070, 100.000], mean action: 1.531 [0.000, 3.000], mean observation: 0.065 [-0.429, 1.000], loss: 5.081289, mean_absolute_error: 19.112820, mean_q: 25.574261
  990334/1500000: episode: 1690, duration: 13.847s, episode steps: 514, steps per second: 37, episode reward: 219.468, mean reward: 0.427 [-2.962, 100.000], mean action: 1.226 [0.000, 3.000], mean observation: 0.133 [-0.560, 1.000], loss: 2.881563, mean_absolute_error: 18.960945, mean_q: 25.382008
  990752/1500000: episode: 1691, duration: 11.408s, episode steps: 418, steps per second: 37, episode reward: 175.087, mean reward: 0.419 [-24.426, 100.000], mean action: 1.062 [0.000, 3.000], mean observation: 0.083 [-0.397, 1.000], loss: 3.190335, mean_absolute_error: 19.123774, mean_q: 25.604517
  991052/1500000: episode: 1692, duration: 8.295s, episode steps: 300, steps per second: 36, episode reward: 176.093, mean reward: 0.587 [-10.129, 100.000], mean action: 1.267 [0.000, 3.000], mean observation: 0.035 [-0.448, 1.000], loss: 3.308366, mean_absolute_error: 19.308558, mean_q: 25.813961
  991637/1500000: episode: 1693, duration: 17.180s, episode steps: 585, steps per second: 34, episode reward: 124.632, mean reward: 0.213 [-18.695, 100.000], mean action: 1.677 [0.000, 3.000], mean observation: 0.123 [-0.354, 1.000], loss: 3.648622, mean_absolute_error: 19.222408, mean_q: 25.726135
  992067/1500000: episode: 1694, duration: 12.519s, episode steps: 430, steps per second: 34, episode reward: 183.641, mean reward: 0.427 [-10.598, 100.000], mean action: 1.302 [0.000, 3.000], mean observation: 0.076 [-0.389, 1.000], loss: 5.165032, mean_absolute_error: 19.085945, mean_q: 25.544205
  992798/1500000: episode: 1695, duration: 22.348s, episode steps: 731, steps per second: 33, episode reward: 181.233, mean reward: 0.248 [-20.059, 100.000], mean action: 1.242 [0.000, 3.000], mean observation: 0.151 [-0.612, 1.000], loss: 2.961239, mean_absolute_error: 19.240284, mean_q: 25.745083
  993180/1500000: episode: 1696, duration: 11.164s, episode steps: 382, steps per second: 34, episode reward: 190.123, mean reward: 0.498 [-17.823, 100.000], mean action: 1.047 [0.000, 3.000], mean observation: 0.085 [-0.525, 1.000], loss: 3.468541, mean_absolute_error: 19.226791, mean_q: 25.704298
  993623/1500000: episode: 1697, duration: 13.230s, episode steps: 443, steps per second: 33, episode reward: 164.322, mean reward: 0.371 [-11.002, 100.000], mean action: 1.447 [0.000, 3.000], mean observation: 0.104 [-0.540, 1.000], loss: 2.294782, mean_absolute_error: 19.147522, mean_q: 25.554888
  993978/1500000: episode: 1698, duration: 10.092s, episode steps: 355, steps per second: 35, episode reward: 208.276, mean reward: 0.587 [-17.469, 100.000], mean action: 1.487 [0.000, 3.000], mean observation: 0.083 [-0.501, 1.000], loss: 2.924330, mean_absolute_error: 19.393850, mean_q: 25.917297
  994474/1500000: episode: 1699, duration: 14.066s, episode steps: 496, steps per second: 35, episode reward: 179.381, mean reward: 0.362 [-18.758, 100.000], mean action: 1.440 [0.000, 3.000], mean observation: 0.116 [-0.631, 1.000], loss: 2.605600, mean_absolute_error: 19.264290, mean_q: 25.752949
  995235/1500000: episode: 1700, duration: 22.339s, episode steps: 761, steps per second: 34, episode reward: 123.432, mean reward: 0.162 [-19.368, 100.000], mean action: 1.222 [0.000, 3.000], mean observation: 0.150 [-0.415, 1.000], loss: 3.098315, mean_absolute_error: 19.450552, mean_q: 25.994051
  996235/1500000: episode: 1701, duration: 29.561s, episode steps: 1000, steps per second: 34, episode reward: -99.451, mean reward: -0.099 [-13.405, 14.921], mean action: 1.813 [0.000, 3.000], mean observation: 0.078 [-0.517, 1.000], loss: 3.391492, mean_absolute_error: 19.447969, mean_q: 25.982281
  996835/1500000: episode: 1702, duration: 16.597s, episode steps: 600, steps per second: 36, episode reward: 133.480, mean reward: 0.222 [-17.409, 100.000], mean action: 2.248 [0.000, 3.000], mean observation: 0.105 [-0.425, 1.000], loss: 2.654794, mean_absolute_error: 19.689688, mean_q: 26.303408
  997287/1500000: episode: 1703, duration: 13.198s, episode steps: 452, steps per second: 34, episode reward: 183.966, mean reward: 0.407 [-17.479, 100.000], mean action: 1.542 [0.000, 3.000], mean observation: 0.102 [-0.351, 1.000], loss: 3.396752, mean_absolute_error: 19.685072, mean_q: 26.307604
  997708/1500000: episode: 1704, duration: 11.862s, episode steps: 421, steps per second: 35, episode reward: 198.702, mean reward: 0.472 [-11.145, 100.000], mean action: 1.280 [0.000, 3.000], mean observation: 0.105 [-0.587, 1.000], loss: 1.971757, mean_absolute_error: 19.624889, mean_q: 26.237894
  998429/1500000: episode: 1705, duration: 20.808s, episode steps: 721, steps per second: 35, episode reward: 141.109, mean reward: 0.196 [-18.297, 100.000], mean action: 1.897 [0.000, 3.000], mean observation: 0.145 [-0.444, 1.000], loss: 2.398175, mean_absolute_error: 19.489565, mean_q: 26.042015
  998710/1500000: episode: 1706, duration: 9.065s, episode steps: 281, steps per second: 31, episode reward: 183.873, mean reward: 0.654 [-2.817, 100.000], mean action: 1.423 [0.000, 3.000], mean observation: 0.054 [-0.410, 1.000], loss: 1.963892, mean_absolute_error: 19.526917, mean_q: 26.051229
  999710/1500000: episode: 1707, duration: 31.254s, episode steps: 1000, steps per second: 32, episode reward: 23.885, mean reward: 0.024 [-19.798, 17.136], mean action: 1.546 [0.000, 3.000], mean observation: 0.169 [-0.393, 1.000], loss: 2.811327, mean_absolute_error: 19.547684, mean_q: 26.116014
 1000447/1500000: episode: 1708, duration: 21.275s, episode steps: 737, steps per second: 35, episode reward: 193.496, mean reward: 0.263 [-17.926, 100.000], mean action: 1.233 [0.000, 3.000], mean observation: 0.135 [-0.598, 1.000], loss: 2.329666, mean_absolute_error: 19.617764, mean_q: 26.211681
 1000832/1500000: episode: 1709, duration: 10.563s, episode steps: 385, steps per second: 36, episode reward: 215.377, mean reward: 0.559 [-8.206, 100.000], mean action: 1.538 [0.000, 3.000], mean observation: 0.080 [-0.440, 1.000], loss: 2.486930, mean_absolute_error: 19.568323, mean_q: 26.130377
 1001216/1500000: episode: 1710, duration: 10.697s, episode steps: 384, steps per second: 36, episode reward: 190.767, mean reward: 0.497 [-3.022, 100.000], mean action: 1.391 [0.000, 3.000], mean observation: 0.063 [-0.397, 1.013], loss: 2.683825, mean_absolute_error: 19.784204, mean_q: 26.450340
 1001559/1500000: episode: 1711, duration: 9.428s, episode steps: 343, steps per second: 36, episode reward: 197.707, mean reward: 0.576 [-19.460, 100.000], mean action: 1.534 [0.000, 3.000], mean observation: 0.084 [-0.626, 1.000], loss: 2.111857, mean_absolute_error: 19.561939, mean_q: 26.155626
 1001931/1500000: episode: 1712, duration: 10.824s, episode steps: 372, steps per second: 34, episode reward: 195.447, mean reward: 0.525 [-17.852, 100.000], mean action: 1.212 [0.000, 3.000], mean observation: 0.116 [-0.431, 1.000], loss: 2.661638, mean_absolute_error: 19.508299, mean_q: 26.075375
 1002302/1500000: episode: 1713, duration: 10.365s, episode steps: 371, steps per second: 36, episode reward: 199.674, mean reward: 0.538 [-19.452, 100.000], mean action: 1.385 [0.000, 3.000], mean observation: 0.069 [-0.454, 1.000], loss: 2.995274, mean_absolute_error: 19.535091, mean_q: 26.084911
 1002653/1500000: episode: 1714, duration: 9.992s, episode steps: 351, steps per second: 35, episode reward: 187.274, mean reward: 0.534 [-10.736, 100.000], mean action: 1.339 [0.000, 3.000], mean observation: 0.082 [-0.490, 1.000], loss: 3.042849, mean_absolute_error: 19.674204, mean_q: 26.259327
 1003348/1500000: episode: 1715, duration: 19.394s, episode steps: 695, steps per second: 36, episode reward: 189.145, mean reward: 0.272 [-19.820, 100.000], mean action: 0.938 [0.000, 3.000], mean observation: 0.162 [-0.564, 1.000], loss: 4.451231, mean_absolute_error: 19.824291, mean_q: 26.480770
 1003708/1500000: episode: 1716, duration: 10.105s, episode steps: 360, steps per second: 36, episode reward: 226.791, mean reward: 0.630 [-17.567, 100.000], mean action: 1.392 [0.000, 3.000], mean observation: 0.061 [-0.677, 1.000], loss: 3.412322, mean_absolute_error: 19.777716, mean_q: 26.368389
 1004284/1500000: episode: 1717, duration: 16.451s, episode steps: 576, steps per second: 35, episode reward: 123.546, mean reward: 0.214 [-19.638, 100.000], mean action: 1.632 [0.000, 3.000], mean observation: 0.056 [-0.601, 1.000], loss: 3.849157, mean_absolute_error: 19.510773, mean_q: 26.004091
 1005284/1500000: episode: 1718, duration: 34.756s, episode steps: 1000, steps per second: 29, episode reward: -112.034, mean reward: -0.112 [-12.790, 12.163], mean action: 1.859 [0.000, 3.000], mean observation: 0.068 [-0.476, 1.000], loss: 2.820210, mean_absolute_error: 19.627790, mean_q: 26.174061
 1005925/1500000: episode: 1719, duration: 19.450s, episode steps: 641, steps per second: 33, episode reward: 177.638, mean reward: 0.277 [-10.499, 100.000], mean action: 1.376 [0.000, 3.000], mean observation: 0.113 [-0.530, 1.000], loss: 2.970378, mean_absolute_error: 19.548365, mean_q: 26.033428
 1006722/1500000: episode: 1720, duration: 24.589s, episode steps: 797, steps per second: 32, episode reward: 90.869, mean reward: 0.114 [-17.904, 100.000], mean action: 2.275 [0.000, 3.000], mean observation: 0.096 [-0.588, 1.000], loss: 2.886700, mean_absolute_error: 19.584249, mean_q: 26.076895
 1007172/1500000: episode: 1721, duration: 12.751s, episode steps: 450, steps per second: 35, episode reward: 195.488, mean reward: 0.434 [-18.888, 100.000], mean action: 1.267 [0.000, 3.000], mean observation: 0.094 [-0.546, 1.000], loss: 2.219147, mean_absolute_error: 19.486967, mean_q: 25.939936
 1007793/1500000: episode: 1722, duration: 18.756s, episode steps: 621, steps per second: 33, episode reward: 124.108, mean reward: 0.200 [-18.060, 100.000], mean action: 2.259 [0.000, 3.000], mean observation: 0.155 [-0.602, 1.000], loss: 3.433417, mean_absolute_error: 19.704855, mean_q: 26.218504
 1008174/1500000: episode: 1723, duration: 10.602s, episode steps: 381, steps per second: 36, episode reward: 193.582, mean reward: 0.508 [-10.715, 100.000], mean action: 1.648 [0.000, 3.000], mean observation: 0.044 [-0.496, 1.000], loss: 2.282226, mean_absolute_error: 19.543985, mean_q: 25.974350
 1008649/1500000: episode: 1724, duration: 13.958s, episode steps: 475, steps per second: 34, episode reward: 170.072, mean reward: 0.358 [-20.061, 100.000], mean action: 1.128 [0.000, 3.000], mean observation: 0.066 [-0.420, 1.000], loss: 3.227716, mean_absolute_error: 19.639832, mean_q: 26.067537
 1009136/1500000: episode: 1725, duration: 13.606s, episode steps: 487, steps per second: 36, episode reward: 194.111, mean reward: 0.399 [-19.104, 100.000], mean action: 1.281 [0.000, 3.000], mean observation: 0.115 [-0.398, 1.000], loss: 3.030633, mean_absolute_error: 19.482185, mean_q: 25.891428
 1009501/1500000: episode: 1726, duration: 10.522s, episode steps: 365, steps per second: 35, episode reward: 213.928, mean reward: 0.586 [-13.595, 100.000], mean action: 1.630 [0.000, 3.000], mean observation: 0.070 [-0.551, 1.000], loss: 4.741356, mean_absolute_error: 19.625519, mean_q: 26.064074
 1009896/1500000: episode: 1727, duration: 11.378s, episode steps: 395, steps per second: 35, episode reward: 195.543, mean reward: 0.495 [-9.764, 100.000], mean action: 1.466 [0.000, 3.000], mean observation: 0.097 [-0.533, 1.000], loss: 3.533679, mean_absolute_error: 19.427202, mean_q: 25.791643
 1010342/1500000: episode: 1728, duration: 12.321s, episode steps: 446, steps per second: 36, episode reward: 207.258, mean reward: 0.465 [-17.616, 100.000], mean action: 1.327 [0.000, 3.000], mean observation: 0.104 [-0.394, 1.000], loss: 2.924806, mean_absolute_error: 19.626232, mean_q: 26.031723
 1011117/1500000: episode: 1729, duration: 22.105s, episode steps: 775, steps per second: 35, episode reward: 205.674, mean reward: 0.265 [-18.576, 100.000], mean action: 0.945 [0.000, 3.000], mean observation: 0.161 [-0.481, 1.000], loss: 3.135420, mean_absolute_error: 19.560198, mean_q: 25.935659
 1011574/1500000: episode: 1730, duration: 12.995s, episode steps: 457, steps per second: 35, episode reward: 211.653, mean reward: 0.463 [-18.819, 100.000], mean action: 1.470 [0.000, 3.000], mean observation: 0.077 [-0.645, 1.000], loss: 2.576729, mean_absolute_error: 19.439791, mean_q: 25.775606
 1012130/1500000: episode: 1731, duration: 16.548s, episode steps: 556, steps per second: 34, episode reward: 180.130, mean reward: 0.324 [-10.347, 100.000], mean action: 1.473 [0.000, 3.000], mean observation: 0.115 [-0.336, 1.000], loss: 2.308029, mean_absolute_error: 19.651634, mean_q: 26.105684
 1012550/1500000: episode: 1732, duration: 12.279s, episode steps: 420, steps per second: 34, episode reward: 207.910, mean reward: 0.495 [-11.014, 100.000], mean action: 1.540 [0.000, 3.000], mean observation: 0.084 [-0.689, 1.000], loss: 2.235970, mean_absolute_error: 19.376320, mean_q: 25.739046
 1013020/1500000: episode: 1733, duration: 13.365s, episode steps: 470, steps per second: 35, episode reward: 187.618, mean reward: 0.399 [-18.587, 100.000], mean action: 1.200 [0.000, 3.000], mean observation: 0.109 [-0.339, 1.000], loss: 2.627950, mean_absolute_error: 19.707958, mean_q: 26.205269
 1013423/1500000: episode: 1734, duration: 10.991s, episode steps: 403, steps per second: 37, episode reward: 209.786, mean reward: 0.521 [-10.639, 100.000], mean action: 1.454 [0.000, 3.000], mean observation: 0.078 [-0.464, 1.000], loss: 3.916780, mean_absolute_error: 19.662123, mean_q: 26.081129
 1013933/1500000: episode: 1735, duration: 14.056s, episode steps: 510, steps per second: 36, episode reward: 101.179, mean reward: 0.198 [-18.715, 100.000], mean action: 2.000 [0.000, 3.000], mean observation: 0.062 [-0.400, 1.000], loss: 3.465506, mean_absolute_error: 19.999001, mean_q: 26.571867
 1014397/1500000: episode: 1736, duration: 13.296s, episode steps: 464, steps per second: 35, episode reward: 205.113, mean reward: 0.442 [-11.521, 100.000], mean action: 1.338 [0.000, 3.000], mean observation: 0.124 [-0.780, 1.000], loss: 2.990028, mean_absolute_error: 19.935921, mean_q: 26.513483
 1014744/1500000: episode: 1737, duration: 9.710s, episode steps: 347, steps per second: 36, episode reward: 242.121, mean reward: 0.698 [-2.805, 100.000], mean action: 1.594 [0.000, 3.000], mean observation: 0.047 [-0.742, 1.000], loss: 3.124766, mean_absolute_error: 20.086308, mean_q: 26.700806
 1015203/1500000: episode: 1738, duration: 13.039s, episode steps: 459, steps per second: 35, episode reward: 204.911, mean reward: 0.446 [-18.514, 100.000], mean action: 1.370 [0.000, 3.000], mean observation: 0.095 [-0.444, 1.000], loss: 2.743185, mean_absolute_error: 19.830048, mean_q: 26.369415
 1015591/1500000: episode: 1739, duration: 11.045s, episode steps: 388, steps per second: 35, episode reward: 227.583, mean reward: 0.587 [-9.855, 100.000], mean action: 1.570 [0.000, 3.000], mean observation: 0.074 [-0.625, 1.016], loss: 2.005051, mean_absolute_error: 19.873703, mean_q: 26.393831
 1015928/1500000: episode: 1740, duration: 9.097s, episode steps: 337, steps per second: 37, episode reward: 179.589, mean reward: 0.533 [-8.099, 100.000], mean action: 1.181 [0.000, 3.000], mean observation: 0.079 [-0.419, 1.000], loss: 3.808826, mean_absolute_error: 19.807499, mean_q: 26.294346
 1016257/1500000: episode: 1741, duration: 9.032s, episode steps: 329, steps per second: 36, episode reward: 234.704, mean reward: 0.713 [-11.123, 100.000], mean action: 1.359 [0.000, 3.000], mean observation: 0.051 [-0.664, 1.000], loss: 3.377680, mean_absolute_error: 20.095104, mean_q: 26.723368
 1016770/1500000: episode: 1742, duration: 14.425s, episode steps: 513, steps per second: 36, episode reward: 157.526, mean reward: 0.307 [-18.912, 100.000], mean action: 1.491 [0.000, 3.000], mean observation: 0.123 [-0.669, 1.000], loss: 3.866184, mean_absolute_error: 20.193722, mean_q: 26.869658
 1017085/1500000: episode: 1743, duration: 8.641s, episode steps: 315, steps per second: 36, episode reward: 191.735, mean reward: 0.609 [-3.230, 100.000], mean action: 1.403 [0.000, 3.000], mean observation: 0.060 [-0.389, 1.000], loss: 3.078359, mean_absolute_error: 20.246769, mean_q: 26.932993
 1018085/1500000: episode: 1744, duration: 28.553s, episode steps: 1000, steps per second: 35, episode reward: 16.481, mean reward: 0.016 [-20.368, 21.382], mean action: 1.836 [0.000, 3.000], mean observation: 0.164 [-0.420, 1.000], loss: 3.969283, mean_absolute_error: 20.213554, mean_q: 26.885498
 1018553/1500000: episode: 1745, duration: 13.454s, episode steps: 468, steps per second: 35, episode reward: 197.765, mean reward: 0.423 [-18.355, 100.000], mean action: 0.959 [0.000, 3.000], mean observation: 0.140 [-0.470, 1.000], loss: 2.517772, mean_absolute_error: 20.264490, mean_q: 26.960257
 1018962/1500000: episode: 1746, duration: 11.189s, episode steps: 409, steps per second: 37, episode reward: 199.806, mean reward: 0.489 [-11.835, 100.000], mean action: 1.345 [0.000, 3.000], mean observation: 0.091 [-0.496, 1.000], loss: 3.808296, mean_absolute_error: 20.506971, mean_q: 27.278959
 1019962/1500000: episode: 1747, duration: 35.775s, episode steps: 1000, steps per second: 28, episode reward: -43.483, mean reward: -0.043 [-19.600, 14.251], mean action: 1.654 [0.000, 3.000], mean observation: 0.094 [-0.603, 1.004], loss: 3.426485, mean_absolute_error: 20.528757, mean_q: 27.306881
 1020381/1500000: episode: 1748, duration: 13.937s, episode steps: 419, steps per second: 30, episode reward: 168.882, mean reward: 0.403 [-9.826, 100.000], mean action: 1.578 [0.000, 3.000], mean observation: 0.054 [-0.488, 1.000], loss: 3.728090, mean_absolute_error: 20.509146, mean_q: 27.273582
 1020769/1500000: episode: 1749, duration: 12.252s, episode steps: 388, steps per second: 32, episode reward: 174.278, mean reward: 0.449 [-9.375, 100.000], mean action: 1.258 [0.000, 3.000], mean observation: 0.086 [-0.417, 1.000], loss: 2.321933, mean_absolute_error: 20.439430, mean_q: 27.193909
 1021157/1500000: episode: 1750, duration: 10.639s, episode steps: 388, steps per second: 36, episode reward: 193.232, mean reward: 0.498 [-17.485, 100.000], mean action: 1.191 [0.000, 3.000], mean observation: 0.099 [-0.467, 1.000], loss: 4.360879, mean_absolute_error: 20.701193, mean_q: 27.554516
 1021722/1500000: episode: 1751, duration: 16.979s, episode steps: 565, steps per second: 33, episode reward: 206.793, mean reward: 0.366 [-19.603, 100.000], mean action: 1.242 [0.000, 3.000], mean observation: 0.124 [-0.521, 1.000], loss: 2.414528, mean_absolute_error: 20.770247, mean_q: 27.646088
 1022282/1500000: episode: 1752, duration: 16.676s, episode steps: 560, steps per second: 34, episode reward: 171.787, mean reward: 0.307 [-19.980, 100.000], mean action: 1.207 [0.000, 3.000], mean observation: 0.103 [-0.396, 1.000], loss: 3.602058, mean_absolute_error: 20.643145, mean_q: 27.470947
 1022858/1500000: episode: 1753, duration: 16.500s, episode steps: 576, steps per second: 35, episode reward: 217.233, mean reward: 0.377 [-17.971, 100.000], mean action: 1.030 [0.000, 3.000], mean observation: 0.125 [-0.456, 1.000], loss: 2.661588, mean_absolute_error: 20.622414, mean_q: 27.452742
 1023351/1500000: episode: 1754, duration: 13.648s, episode steps: 493, steps per second: 36, episode reward: 194.629, mean reward: 0.395 [-9.349, 100.000], mean action: 1.318 [0.000, 3.000], mean observation: 0.086 [-0.419, 1.009], loss: 4.014663, mean_absolute_error: 20.625957, mean_q: 27.396545
 1023815/1500000: episode: 1755, duration: 13.023s, episode steps: 464, steps per second: 36, episode reward: 181.945, mean reward: 0.392 [-18.010, 100.000], mean action: 1.252 [0.000, 3.000], mean observation: 0.095 [-0.358, 1.000], loss: 2.605449, mean_absolute_error: 20.767265, mean_q: 27.639174
 1024125/1500000: episode: 1756, duration: 8.474s, episode steps: 310, steps per second: 37, episode reward: -67.898, mean reward: -0.219 [-100.000, 22.888], mean action: 1.616 [0.000, 3.000], mean observation: 0.005 [-0.426, 1.000], loss: 3.002898, mean_absolute_error: 20.670284, mean_q: 27.479452
 1025074/1500000: episode: 1757, duration: 26.543s, episode steps: 949, steps per second: 36, episode reward: 130.038, mean reward: 0.137 [-19.925, 100.000], mean action: 1.256 [0.000, 3.000], mean observation: 0.095 [-0.653, 1.000], loss: 3.222932, mean_absolute_error: 20.876659, mean_q: 27.759449
 1025552/1500000: episode: 1758, duration: 13.184s, episode steps: 478, steps per second: 36, episode reward: 100.924, mean reward: 0.211 [-11.508, 100.000], mean action: 2.144 [0.000, 3.000], mean observation: 0.055 [-0.702, 1.000], loss: 3.515579, mean_absolute_error: 20.807682, mean_q: 27.662586
 1025946/1500000: episode: 1759, duration: 11.540s, episode steps: 394, steps per second: 34, episode reward: 206.797, mean reward: 0.525 [-7.434, 100.000], mean action: 1.589 [0.000, 3.000], mean observation: 0.082 [-0.459, 1.000], loss: 2.564428, mean_absolute_error: 20.965506, mean_q: 27.939056
 1026274/1500000: episode: 1760, duration: 9.661s, episode steps: 328, steps per second: 34, episode reward: 199.910, mean reward: 0.609 [-10.954, 100.000], mean action: 1.503 [0.000, 3.000], mean observation: 0.068 [-0.374, 1.000], loss: 3.545991, mean_absolute_error: 21.044762, mean_q: 28.074219
 1026662/1500000: episode: 1761, duration: 10.576s, episode steps: 388, steps per second: 37, episode reward: 192.164, mean reward: 0.495 [-11.267, 100.000], mean action: 1.485 [0.000, 3.000], mean observation: 0.042 [-0.595, 1.000], loss: 2.740816, mean_absolute_error: 21.202232, mean_q: 28.229076
 1027029/1500000: episode: 1762, duration: 10.462s, episode steps: 367, steps per second: 35, episode reward: 219.761, mean reward: 0.599 [-19.489, 100.000], mean action: 1.452 [0.000, 3.000], mean observation: 0.085 [-0.645, 1.000], loss: 2.765440, mean_absolute_error: 21.349977, mean_q: 28.478449
 1027439/1500000: episode: 1763, duration: 11.396s, episode steps: 410, steps per second: 36, episode reward: 188.630, mean reward: 0.460 [-12.160, 100.000], mean action: 1.249 [0.000, 3.000], mean observation: 0.087 [-0.397, 1.000], loss: 2.539541, mean_absolute_error: 21.191240, mean_q: 28.209177
 1027769/1500000: episode: 1764, duration: 8.868s, episode steps: 330, steps per second: 37, episode reward: 189.802, mean reward: 0.575 [-9.925, 100.000], mean action: 1.288 [0.000, 3.000], mean observation: 0.067 [-0.508, 1.000], loss: 4.812426, mean_absolute_error: 21.344603, mean_q: 28.427639
 1028400/1500000: episode: 1765, duration: 17.817s, episode steps: 631, steps per second: 35, episode reward: 186.782, mean reward: 0.296 [-21.614, 100.000], mean action: 1.211 [0.000, 3.000], mean observation: 0.097 [-0.554, 1.000], loss: 4.121792, mean_absolute_error: 21.412292, mean_q: 28.516932
 1028899/1500000: episode: 1766, duration: 13.947s, episode steps: 499, steps per second: 36, episode reward: 186.997, mean reward: 0.375 [-18.770, 100.000], mean action: 1.174 [0.000, 3.000], mean observation: 0.086 [-0.489, 1.000], loss: 3.678713, mean_absolute_error: 21.653831, mean_q: 28.874784
 1029347/1500000: episode: 1767, duration: 14.142s, episode steps: 448, steps per second: 32, episode reward: 136.078, mean reward: 0.304 [-18.177, 100.000], mean action: 1.569 [0.000, 3.000], mean observation: 0.099 [-0.440, 1.000], loss: 2.635536, mean_absolute_error: 21.631926, mean_q: 28.819906
 1029867/1500000: episode: 1768, duration: 14.913s, episode steps: 520, steps per second: 35, episode reward: 96.145, mean reward: 0.185 [-13.627, 100.000], mean action: 1.631 [0.000, 3.000], mean observation: 0.013 [-0.515, 1.000], loss: 2.528996, mean_absolute_error: 21.669422, mean_q: 28.883970
 1030157/1500000: episode: 1769, duration: 7.889s, episode steps: 290, steps per second: 37, episode reward: 235.554, mean reward: 0.812 [-2.815, 100.000], mean action: 1.662 [0.000, 3.000], mean observation: 0.013 [-0.724, 1.000], loss: 3.014004, mean_absolute_error: 21.382051, mean_q: 28.483822
 1030781/1500000: episode: 1770, duration: 18.197s, episode steps: 624, steps per second: 34, episode reward: 146.241, mean reward: 0.234 [-18.858, 100.000], mean action: 1.449 [0.000, 3.000], mean observation: 0.134 [-0.431, 1.000], loss: 3.313245, mean_absolute_error: 21.603910, mean_q: 28.821625
 1031285/1500000: episode: 1771, duration: 14.341s, episode steps: 504, steps per second: 35, episode reward: 170.359, mean reward: 0.338 [-19.791, 100.000], mean action: 1.117 [0.000, 3.000], mean observation: 0.117 [-0.330, 1.000], loss: 4.255701, mean_absolute_error: 21.602127, mean_q: 28.804005
 1031920/1500000: episode: 1772, duration: 18.775s, episode steps: 635, steps per second: 34, episode reward: 164.640, mean reward: 0.259 [-19.674, 100.000], mean action: 0.893 [0.000, 3.000], mean observation: 0.102 [-0.415, 1.000], loss: 2.559551, mean_absolute_error: 21.549120, mean_q: 28.729338
 1032301/1500000: episode: 1773, duration: 10.688s, episode steps: 381, steps per second: 36, episode reward: 187.416, mean reward: 0.492 [-18.108, 100.000], mean action: 1.415 [0.000, 3.000], mean observation: 0.053 [-0.557, 1.000], loss: 2.975753, mean_absolute_error: 21.792463, mean_q: 29.111118
 1032707/1500000: episode: 1774, duration: 11.254s, episode steps: 406, steps per second: 36, episode reward: 201.632, mean reward: 0.497 [-20.733, 100.000], mean action: 1.330 [0.000, 3.000], mean observation: 0.081 [-0.458, 1.000], loss: 1.858559, mean_absolute_error: 21.560139, mean_q: 28.831488
 1033110/1500000: episode: 1775, duration: 11.365s, episode steps: 403, steps per second: 35, episode reward: 171.720, mean reward: 0.426 [-17.697, 100.000], mean action: 1.695 [0.000, 3.000], mean observation: 0.072 [-0.292, 1.000], loss: 2.974913, mean_absolute_error: 21.491686, mean_q: 28.711302
 1033485/1500000: episode: 1776, duration: 10.648s, episode steps: 375, steps per second: 35, episode reward: 178.523, mean reward: 0.476 [-20.131, 100.000], mean action: 1.413 [0.000, 3.000], mean observation: 0.054 [-0.362, 1.000], loss: 5.559499, mean_absolute_error: 21.449263, mean_q: 28.568209
 1033867/1500000: episode: 1777, duration: 10.331s, episode steps: 382, steps per second: 37, episode reward: 199.846, mean reward: 0.523 [-13.381, 100.000], mean action: 1.118 [0.000, 3.000], mean observation: 0.033 [-0.706, 1.000], loss: 1.683944, mean_absolute_error: 21.446859, mean_q: 28.621033
 1034340/1500000: episode: 1778, duration: 13.374s, episode steps: 473, steps per second: 35, episode reward: 107.533, mean reward: 0.227 [-17.277, 100.000], mean action: 1.863 [0.000, 3.000], mean observation: 0.015 [-0.490, 1.000], loss: 4.094277, mean_absolute_error: 21.662067, mean_q: 28.922022
 1035110/1500000: episode: 1779, duration: 21.887s, episode steps: 770, steps per second: 35, episode reward: 129.529, mean reward: 0.168 [-19.944, 100.000], mean action: 1.384 [0.000, 3.000], mean observation: 0.125 [-0.561, 1.000], loss: 4.239554, mean_absolute_error: 21.768415, mean_q: 29.024727
 1035826/1500000: episode: 1780, duration: 22.467s, episode steps: 716, steps per second: 32, episode reward: 143.157, mean reward: 0.200 [-18.323, 100.000], mean action: 2.038 [0.000, 3.000], mean observation: 0.130 [-0.361, 1.000], loss: 2.986387, mean_absolute_error: 21.574242, mean_q: 28.738470
 1036156/1500000: episode: 1781, duration: 10.961s, episode steps: 330, steps per second: 30, episode reward: 186.392, mean reward: 0.565 [-17.524, 100.000], mean action: 1.261 [0.000, 3.000], mean observation: 0.066 [-0.487, 1.000], loss: 3.414155, mean_absolute_error: 21.915920, mean_q: 29.291868
 1036614/1500000: episode: 1782, duration: 14.395s, episode steps: 458, steps per second: 32, episode reward: 178.753, mean reward: 0.390 [-18.580, 100.000], mean action: 1.081 [0.000, 3.000], mean observation: 0.124 [-0.398, 1.000], loss: 4.792238, mean_absolute_error: 22.008768, mean_q: 29.351768
 1037073/1500000: episode: 1783, duration: 14.900s, episode steps: 459, steps per second: 31, episode reward: 161.255, mean reward: 0.351 [-14.427, 100.000], mean action: 1.074 [0.000, 3.000], mean observation: 0.070 [-0.426, 1.000], loss: 2.614268, mean_absolute_error: 22.168104, mean_q: 29.593197
 1037423/1500000: episode: 1784, duration: 10.885s, episode steps: 350, steps per second: 32, episode reward: 153.711, mean reward: 0.439 [-18.567, 100.000], mean action: 1.480 [0.000, 3.000], mean observation: 0.024 [-0.468, 1.000], loss: 3.353758, mean_absolute_error: 22.008242, mean_q: 29.393639
 1037960/1500000: episode: 1785, duration: 20.074s, episode steps: 537, steps per second: 27, episode reward: 169.923, mean reward: 0.316 [-18.314, 100.000], mean action: 1.017 [0.000, 3.000], mean observation: 0.057 [-0.450, 1.014], loss: 3.012809, mean_absolute_error: 22.101187, mean_q: 29.533743
 1038299/1500000: episode: 1786, duration: 13.487s, episode steps: 339, steps per second: 25, episode reward: 248.642, mean reward: 0.733 [-19.688, 100.000], mean action: 1.310 [0.000, 3.000], mean observation: 0.099 [-0.681, 1.000], loss: 3.757675, mean_absolute_error: 22.161913, mean_q: 29.569754
 1038601/1500000: episode: 1787, duration: 13.530s, episode steps: 302, steps per second: 22, episode reward: 245.306, mean reward: 0.812 [-3.472, 100.000], mean action: 1.543 [0.000, 3.000], mean observation: 0.048 [-0.706, 1.000], loss: 4.238852, mean_absolute_error: 22.017212, mean_q: 29.431700
 1039307/1500000: episode: 1788, duration: 16.562s, episode steps: 706, steps per second: 43, episode reward: 183.280, mean reward: 0.260 [-13.836, 100.000], mean action: 1.040 [0.000, 3.000], mean observation: 0.058 [-0.687, 1.000], loss: 2.556882, mean_absolute_error: 22.226557, mean_q: 29.730236
 1039931/1500000: episode: 1789, duration: 10.483s, episode steps: 624, steps per second: 60, episode reward: 100.473, mean reward: 0.161 [-11.665, 100.000], mean action: 2.027 [0.000, 3.000], mean observation: 0.069 [-0.422, 1.000], loss: 3.170428, mean_absolute_error: 22.369738, mean_q: 29.896700
 1040525/1500000: episode: 1790, duration: 13.269s, episode steps: 594, steps per second: 45, episode reward: 157.356, mean reward: 0.265 [-20.858, 100.000], mean action: 1.919 [0.000, 3.000], mean observation: 0.140 [-0.457, 1.000], loss: 4.147410, mean_absolute_error: 22.483639, mean_q: 30.039171
 1040877/1500000: episode: 1791, duration: 9.605s, episode steps: 352, steps per second: 37, episode reward: 169.657, mean reward: 0.482 [-10.745, 100.000], mean action: 1.386 [0.000, 3.000], mean observation: 0.040 [-0.411, 1.000], loss: 3.395127, mean_absolute_error: 22.395626, mean_q: 29.902130
 1041399/1500000: episode: 1792, duration: 11.986s, episode steps: 522, steps per second: 44, episode reward: 145.376, mean reward: 0.278 [-20.019, 100.000], mean action: 1.073 [0.000, 3.000], mean observation: 0.091 [-0.456, 1.000], loss: 2.750366, mean_absolute_error: 22.389198, mean_q: 29.906628
 1041834/1500000: episode: 1793, duration: 7.911s, episode steps: 435, steps per second: 55, episode reward: 189.862, mean reward: 0.436 [-17.453, 100.000], mean action: 1.446 [0.000, 3.000], mean observation: 0.076 [-0.495, 1.000], loss: 4.583004, mean_absolute_error: 22.363026, mean_q: 29.848640
 1042193/1500000: episode: 1794, duration: 10.356s, episode steps: 359, steps per second: 35, episode reward: 145.543, mean reward: 0.405 [-9.915, 100.000], mean action: 1.694 [0.000, 3.000], mean observation: -0.005 [-0.561, 1.000], loss: 3.050122, mean_absolute_error: 22.178370, mean_q: 29.599148
 1042464/1500000: episode: 1795, duration: 4.174s, episode steps: 271, steps per second: 65, episode reward: 209.416, mean reward: 0.773 [-7.746, 100.000], mean action: 1.542 [0.000, 3.000], mean observation: 0.051 [-0.597, 1.000], loss: 2.959974, mean_absolute_error: 22.283964, mean_q: 29.786037
 1042762/1500000: episode: 1796, duration: 9.331s, episode steps: 298, steps per second: 32, episode reward: 205.630, mean reward: 0.690 [-8.990, 100.000], mean action: 1.544 [0.000, 3.000], mean observation: 0.071 [-0.446, 1.000], loss: 3.272619, mean_absolute_error: 22.474882, mean_q: 29.998899
 1043186/1500000: episode: 1797, duration: 9.489s, episode steps: 424, steps per second: 45, episode reward: 192.349, mean reward: 0.454 [-17.466, 100.000], mean action: 1.396 [0.000, 3.000], mean observation: 0.079 [-0.486, 1.000], loss: 3.184681, mean_absolute_error: 22.365469, mean_q: 29.900581
 1043524/1500000: episode: 1798, duration: 6.906s, episode steps: 338, steps per second: 49, episode reward: 194.405, mean reward: 0.575 [-11.409, 100.000], mean action: 1.651 [0.000, 3.000], mean observation: 0.051 [-0.407, 1.000], loss: 2.829341, mean_absolute_error: 22.426254, mean_q: 29.954943
 1044138/1500000: episode: 1799, duration: 14.128s, episode steps: 614, steps per second: 43, episode reward: 176.338, mean reward: 0.287 [-17.504, 100.000], mean action: 1.581 [0.000, 3.000], mean observation: 0.111 [-0.777, 1.000], loss: 3.411297, mean_absolute_error: 22.653679, mean_q: 30.300470
 1044686/1500000: episode: 1800, duration: 13.305s, episode steps: 548, steps per second: 41, episode reward: 177.286, mean reward: 0.324 [-18.779, 100.000], mean action: 1.303 [0.000, 3.000], mean observation: 0.103 [-0.379, 1.000], loss: 4.915619, mean_absolute_error: 22.697800, mean_q: 30.330894
 1045156/1500000: episode: 1801, duration: 11.271s, episode steps: 470, steps per second: 42, episode reward: 167.681, mean reward: 0.357 [-12.318, 100.000], mean action: 1.628 [0.000, 3.000], mean observation: 0.041 [-0.436, 1.027], loss: 2.975094, mean_absolute_error: 22.891787, mean_q: 30.614943
 1045452/1500000: episode: 1802, duration: 4.898s, episode steps: 296, steps per second: 60, episode reward: 184.245, mean reward: 0.622 [-2.827, 100.000], mean action: 1.520 [0.000, 3.000], mean observation: 0.047 [-0.503, 1.000], loss: 3.556265, mean_absolute_error: 22.739130, mean_q: 30.430632
 1046213/1500000: episode: 1803, duration: 16.222s, episode steps: 761, steps per second: 47, episode reward: 151.439, mean reward: 0.199 [-20.224, 100.000], mean action: 1.908 [0.000, 3.000], mean observation: 0.135 [-0.380, 1.000], loss: 2.928907, mean_absolute_error: 22.786753, mean_q: 30.458202
 1046570/1500000: episode: 1804, duration: 9.608s, episode steps: 357, steps per second: 37, episode reward: 224.571, mean reward: 0.629 [-9.869, 100.000], mean action: 1.426 [0.000, 3.000], mean observation: 0.042 [-0.681, 1.000], loss: 4.314431, mean_absolute_error: 22.889723, mean_q: 30.603199
 1047052/1500000: episode: 1805, duration: 9.621s, episode steps: 482, steps per second: 50, episode reward: 186.837, mean reward: 0.388 [-19.372, 100.000], mean action: 1.369 [0.000, 3.000], mean observation: 0.065 [-0.410, 1.000], loss: 2.552301, mean_absolute_error: 22.770739, mean_q: 30.457623
 1047470/1500000: episode: 1806, duration: 8.323s, episode steps: 418, steps per second: 50, episode reward: 198.711, mean reward: 0.475 [-19.718, 100.000], mean action: 1.433 [0.000, 3.000], mean observation: 0.078 [-0.384, 1.000], loss: 3.455730, mean_absolute_error: 22.802814, mean_q: 30.476254
 1048183/1500000: episode: 1807, duration: 15.502s, episode steps: 713, steps per second: 46, episode reward: 186.562, mean reward: 0.262 [-20.396, 100.000], mean action: 0.840 [0.000, 3.000], mean observation: 0.139 [-0.419, 1.000], loss: 4.088936, mean_absolute_error: 22.990791, mean_q: 30.729609
 1048547/1500000: episode: 1808, duration: 9.551s, episode steps: 364, steps per second: 38, episode reward: 234.621, mean reward: 0.645 [-10.468, 100.000], mean action: 1.445 [0.000, 3.000], mean observation: 0.082 [-0.526, 1.000], loss: 2.187792, mean_absolute_error: 23.164057, mean_q: 30.987734
 1048844/1500000: episode: 1809, duration: 4.699s, episode steps: 297, steps per second: 63, episode reward: 243.600, mean reward: 0.820 [-3.023, 100.000], mean action: 1.781 [0.000, 3.000], mean observation: 0.027 [-0.789, 1.000], loss: 4.145614, mean_absolute_error: 23.331337, mean_q: 31.183929
 1049141/1500000: episode: 1810, duration: 8.473s, episode steps: 297, steps per second: 35, episode reward: 195.099, mean reward: 0.657 [-8.353, 100.000], mean action: 1.481 [0.000, 3.000], mean observation: 0.059 [-0.450, 1.000], loss: 2.478422, mean_absolute_error: 23.312469, mean_q: 31.200119
 1049643/1500000: episode: 1811, duration: 8.060s, episode steps: 502, steps per second: 62, episode reward: 145.226, mean reward: 0.289 [-17.287, 100.000], mean action: 1.418 [0.000, 3.000], mean observation: 0.080 [-0.464, 1.000], loss: 2.542616, mean_absolute_error: 23.292459, mean_q: 31.185238
 1050232/1500000: episode: 1812, duration: 13.033s, episode steps: 589, steps per second: 45, episode reward: 147.006, mean reward: 0.250 [-18.664, 100.000], mean action: 1.630 [0.000, 3.000], mean observation: 0.099 [-0.424, 1.000], loss: 3.186703, mean_absolute_error: 23.366514, mean_q: 31.281601
 1050549/1500000: episode: 1813, duration: 8.668s, episode steps: 317, steps per second: 37, episode reward: 164.161, mean reward: 0.518 [-10.086, 100.000], mean action: 1.312 [0.000, 3.000], mean observation: 0.023 [-0.511, 1.000], loss: 4.901952, mean_absolute_error: 23.849251, mean_q: 31.976988
 1050909/1500000: episode: 1814, duration: 5.541s, episode steps: 360, steps per second: 65, episode reward: 222.634, mean reward: 0.618 [-11.661, 100.000], mean action: 1.531 [0.000, 3.000], mean observation: 0.062 [-0.624, 1.000], loss: 3.531080, mean_absolute_error: 23.530571, mean_q: 31.538877
 1051215/1500000: episode: 1815, duration: 6.138s, episode steps: 306, steps per second: 50, episode reward: 226.329, mean reward: 0.740 [-9.128, 100.000], mean action: 1.422 [0.000, 3.000], mean observation: 0.062 [-0.611, 1.000], loss: 4.067111, mean_absolute_error: 23.558338, mean_q: 31.562605
 1051532/1500000: episode: 1816, duration: 6.916s, episode steps: 317, steps per second: 46, episode reward: 230.329, mean reward: 0.727 [-10.911, 100.000], mean action: 1.123 [0.000, 3.000], mean observation: 0.098 [-0.581, 1.000], loss: 3.979870, mean_absolute_error: 23.713245, mean_q: 31.813251
 1051935/1500000: episode: 1817, duration: 7.765s, episode steps: 403, steps per second: 52, episode reward: 194.308, mean reward: 0.482 [-9.311, 100.000], mean action: 1.169 [0.000, 3.000], mean observation: 0.098 [-0.479, 1.000], loss: 3.812423, mean_absolute_error: 23.881512, mean_q: 32.045017
 1052424/1500000: episode: 1818, duration: 9.864s, episode steps: 489, steps per second: 50, episode reward: 217.537, mean reward: 0.445 [-19.856, 100.000], mean action: 1.084 [0.000, 3.000], mean observation: 0.123 [-0.529, 1.000], loss: 2.856151, mean_absolute_error: 23.805143, mean_q: 31.917490
 1052811/1500000: episode: 1819, duration: 9.663s, episode steps: 387, steps per second: 40, episode reward: 221.100, mean reward: 0.571 [-2.829, 100.000], mean action: 1.419 [0.000, 3.000], mean observation: 0.083 [-0.599, 1.000], loss: 4.034368, mean_absolute_error: 23.824268, mean_q: 31.954714
 1053123/1500000: episode: 1820, duration: 5.102s, episode steps: 312, steps per second: 61, episode reward: 200.086, mean reward: 0.641 [-10.391, 100.000], mean action: 1.571 [0.000, 3.000], mean observation: 0.063 [-0.387, 1.000], loss: 3.418370, mean_absolute_error: 24.076935, mean_q: 32.295517
 1053500/1500000: episode: 1821, duration: 9.167s, episode steps: 377, steps per second: 41, episode reward: 184.473, mean reward: 0.489 [-13.358, 100.000], mean action: 1.340 [0.000, 3.000], mean observation: 0.035 [-0.478, 1.000], loss: 3.329516, mean_absolute_error: 23.854210, mean_q: 32.009689
 1053801/1500000: episode: 1822, duration: 4.536s, episode steps: 301, steps per second: 66, episode reward: 190.437, mean reward: 0.633 [-12.584, 100.000], mean action: 1.631 [0.000, 3.000], mean observation: 0.058 [-0.364, 1.000], loss: 2.417750, mean_absolute_error: 24.014408, mean_q: 32.250412
 1054175/1500000: episode: 1823, duration: 8.746s, episode steps: 374, steps per second: 43, episode reward: 183.513, mean reward: 0.491 [-12.604, 100.000], mean action: 1.377 [0.000, 3.000], mean observation: 0.083 [-0.481, 1.000], loss: 1.869913, mean_absolute_error: 24.072657, mean_q: 32.309135
 1054677/1500000: episode: 1824, duration: 8.670s, episode steps: 502, steps per second: 58, episode reward: 212.664, mean reward: 0.424 [-19.248, 100.000], mean action: 1.040 [0.000, 3.000], mean observation: 0.143 [-0.544, 1.000], loss: 3.056837, mean_absolute_error: 24.185970, mean_q: 32.435863
 1055233/1500000: episode: 1825, duration: 12.367s, episode steps: 556, steps per second: 45, episode reward: 199.720, mean reward: 0.359 [-17.760, 100.000], mean action: 1.018 [0.000, 3.000], mean observation: 0.120 [-0.573, 1.000], loss: 2.098541, mean_absolute_error: 24.335945, mean_q: 32.650990
 1055570/1500000: episode: 1826, duration: 6.398s, episode steps: 337, steps per second: 53, episode reward: 223.506, mean reward: 0.663 [-7.390, 100.000], mean action: 1.353 [0.000, 3.000], mean observation: 0.096 [-0.499, 1.000], loss: 1.814402, mean_absolute_error: 24.279787, mean_q: 32.569206
 1055881/1500000: episode: 1827, duration: 7.035s, episode steps: 311, steps per second: 44, episode reward: 218.658, mean reward: 0.703 [-10.174, 100.000], mean action: 1.421 [0.000, 3.000], mean observation: 0.073 [-0.447, 1.015], loss: 3.088879, mean_absolute_error: 24.287289, mean_q: 32.589142
 1056881/1500000: episode: 1828, duration: 19.463s, episode steps: 1000, steps per second: 51, episode reward: 89.538, mean reward: 0.090 [-20.340, 21.785], mean action: 1.015 [0.000, 3.000], mean observation: 0.189 [-0.731, 1.000], loss: 2.688148, mean_absolute_error: 24.488073, mean_q: 32.888294
 1057169/1500000: episode: 1829, duration: 8.050s, episode steps: 288, steps per second: 36, episode reward: 183.722, mean reward: 0.638 [-13.954, 100.000], mean action: 1.479 [0.000, 3.000], mean observation: 0.003 [-0.696, 1.000], loss: 2.772685, mean_absolute_error: 24.846378, mean_q: 33.408371
 1057458/1500000: episode: 1830, duration: 4.476s, episode steps: 289, steps per second: 65, episode reward: 241.146, mean reward: 0.834 [-13.085, 100.000], mean action: 1.699 [0.000, 3.000], mean observation: 0.030 [-0.743, 1.000], loss: 3.201810, mean_absolute_error: 24.894381, mean_q: 33.410980
 1057863/1500000: episode: 1831, duration: 9.708s, episode steps: 405, steps per second: 42, episode reward: 182.906, mean reward: 0.452 [-19.359, 100.000], mean action: 1.089 [0.000, 3.000], mean observation: 0.075 [-0.696, 1.000], loss: 2.567686, mean_absolute_error: 24.730093, mean_q: 33.197845
 1058161/1500000: episode: 1832, duration: 4.812s, episode steps: 298, steps per second: 62, episode reward: 202.442, mean reward: 0.679 [-2.926, 100.000], mean action: 1.312 [0.000, 3.000], mean observation: 0.083 [-0.501, 1.000], loss: 3.658623, mean_absolute_error: 25.142044, mean_q: 33.791328
 1058460/1500000: episode: 1833, duration: 4.928s, episode steps: 299, steps per second: 61, episode reward: 183.401, mean reward: 0.613 [-14.796, 100.000], mean action: 1.699 [0.000, 3.000], mean observation: -0.004 [-0.733, 1.000], loss: 2.217466, mean_absolute_error: 25.114752, mean_q: 33.769650
 1058771/1500000: episode: 1834, duration: 8.051s, episode steps: 311, steps per second: 39, episode reward: 202.541, mean reward: 0.651 [-9.711, 100.000], mean action: 1.267 [0.000, 3.000], mean observation: 0.064 [-0.474, 1.000], loss: 2.358382, mean_absolute_error: 25.313112, mean_q: 34.038189
 1059100/1500000: episode: 1835, duration: 5.134s, episode steps: 329, steps per second: 64, episode reward: 215.064, mean reward: 0.654 [-17.614, 100.000], mean action: 1.301 [0.000, 3.000], mean observation: 0.081 [-0.444, 1.000], loss: 4.449475, mean_absolute_error: 25.228979, mean_q: 33.908165
 1059434/1500000: episode: 1836, duration: 8.639s, episode steps: 334, steps per second: 39, episode reward: 231.239, mean reward: 0.692 [-2.782, 100.000], mean action: 1.386 [0.000, 3.000], mean observation: 0.091 [-0.444, 1.000], loss: 2.248050, mean_absolute_error: 25.258610, mean_q: 33.941650
 1059919/1500000: episode: 1837, duration: 7.556s, episode steps: 485, steps per second: 64, episode reward: 207.272, mean reward: 0.427 [-18.845, 100.000], mean action: 0.918 [0.000, 3.000], mean observation: 0.120 [-0.601, 1.000], loss: 3.688823, mean_absolute_error: 25.390589, mean_q: 34.126305
 1060221/1500000: episode: 1838, duration: 8.365s, episode steps: 302, steps per second: 36, episode reward: 201.413, mean reward: 0.667 [-12.367, 100.000], mean action: 1.517 [0.000, 3.000], mean observation: 0.058 [-0.380, 1.000], loss: 4.016064, mean_absolute_error: 25.424503, mean_q: 34.171967
 1060513/1500000: episode: 1839, duration: 4.863s, episode steps: 292, steps per second: 60, episode reward: 125.256, mean reward: 0.429 [-14.604, 100.000], mean action: 1.363 [0.000, 3.000], mean observation: 0.005 [-0.519, 1.000], loss: 3.264979, mean_absolute_error: 25.625708, mean_q: 34.437828
 1060935/1500000: episode: 1840, duration: 9.985s, episode steps: 422, steps per second: 42, episode reward: 160.077, mean reward: 0.379 [-17.550, 100.000], mean action: 1.427 [0.000, 3.000], mean observation: 0.073 [-0.458, 1.000], loss: 2.547202, mean_absolute_error: 25.747566, mean_q: 34.658749
 1061263/1500000: episode: 1841, duration: 5.101s, episode steps: 328, steps per second: 64, episode reward: 212.629, mean reward: 0.648 [-2.757, 100.000], mean action: 1.348 [0.000, 3.000], mean observation: 0.081 [-0.541, 1.000], loss: 2.225706, mean_absolute_error: 25.683195, mean_q: 34.539734
 1061676/1500000: episode: 1842, duration: 10.090s, episode steps: 413, steps per second: 41, episode reward: 182.193, mean reward: 0.441 [-19.244, 100.000], mean action: 1.375 [0.000, 3.000], mean observation: 0.091 [-0.388, 1.000], loss: 5.065589, mean_absolute_error: 25.737307, mean_q: 34.582108
 1062025/1500000: episode: 1843, duration: 5.273s, episode steps: 349, steps per second: 66, episode reward: 201.900, mean reward: 0.579 [-2.891, 100.000], mean action: 1.347 [0.000, 3.000], mean observation: 0.079 [-0.502, 1.000], loss: 4.678633, mean_absolute_error: 25.931856, mean_q: 34.859051
 1062344/1500000: episode: 1844, duration: 8.650s, episode steps: 319, steps per second: 37, episode reward: 228.421, mean reward: 0.716 [-9.462, 100.000], mean action: 1.414 [0.000, 3.000], mean observation: 0.069 [-0.652, 1.000], loss: 4.293306, mean_absolute_error: 25.832857, mean_q: 34.720318
 1062757/1500000: episode: 1845, duration: 6.454s, episode steps: 413, steps per second: 64, episode reward: 153.713, mean reward: 0.372 [-13.417, 100.000], mean action: 2.116 [0.000, 3.000], mean observation: 0.097 [-0.533, 1.000], loss: 3.144142, mean_absolute_error: 26.026455, mean_q: 35.002056
 1063140/1500000: episode: 1846, duration: 9.385s, episode steps: 383, steps per second: 41, episode reward: 206.089, mean reward: 0.538 [-3.018, 100.000], mean action: 1.339 [0.000, 3.000], mean observation: 0.089 [-0.368, 1.000], loss: 3.854781, mean_absolute_error: 25.821321, mean_q: 34.689213
 1063451/1500000: episode: 1847, duration: 4.770s, episode steps: 311, steps per second: 65, episode reward: 181.509, mean reward: 0.584 [-3.052, 100.000], mean action: 1.476 [0.000, 3.000], mean observation: 0.052 [-0.390, 1.000], loss: 3.743033, mean_absolute_error: 25.744732, mean_q: 34.600147
 1063834/1500000: episode: 1848, duration: 9.508s, episode steps: 383, steps per second: 40, episode reward: 157.469, mean reward: 0.411 [-11.761, 100.000], mean action: 1.924 [0.000, 3.000], mean observation: 0.050 [-0.575, 1.000], loss: 3.290121, mean_absolute_error: 26.051964, mean_q: 35.027462
 1064317/1500000: episode: 1849, duration: 7.652s, episode steps: 483, steps per second: 63, episode reward: 184.101, mean reward: 0.381 [-17.511, 100.000], mean action: 1.145 [0.000, 3.000], mean observation: 0.111 [-0.346, 1.000], loss: 3.411049, mean_absolute_error: 25.826572, mean_q: 34.706799
 1064766/1500000: episode: 1850, duration: 10.486s, episode steps: 449, steps per second: 43, episode reward: 181.361, mean reward: 0.404 [-22.989, 100.000], mean action: 1.192 [0.000, 3.000], mean observation: 0.099 [-0.373, 1.000], loss: 2.506618, mean_absolute_error: 26.079369, mean_q: 35.096882
 1065285/1500000: episode: 1851, duration: 10.136s, episode steps: 519, steps per second: 51, episode reward: 206.648, mean reward: 0.398 [-18.112, 100.000], mean action: 0.961 [0.000, 3.000], mean observation: 0.136 [-0.435, 1.000], loss: 3.441938, mean_absolute_error: 25.908918, mean_q: 34.818108
 1065655/1500000: episode: 1852, duration: 7.067s, episode steps: 370, steps per second: 52, episode reward: 221.325, mean reward: 0.598 [-2.825, 100.000], mean action: 1.457 [0.000, 3.000], mean observation: 0.074 [-0.469, 1.000], loss: 2.358951, mean_absolute_error: 25.814610, mean_q: 34.693977
 1066007/1500000: episode: 1853, duration: 7.243s, episode steps: 352, steps per second: 49, episode reward: 147.558, mean reward: 0.419 [-18.032, 100.000], mean action: 1.298 [0.000, 3.000], mean observation: 0.025 [-0.557, 1.000], loss: 2.699322, mean_absolute_error: 25.860502, mean_q: 34.813648
 1066601/1500000: episode: 1854, duration: 11.112s, episode steps: 594, steps per second: 53, episode reward: 215.169, mean reward: 0.362 [-17.498, 100.000], mean action: 1.202 [0.000, 3.000], mean observation: 0.127 [-0.501, 1.000], loss: 3.220489, mean_absolute_error: 25.793787, mean_q: 34.697201
 1066914/1500000: episode: 1855, duration: 8.377s, episode steps: 313, steps per second: 37, episode reward: 205.905, mean reward: 0.658 [-17.362, 100.000], mean action: 1.281 [0.000, 3.000], mean observation: 0.065 [-0.441, 1.000], loss: 2.704866, mean_absolute_error: 25.855110, mean_q: 34.790596
 1067285/1500000: episode: 1856, duration: 5.736s, episode steps: 371, steps per second: 65, episode reward: 231.042, mean reward: 0.623 [-10.733, 100.000], mean action: 1.375 [0.000, 3.000], mean observation: 0.034 [-0.741, 1.000], loss: 3.328513, mean_absolute_error: 25.903997, mean_q: 34.870941
 1067669/1500000: episode: 1857, duration: 9.809s, episode steps: 384, steps per second: 39, episode reward: 248.616, mean reward: 0.647 [-9.369, 100.000], mean action: 1.112 [0.000, 3.000], mean observation: 0.054 [-0.775, 1.000], loss: 3.935610, mean_absolute_error: 25.808748, mean_q: 34.709644
 1067925/1500000: episode: 1858, duration: 4.305s, episode steps: 256, steps per second: 59, episode reward: 244.025, mean reward: 0.953 [-7.567, 100.000], mean action: 1.590 [0.000, 3.000], mean observation: 0.038 [-0.747, 1.000], loss: 3.230126, mean_absolute_error: 26.126282, mean_q: 35.128880
 1068386/1500000: episode: 1859, duration: 10.796s, episode steps: 461, steps per second: 43, episode reward: 197.988, mean reward: 0.429 [-12.416, 100.000], mean action: 1.388 [0.000, 3.000], mean observation: 0.099 [-0.357, 1.000], loss: 3.806239, mean_absolute_error: 25.976431, mean_q: 34.980785
 1069299/1500000: episode: 1860, duration: 18.772s, episode steps: 913, steps per second: 49, episode reward: 179.122, mean reward: 0.196 [-19.457, 100.000], mean action: 1.817 [0.000, 3.000], mean observation: 0.167 [-0.576, 1.000], loss: 3.141627, mean_absolute_error: 25.935938, mean_q: 34.887394
 1069829/1500000: episode: 1861, duration: 11.911s, episode steps: 530, steps per second: 44, episode reward: 177.374, mean reward: 0.335 [-19.505, 100.000], mean action: 0.934 [0.000, 3.000], mean observation: 0.140 [-0.463, 1.000], loss: 3.382039, mean_absolute_error: 25.912607, mean_q: 34.852440
 1070099/1500000: episode: 1862, duration: 4.978s, episode steps: 270, steps per second: 54, episode reward: 260.571, mean reward: 0.965 [-10.255, 100.000], mean action: 1.778 [0.000, 3.000], mean observation: 0.018 [-0.788, 1.000], loss: 3.394496, mean_absolute_error: 25.995853, mean_q: 34.964020
 1070467/1500000: episode: 1863, duration: 9.935s, episode steps: 368, steps per second: 37, episode reward: 190.043, mean reward: 0.516 [-17.898, 100.000], mean action: 1.092 [0.000, 3.000], mean observation: 0.063 [-0.472, 1.000], loss: 2.242486, mean_absolute_error: 25.986647, mean_q: 34.989857
 1070829/1500000: episode: 1864, duration: 9.732s, episode steps: 362, steps per second: 37, episode reward: 214.295, mean reward: 0.592 [-3.439, 100.000], mean action: 1.635 [0.000, 3.000], mean observation: 0.078 [-0.504, 1.000], loss: 3.398173, mean_absolute_error: 25.987564, mean_q: 34.928764
 1071279/1500000: episode: 1865, duration: 9.828s, episode steps: 450, steps per second: 46, episode reward: 193.909, mean reward: 0.431 [-18.965, 100.000], mean action: 1.316 [0.000, 3.000], mean observation: 0.107 [-0.485, 1.000], loss: 3.451744, mean_absolute_error: 26.156673, mean_q: 35.148010
 1071741/1500000: episode: 1866, duration: 9.379s, episode steps: 462, steps per second: 49, episode reward: 153.268, mean reward: 0.332 [-19.811, 100.000], mean action: 1.223 [0.000, 3.000], mean observation: 0.078 [-0.517, 1.000], loss: 2.302408, mean_absolute_error: 26.152876, mean_q: 35.141804
 1072094/1500000: episode: 1867, duration: 9.170s, episode steps: 353, steps per second: 38, episode reward: 215.427, mean reward: 0.610 [-10.639, 100.000], mean action: 1.295 [0.000, 3.000], mean observation: 0.085 [-0.548, 1.000], loss: 2.943765, mean_absolute_error: 26.036961, mean_q: 35.008057
 1072441/1500000: episode: 1868, duration: 5.315s, episode steps: 347, steps per second: 65, episode reward: 219.506, mean reward: 0.633 [-9.988, 100.000], mean action: 1.686 [0.000, 3.000], mean observation: 0.055 [-0.527, 1.022], loss: 2.483969, mean_absolute_error: 26.201025, mean_q: 35.243965
 1072888/1500000: episode: 1869, duration: 10.667s, episode steps: 447, steps per second: 42, episode reward: 198.063, mean reward: 0.443 [-17.634, 100.000], mean action: 1.181 [0.000, 3.000], mean observation: 0.073 [-0.403, 1.000], loss: 3.848530, mean_absolute_error: 26.268089, mean_q: 35.316357
 1073207/1500000: episode: 1870, duration: 4.988s, episode steps: 319, steps per second: 64, episode reward: 197.426, mean reward: 0.619 [-11.558, 100.000], mean action: 1.486 [0.000, 3.000], mean observation: 0.060 [-0.520, 1.000], loss: 3.478995, mean_absolute_error: 26.286819, mean_q: 35.330101
 1073516/1500000: episode: 1871, duration: 8.512s, episode steps: 309, steps per second: 36, episode reward: 171.226, mean reward: 0.554 [-15.334, 100.000], mean action: 1.456 [0.000, 3.000], mean observation: 0.016 [-0.809, 1.000], loss: 2.603954, mean_absolute_error: 26.391222, mean_q: 35.488834
 1073870/1500000: episode: 1872, duration: 5.480s, episode steps: 354, steps per second: 65, episode reward: 236.114, mean reward: 0.667 [-11.479, 100.000], mean action: 1.545 [0.000, 3.000], mean observation: 0.059 [-0.626, 1.045], loss: 3.161932, mean_absolute_error: 26.446190, mean_q: 35.530590
 1074459/1500000: episode: 1873, duration: 12.943s, episode steps: 589, steps per second: 46, episode reward: 200.989, mean reward: 0.341 [-18.965, 100.000], mean action: 0.930 [0.000, 3.000], mean observation: 0.135 [-0.534, 1.000], loss: 3.369076, mean_absolute_error: 26.411716, mean_q: 35.486824
 1074824/1500000: episode: 1874, duration: 6.375s, episode steps: 365, steps per second: 57, episode reward: 219.037, mean reward: 0.600 [-3.142, 100.000], mean action: 1.340 [0.000, 3.000], mean observation: 0.087 [-0.450, 1.000], loss: 3.372080, mean_absolute_error: 26.593048, mean_q: 35.738045
 1075299/1500000: episode: 1875, duration: 10.267s, episode steps: 475, steps per second: 46, episode reward: 201.715, mean reward: 0.425 [-17.614, 100.000], mean action: 1.251 [0.000, 3.000], mean observation: 0.102 [-0.451, 1.004], loss: 3.215608, mean_absolute_error: 26.693092, mean_q: 35.859039
 1075737/1500000: episode: 1876, duration: 10.791s, episode steps: 438, steps per second: 41, episode reward: 167.086, mean reward: 0.381 [-14.477, 100.000], mean action: 1.315 [0.000, 3.000], mean observation: 0.058 [-0.632, 1.000], loss: 3.012296, mean_absolute_error: 26.780233, mean_q: 36.008522
 1076244/1500000: episode: 1877, duration: 10.087s, episode steps: 507, steps per second: 50, episode reward: 206.654, mean reward: 0.408 [-18.754, 100.000], mean action: 0.976 [0.000, 3.000], mean observation: 0.115 [-0.484, 1.030], loss: 3.709552, mean_absolute_error: 26.624985, mean_q: 35.784042
 1076500/1500000: episode: 1878, duration: 6.192s, episode steps: 256, steps per second: 41, episode reward: 221.137, mean reward: 0.864 [-3.220, 100.000], mean action: 1.578 [0.000, 3.000], mean observation: 0.006 [-0.732, 1.000], loss: 2.393400, mean_absolute_error: 27.071556, mean_q: 36.426998
 1076811/1500000: episode: 1879, duration: 6.037s, episode steps: 311, steps per second: 52, episode reward: 226.115, mean reward: 0.727 [-3.352, 100.000], mean action: 1.514 [0.000, 3.000], mean observation: 0.082 [-0.593, 1.000], loss: 3.424924, mean_absolute_error: 26.974430, mean_q: 36.272861
 1077186/1500000: episode: 1880, duration: 9.078s, episode steps: 375, steps per second: 41, episode reward: 212.521, mean reward: 0.567 [-10.712, 100.000], mean action: 1.309 [0.000, 3.000], mean observation: 0.089 [-0.501, 1.000], loss: 2.826671, mean_absolute_error: 27.031044, mean_q: 36.375767
 1077467/1500000: episode: 1881, duration: 6.646s, episode steps: 281, steps per second: 42, episode reward: 173.957, mean reward: 0.619 [-2.769, 100.000], mean action: 1.456 [0.000, 3.000], mean observation: 0.024 [-0.472, 1.000], loss: 2.262907, mean_absolute_error: 27.276104, mean_q: 36.680557
 1077841/1500000: episode: 1882, duration: 9.220s, episode steps: 374, steps per second: 41, episode reward: 224.780, mean reward: 0.601 [-11.639, 100.000], mean action: 1.193 [0.000, 3.000], mean observation: 0.113 [-0.644, 1.000], loss: 3.636161, mean_absolute_error: 27.175673, mean_q: 36.517956
 1078243/1500000: episode: 1883, duration: 8.876s, episode steps: 402, steps per second: 45, episode reward: 246.220, mean reward: 0.612 [-18.296, 100.000], mean action: 1.443 [0.000, 3.000], mean observation: 0.079 [-0.682, 1.054], loss: 2.727019, mean_absolute_error: 27.379116, mean_q: 36.826973
 1078633/1500000: episode: 1884, duration: 4.513s, episode steps: 390, steps per second: 86, episode reward: 176.130, mean reward: 0.452 [-9.880, 100.000], mean action: 1.318 [0.000, 3.000], mean observation: 0.077 [-0.379, 1.000], loss: 3.404058, mean_absolute_error: 27.670059, mean_q: 37.170238
 1079086/1500000: episode: 1885, duration: 7.813s, episode steps: 453, steps per second: 58, episode reward: 230.586, mean reward: 0.509 [-17.628, 100.000], mean action: 1.073 [0.000, 3.000], mean observation: 0.121 [-0.625, 1.029], loss: 4.019735, mean_absolute_error: 27.545374, mean_q: 37.052498
 1079324/1500000: episode: 1886, duration: 3.311s, episode steps: 238, steps per second: 72, episode reward: 199.624, mean reward: 0.839 [-3.154, 100.000], mean action: 1.399 [0.000, 3.000], mean observation: 0.034 [-0.502, 1.000], loss: 3.881718, mean_absolute_error: 27.747330, mean_q: 37.306763
 1079582/1500000: episode: 1887, duration: 2.873s, episode steps: 258, steps per second: 90, episode reward: 212.135, mean reward: 0.822 [-8.714, 100.000], mean action: 1.473 [0.000, 3.000], mean observation: 0.034 [-0.668, 1.000], loss: 3.346882, mean_absolute_error: 27.849548, mean_q: 37.439648
 1079937/1500000: episode: 1888, duration: 4.166s, episode steps: 355, steps per second: 85, episode reward: 166.848, mean reward: 0.470 [-19.688, 100.000], mean action: 2.169 [0.000, 3.000], mean observation: 0.057 [-0.579, 1.000], loss: 2.582469, mean_absolute_error: 28.075190, mean_q: 37.715599
 1080201/1500000: episode: 1889, duration: 3.085s, episode steps: 264, steps per second: 86, episode reward: 234.871, mean reward: 0.890 [-9.655, 100.000], mean action: 1.523 [0.000, 3.000], mean observation: 0.013 [-0.717, 1.000], loss: 3.140863, mean_absolute_error: 27.953392, mean_q: 37.597736
 1080622/1500000: episode: 1890, duration: 7.693s, episode steps: 421, steps per second: 55, episode reward: 198.840, mean reward: 0.472 [-17.694, 100.000], mean action: 1.342 [0.000, 3.000], mean observation: 0.078 [-0.562, 1.031], loss: 3.971035, mean_absolute_error: 28.027905, mean_q: 37.690701
 1081014/1500000: episode: 1891, duration: 4.516s, episode steps: 392, steps per second: 87, episode reward: 190.375, mean reward: 0.486 [-4.984, 100.000], mean action: 1.357 [0.000, 3.000], mean observation: 0.097 [-0.528, 1.000], loss: 3.469927, mean_absolute_error: 28.223785, mean_q: 37.950848
 1081390/1500000: episode: 1892, duration: 4.539s, episode steps: 376, steps per second: 83, episode reward: 242.579, mean reward: 0.645 [-8.138, 100.000], mean action: 1.566 [0.000, 3.000], mean observation: 0.062 [-0.771, 1.000], loss: 2.456338, mean_absolute_error: 28.213091, mean_q: 37.942112
 1081890/1500000: episode: 1893, duration: 6.223s, episode steps: 500, steps per second: 80, episode reward: 224.748, mean reward: 0.449 [-19.430, 100.000], mean action: 0.948 [0.000, 3.000], mean observation: 0.127 [-0.539, 1.000], loss: 3.848248, mean_absolute_error: 28.221918, mean_q: 37.971004
 1082179/1500000: episode: 1894, duration: 3.379s, episode steps: 289, steps per second: 86, episode reward: 215.490, mean reward: 0.746 [-11.604, 100.000], mean action: 1.294 [0.000, 3.000], mean observation: 0.074 [-0.580, 1.000], loss: 3.213246, mean_absolute_error: 28.407253, mean_q: 38.213734
 1082934/1500000: episode: 1895, duration: 12.538s, episode steps: 755, steps per second: 60, episode reward: 212.753, mean reward: 0.282 [-17.640, 100.000], mean action: 0.776 [0.000, 3.000], mean observation: 0.129 [-0.464, 1.000], loss: 2.784218, mean_absolute_error: 28.195255, mean_q: 37.959358
 1083720/1500000: episode: 1896, duration: 10.808s, episode steps: 786, steps per second: 73, episode reward: 205.589, mean reward: 0.262 [-22.741, 100.000], mean action: 0.696 [0.000, 3.000], mean observation: 0.150 [-0.471, 1.000], loss: 3.328763, mean_absolute_error: 28.542009, mean_q: 38.415798
 1084021/1500000: episode: 1897, duration: 3.471s, episode steps: 301, steps per second: 87, episode reward: 217.998, mean reward: 0.724 [-17.592, 100.000], mean action: 1.618 [0.000, 3.000], mean observation: 0.033 [-0.700, 1.000], loss: 4.109987, mean_absolute_error: 28.541529, mean_q: 38.412365
 1084453/1500000: episode: 1898, duration: 5.342s, episode steps: 432, steps per second: 81, episode reward: 145.061, mean reward: 0.336 [-9.832, 100.000], mean action: 1.968 [0.000, 3.000], mean observation: 0.004 [-0.514, 1.000], loss: 3.619594, mean_absolute_error: 28.447691, mean_q: 38.195171
 1084937/1500000: episode: 1899, duration: 8.346s, episode steps: 484, steps per second: 58, episode reward: 146.478, mean reward: 0.303 [-18.472, 100.000], mean action: 0.814 [0.000, 3.000], mean observation: 0.106 [-0.488, 1.000], loss: 3.052350, mean_absolute_error: 28.762045, mean_q: 38.702641
 1085285/1500000: episode: 1900, duration: 3.999s, episode steps: 348, steps per second: 87, episode reward: 190.214, mean reward: 0.547 [-8.358, 100.000], mean action: 1.259 [0.000, 3.000], mean observation: 0.052 [-0.508, 1.000], loss: 4.699063, mean_absolute_error: 28.666803, mean_q: 38.548164
 1085625/1500000: episode: 1901, duration: 4.061s, episode steps: 340, steps per second: 84, episode reward: 156.593, mean reward: 0.461 [-18.255, 100.000], mean action: 1.938 [0.000, 3.000], mean observation: 0.052 [-0.745, 1.000], loss: 3.564234, mean_absolute_error: 28.528561, mean_q: 38.432163
 1086068/1500000: episode: 1902, duration: 5.487s, episode steps: 443, steps per second: 81, episode reward: 187.240, mean reward: 0.423 [-17.632, 100.000], mean action: 1.609 [0.000, 3.000], mean observation: 0.088 [-0.631, 1.004], loss: 2.731596, mean_absolute_error: 28.536407, mean_q: 38.394100
 1086453/1500000: episode: 1903, duration: 4.639s, episode steps: 385, steps per second: 83, episode reward: 165.139, mean reward: 0.429 [-17.712, 100.000], mean action: 1.875 [0.000, 3.000], mean observation: 0.088 [-0.504, 1.000], loss: 2.668016, mean_absolute_error: 28.694174, mean_q: 38.626602
 1086750/1500000: episode: 1904, duration: 3.582s, episode steps: 297, steps per second: 83, episode reward: 198.015, mean reward: 0.667 [-5.670, 100.000], mean action: 1.074 [0.000, 3.000], mean observation: 0.065 [-0.632, 1.011], loss: 5.238513, mean_absolute_error: 28.518183, mean_q: 38.370468
 1087170/1500000: episode: 1905, duration: 7.642s, episode steps: 420, steps per second: 55, episode reward: 209.817, mean reward: 0.500 [-18.262, 100.000], mean action: 1.171 [0.000, 3.000], mean observation: 0.116 [-0.492, 1.000], loss: 3.254705, mean_absolute_error: 28.481680, mean_q: 38.314362
 1087746/1500000: episode: 1906, duration: 7.206s, episode steps: 576, steps per second: 80, episode reward: 213.861, mean reward: 0.371 [-18.682, 100.000], mean action: 0.984 [0.000, 3.000], mean observation: 0.126 [-0.437, 1.014], loss: 3.156780, mean_absolute_error: 28.773552, mean_q: 38.722580
 1088106/1500000: episode: 1907, duration: 4.448s, episode steps: 360, steps per second: 81, episode reward: 228.776, mean reward: 0.635 [-11.889, 100.000], mean action: 1.439 [0.000, 3.000], mean observation: 0.040 [-0.779, 1.000], loss: 3.131475, mean_absolute_error: 28.768003, mean_q: 38.740841
 1088440/1500000: episode: 1908, duration: 3.950s, episode steps: 334, steps per second: 85, episode reward: 193.929, mean reward: 0.581 [-7.594, 100.000], mean action: 1.120 [0.000, 3.000], mean observation: 0.086 [-0.430, 1.000], loss: 3.432568, mean_absolute_error: 28.818342, mean_q: 38.784389
 1088755/1500000: episode: 1909, duration: 3.603s, episode steps: 315, steps per second: 87, episode reward: 234.379, mean reward: 0.744 [-4.615, 100.000], mean action: 1.406 [0.000, 3.000], mean observation: 0.073 [-0.679, 1.000], loss: 1.591607, mean_absolute_error: 28.533762, mean_q: 38.409988
 1089227/1500000: episode: 1910, duration: 5.846s, episode steps: 472, steps per second: 81, episode reward: 234.916, mean reward: 0.498 [-22.854, 100.000], mean action: 1.036 [0.000, 3.000], mean observation: 0.120 [-0.675, 1.000], loss: 3.572426, mean_absolute_error: 28.718838, mean_q: 38.609573
 1089485/1500000: episode: 1911, duration: 3.150s, episode steps: 258, steps per second: 82, episode reward: 195.051, mean reward: 0.756 [-17.443, 100.000], mean action: 1.612 [0.000, 3.000], mean observation: 0.020 [-0.525, 1.000], loss: 3.215452, mean_absolute_error: 28.695141, mean_q: 38.603077
 1089820/1500000: episode: 1912, duration: 4.015s, episode steps: 335, steps per second: 83, episode reward: 218.867, mean reward: 0.653 [-9.594, 100.000], mean action: 1.122 [0.000, 3.000], mean observation: 0.091 [-0.452, 1.012], loss: 4.201461, mean_absolute_error: 28.780445, mean_q: 38.733170
 1090440/1500000: episode: 1913, duration: 9.989s, episode steps: 620, steps per second: 62, episode reward: 183.610, mean reward: 0.296 [-17.394, 100.000], mean action: 0.935 [0.000, 3.000], mean observation: 0.126 [-0.443, 1.000], loss: 3.087449, mean_absolute_error: 28.760572, mean_q: 38.706841
 1090782/1500000: episode: 1914, duration: 4.098s, episode steps: 342, steps per second: 83, episode reward: 230.436, mean reward: 0.674 [-10.041, 100.000], mean action: 1.596 [0.000, 3.000], mean observation: 0.048 [-0.722, 1.000], loss: 1.924167, mean_absolute_error: 28.540552, mean_q: 38.440277
 1090990/1500000: episode: 1915, duration: 2.422s, episode steps: 208, steps per second: 86, episode reward: -39.139, mean reward: -0.188 [-100.000, 13.159], mean action: 1.577 [0.000, 3.000], mean observation: -0.014 [-0.532, 1.000], loss: 2.069490, mean_absolute_error: 28.526085, mean_q: 38.421654
 1091417/1500000: episode: 1916, duration: 4.983s, episode steps: 427, steps per second: 86, episode reward: 224.234, mean reward: 0.525 [-10.623, 100.000], mean action: 1.183 [0.000, 3.000], mean observation: 0.098 [-0.525, 1.000], loss: 4.979661, mean_absolute_error: 28.836088, mean_q: 38.848698
 1091809/1500000: episode: 1917, duration: 4.657s, episode steps: 392, steps per second: 84, episode reward: 227.830, mean reward: 0.581 [-10.772, 100.000], mean action: 1.469 [0.000, 3.000], mean observation: 0.082 [-0.552, 1.026], loss: 2.610713, mean_absolute_error: 28.700787, mean_q: 38.645351
 1092632/1500000: episode: 1918, duration: 10.504s, episode steps: 823, steps per second: 78, episode reward: 114.125, mean reward: 0.139 [-19.153, 100.000], mean action: 2.079 [0.000, 3.000], mean observation: 0.159 [-0.434, 1.000], loss: 3.986813, mean_absolute_error: 28.691065, mean_q: 38.661091
 1092993/1500000: episode: 1919, duration: 4.352s, episode steps: 361, steps per second: 83, episode reward: 215.445, mean reward: 0.597 [-8.998, 100.000], mean action: 1.374 [0.000, 3.000], mean observation: 0.082 [-0.482, 1.000], loss: 3.724176, mean_absolute_error: 28.779400, mean_q: 38.714382
 1093326/1500000: episode: 1920, duration: 4.693s, episode steps: 333, steps per second: 71, episode reward: 174.018, mean reward: 0.523 [-18.390, 100.000], mean action: 1.051 [0.000, 3.000], mean observation: 0.106 [-0.473, 1.000], loss: 3.099118, mean_absolute_error: 28.822107, mean_q: 38.800102
 1093725/1500000: episode: 1921, duration: 6.874s, episode steps: 399, steps per second: 58, episode reward: 199.215, mean reward: 0.499 [-18.502, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.101 [-0.472, 1.000], loss: 3.411087, mean_absolute_error: 28.848757, mean_q: 38.843094
 1094100/1500000: episode: 1922, duration: 4.360s, episode steps: 375, steps per second: 86, episode reward: 224.210, mean reward: 0.598 [-12.198, 100.000], mean action: 1.288 [0.000, 3.000], mean observation: 0.085 [-0.534, 1.000], loss: 4.402652, mean_absolute_error: 29.025291, mean_q: 39.077213
 1094609/1500000: episode: 1923, duration: 6.291s, episode steps: 509, steps per second: 81, episode reward: 189.239, mean reward: 0.372 [-19.928, 100.000], mean action: 1.033 [0.000, 3.000], mean observation: 0.125 [-0.347, 1.000], loss: 3.296490, mean_absolute_error: 29.070574, mean_q: 39.149689
 1095038/1500000: episode: 1924, duration: 5.264s, episode steps: 429, steps per second: 81, episode reward: 202.613, mean reward: 0.472 [-19.403, 100.000], mean action: 0.956 [0.000, 3.000], mean observation: 0.100 [-0.956, 1.000], loss: 3.943549, mean_absolute_error: 29.137308, mean_q: 39.266445
 1095411/1500000: episode: 1925, duration: 4.484s, episode steps: 373, steps per second: 83, episode reward: 196.285, mean reward: 0.526 [-10.828, 100.000], mean action: 1.048 [0.000, 3.000], mean observation: 0.098 [-0.385, 1.000], loss: 3.747880, mean_absolute_error: 29.111427, mean_q: 39.196808
 1095701/1500000: episode: 1926, duration: 5.852s, episode steps: 290, steps per second: 50, episode reward: 206.052, mean reward: 0.711 [-10.493, 100.000], mean action: 1.400 [0.000, 3.000], mean observation: 0.059 [-0.413, 1.001], loss: 2.786438, mean_absolute_error: 28.985796, mean_q: 39.045723
 1096084/1500000: episode: 1927, duration: 4.438s, episode steps: 383, steps per second: 86, episode reward: 179.978, mean reward: 0.470 [-17.712, 100.000], mean action: 1.084 [0.000, 3.000], mean observation: 0.109 [-0.465, 1.000], loss: 2.463740, mean_absolute_error: 29.327610, mean_q: 39.475014
 1096466/1500000: episode: 1928, duration: 4.455s, episode steps: 382, steps per second: 86, episode reward: 218.980, mean reward: 0.573 [-9.965, 100.000], mean action: 1.118 [0.000, 3.000], mean observation: 0.107 [-0.539, 1.000], loss: 3.708921, mean_absolute_error: 29.269493, mean_q: 39.407482
 1096949/1500000: episode: 1929, duration: 5.784s, episode steps: 483, steps per second: 84, episode reward: 247.674, mean reward: 0.513 [-17.716, 100.000], mean action: 0.996 [0.000, 3.000], mean observation: 0.138 [-0.596, 1.000], loss: 2.403505, mean_absolute_error: 29.104664, mean_q: 39.174431
 1097337/1500000: episode: 1930, duration: 4.677s, episode steps: 388, steps per second: 83, episode reward: 198.529, mean reward: 0.512 [-9.026, 100.000], mean action: 1.175 [0.000, 3.000], mean observation: 0.110 [-0.405, 1.000], loss: 3.992284, mean_absolute_error: 29.205231, mean_q: 39.317127
 1097649/1500000: episode: 1931, duration: 5.104s, episode steps: 312, steps per second: 61, episode reward: 208.621, mean reward: 0.669 [-17.522, 100.000], mean action: 1.324 [0.000, 3.000], mean observation: 0.086 [-0.536, 1.000], loss: 3.740435, mean_absolute_error: 29.127466, mean_q: 39.223759
 1098010/1500000: episode: 1932, duration: 5.510s, episode steps: 361, steps per second: 66, episode reward: 205.310, mean reward: 0.569 [-17.400, 100.000], mean action: 0.970 [0.000, 3.000], mean observation: 0.098 [-0.526, 1.000], loss: 4.165354, mean_absolute_error: 29.228077, mean_q: 39.347828
 1098299/1500000: episode: 1933, duration: 3.430s, episode steps: 289, steps per second: 84, episode reward: 224.466, mean reward: 0.777 [-10.125, 100.000], mean action: 1.578 [0.000, 3.000], mean observation: 0.013 [-0.705, 1.000], loss: 3.077889, mean_absolute_error: 29.319143, mean_q: 39.504513
 1098720/1500000: episode: 1934, duration: 5.168s, episode steps: 421, steps per second: 81, episode reward: 241.360, mean reward: 0.573 [-8.971, 100.000], mean action: 1.116 [0.000, 3.000], mean observation: 0.129 [-0.600, 1.000], loss: 2.719642, mean_absolute_error: 29.399799, mean_q: 39.603035
 1099228/1500000: episode: 1935, duration: 5.952s, episode steps: 508, steps per second: 85, episode reward: 190.789, mean reward: 0.376 [-18.052, 100.000], mean action: 1.100 [0.000, 3.000], mean observation: 0.109 [-0.494, 1.000], loss: 3.359022, mean_absolute_error: 29.349257, mean_q: 39.511814
 1099566/1500000: episode: 1936, duration: 6.440s, episode steps: 338, steps per second: 52, episode reward: 220.637, mean reward: 0.653 [-10.515, 100.000], mean action: 1.355 [0.000, 3.000], mean observation: 0.091 [-0.406, 1.000], loss: 4.601056, mean_absolute_error: 29.439569, mean_q: 39.600174
 1099828/1500000: episode: 1937, duration: 2.829s, episode steps: 262, steps per second: 93, episode reward: 171.453, mean reward: 0.654 [-10.965, 100.000], mean action: 1.031 [0.000, 3.000], mean observation: 0.049 [-0.522, 1.000], loss: 4.556083, mean_absolute_error: 29.165882, mean_q: 39.287342
 1100157/1500000: episode: 1938, duration: 4.038s, episode steps: 329, steps per second: 81, episode reward: 182.706, mean reward: 0.555 [-10.602, 100.000], mean action: 1.185 [0.000, 3.000], mean observation: 0.052 [-0.525, 1.000], loss: 4.732294, mean_absolute_error: 29.527380, mean_q: 39.770317
 1100555/1500000: episode: 1939, duration: 4.732s, episode steps: 398, steps per second: 84, episode reward: 210.017, mean reward: 0.528 [-17.349, 100.000], mean action: 0.945 [0.000, 3.000], mean observation: 0.112 [-0.433, 1.005], loss: 3.515312, mean_absolute_error: 29.335495, mean_q: 39.540173
 1100863/1500000: episode: 1940, duration: 3.705s, episode steps: 308, steps per second: 83, episode reward: 211.491, mean reward: 0.687 [-11.642, 100.000], mean action: 1.617 [0.000, 3.000], mean observation: 0.040 [-0.595, 1.000], loss: 4.552479, mean_absolute_error: 29.280767, mean_q: 39.443100
 1101193/1500000: episode: 1941, duration: 6.626s, episode steps: 330, steps per second: 50, episode reward: 174.529, mean reward: 0.529 [-18.489, 100.000], mean action: 1.097 [0.000, 3.000], mean observation: 0.072 [-0.459, 1.000], loss: 4.999797, mean_absolute_error: 29.597881, mean_q: 39.843227
 1101463/1500000: episode: 1942, duration: 3.022s, episode steps: 270, steps per second: 89, episode reward: 181.337, mean reward: 0.672 [-2.952, 100.000], mean action: 1.493 [0.000, 3.000], mean observation: 0.050 [-0.416, 1.000], loss: 4.002297, mean_absolute_error: 29.511791, mean_q: 39.728481
 1101754/1500000: episode: 1943, duration: 3.394s, episode steps: 291, steps per second: 86, episode reward: 191.388, mean reward: 0.658 [-2.838, 100.000], mean action: 1.182 [0.000, 3.000], mean observation: 0.068 [-0.494, 1.000], loss: 3.785874, mean_absolute_error: 29.289707, mean_q: 39.461685
 1102080/1500000: episode: 1944, duration: 3.539s, episode steps: 326, steps per second: 92, episode reward: 242.424, mean reward: 0.744 [-8.401, 100.000], mean action: 1.196 [0.000, 3.000], mean observation: 0.059 [-0.729, 1.000], loss: 4.286457, mean_absolute_error: 29.791435, mean_q: 40.159988
 1102683/1500000: episode: 1945, duration: 10.205s, episode steps: 603, steps per second: 59, episode reward: 203.183, mean reward: 0.337 [-18.567, 100.000], mean action: 0.799 [0.000, 3.000], mean observation: 0.148 [-0.367, 1.000], loss: 3.639731, mean_absolute_error: 29.598381, mean_q: 39.829777
 1103279/1500000: episode: 1946, duration: 7.413s, episode steps: 596, steps per second: 80, episode reward: 230.657, mean reward: 0.387 [-17.797, 100.000], mean action: 1.003 [0.000, 3.000], mean observation: 0.135 [-0.692, 1.000], loss: 3.147500, mean_absolute_error: 29.894432, mean_q: 40.281975
 1103557/1500000: episode: 1947, duration: 3.278s, episode steps: 278, steps per second: 85, episode reward: 254.110, mean reward: 0.914 [-3.469, 100.000], mean action: 1.550 [0.000, 3.000], mean observation: 0.029 [-0.799, 1.000], loss: 3.931239, mean_absolute_error: 29.676104, mean_q: 39.983620
 1103990/1500000: episode: 1948, duration: 7.583s, episode steps: 433, steps per second: 57, episode reward: 220.267, mean reward: 0.509 [-9.001, 100.000], mean action: 1.125 [0.000, 3.000], mean observation: 0.094 [-0.592, 1.000], loss: 5.943694, mean_absolute_error: 29.955339, mean_q: 40.375252
 1104522/1500000: episode: 1949, duration: 6.823s, episode steps: 532, steps per second: 78, episode reward: 225.631, mean reward: 0.424 [-19.312, 100.000], mean action: 0.991 [0.000, 3.000], mean observation: 0.110 [-0.699, 1.000], loss: 3.988815, mean_absolute_error: 29.996164, mean_q: 40.438595
 1104943/1500000: episode: 1950, duration: 4.757s, episode steps: 421, steps per second: 89, episode reward: 178.192, mean reward: 0.423 [-17.675, 100.000], mean action: 1.411 [0.000, 3.000], mean observation: 0.052 [-0.535, 1.000], loss: 3.578463, mean_absolute_error: 30.048027, mean_q: 40.489563
 1105403/1500000: episode: 1951, duration: 8.079s, episode steps: 460, steps per second: 57, episode reward: 197.955, mean reward: 0.430 [-21.181, 100.000], mean action: 1.030 [0.000, 3.000], mean observation: 0.124 [-0.478, 1.000], loss: 3.966881, mean_absolute_error: 30.006607, mean_q: 40.428108
 1105806/1500000: episode: 1952, duration: 5.217s, episode steps: 403, steps per second: 77, episode reward: 230.203, mean reward: 0.571 [-17.561, 100.000], mean action: 1.196 [0.000, 3.000], mean observation: 0.100 [-0.458, 1.000], loss: 3.269053, mean_absolute_error: 30.190365, mean_q: 40.682461
 1106109/1500000: episode: 1953, duration: 3.457s, episode steps: 303, steps per second: 88, episode reward: 212.893, mean reward: 0.703 [-9.105, 100.000], mean action: 1.459 [0.000, 3.000], mean observation: 0.063 [-0.567, 1.000], loss: 2.964868, mean_absolute_error: 30.051641, mean_q: 40.487862
 1106500/1500000: episode: 1954, duration: 4.805s, episode steps: 391, steps per second: 81, episode reward: 169.679, mean reward: 0.434 [-20.403, 100.000], mean action: 1.118 [0.000, 3.000], mean observation: 0.092 [-0.454, 1.000], loss: 3.545506, mean_absolute_error: 30.174372, mean_q: 40.655121
 1106748/1500000: episode: 1955, duration: 4.667s, episode steps: 248, steps per second: 53, episode reward: 196.126, mean reward: 0.791 [-3.004, 100.000], mean action: 1.294 [0.000, 3.000], mean observation: 0.045 [-0.525, 1.000], loss: 2.456544, mean_absolute_error: 29.972263, mean_q: 40.402828
 1107046/1500000: episode: 1956, duration: 4.681s, episode steps: 298, steps per second: 64, episode reward: 169.547, mean reward: 0.569 [-3.927, 100.000], mean action: 1.570 [0.000, 3.000], mean observation: 0.045 [-0.476, 1.000], loss: 3.327987, mean_absolute_error: 30.166901, mean_q: 40.610115
 1107398/1500000: episode: 1957, duration: 4.084s, episode steps: 352, steps per second: 86, episode reward: 174.182, mean reward: 0.495 [-17.843, 100.000], mean action: 1.330 [0.000, 3.000], mean observation: 0.087 [-0.446, 1.000], loss: 3.565308, mean_absolute_error: 30.249891, mean_q: 40.706764
 1107655/1500000: episode: 1958, duration: 2.958s, episode steps: 257, steps per second: 87, episode reward: 246.854, mean reward: 0.961 [-3.039, 100.000], mean action: 1.704 [0.000, 3.000], mean observation: 0.016 [-0.779, 1.000], loss: 1.843993, mean_absolute_error: 30.022682, mean_q: 40.424938
 1108006/1500000: episode: 1959, duration: 5.395s, episode steps: 351, steps per second: 65, episode reward: 244.016, mean reward: 0.695 [-17.954, 100.000], mean action: 1.459 [0.000, 3.000], mean observation: 0.059 [-0.794, 1.000], loss: 3.712926, mean_absolute_error: 30.310244, mean_q: 40.796623
 1108372/1500000: episode: 1960, duration: 5.852s, episode steps: 366, steps per second: 63, episode reward: 219.671, mean reward: 0.600 [-19.948, 100.000], mean action: 1.333 [0.000, 3.000], mean observation: 0.062 [-0.579, 1.000], loss: 4.532621, mean_absolute_error: 30.374800, mean_q: 40.931015
 1108833/1500000: episode: 1961, duration: 5.295s, episode steps: 461, steps per second: 87, episode reward: 247.018, mean reward: 0.536 [-10.983, 100.000], mean action: 1.191 [0.000, 3.000], mean observation: 0.129 [-0.739, 1.000], loss: 3.107770, mean_absolute_error: 30.290443, mean_q: 40.795357
 1109212/1500000: episode: 1962, duration: 5.174s, episode steps: 379, steps per second: 73, episode reward: 219.197, mean reward: 0.578 [-18.327, 100.000], mean action: 1.285 [0.000, 3.000], mean observation: 0.086 [-0.525, 1.000], loss: 3.241249, mean_absolute_error: 30.583763, mean_q: 41.155575
 1109583/1500000: episode: 1963, duration: 6.284s, episode steps: 371, steps per second: 59, episode reward: 221.190, mean reward: 0.596 [-2.788, 100.000], mean action: 1.396 [0.000, 3.000], mean observation: 0.077 [-0.487, 1.046], loss: 3.323802, mean_absolute_error: 30.646801, mean_q: 41.279728
 1109879/1500000: episode: 1964, duration: 3.617s, episode steps: 296, steps per second: 82, episode reward: 233.097, mean reward: 0.787 [-10.041, 100.000], mean action: 1.473 [0.000, 3.000], mean observation: 0.043 [-0.676, 1.000], loss: 2.840993, mean_absolute_error: 30.489040, mean_q: 41.091724
 1110128/1500000: episode: 1965, duration: 3.065s, episode steps: 249, steps per second: 81, episode reward: -59.398, mean reward: -0.239 [-100.000, 10.981], mean action: 1.908 [0.000, 3.000], mean observation: -0.001 [-0.887, 1.030], loss: 3.271099, mean_absolute_error: 30.544674, mean_q: 41.164574
 1110448/1500000: episode: 1966, duration: 6.332s, episode steps: 320, steps per second: 51, episode reward: 249.690, mean reward: 0.780 [-10.069, 100.000], mean action: 1.381 [0.000, 3.000], mean observation: 0.098 [-0.625, 1.000], loss: 3.662463, mean_absolute_error: 30.745670, mean_q: 41.371742
 1110770/1500000: episode: 1967, duration: 4.082s, episode steps: 322, steps per second: 79, episode reward: 198.893, mean reward: 0.618 [-8.235, 100.000], mean action: 1.087 [0.000, 3.000], mean observation: 0.094 [-0.474, 1.000], loss: 3.575402, mean_absolute_error: 30.612947, mean_q: 41.227234
 1111040/1500000: episode: 1968, duration: 3.174s, episode steps: 270, steps per second: 85, episode reward: 170.957, mean reward: 0.633 [-10.213, 100.000], mean action: 1.267 [0.000, 3.000], mean observation: 0.032 [-0.503, 1.000], loss: 2.434026, mean_absolute_error: 30.809990, mean_q: 41.530663
 1111649/1500000: episode: 1969, duration: 8.932s, episode steps: 609, steps per second: 68, episode reward: 167.804, mean reward: 0.276 [-18.907, 100.000], mean action: 0.808 [0.000, 3.000], mean observation: 0.085 [-0.578, 1.000], loss: 4.243072, mean_absolute_error: 30.535618, mean_q: 41.128891
 1112061/1500000: episode: 1970, duration: 6.863s, episode steps: 412, steps per second: 60, episode reward: 221.068, mean reward: 0.537 [-3.132, 100.000], mean action: 1.226 [0.000, 3.000], mean observation: 0.094 [-0.514, 1.017], loss: 3.999707, mean_absolute_error: 30.663113, mean_q: 41.331978
 1112489/1500000: episode: 1971, duration: 5.104s, episode steps: 428, steps per second: 84, episode reward: 198.399, mean reward: 0.464 [-17.640, 100.000], mean action: 1.150 [0.000, 3.000], mean observation: 0.085 [-0.497, 1.000], loss: 3.316123, mean_absolute_error: 30.324856, mean_q: 40.854397
 1112758/1500000: episode: 1972, duration: 4.089s, episode steps: 269, steps per second: 66, episode reward: 205.550, mean reward: 0.764 [-13.580, 100.000], mean action: 1.171 [0.000, 3.000], mean observation: 0.053 [-0.526, 1.000], loss: 4.976647, mean_absolute_error: 30.540529, mean_q: 41.140438
 1113154/1500000: episode: 1973, duration: 6.678s, episode steps: 396, steps per second: 59, episode reward: 227.725, mean reward: 0.575 [-9.358, 100.000], mean action: 1.187 [0.000, 3.000], mean observation: 0.111 [-0.505, 1.000], loss: 3.080990, mean_absolute_error: 30.472755, mean_q: 41.044136
 1113663/1500000: episode: 1974, duration: 6.332s, episode steps: 509, steps per second: 80, episode reward: 219.161, mean reward: 0.431 [-21.700, 100.000], mean action: 0.996 [0.000, 3.000], mean observation: 0.120 [-0.556, 1.000], loss: 4.085613, mean_absolute_error: 30.467329, mean_q: 41.023014
 1113963/1500000: episode: 1975, duration: 4.082s, episode steps: 300, steps per second: 73, episode reward: 211.152, mean reward: 0.704 [-11.703, 100.000], mean action: 1.463 [0.000, 3.000], mean observation: 0.068 [-0.400, 1.000], loss: 3.446510, mean_absolute_error: 30.687214, mean_q: 41.327942
 1114400/1500000: episode: 1976, duration: 7.645s, episode steps: 437, steps per second: 57, episode reward: 190.196, mean reward: 0.435 [-18.565, 100.000], mean action: 0.966 [0.000, 3.000], mean observation: 0.106 [-0.414, 1.000], loss: 3.416675, mean_absolute_error: 30.578081, mean_q: 41.176167
 1114663/1500000: episode: 1977, duration: 3.051s, episode steps: 263, steps per second: 86, episode reward: -51.947, mean reward: -0.198 [-100.000, 11.646], mean action: 1.658 [0.000, 3.000], mean observation: 0.003 [-0.516, 1.000], loss: 2.650879, mean_absolute_error: 30.655146, mean_q: 41.277405
 1114902/1500000: episode: 1978, duration: 2.803s, episode steps: 239, steps per second: 85, episode reward: 202.729, mean reward: 0.848 [-8.986, 100.000], mean action: 1.523 [0.000, 3.000], mean observation: 0.019 [-0.628, 1.000], loss: 6.695579, mean_absolute_error: 30.618229, mean_q: 41.203197
 1115232/1500000: episode: 1979, duration: 6.617s, episode steps: 330, steps per second: 50, episode reward: 174.984, mean reward: 0.530 [-9.279, 100.000], mean action: 1.370 [0.000, 3.000], mean observation: 0.068 [-0.452, 1.000], loss: 3.808104, mean_absolute_error: 30.471241, mean_q: 41.035553
 1115624/1500000: episode: 1980, duration: 5.104s, episode steps: 392, steps per second: 77, episode reward: 207.429, mean reward: 0.529 [-18.059, 100.000], mean action: 1.048 [0.000, 3.000], mean observation: 0.106 [-0.477, 1.000], loss: 2.884492, mean_absolute_error: 30.661701, mean_q: 41.299023
 1115962/1500000: episode: 1981, duration: 3.802s, episode steps: 338, steps per second: 89, episode reward: 198.316, mean reward: 0.587 [-17.356, 100.000], mean action: 1.157 [0.000, 3.000], mean observation: 0.070 [-0.673, 1.002], loss: 4.036363, mean_absolute_error: 30.422852, mean_q: 40.981503
 1116344/1500000: episode: 1982, duration: 6.576s, episode steps: 382, steps per second: 58, episode reward: 222.499, mean reward: 0.582 [-17.400, 100.000], mean action: 1.280 [0.000, 3.000], mean observation: 0.099 [-0.515, 1.000], loss: 4.707877, mean_absolute_error: 30.447542, mean_q: 40.994999
 1116726/1500000: episode: 1983, duration: 5.443s, episode steps: 382, steps per second: 70, episode reward: 233.180, mean reward: 0.610 [-17.405, 100.000], mean action: 1.301 [0.000, 3.000], mean observation: 0.104 [-0.515, 1.000], loss: 3.594762, mean_absolute_error: 30.666286, mean_q: 41.344383
 1117117/1500000: episode: 1984, duration: 4.752s, episode steps: 391, steps per second: 82, episode reward: 238.242, mean reward: 0.609 [-17.332, 100.000], mean action: 1.199 [0.000, 3.000], mean observation: 0.078 [-0.761, 1.000], loss: 3.798677, mean_absolute_error: 30.503189, mean_q: 41.102997
 1117633/1500000: episode: 1985, duration: 9.599s, episode steps: 516, steps per second: 54, episode reward: 213.482, mean reward: 0.414 [-19.329, 100.000], mean action: 1.182 [0.000, 3.000], mean observation: 0.126 [-0.440, 1.000], loss: 3.129197, mean_absolute_error: 30.551472, mean_q: 41.130074
 1117893/1500000: episode: 1986, duration: 3.015s, episode steps: 260, steps per second: 86, episode reward: 193.417, mean reward: 0.744 [-12.599, 100.000], mean action: 1.358 [0.000, 3.000], mean observation: 0.046 [-0.411, 1.000], loss: 3.503462, mean_absolute_error: 30.486544, mean_q: 41.060455
 1118215/1500000: episode: 1987, duration: 3.746s, episode steps: 322, steps per second: 86, episode reward: 196.188, mean reward: 0.609 [-17.495, 100.000], mean action: 1.016 [0.000, 3.000], mean observation: 0.089 [-0.470, 1.000], loss: 3.312106, mean_absolute_error: 30.529497, mean_q: 41.141144
 1118585/1500000: episode: 1988, duration: 6.809s, episode steps: 370, steps per second: 54, episode reward: 178.619, mean reward: 0.483 [-18.522, 100.000], mean action: 1.330 [0.000, 3.000], mean observation: 0.042 [-0.652, 1.000], loss: 2.602470, mean_absolute_error: 30.508072, mean_q: 41.124104
 1118947/1500000: episode: 1989, duration: 5.123s, episode steps: 362, steps per second: 71, episode reward: 217.415, mean reward: 0.601 [-17.860, 100.000], mean action: 1.282 [0.000, 3.000], mean observation: 0.082 [-0.484, 1.000], loss: 2.729609, mean_absolute_error: 30.428915, mean_q: 40.990211
 1119432/1500000: episode: 1990, duration: 5.859s, episode steps: 485, steps per second: 83, episode reward: 240.545, mean reward: 0.496 [-18.682, 100.000], mean action: 1.097 [0.000, 3.000], mean observation: 0.131 [-0.609, 1.000], loss: 2.717077, mean_absolute_error: 30.566629, mean_q: 41.195683
 1119822/1500000: episode: 1991, duration: 7.989s, episode steps: 390, steps per second: 49, episode reward: 229.869, mean reward: 0.589 [-8.621, 100.000], mean action: 1.321 [0.000, 3.000], mean observation: 0.101 [-0.445, 1.026], loss: 4.154947, mean_absolute_error: 30.474422, mean_q: 41.069069
 1120409/1500000: episode: 1992, duration: 7.447s, episode steps: 587, steps per second: 79, episode reward: 173.827, mean reward: 0.296 [-18.507, 100.000], mean action: 1.538 [0.000, 3.000], mean observation: 0.120 [-0.646, 1.000], loss: 2.510215, mean_absolute_error: 30.612373, mean_q: 41.278606
 1120653/1500000: episode: 1993, duration: 2.956s, episode steps: 244, steps per second: 83, episode reward: 208.431, mean reward: 0.854 [-2.946, 100.000], mean action: 1.463 [0.000, 3.000], mean observation: 0.027 [-0.682, 1.000], loss: 4.380326, mean_absolute_error: 30.800425, mean_q: 41.478661
 1120992/1500000: episode: 1994, duration: 7.106s, episode steps: 339, steps per second: 48, episode reward: 227.807, mean reward: 0.672 [-9.915, 100.000], mean action: 1.348 [0.000, 3.000], mean observation: 0.082 [-0.494, 1.018], loss: 4.046001, mean_absolute_error: 30.760933, mean_q: 41.426132
 1121696/1500000: episode: 1995, duration: 8.428s, episode steps: 704, steps per second: 84, episode reward: 188.426, mean reward: 0.268 [-18.576, 100.000], mean action: 1.783 [0.000, 3.000], mean observation: 0.165 [-0.475, 1.000], loss: 4.374346, mean_absolute_error: 30.807720, mean_q: 41.502071
 1122079/1500000: episode: 1996, duration: 7.972s, episode steps: 383, steps per second: 48, episode reward: 232.311, mean reward: 0.607 [-10.513, 100.000], mean action: 1.423 [0.000, 3.000], mean observation: 0.084 [-0.538, 1.084], loss: 3.001193, mean_absolute_error: 31.192638, mean_q: 42.037674
 1122331/1500000: episode: 1997, duration: 2.845s, episode steps: 252, steps per second: 89, episode reward: 226.783, mean reward: 0.900 [-10.074, 100.000], mean action: 1.341 [0.000, 3.000], mean observation: 0.064 [-0.604, 1.000], loss: 2.547350, mean_absolute_error: 31.183012, mean_q: 42.055897
 1122667/1500000: episode: 1998, duration: 4.008s, episode steps: 336, steps per second: 84, episode reward: 208.823, mean reward: 0.621 [-17.938, 100.000], mean action: 1.185 [0.000, 3.000], mean observation: 0.099 [-0.413, 1.000], loss: 3.736008, mean_absolute_error: 31.089920, mean_q: 41.863792
 1122950/1500000: episode: 1999, duration: 5.522s, episode steps: 283, steps per second: 51, episode reward: 187.109, mean reward: 0.661 [-11.524, 100.000], mean action: 1.088 [0.000, 3.000], mean observation: 0.063 [-0.484, 1.000], loss: 3.788677, mean_absolute_error: 31.351734, mean_q: 42.283749
 1123229/1500000: episode: 2000, duration: 4.263s, episode steps: 279, steps per second: 65, episode reward: 213.768, mean reward: 0.766 [-10.277, 100.000], mean action: 1.477 [0.000, 3.000], mean observation: 0.052 [-0.522, 1.000], loss: 4.298899, mean_absolute_error: 31.175428, mean_q: 41.941978
 1123557/1500000: episode: 2001, duration: 3.626s, episode steps: 328, steps per second: 90, episode reward: 241.177, mean reward: 0.735 [-2.686, 100.000], mean action: 1.305 [0.000, 3.000], mean observation: 0.088 [-0.474, 1.000], loss: 3.094326, mean_absolute_error: 31.142000, mean_q: 41.966587
 1123898/1500000: episode: 2002, duration: 4.264s, episode steps: 341, steps per second: 80, episode reward: 221.557, mean reward: 0.650 [-10.322, 100.000], mean action: 1.419 [0.000, 3.000], mean observation: 0.060 [-0.724, 1.000], loss: 4.059485, mean_absolute_error: 31.208139, mean_q: 42.063011
 1124254/1500000: episode: 2003, duration: 7.184s, episode steps: 356, steps per second: 50, episode reward: 249.150, mean reward: 0.700 [-17.527, 100.000], mean action: 1.256 [0.000, 3.000], mean observation: 0.122 [-0.714, 1.000], loss: 2.947221, mean_absolute_error: 31.443146, mean_q: 42.374847
 1124579/1500000: episode: 2004, duration: 3.767s, episode steps: 325, steps per second: 86, episode reward: 211.189, mean reward: 0.650 [-8.425, 100.000], mean action: 1.723 [0.000, 3.000], mean observation: 0.052 [-0.544, 1.000], loss: 2.740384, mean_absolute_error: 31.545811, mean_q: 42.513138
 1125579/1500000: episode: 2005, duration: 16.149s, episode steps: 1000, steps per second: 62, episode reward: 29.275, mean reward: 0.029 [-18.573, 12.616], mean action: 1.074 [0.000, 3.000], mean observation: 0.129 [-0.592, 1.000], loss: 3.593685, mean_absolute_error: 31.632753, mean_q: 42.613419
 1125855/1500000: episode: 2006, duration: 3.166s, episode steps: 276, steps per second: 87, episode reward: 159.403, mean reward: 0.578 [-12.518, 100.000], mean action: 1.214 [0.000, 3.000], mean observation: 0.037 [-0.481, 1.000], loss: 3.550642, mean_absolute_error: 31.670952, mean_q: 42.648495
 1126294/1500000: episode: 2007, duration: 8.985s, episode steps: 439, steps per second: 49, episode reward: 160.298, mean reward: 0.365 [-18.787, 100.000], mean action: 1.319 [0.000, 3.000], mean observation: 0.103 [-0.482, 1.000], loss: 2.777953, mean_absolute_error: 31.969095, mean_q: 43.099228
 1126599/1500000: episode: 2008, duration: 3.679s, episode steps: 305, steps per second: 83, episode reward: 247.091, mean reward: 0.810 [-8.929, 100.000], mean action: 1.508 [0.000, 3.000], mean observation: 0.041 [-0.713, 1.000], loss: 3.624853, mean_absolute_error: 31.988316, mean_q: 43.133152
 1127035/1500000: episode: 2009, duration: 8.264s, episode steps: 436, steps per second: 53, episode reward: 212.880, mean reward: 0.488 [-19.306, 100.000], mean action: 0.860 [0.000, 3.000], mean observation: 0.108 [-0.990, 1.000], loss: 6.132524, mean_absolute_error: 31.943569, mean_q: 43.015640
 1127493/1500000: episode: 2010, duration: 5.868s, episode steps: 458, steps per second: 78, episode reward: 190.239, mean reward: 0.415 [-21.357, 100.000], mean action: 2.306 [0.000, 3.000], mean observation: 0.132 [-0.636, 1.000], loss: 3.731342, mean_absolute_error: 31.837687, mean_q: 42.910496
 1127921/1500000: episode: 2011, duration: 7.200s, episode steps: 428, steps per second: 59, episode reward: 241.932, mean reward: 0.565 [-18.414, 100.000], mean action: 1.040 [0.000, 3.000], mean observation: 0.103 [-0.743, 1.000], loss: 3.860262, mean_absolute_error: 31.790760, mean_q: 42.853001
 1128294/1500000: episode: 2012, duration: 5.905s, episode steps: 373, steps per second: 63, episode reward: 176.277, mean reward: 0.473 [-17.862, 100.000], mean action: 1.201 [0.000, 3.000], mean observation: 0.060 [-0.549, 1.000], loss: 3.650846, mean_absolute_error: 31.834490, mean_q: 42.901688
 1129294/1500000: episode: 2013, duration: 15.765s, episode steps: 1000, steps per second: 63, episode reward: 111.647, mean reward: 0.112 [-19.735, 22.664], mean action: 2.151 [0.000, 3.000], mean observation: 0.184 [-1.023, 1.000], loss: 3.149903, mean_absolute_error: 31.859816, mean_q: 42.941776
 1129558/1500000: episode: 2014, duration: 3.039s, episode steps: 264, steps per second: 87, episode reward: 156.481, mean reward: 0.593 [-13.795, 100.000], mean action: 1.182 [0.000, 3.000], mean observation: 0.038 [-0.527, 1.000], loss: 3.799860, mean_absolute_error: 31.930153, mean_q: 42.997543
 1129968/1500000: episode: 2015, duration: 7.265s, episode steps: 410, steps per second: 56, episode reward: 232.352, mean reward: 0.567 [-2.918, 100.000], mean action: 1.449 [0.000, 3.000], mean observation: 0.050 [-0.771, 1.000], loss: 5.021647, mean_absolute_error: 32.018055, mean_q: 43.133171
 1130646/1500000: episode: 2016, duration: 9.329s, episode steps: 678, steps per second: 73, episode reward: 193.591, mean reward: 0.286 [-20.379, 100.000], mean action: 1.816 [0.000, 3.000], mean observation: 0.163 [-0.486, 1.000], loss: 4.573460, mean_absolute_error: 31.991899, mean_q: 43.118900
 1130882/1500000: episode: 2017, duration: 4.579s, episode steps: 236, steps per second: 52, episode reward: 139.629, mean reward: 0.592 [-8.967, 100.000], mean action: 1.390 [0.000, 3.000], mean observation: 0.038 [-0.910, 1.000], loss: 3.250857, mean_absolute_error: 32.023026, mean_q: 43.172859
 1131113/1500000: episode: 2018, duration: 4.242s, episode steps: 231, steps per second: 54, episode reward: 256.019, mean reward: 1.108 [-7.921, 100.000], mean action: 1.368 [0.000, 3.000], mean observation: 0.123 [-0.921, 1.091], loss: 3.106404, mean_absolute_error: 32.269352, mean_q: 43.477428
 1131596/1500000: episode: 2019, duration: 5.824s, episode steps: 483, steps per second: 83, episode reward: 205.586, mean reward: 0.426 [-19.171, 100.000], mean action: 0.983 [0.000, 3.000], mean observation: 0.095 [-0.605, 1.000], loss: 3.045653, mean_absolute_error: 32.075310, mean_q: 43.248745
 1131933/1500000: episode: 2020, duration: 6.290s, episode steps: 337, steps per second: 54, episode reward: 244.480, mean reward: 0.725 [-18.044, 100.000], mean action: 1.199 [0.000, 3.000], mean observation: 0.111 [-0.695, 1.000], loss: 4.462313, mean_absolute_error: 32.106968, mean_q: 43.266212
 1132307/1500000: episode: 2021, duration: 5.495s, episode steps: 374, steps per second: 68, episode reward: 192.801, mean reward: 0.516 [-10.714, 100.000], mean action: 1.206 [0.000, 3.000], mean observation: 0.105 [-0.360, 1.000], loss: 3.653489, mean_absolute_error: 32.097065, mean_q: 43.202110
 1132627/1500000: episode: 2022, duration: 3.781s, episode steps: 320, steps per second: 85, episode reward: 172.926, mean reward: 0.540 [-9.378, 100.000], mean action: 1.222 [0.000, 3.000], mean observation: 0.076 [-0.448, 1.000], loss: 3.794451, mean_absolute_error: 32.376663, mean_q: 43.610546
 1133019/1500000: episode: 2023, duration: 8.076s, episode steps: 392, steps per second: 49, episode reward: 201.104, mean reward: 0.513 [-21.251, 100.000], mean action: 0.987 [0.000, 3.000], mean observation: 0.124 [-0.728, 1.000], loss: 3.883444, mean_absolute_error: 32.147408, mean_q: 43.301346
 1133371/1500000: episode: 2024, duration: 4.184s, episode steps: 352, steps per second: 84, episode reward: 202.908, mean reward: 0.576 [-2.659, 100.000], mean action: 1.205 [0.000, 3.000], mean observation: 0.088 [-0.443, 1.000], loss: 2.288685, mean_absolute_error: 32.373135, mean_q: 43.646297
 1133714/1500000: episode: 2025, duration: 4.525s, episode steps: 343, steps per second: 76, episode reward: 215.432, mean reward: 0.628 [-17.458, 100.000], mean action: 1.382 [0.000, 3.000], mean observation: 0.104 [-0.679, 1.006], loss: 4.895733, mean_absolute_error: 32.281052, mean_q: 43.485310
 1134192/1500000: episode: 2026, duration: 8.540s, episode steps: 478, steps per second: 56, episode reward: 242.700, mean reward: 0.508 [-17.912, 100.000], mean action: 0.902 [0.000, 3.000], mean observation: 0.147 [-0.526, 1.072], loss: 3.144192, mean_absolute_error: 32.267300, mean_q: 43.499592
 1134637/1500000: episode: 2027, duration: 5.278s, episode steps: 445, steps per second: 84, episode reward: 157.413, mean reward: 0.354 [-17.737, 100.000], mean action: 0.840 [0.000, 3.000], mean observation: 0.096 [-0.599, 1.000], loss: 3.883503, mean_absolute_error: 32.511623, mean_q: 43.742584
 1134939/1500000: episode: 2028, duration: 6.638s, episode steps: 302, steps per second: 45, episode reward: 215.678, mean reward: 0.714 [-3.279, 100.000], mean action: 1.209 [0.000, 3.000], mean observation: 0.077 [-0.465, 1.000], loss: 3.442780, mean_absolute_error: 32.540009, mean_q: 43.844929
 1135386/1500000: episode: 2029, duration: 5.795s, episode steps: 447, steps per second: 77, episode reward: 213.000, mean reward: 0.477 [-19.325, 100.000], mean action: 1.047 [0.000, 3.000], mean observation: 0.126 [-0.427, 1.000], loss: 3.172156, mean_absolute_error: 32.466282, mean_q: 43.769512
 1135735/1500000: episode: 2030, duration: 4.295s, episode steps: 349, steps per second: 81, episode reward: 215.044, mean reward: 0.616 [-9.185, 100.000], mean action: 1.261 [0.000, 3.000], mean observation: 0.090 [-0.532, 1.000], loss: 3.568045, mean_absolute_error: 32.765316, mean_q: 44.132431
 1136093/1500000: episode: 2031, duration: 7.822s, episode steps: 358, steps per second: 46, episode reward: 187.838, mean reward: 0.525 [-17.487, 100.000], mean action: 1.061 [0.000, 3.000], mean observation: 0.096 [-0.450, 1.000], loss: 3.300620, mean_absolute_error: 32.559086, mean_q: 43.879303
 1136684/1500000: episode: 2032, duration: 7.070s, episode steps: 591, steps per second: 84, episode reward: 206.359, mean reward: 0.349 [-18.095, 100.000], mean action: 0.797 [0.000, 3.000], mean observation: 0.148 [-0.393, 1.000], loss: 3.545387, mean_absolute_error: 32.589333, mean_q: 43.874123
 1137067/1500000: episode: 2033, duration: 8.124s, episode steps: 383, steps per second: 47, episode reward: 232.602, mean reward: 0.607 [-18.302, 100.000], mean action: 1.120 [0.000, 3.000], mean observation: 0.102 [-0.669, 1.000], loss: 3.276832, mean_absolute_error: 32.749249, mean_q: 44.114292
 1137444/1500000: episode: 2034, duration: 4.742s, episode steps: 377, steps per second: 80, episode reward: 170.300, mean reward: 0.452 [-17.268, 100.000], mean action: 1.194 [0.000, 3.000], mean observation: 0.084 [-0.526, 1.000], loss: 3.045007, mean_absolute_error: 32.772072, mean_q: 44.139515
 1137741/1500000: episode: 2035, duration: 5.328s, episode steps: 297, steps per second: 56, episode reward: 221.236, mean reward: 0.745 [-6.203, 100.000], mean action: 1.313 [0.000, 3.000], mean observation: 0.077 [-0.451, 1.002], loss: 4.423621, mean_absolute_error: 32.948345, mean_q: 44.371624
 1138101/1500000: episode: 2036, duration: 6.315s, episode steps: 360, steps per second: 57, episode reward: 253.031, mean reward: 0.703 [-9.173, 100.000], mean action: 1.453 [0.000, 3.000], mean observation: 0.061 [-0.783, 1.000], loss: 3.695472, mean_absolute_error: 32.839725, mean_q: 44.252712
 1138549/1500000: episode: 2037, duration: 6.179s, episode steps: 448, steps per second: 73, episode reward: 225.198, mean reward: 0.503 [-19.500, 100.000], mean action: 1.020 [0.000, 3.000], mean observation: 0.109 [-0.472, 1.000], loss: 3.562353, mean_absolute_error: 32.785084, mean_q: 44.165882
 1138898/1500000: episode: 2038, duration: 7.540s, episode steps: 349, steps per second: 46, episode reward: 187.410, mean reward: 0.537 [-2.525, 100.000], mean action: 1.046 [0.000, 3.000], mean observation: 0.095 [-0.419, 1.000], loss: 2.936450, mean_absolute_error: 33.168533, mean_q: 44.709126
 1139205/1500000: episode: 2039, duration: 3.548s, episode steps: 307, steps per second: 87, episode reward: 172.818, mean reward: 0.563 [-2.978, 100.000], mean action: 0.980 [0.000, 3.000], mean observation: 0.084 [-0.505, 1.000], loss: 3.380238, mean_absolute_error: 32.872169, mean_q: 44.282810
 1139700/1500000: episode: 2040, duration: 10.099s, episode steps: 495, steps per second: 49, episode reward: 236.732, mean reward: 0.478 [-21.203, 100.000], mean action: 0.883 [0.000, 3.000], mean observation: 0.145 [-0.659, 1.000], loss: 3.710503, mean_absolute_error: 33.031929, mean_q: 44.552486
 1140046/1500000: episode: 2041, duration: 4.351s, episode steps: 346, steps per second: 80, episode reward: 208.261, mean reward: 0.602 [-19.111, 100.000], mean action: 1.344 [0.000, 3.000], mean observation: 0.102 [-0.504, 1.009], loss: 3.856899, mean_absolute_error: 32.921051, mean_q: 44.386768
 1140355/1500000: episode: 2042, duration: 5.189s, episode steps: 309, steps per second: 60, episode reward: 194.066, mean reward: 0.628 [-9.493, 100.000], mean action: 1.968 [0.000, 3.000], mean observation: 0.038 [-0.830, 1.000], loss: 3.103260, mean_absolute_error: 32.972931, mean_q: 44.475708
 1140677/1500000: episode: 2043, duration: 6.334s, episode steps: 322, steps per second: 51, episode reward: 142.233, mean reward: 0.442 [-13.787, 100.000], mean action: 2.000 [0.000, 3.000], mean observation: 0.068 [-0.474, 1.000], loss: 4.414009, mean_absolute_error: 32.880527, mean_q: 44.376770
 1141677/1500000: episode: 2044, duration: 18.048s, episode steps: 1000, steps per second: 55, episode reward: 107.465, mean reward: 0.107 [-18.827, 27.212], mean action: 1.729 [0.000, 3.000], mean observation: 0.172 [-0.799, 1.000], loss: 3.330396, mean_absolute_error: 32.853447, mean_q: 44.284203
 1142017/1500000: episode: 2045, duration: 5.664s, episode steps: 340, steps per second: 60, episode reward: 230.998, mean reward: 0.679 [-9.541, 100.000], mean action: 1.362 [0.000, 3.000], mean observation: 0.094 [-0.443, 1.000], loss: 3.462287, mean_absolute_error: 32.932728, mean_q: 44.412197
 1142401/1500000: episode: 2046, duration: 6.874s, episode steps: 384, steps per second: 56, episode reward: 239.172, mean reward: 0.623 [-8.385, 100.000], mean action: 0.995 [0.000, 3.000], mean observation: 0.100 [-0.573, 1.041], loss: 3.441973, mean_absolute_error: 33.057083, mean_q: 44.571369
 1143401/1500000: episode: 2047, duration: 16.864s, episode steps: 1000, steps per second: 59, episode reward: 54.979, mean reward: 0.055 [-19.141, 22.277], mean action: 2.224 [0.000, 3.000], mean observation: 0.190 [-0.514, 1.000], loss: 3.797796, mean_absolute_error: 32.969757, mean_q: 44.447712
 1143844/1500000: episode: 2048, duration: 7.340s, episode steps: 443, steps per second: 60, episode reward: 150.033, mean reward: 0.339 [-17.317, 100.000], mean action: 2.106 [0.000, 3.000], mean observation: 0.074 [-0.540, 1.000], loss: 3.098560, mean_absolute_error: 32.718552, mean_q: 44.111507
 1144107/1500000: episode: 2049, duration: 4.908s, episode steps: 263, steps per second: 54, episode reward: 186.799, mean reward: 0.710 [-11.985, 100.000], mean action: 1.160 [0.000, 3.000], mean observation: 0.059 [-0.476, 1.000], loss: 5.180646, mean_absolute_error: 32.780998, mean_q: 44.151073
 1144411/1500000: episode: 2050, duration: 3.648s, episode steps: 304, steps per second: 83, episode reward: 247.861, mean reward: 0.815 [-3.237, 100.000], mean action: 1.490 [0.000, 3.000], mean observation: 0.032 [-0.766, 1.000], loss: 3.383587, mean_absolute_error: 32.751591, mean_q: 44.125202
 1144870/1500000: episode: 2051, duration: 9.523s, episode steps: 459, steps per second: 48, episode reward: 165.984, mean reward: 0.362 [-17.130, 100.000], mean action: 1.325 [0.000, 3.000], mean observation: 0.088 [-0.816, 1.000], loss: 3.643145, mean_absolute_error: 32.871349, mean_q: 44.276520
 1145175/1500000: episode: 2052, duration: 4.061s, episode steps: 305, steps per second: 75, episode reward: 199.043, mean reward: 0.653 [-17.385, 100.000], mean action: 1.282 [0.000, 3.000], mean observation: 0.042 [-0.706, 1.000], loss: 3.107720, mean_absolute_error: 33.031521, mean_q: 44.518303
 1145505/1500000: episode: 2053, duration: 4.336s, episode steps: 330, steps per second: 76, episode reward: 170.293, mean reward: 0.516 [-18.423, 100.000], mean action: 1.448 [0.000, 3.000], mean observation: 0.060 [-0.653, 1.000], loss: 3.899155, mean_absolute_error: 33.022438, mean_q: 44.474426
 1145820/1500000: episode: 2054, duration: 7.180s, episode steps: 315, steps per second: 44, episode reward: 228.884, mean reward: 0.727 [-19.238, 100.000], mean action: 1.029 [0.000, 3.000], mean observation: 0.108 [-0.601, 1.000], loss: 4.375171, mean_absolute_error: 33.161255, mean_q: 44.672726
 1146179/1500000: episode: 2055, duration: 4.185s, episode steps: 359, steps per second: 86, episode reward: 213.705, mean reward: 0.595 [-19.568, 100.000], mean action: 1.150 [0.000, 3.000], mean observation: 0.116 [-0.555, 1.000], loss: 3.240560, mean_absolute_error: 32.915451, mean_q: 44.372246
 1146475/1500000: episode: 2056, duration: 4.513s, episode steps: 296, steps per second: 66, episode reward: 218.117, mean reward: 0.737 [-19.272, 100.000], mean action: 1.189 [0.000, 3.000], mean observation: 0.092 [-0.472, 1.000], loss: 3.928992, mean_absolute_error: 32.821716, mean_q: 44.207481
 1146751/1500000: episode: 2057, duration: 5.839s, episode steps: 276, steps per second: 47, episode reward: 160.580, mean reward: 0.582 [-9.002, 100.000], mean action: 1.678 [0.000, 3.000], mean observation: 0.076 [-0.592, 1.000], loss: 2.340674, mean_absolute_error: 32.820385, mean_q: 44.274403
 1147378/1500000: episode: 2058, duration: 8.814s, episode steps: 627, steps per second: 71, episode reward: 150.054, mean reward: 0.239 [-18.614, 100.000], mean action: 1.038 [0.000, 3.000], mean observation: 0.152 [-0.584, 1.000], loss: 4.279084, mean_absolute_error: 33.030560, mean_q: 44.497597
 1147622/1500000: episode: 2059, duration: 5.706s, episode steps: 244, steps per second: 43, episode reward: 217.584, mean reward: 0.892 [-17.580, 100.000], mean action: 1.090 [0.000, 3.000], mean observation: 0.068 [-0.512, 1.000], loss: 4.278296, mean_absolute_error: 32.935596, mean_q: 44.395485
 1147900/1500000: episode: 2060, duration: 3.264s, episode steps: 278, steps per second: 85, episode reward: 154.634, mean reward: 0.556 [-19.485, 100.000], mean action: 1.029 [0.000, 3.000], mean observation: 0.048 [-0.509, 1.000], loss: 2.900987, mean_absolute_error: 32.937054, mean_q: 44.390469
 1148249/1500000: episode: 2061, duration: 4.861s, episode steps: 349, steps per second: 72, episode reward: 172.839, mean reward: 0.495 [-13.447, 100.000], mean action: 1.880 [0.000, 3.000], mean observation: 0.064 [-0.548, 1.008], loss: 2.951511, mean_absolute_error: 33.097591, mean_q: 44.624439
 1148516/1500000: episode: 2062, duration: 6.106s, episode steps: 267, steps per second: 44, episode reward: 238.342, mean reward: 0.893 [-16.492, 100.000], mean action: 1.270 [0.000, 3.000], mean observation: 0.090 [-0.592, 1.000], loss: 3.564442, mean_absolute_error: 33.027763, mean_q: 44.545280
 1148916/1500000: episode: 2063, duration: 4.980s, episode steps: 400, steps per second: 80, episode reward: 198.609, mean reward: 0.497 [-12.043, 100.000], mean action: 2.132 [0.000, 3.000], mean observation: 0.077 [-0.804, 1.000], loss: 4.304260, mean_absolute_error: 33.180317, mean_q: 44.734844
 1149319/1500000: episode: 2064, duration: 8.467s, episode steps: 403, steps per second: 48, episode reward: 216.539, mean reward: 0.537 [-19.701, 100.000], mean action: 0.829 [0.000, 3.000], mean observation: 0.125 [-0.571, 1.000], loss: 4.521419, mean_absolute_error: 33.048595, mean_q: 44.527382
 1149749/1500000: episode: 2065, duration: 5.484s, episode steps: 430, steps per second: 78, episode reward: 233.768, mean reward: 0.544 [-17.946, 100.000], mean action: 1.095 [0.000, 3.000], mean observation: 0.091 [-0.597, 1.001], loss: 4.312860, mean_absolute_error: 33.368118, mean_q: 44.923218
 1150028/1500000: episode: 2066, duration: 3.930s, episode steps: 279, steps per second: 71, episode reward: 251.029, mean reward: 0.900 [-19.828, 100.000], mean action: 1.233 [0.000, 3.000], mean observation: 0.101 [-0.545, 1.000], loss: 2.295332, mean_absolute_error: 33.079166, mean_q: 44.603363
 1150342/1500000: episode: 2067, duration: 6.835s, episode steps: 314, steps per second: 46, episode reward: 199.938, mean reward: 0.637 [-5.032, 100.000], mean action: 1.433 [0.000, 3.000], mean observation: 0.057 [-0.505, 1.000], loss: 3.773654, mean_absolute_error: 33.351185, mean_q: 44.882584
 1150666/1500000: episode: 2068, duration: 3.837s, episode steps: 324, steps per second: 84, episode reward: 247.092, mean reward: 0.763 [-17.602, 100.000], mean action: 1.364 [0.000, 3.000], mean observation: 0.034 [-0.777, 1.000], loss: 3.699140, mean_absolute_error: 33.347927, mean_q: 44.952301
 1151119/1500000: episode: 2069, duration: 9.514s, episode steps: 453, steps per second: 48, episode reward: 219.713, mean reward: 0.485 [-12.381, 100.000], mean action: 1.174 [0.000, 3.000], mean observation: 0.088 [-0.699, 1.000], loss: 3.289788, mean_absolute_error: 33.396481, mean_q: 45.043221
 1151552/1500000: episode: 2070, duration: 5.578s, episode steps: 433, steps per second: 78, episode reward: 179.554, mean reward: 0.415 [-17.514, 100.000], mean action: 1.379 [0.000, 3.000], mean observation: 0.069 [-0.632, 1.000], loss: 5.906593, mean_absolute_error: 33.344612, mean_q: 44.959946
 1151957/1500000: episode: 2071, duration: 7.519s, episode steps: 405, steps per second: 54, episode reward: 204.506, mean reward: 0.505 [-10.161, 100.000], mean action: 1.237 [0.000, 3.000], mean observation: 0.107 [-0.588, 1.000], loss: 2.856994, mean_absolute_error: 33.547684, mean_q: 45.232075
 1152350/1500000: episode: 2072, duration: 6.076s, episode steps: 393, steps per second: 65, episode reward: 219.854, mean reward: 0.559 [-18.942, 100.000], mean action: 1.288 [0.000, 3.000], mean observation: 0.108 [-0.556, 1.000], loss: 3.679872, mean_absolute_error: 33.245190, mean_q: 44.827480
 1153350/1500000: episode: 2073, duration: 16.475s, episode steps: 1000, steps per second: 61, episode reward: 65.391, mean reward: 0.065 [-18.333, 12.186], mean action: 1.586 [0.000, 3.000], mean observation: 0.169 [-0.574, 1.000], loss: 2.962028, mean_absolute_error: 33.184715, mean_q: 44.748627
 1153737/1500000: episode: 2074, duration: 8.106s, episode steps: 387, steps per second: 48, episode reward: 213.211, mean reward: 0.551 [-2.581, 100.000], mean action: 1.036 [0.000, 3.000], mean observation: 0.108 [-0.414, 1.000], loss: 3.663983, mean_absolute_error: 33.190166, mean_q: 44.733246
 1154164/1500000: episode: 2075, duration: 5.572s, episode steps: 427, steps per second: 77, episode reward: 170.421, mean reward: 0.399 [-10.371, 100.000], mean action: 1.302 [0.000, 3.000], mean observation: 0.098 [-0.609, 1.000], loss: 4.869848, mean_absolute_error: 33.341606, mean_q: 44.965694
 1155164/1500000: episode: 2076, duration: 16.074s, episode steps: 1000, steps per second: 62, episode reward: 83.231, mean reward: 0.083 [-21.899, 13.685], mean action: 1.503 [0.000, 3.000], mean observation: 0.193 [-0.563, 1.000], loss: 3.467295, mean_absolute_error: 33.273956, mean_q: 44.884083
 1156164/1500000: episode: 2077, duration: 17.880s, episode steps: 1000, steps per second: 56, episode reward: 80.164, mean reward: 0.080 [-18.323, 12.014], mean action: 1.875 [0.000, 3.000], mean observation: 0.184 [-0.519, 1.000], loss: 4.036333, mean_absolute_error: 33.422226, mean_q: 45.044041
 1156466/1500000: episode: 2078, duration: 6.688s, episode steps: 302, steps per second: 45, episode reward: 245.415, mean reward: 0.813 [-8.866, 100.000], mean action: 1.394 [0.000, 3.000], mean observation: 0.051 [-0.714, 1.000], loss: 5.788586, mean_absolute_error: 33.495239, mean_q: 45.137821
 1156959/1500000: episode: 2079, duration: 6.037s, episode steps: 493, steps per second: 82, episode reward: 185.778, mean reward: 0.377 [-17.745, 100.000], mean action: 1.150 [0.000, 3.000], mean observation: 0.114 [-0.456, 1.000], loss: 3.409730, mean_absolute_error: 33.327587, mean_q: 44.958099
 1157352/1500000: episode: 2080, duration: 9.500s, episode steps: 393, steps per second: 41, episode reward: 177.871, mean reward: 0.453 [-18.655, 100.000], mean action: 1.305 [0.000, 3.000], mean observation: 0.087 [-0.469, 1.000], loss: 3.508536, mean_absolute_error: 33.286144, mean_q: 44.870617
 1157961/1500000: episode: 2081, duration: 9.385s, episode steps: 609, steps per second: 65, episode reward: 206.789, mean reward: 0.340 [-17.751, 100.000], mean action: 1.095 [0.000, 3.000], mean observation: 0.135 [-0.363, 1.000], loss: 3.658171, mean_absolute_error: 33.307255, mean_q: 44.905594
 1158330/1500000: episode: 2082, duration: 6.825s, episode steps: 369, steps per second: 54, episode reward: 214.011, mean reward: 0.580 [-2.925, 100.000], mean action: 1.425 [0.000, 3.000], mean observation: 0.055 [-0.562, 1.000], loss: 2.739705, mean_absolute_error: 33.247318, mean_q: 44.839188
 1158741/1500000: episode: 2083, duration: 7.182s, episode steps: 411, steps per second: 57, episode reward: 202.905, mean reward: 0.494 [-19.619, 100.000], mean action: 1.275 [0.000, 3.000], mean observation: 0.073 [-0.558, 1.000], loss: 4.114362, mean_absolute_error: 32.993160, mean_q: 44.460243
 1159291/1500000: episode: 2084, duration: 9.386s, episode steps: 550, steps per second: 59, episode reward: 198.103, mean reward: 0.360 [-11.863, 100.000], mean action: 1.035 [0.000, 3.000], mean observation: 0.134 [-0.373, 1.000], loss: 4.880599, mean_absolute_error: 32.940765, mean_q: 44.346474
 1159692/1500000: episode: 2085, duration: 8.273s, episode steps: 401, steps per second: 48, episode reward: 203.508, mean reward: 0.508 [-14.259, 100.000], mean action: 1.327 [0.000, 3.000], mean observation: 0.090 [-0.407, 1.000], loss: 3.131372, mean_absolute_error: 33.024414, mean_q: 44.521889
 1160222/1500000: episode: 2086, duration: 7.494s, episode steps: 530, steps per second: 71, episode reward: 157.801, mean reward: 0.298 [-19.264, 100.000], mean action: 1.883 [0.000, 3.000], mean observation: 0.140 [-0.525, 1.000], loss: 3.816236, mean_absolute_error: 33.066929, mean_q: 44.544846
 1160493/1500000: episode: 2087, duration: 5.306s, episode steps: 271, steps per second: 51, episode reward: 200.875, mean reward: 0.741 [-9.465, 100.000], mean action: 1.716 [0.000, 3.000], mean observation: 0.025 [-0.581, 1.000], loss: 3.226697, mean_absolute_error: 33.031590, mean_q: 44.472374
 1160790/1500000: episode: 2088, duration: 5.922s, episode steps: 297, steps per second: 50, episode reward: 187.933, mean reward: 0.633 [-9.517, 100.000], mean action: 1.397 [0.000, 3.000], mean observation: 0.046 [-0.555, 1.000], loss: 3.325876, mean_absolute_error: 33.184349, mean_q: 44.753990
 1161289/1500000: episode: 2089, duration: 6.706s, episode steps: 499, steps per second: 74, episode reward: 229.857, mean reward: 0.461 [-17.447, 100.000], mean action: 1.238 [0.000, 3.000], mean observation: 0.077 [-0.742, 1.004], loss: 3.452296, mean_absolute_error: 33.292210, mean_q: 44.872143
 1161664/1500000: episode: 2090, duration: 8.091s, episode steps: 375, steps per second: 46, episode reward: 216.428, mean reward: 0.577 [-20.115, 100.000], mean action: 1.243 [0.000, 3.000], mean observation: 0.091 [-0.388, 1.000], loss: 3.498127, mean_absolute_error: 33.338196, mean_q: 44.965218
 1162499/1500000: episode: 2091, duration: 14.611s, episode steps: 835, steps per second: 57, episode reward: 188.222, mean reward: 0.225 [-20.412, 100.000], mean action: 1.786 [0.000, 3.000], mean observation: 0.162 [-0.561, 1.000], loss: 3.730285, mean_absolute_error: 33.199295, mean_q: 44.714931
 1162812/1500000: episode: 2092, duration: 3.763s, episode steps: 313, steps per second: 83, episode reward: 192.126, mean reward: 0.614 [-3.998, 100.000], mean action: 1.633 [0.000, 3.000], mean observation: 0.058 [-0.598, 1.029], loss: 4.151378, mean_absolute_error: 33.342728, mean_q: 44.905575
 1163125/1500000: episode: 2093, duration: 7.032s, episode steps: 313, steps per second: 45, episode reward: 218.342, mean reward: 0.698 [-11.312, 100.000], mean action: 2.083 [0.000, 3.000], mean observation: 0.048 [-0.743, 1.000], loss: 3.220268, mean_absolute_error: 33.235775, mean_q: 44.789555
 1163456/1500000: episode: 2094, duration: 5.136s, episode steps: 331, steps per second: 64, episode reward: 181.049, mean reward: 0.547 [-17.476, 100.000], mean action: 1.021 [0.000, 3.000], mean observation: 0.104 [-0.496, 1.000], loss: 2.721344, mean_absolute_error: 33.349525, mean_q: 44.954193
 1163936/1500000: episode: 2095, duration: 8.082s, episode steps: 480, steps per second: 59, episode reward: 225.971, mean reward: 0.471 [-17.370, 100.000], mean action: 1.231 [0.000, 3.000], mean observation: 0.110 [-0.740, 1.000], loss: 3.536490, mean_absolute_error: 33.380035, mean_q: 44.984219
 1164240/1500000: episode: 2096, duration: 5.676s, episode steps: 304, steps per second: 54, episode reward: 218.794, mean reward: 0.720 [-19.127, 100.000], mean action: 1.016 [0.000, 3.000], mean observation: 0.104 [-0.550, 1.000], loss: 2.667616, mean_absolute_error: 33.479820, mean_q: 45.113495
 1164571/1500000: episode: 2097, duration: 3.814s, episode steps: 331, steps per second: 87, episode reward: 215.709, mean reward: 0.652 [-18.302, 100.000], mean action: 1.502 [0.000, 3.000], mean observation: 0.061 [-0.575, 1.001], loss: 5.055099, mean_absolute_error: 33.664383, mean_q: 45.373772
 1164892/1500000: episode: 2098, duration: 7.372s, episode steps: 321, steps per second: 44, episode reward: 204.464, mean reward: 0.637 [-10.607, 100.000], mean action: 1.012 [0.000, 3.000], mean observation: 0.109 [-0.415, 1.000], loss: 3.524888, mean_absolute_error: 33.683968, mean_q: 45.390324
 1165203/1500000: episode: 2099, duration: 4.497s, episode steps: 311, steps per second: 69, episode reward: 114.300, mean reward: 0.368 [-11.667, 100.000], mean action: 1.939 [0.000, 3.000], mean observation: -0.011 [-0.724, 1.000], loss: 3.911381, mean_absolute_error: 33.934402, mean_q: 45.745487
 1165561/1500000: episode: 2100, duration: 4.599s, episode steps: 358, steps per second: 78, episode reward: 228.834, mean reward: 0.639 [-21.745, 100.000], mean action: 1.067 [0.000, 3.000], mean observation: 0.116 [-0.624, 1.000], loss: 3.325154, mean_absolute_error: 33.914825, mean_q: 45.741108
 1165687/1500000: episode: 2101, duration: 3.541s, episode steps: 126, steps per second: 36, episode reward: -23.508, mean reward: -0.187 [-100.000, 16.779], mean action: 1.683 [0.000, 3.000], mean observation: 0.067 [-0.981, 1.000], loss: 4.286320, mean_absolute_error: 33.972607, mean_q: 45.773129
 1165949/1500000: episode: 2102, duration: 5.276s, episode steps: 262, steps per second: 50, episode reward: 189.535, mean reward: 0.723 [-17.529, 100.000], mean action: 0.962 [0.000, 3.000], mean observation: 0.064 [-0.671, 1.000], loss: 4.020909, mean_absolute_error: 34.041973, mean_q: 45.901764
 1166345/1500000: episode: 2103, duration: 4.669s, episode steps: 396, steps per second: 85, episode reward: 160.902, mean reward: 0.406 [-12.018, 100.000], mean action: 1.457 [0.000, 3.000], mean observation: 0.137 [-0.681, 1.000], loss: 3.683223, mean_absolute_error: 34.150471, mean_q: 46.049992
 1166700/1500000: episode: 2104, duration: 8.629s, episode steps: 355, steps per second: 41, episode reward: 198.658, mean reward: 0.560 [-11.872, 100.000], mean action: 1.420 [0.000, 3.000], mean observation: 0.054 [-0.722, 1.000], loss: 3.387039, mean_absolute_error: 34.099224, mean_q: 45.959717
 1167111/1500000: episode: 2105, duration: 5.382s, episode steps: 411, steps per second: 76, episode reward: 224.396, mean reward: 0.546 [-7.908, 100.000], mean action: 1.258 [0.000, 3.000], mean observation: 0.069 [-0.583, 1.005], loss: 4.057443, mean_absolute_error: 33.936558, mean_q: 45.741749
 1167656/1500000: episode: 2106, duration: 11.088s, episode steps: 545, steps per second: 49, episode reward: 260.249, mean reward: 0.478 [-18.598, 100.000], mean action: 0.818 [0.000, 3.000], mean observation: 0.179 [-0.660, 1.000], loss: 3.196800, mean_absolute_error: 34.204872, mean_q: 46.149139
 1167972/1500000: episode: 2107, duration: 3.518s, episode steps: 316, steps per second: 90, episode reward: 197.286, mean reward: 0.624 [-10.099, 100.000], mean action: 0.816 [0.000, 3.000], mean observation: 0.078 [-0.521, 1.000], loss: 4.209514, mean_absolute_error: 34.377151, mean_q: 46.330784
 1168446/1500000: episode: 2108, duration: 10.493s, episode steps: 474, steps per second: 45, episode reward: 184.876, mean reward: 0.390 [-17.695, 100.000], mean action: 0.766 [0.000, 3.000], mean observation: 0.118 [-0.704, 1.000], loss: 3.950719, mean_absolute_error: 34.462116, mean_q: 46.453205
 1168979/1500000: episode: 2109, duration: 6.620s, episode steps: 533, steps per second: 81, episode reward: 184.068, mean reward: 0.345 [-18.622, 100.000], mean action: 1.193 [0.000, 3.000], mean observation: 0.096 [-0.632, 1.000], loss: 4.134881, mean_absolute_error: 34.185349, mean_q: 46.094330
 1169241/1500000: episode: 2110, duration: 7.055s, episode steps: 262, steps per second: 37, episode reward: -48.364, mean reward: -0.185 [-100.000, 17.759], mean action: 1.599 [0.000, 3.000], mean observation: 0.008 [-0.530, 1.000], loss: 2.705216, mean_absolute_error: 34.251637, mean_q: 46.238457
 1169452/1500000: episode: 2111, duration: 3.596s, episode steps: 211, steps per second: 59, episode reward: 177.288, mean reward: 0.840 [-2.950, 100.000], mean action: 1.384 [0.000, 3.000], mean observation: 0.034 [-0.743, 1.000], loss: 3.258402, mean_absolute_error: 34.564228, mean_q: 46.597599
 1169720/1500000: episode: 2112, duration: 2.988s, episode steps: 268, steps per second: 90, episode reward: 226.511, mean reward: 0.845 [-19.744, 100.000], mean action: 1.265 [0.000, 3.000], mean observation: 0.097 [-0.564, 1.000], loss: 2.920797, mean_absolute_error: 34.415062, mean_q: 46.466858
 1169991/1500000: episode: 2113, duration: 5.864s, episode steps: 271, steps per second: 46, episode reward: 186.153, mean reward: 0.687 [-11.014, 100.000], mean action: 1.299 [0.000, 3.000], mean observation: 0.077 [-0.450, 1.000], loss: 3.691443, mean_absolute_error: 34.542908, mean_q: 46.560856
 1170277/1500000: episode: 2114, duration: 5.313s, episode steps: 286, steps per second: 54, episode reward: 223.815, mean reward: 0.783 [-2.711, 100.000], mean action: 1.346 [0.000, 3.000], mean observation: 0.084 [-0.571, 1.000], loss: 3.698739, mean_absolute_error: 34.776791, mean_q: 46.893116
 1170830/1500000: episode: 2115, duration: 8.747s, episode steps: 553, steps per second: 63, episode reward: 227.305, mean reward: 0.411 [-17.398, 100.000], mean action: 0.834 [0.000, 3.000], mean observation: 0.115 [-0.665, 1.000], loss: 3.104038, mean_absolute_error: 34.730724, mean_q: 46.837677
 1171161/1500000: episode: 2116, duration: 6.346s, episode steps: 331, steps per second: 52, episode reward: 182.389, mean reward: 0.551 [-13.759, 100.000], mean action: 1.840 [0.000, 3.000], mean observation: 0.049 [-0.511, 1.000], loss: 3.101300, mean_absolute_error: 34.739883, mean_q: 46.874790
 1171581/1500000: episode: 2117, duration: 5.327s, episode steps: 420, steps per second: 79, episode reward: 204.932, mean reward: 0.488 [-20.065, 100.000], mean action: 0.969 [0.000, 3.000], mean observation: 0.079 [-0.685, 1.000], loss: 3.624493, mean_absolute_error: 34.734879, mean_q: 46.848343
 1172209/1500000: episode: 2118, duration: 11.893s, episode steps: 628, steps per second: 53, episode reward: 184.607, mean reward: 0.294 [-18.959, 100.000], mean action: 1.215 [0.000, 3.000], mean observation: 0.112 [-0.915, 1.000], loss: 3.621551, mean_absolute_error: 34.826199, mean_q: 46.953762
 1172568/1500000: episode: 2119, duration: 6.385s, episode steps: 359, steps per second: 56, episode reward: 206.399, mean reward: 0.575 [-19.092, 100.000], mean action: 1.109 [0.000, 3.000], mean observation: 0.106 [-0.414, 1.000], loss: 3.616622, mean_absolute_error: 34.794720, mean_q: 46.901588
 1172759/1500000: episode: 2120, duration: 4.590s, episode steps: 191, steps per second: 42, episode reward: -30.138, mean reward: -0.158 [-100.000, 16.763], mean action: 1.654 [0.000, 3.000], mean observation: 0.004 [-0.525, 1.000], loss: 4.116152, mean_absolute_error: 34.852383, mean_q: 47.010429
 1173352/1500000: episode: 2121, duration: 9.015s, episode steps: 593, steps per second: 66, episode reward: 161.803, mean reward: 0.273 [-19.037, 100.000], mean action: 2.273 [0.000, 3.000], mean observation: 0.138 [-0.498, 1.010], loss: 4.941715, mean_absolute_error: 34.913357, mean_q: 47.085346
 1173845/1500000: episode: 2122, duration: 9.171s, episode steps: 493, steps per second: 54, episode reward: 243.459, mean reward: 0.494 [-18.014, 100.000], mean action: 0.856 [0.000, 3.000], mean observation: 0.165 [-0.694, 1.000], loss: 4.543804, mean_absolute_error: 34.988956, mean_q: 47.187042
 1174166/1500000: episode: 2123, duration: 4.028s, episode steps: 321, steps per second: 80, episode reward: 139.036, mean reward: 0.433 [-18.226, 100.000], mean action: 1.383 [0.000, 3.000], mean observation: 0.071 [-0.817, 1.000], loss: 4.090003, mean_absolute_error: 35.135315, mean_q: 47.403614
 1174450/1500000: episode: 2124, duration: 7.149s, episode steps: 284, steps per second: 40, episode reward: 224.579, mean reward: 0.791 [-9.883, 100.000], mean action: 1.507 [0.000, 3.000], mean observation: 0.033 [-0.709, 1.000], loss: 4.473870, mean_absolute_error: 35.332176, mean_q: 47.670357
 1174710/1500000: episode: 2125, duration: 3.747s, episode steps: 260, steps per second: 69, episode reward: 204.290, mean reward: 0.786 [-10.075, 100.000], mean action: 1.192 [0.000, 3.000], mean observation: 0.074 [-0.546, 1.014], loss: 3.492533, mean_absolute_error: 35.418076, mean_q: 47.842400
 1175001/1500000: episode: 2126, duration: 3.507s, episode steps: 291, steps per second: 83, episode reward: 212.285, mean reward: 0.730 [-13.325, 100.000], mean action: 1.261 [0.000, 3.000], mean observation: 0.042 [-0.687, 1.000], loss: 4.230668, mean_absolute_error: 35.496788, mean_q: 47.885101
 1175212/1500000: episode: 2127, duration: 4.805s, episode steps: 211, steps per second: 44, episode reward: -34.714, mean reward: -0.165 [-100.000, 12.959], mean action: 1.536 [0.000, 3.000], mean observation: -0.011 [-0.554, 1.000], loss: 2.704095, mean_absolute_error: 35.330776, mean_q: 47.647308
 1175439/1500000: episode: 2128, duration: 5.052s, episode steps: 227, steps per second: 45, episode reward: -14.414, mean reward: -0.063 [-100.000, 17.711], mean action: 1.709 [0.000, 3.000], mean observation: -0.037 [-0.849, 1.000], loss: 4.942511, mean_absolute_error: 35.410736, mean_q: 47.805729
 1175702/1500000: episode: 2129, duration: 3.136s, episode steps: 263, steps per second: 84, episode reward: 156.987, mean reward: 0.597 [-12.920, 100.000], mean action: 1.814 [0.000, 3.000], mean observation: 0.024 [-0.721, 1.000], loss: 4.181163, mean_absolute_error: 35.448231, mean_q: 47.833027
 1176074/1500000: episode: 2130, duration: 7.347s, episode steps: 372, steps per second: 51, episode reward: 201.564, mean reward: 0.542 [-16.018, 100.000], mean action: 1.871 [0.000, 3.000], mean observation: 0.124 [-0.767, 1.000], loss: 5.300837, mean_absolute_error: 35.525333, mean_q: 47.886173
 1176333/1500000: episode: 2131, duration: 4.974s, episode steps: 259, steps per second: 52, episode reward: 205.652, mean reward: 0.794 [-13.303, 100.000], mean action: 1.398 [0.000, 3.000], mean observation: 0.025 [-0.934, 1.000], loss: 3.041942, mean_absolute_error: 35.372498, mean_q: 47.760147
 1176630/1500000: episode: 2132, duration: 3.293s, episode steps: 297, steps per second: 90, episode reward: 222.856, mean reward: 0.750 [-2.812, 100.000], mean action: 1.367 [0.000, 3.000], mean observation: 0.088 [-0.451, 1.000], loss: 3.565878, mean_absolute_error: 35.258102, mean_q: 47.587139
 1176971/1500000: episode: 2133, duration: 7.050s, episode steps: 341, steps per second: 48, episode reward: 187.830, mean reward: 0.551 [-2.771, 100.000], mean action: 1.375 [0.000, 3.000], mean observation: 0.081 [-0.355, 1.000], loss: 4.366173, mean_absolute_error: 35.606842, mean_q: 48.007824
 1177361/1500000: episode: 2134, duration: 6.516s, episode steps: 390, steps per second: 60, episode reward: 204.946, mean reward: 0.526 [-2.979, 100.000], mean action: 0.997 [0.000, 3.000], mean observation: 0.119 [-0.377, 1.000], loss: 4.815280, mean_absolute_error: 35.515430, mean_q: 47.869297
 1177788/1500000: episode: 2135, duration: 6.741s, episode steps: 427, steps per second: 63, episode reward: 232.237, mean reward: 0.544 [-17.339, 100.000], mean action: 1.087 [0.000, 3.000], mean observation: 0.133 [-0.472, 1.000], loss: 3.644644, mean_absolute_error: 35.552845, mean_q: 47.926613
 1178123/1500000: episode: 2136, duration: 6.813s, episode steps: 335, steps per second: 49, episode reward: 214.671, mean reward: 0.641 [-19.896, 100.000], mean action: 1.081 [0.000, 3.000], mean observation: 0.109 [-0.391, 1.000], loss: 4.953994, mean_absolute_error: 35.582218, mean_q: 47.988079
 1178455/1500000: episode: 2137, duration: 3.861s, episode steps: 332, steps per second: 86, episode reward: 236.424, mean reward: 0.712 [-12.359, 100.000], mean action: 1.373 [0.000, 3.000], mean observation: 0.058 [-0.587, 1.000], loss: 5.226601, mean_absolute_error: 35.387646, mean_q: 47.730846
 1179266/1500000: episode: 2138, duration: 14.627s, episode steps: 811, steps per second: 55, episode reward: 138.126, mean reward: 0.170 [-18.873, 100.000], mean action: 2.037 [0.000, 3.000], mean observation: 0.161 [-0.526, 1.000], loss: 4.151670, mean_absolute_error: 35.423935, mean_q: 47.764652
 1179526/1500000: episode: 2139, duration: 4.470s, episode steps: 260, steps per second: 58, episode reward: 236.365, mean reward: 0.909 [-2.808, 100.000], mean action: 1.538 [0.000, 3.000], mean observation: 0.019 [-0.731, 1.000], loss: 4.160293, mean_absolute_error: 35.787663, mean_q: 48.219749
 1179921/1500000: episode: 2140, duration: 8.442s, episode steps: 395, steps per second: 47, episode reward: 185.051, mean reward: 0.468 [-17.333, 100.000], mean action: 1.570 [0.000, 3.000], mean observation: 0.078 [-0.487, 1.000], loss: 4.232364, mean_absolute_error: 35.623524, mean_q: 48.030499
 1180312/1500000: episode: 2141, duration: 5.697s, episode steps: 391, steps per second: 69, episode reward: 218.453, mean reward: 0.559 [-20.433, 100.000], mean action: 1.061 [0.000, 3.000], mean observation: 0.091 [-0.656, 1.000], loss: 2.444512, mean_absolute_error: 35.338703, mean_q: 47.716522
 1180589/1500000: episode: 2142, duration: 7.026s, episode steps: 277, steps per second: 39, episode reward: 222.813, mean reward: 0.804 [-3.282, 100.000], mean action: 1.632 [0.000, 3.000], mean observation: 0.074 [-0.498, 1.000], loss: 5.016866, mean_absolute_error: 35.639778, mean_q: 48.037823
 1180864/1500000: episode: 2143, duration: 3.589s, episode steps: 275, steps per second: 77, episode reward: 227.444, mean reward: 0.827 [-9.613, 100.000], mean action: 1.342 [0.000, 3.000], mean observation: 0.078 [-0.532, 1.000], loss: 3.940207, mean_absolute_error: 35.544888, mean_q: 47.908287
 1181209/1500000: episode: 2144, duration: 5.182s, episode steps: 345, steps per second: 67, episode reward: 213.747, mean reward: 0.620 [-17.454, 100.000], mean action: 1.301 [0.000, 3.000], mean observation: 0.070 [-0.477, 1.000], loss: 3.009798, mean_absolute_error: 35.587273, mean_q: 47.968910
 1181478/1500000: episode: 2145, duration: 6.734s, episode steps: 269, steps per second: 40, episode reward: 233.449, mean reward: 0.868 [-9.230, 100.000], mean action: 1.275 [0.000, 3.000], mean observation: 0.089 [-0.587, 1.000], loss: 3.550970, mean_absolute_error: 35.659443, mean_q: 48.048943
 1181953/1500000: episode: 2146, duration: 5.886s, episode steps: 475, steps per second: 81, episode reward: 211.121, mean reward: 0.444 [-19.530, 100.000], mean action: 1.023 [0.000, 3.000], mean observation: 0.135 [-0.531, 1.000], loss: 2.721646, mean_absolute_error: 35.449093, mean_q: 47.786530
 1182191/1500000: episode: 2147, duration: 4.985s, episode steps: 238, steps per second: 48, episode reward: 163.417, mean reward: 0.687 [-3.568, 100.000], mean action: 1.218 [0.000, 3.000], mean observation: 0.031 [-0.497, 1.000], loss: 4.137282, mean_absolute_error: 35.559864, mean_q: 47.934586
 1182486/1500000: episode: 2148, duration: 6.314s, episode steps: 295, steps per second: 47, episode reward: 202.631, mean reward: 0.687 [-3.100, 100.000], mean action: 1.241 [0.000, 3.000], mean observation: 0.051 [-0.496, 1.000], loss: 2.387436, mean_absolute_error: 35.221897, mean_q: 47.531357
 1182839/1500000: episode: 2149, duration: 4.245s, episode steps: 353, steps per second: 83, episode reward: 221.474, mean reward: 0.627 [-8.610, 100.000], mean action: 1.470 [0.000, 3.000], mean observation: 0.092 [-0.478, 1.000], loss: 3.204994, mean_absolute_error: 35.254887, mean_q: 47.534073
 1183361/1500000: episode: 2150, duration: 11.024s, episode steps: 522, steps per second: 47, episode reward: 232.735, mean reward: 0.446 [-18.360, 100.000], mean action: 0.841 [0.000, 3.000], mean observation: 0.148 [-0.506, 1.000], loss: 4.114039, mean_absolute_error: 34.999790, mean_q: 47.205048
 1183657/1500000: episode: 2151, duration: 3.599s, episode steps: 296, steps per second: 82, episode reward: 254.575, mean reward: 0.860 [-9.397, 100.000], mean action: 1.480 [0.000, 3.000], mean observation: 0.043 [-0.868, 1.000], loss: 5.605259, mean_absolute_error: 35.351036, mean_q: 47.579182
 1184002/1500000: episode: 2152, duration: 7.045s, episode steps: 345, steps per second: 49, episode reward: 177.175, mean reward: 0.514 [-17.336, 100.000], mean action: 1.261 [0.000, 3.000], mean observation: 0.075 [-0.435, 1.000], loss: 4.775702, mean_absolute_error: 35.330147, mean_q: 47.630424
 1184333/1500000: episode: 2153, duration: 6.214s, episode steps: 331, steps per second: 53, episode reward: 187.604, mean reward: 0.567 [-11.648, 100.000], mean action: 1.227 [0.000, 3.000], mean observation: 0.072 [-0.474, 1.000], loss: 2.859093, mean_absolute_error: 35.222919, mean_q: 47.518967
 1184676/1500000: episode: 2154, duration: 4.451s, episode steps: 343, steps per second: 77, episode reward: 195.386, mean reward: 0.570 [-17.655, 100.000], mean action: 1.131 [0.000, 3.000], mean observation: 0.083 [-0.383, 1.000], loss: 4.271603, mean_absolute_error: 35.090588, mean_q: 47.326694
 1185432/1500000: episode: 2155, duration: 15.707s, episode steps: 756, steps per second: 48, episode reward: 209.360, mean reward: 0.277 [-17.461, 100.000], mean action: 2.139 [0.000, 3.000], mean observation: 0.153 [-0.699, 1.020], loss: 4.869487, mean_absolute_error: 35.095196, mean_q: 47.328762
 1185619/1500000: episode: 2156, duration: 5.031s, episode steps: 187, steps per second: 37, episode reward: 154.377, mean reward: 0.826 [-18.011, 100.000], mean action: 1.342 [0.000, 3.000], mean observation: -0.019 [-1.104, 1.000], loss: 5.627973, mean_absolute_error: 35.336838, mean_q: 47.615589
 1186056/1500000: episode: 2157, duration: 8.122s, episode steps: 437, steps per second: 54, episode reward: 198.840, mean reward: 0.455 [-18.932, 100.000], mean action: 1.007 [0.000, 3.000], mean observation: 0.125 [-0.406, 1.000], loss: 4.339509, mean_absolute_error: 35.487770, mean_q: 47.859245
 1186310/1500000: episode: 2158, duration: 5.818s, episode steps: 254, steps per second: 44, episode reward: 179.970, mean reward: 0.709 [-2.544, 100.000], mean action: 1.319 [0.000, 3.000], mean observation: 0.041 [-0.436, 1.000], loss: 2.486654, mean_absolute_error: 35.209488, mean_q: 47.537285
 1186791/1500000: episode: 2159, duration: 8.443s, episode steps: 481, steps per second: 57, episode reward: 221.308, mean reward: 0.460 [-18.203, 100.000], mean action: 1.129 [0.000, 3.000], mean observation: 0.100 [-0.613, 1.000], loss: 3.808338, mean_absolute_error: 35.297745, mean_q: 47.582016
 1187056/1500000: episode: 2160, duration: 3.690s, episode steps: 265, steps per second: 72, episode reward: 193.284, mean reward: 0.729 [-17.659, 100.000], mean action: 1.015 [0.000, 3.000], mean observation: 0.067 [-0.487, 1.000], loss: 6.000646, mean_absolute_error: 35.190037, mean_q: 47.462601
 1187337/1500000: episode: 2161, duration: 7.239s, episode steps: 281, steps per second: 39, episode reward: 214.655, mean reward: 0.764 [-9.883, 100.000], mean action: 1.349 [0.000, 3.000], mean observation: 0.068 [-0.496, 1.000], loss: 5.574038, mean_absolute_error: 35.095779, mean_q: 47.341457
 1187856/1500000: episode: 2162, duration: 7.113s, episode steps: 519, steps per second: 73, episode reward: 229.847, mean reward: 0.443 [-3.340, 100.000], mean action: 1.291 [0.000, 3.000], mean observation: 0.095 [-0.797, 1.013], loss: 3.758098, mean_absolute_error: 35.295837, mean_q: 47.581036
 1188318/1500000: episode: 2163, duration: 10.654s, episode steps: 462, steps per second: 43, episode reward: 240.364, mean reward: 0.520 [-10.369, 100.000], mean action: 1.035 [0.000, 3.000], mean observation: 0.142 [-0.561, 1.000], loss: 3.715327, mean_absolute_error: 35.183823, mean_q: 47.475632
 1188669/1500000: episode: 2164, duration: 3.981s, episode steps: 351, steps per second: 88, episode reward: 247.285, mean reward: 0.705 [-18.370, 100.000], mean action: 1.473 [0.000, 3.000], mean observation: 0.106 [-0.600, 1.000], loss: 4.722168, mean_absolute_error: 35.249599, mean_q: 47.512554
 1188971/1500000: episode: 2165, duration: 6.977s, episode steps: 302, steps per second: 43, episode reward: 185.751, mean reward: 0.615 [-9.198, 100.000], mean action: 1.252 [0.000, 3.000], mean observation: 0.081 [-0.414, 1.000], loss: 3.729615, mean_absolute_error: 35.379398, mean_q: 47.700768
 1189224/1500000: episode: 2166, duration: 4.661s, episode steps: 253, steps per second: 54, episode reward: 195.480, mean reward: 0.773 [-2.813, 100.000], mean action: 1.170 [0.000, 3.000], mean observation: 0.034 [-0.498, 1.000], loss: 3.326557, mean_absolute_error: 35.621887, mean_q: 48.036652
 1189712/1500000: episode: 2167, duration: 7.247s, episode steps: 488, steps per second: 67, episode reward: 210.792, mean reward: 0.432 [-17.387, 100.000], mean action: 2.162 [0.000, 3.000], mean observation: 0.117 [-0.881, 1.000], loss: 5.846667, mean_absolute_error: 35.376465, mean_q: 47.674229
 1190145/1500000: episode: 2168, duration: 9.166s, episode steps: 433, steps per second: 47, episode reward: 205.298, mean reward: 0.474 [-19.397, 100.000], mean action: 1.194 [0.000, 3.000], mean observation: 0.122 [-0.362, 1.000], loss: 3.786867, mean_absolute_error: 35.445446, mean_q: 47.812824
 1190624/1500000: episode: 2169, duration: 7.879s, episode steps: 479, steps per second: 61, episode reward: 203.064, mean reward: 0.424 [-9.051, 100.000], mean action: 1.296 [0.000, 3.000], mean observation: 0.095 [-0.476, 1.000], loss: 3.794236, mean_absolute_error: 35.342762, mean_q: 47.669552
 1191030/1500000: episode: 2170, duration: 8.082s, episode steps: 406, steps per second: 50, episode reward: 204.774, mean reward: 0.504 [-11.876, 100.000], mean action: 1.404 [0.000, 3.000], mean observation: 0.075 [-0.454, 1.000], loss: 4.553428, mean_absolute_error: 35.258198, mean_q: 47.519142
 1191362/1500000: episode: 2171, duration: 4.786s, episode steps: 332, steps per second: 69, episode reward: 164.276, mean reward: 0.495 [-13.846, 100.000], mean action: 2.075 [0.000, 3.000], mean observation: 0.058 [-0.589, 1.000], loss: 4.092814, mean_absolute_error: 35.285572, mean_q: 47.625443
 1191778/1500000: episode: 2172, duration: 9.249s, episode steps: 416, steps per second: 45, episode reward: 241.375, mean reward: 0.580 [-10.291, 100.000], mean action: 1.238 [0.000, 3.000], mean observation: 0.072 [-0.777, 1.000], loss: 5.158968, mean_absolute_error: 35.436340, mean_q: 47.770134
 1192446/1500000: episode: 2173, duration: 12.092s, episode steps: 668, steps per second: 55, episode reward: 193.536, mean reward: 0.290 [-19.335, 100.000], mean action: 1.010 [0.000, 3.000], mean observation: 0.115 [-0.608, 1.000], loss: 3.859812, mean_absolute_error: 35.431988, mean_q: 47.779278
 1192765/1500000: episode: 2174, duration: 5.448s, episode steps: 319, steps per second: 59, episode reward: 228.412, mean reward: 0.716 [-10.232, 100.000], mean action: 1.389 [0.000, 3.000], mean observation: 0.087 [-0.522, 1.000], loss: 4.202984, mean_absolute_error: 35.374260, mean_q: 47.694035
 1193131/1500000: episode: 2175, duration: 6.376s, episode steps: 366, steps per second: 57, episode reward: 224.654, mean reward: 0.614 [-18.357, 100.000], mean action: 1.158 [0.000, 3.000], mean observation: 0.107 [-0.411, 1.015], loss: 3.482906, mean_absolute_error: 35.410286, mean_q: 47.751846
 1193384/1500000: episode: 2176, duration: 6.111s, episode steps: 253, steps per second: 41, episode reward: 193.166, mean reward: 0.764 [-2.890, 100.000], mean action: 1.644 [0.000, 3.000], mean observation: 0.050 [-0.464, 1.000], loss: 3.285163, mean_absolute_error: 35.593510, mean_q: 48.056595
 1193691/1500000: episode: 2177, duration: 3.853s, episode steps: 307, steps per second: 80, episode reward: 201.198, mean reward: 0.655 [-2.831, 100.000], mean action: 1.466 [0.000, 3.000], mean observation: 0.077 [-0.459, 1.000], loss: 3.683742, mean_absolute_error: 35.216675, mean_q: 47.451523
 1194019/1500000: episode: 2178, duration: 6.256s, episode steps: 328, steps per second: 52, episode reward: 204.623, mean reward: 0.624 [-2.864, 100.000], mean action: 1.284 [0.000, 3.000], mean observation: 0.099 [-0.387, 1.000], loss: 4.745817, mean_absolute_error: 35.516418, mean_q: 47.919998
 1194489/1500000: episode: 2179, duration: 8.627s, episode steps: 470, steps per second: 54, episode reward: 187.167, mean reward: 0.398 [-19.523, 100.000], mean action: 0.966 [0.000, 3.000], mean observation: 0.121 [-0.444, 1.000], loss: 2.864468, mean_absolute_error: 35.331158, mean_q: 47.678558
 1194770/1500000: episode: 2180, duration: 4.123s, episode steps: 281, steps per second: 68, episode reward: 211.873, mean reward: 0.754 [-2.992, 100.000], mean action: 1.441 [0.000, 3.000], mean observation: 0.034 [-0.601, 1.000], loss: 3.571601, mean_absolute_error: 35.292236, mean_q: 47.566734
 1195112/1500000: episode: 2181, duration: 8.495s, episode steps: 342, steps per second: 40, episode reward: 206.707, mean reward: 0.604 [-17.017, 100.000], mean action: 1.883 [0.000, 3.000], mean observation: 0.110 [-0.526, 1.000], loss: 2.103908, mean_absolute_error: 35.269886, mean_q: 47.564201
 1195504/1500000: episode: 2182, duration: 4.444s, episode steps: 392, steps per second: 88, episode reward: 197.053, mean reward: 0.503 [-11.776, 100.000], mean action: 1.237 [0.000, 3.000], mean observation: 0.089 [-0.501, 1.000], loss: 4.559660, mean_absolute_error: 35.359558, mean_q: 47.645496
 1195799/1500000: episode: 2183, duration: 5.945s, episode steps: 295, steps per second: 50, episode reward: 170.316, mean reward: 0.577 [-12.182, 100.000], mean action: 1.915 [0.000, 3.000], mean observation: 0.048 [-0.581, 1.000], loss: 4.227140, mean_absolute_error: 35.293255, mean_q: 47.621056
 1196166/1500000: episode: 2184, duration: 7.257s, episode steps: 367, steps per second: 51, episode reward: 214.588, mean reward: 0.585 [-10.063, 100.000], mean action: 1.003 [0.000, 3.000], mean observation: 0.113 [-0.414, 1.000], loss: 3.835304, mean_absolute_error: 35.120697, mean_q: 47.370613
 1196438/1500000: episode: 2185, duration: 3.210s, episode steps: 272, steps per second: 85, episode reward: 190.116, mean reward: 0.699 [-2.576, 100.000], mean action: 0.971 [0.000, 3.000], mean observation: 0.073 [-0.496, 1.000], loss: 4.499812, mean_absolute_error: 35.372505, mean_q: 47.726208
 1196754/1500000: episode: 2186, duration: 8.032s, episode steps: 316, steps per second: 39, episode reward: 192.406, mean reward: 0.609 [-15.998, 100.000], mean action: 1.434 [0.000, 3.000], mean observation: 0.011 [-0.903, 1.000], loss: 2.665690, mean_absolute_error: 35.346771, mean_q: 47.681393
 1196978/1500000: episode: 2187, duration: 3.865s, episode steps: 224, steps per second: 58, episode reward: 154.394, mean reward: 0.689 [-3.074, 100.000], mean action: 1.250 [0.000, 3.000], mean observation: 0.051 [-0.985, 1.000], loss: 4.841493, mean_absolute_error: 35.224705, mean_q: 47.521534
 1197381/1500000: episode: 2188, duration: 5.614s, episode steps: 403, steps per second: 72, episode reward: 175.325, mean reward: 0.435 [-12.449, 100.000], mean action: 1.325 [0.000, 3.000], mean observation: 0.027 [-0.778, 1.000], loss: 3.577663, mean_absolute_error: 35.393181, mean_q: 47.724766
 1197700/1500000: episode: 2189, duration: 8.057s, episode steps: 319, steps per second: 40, episode reward: 211.474, mean reward: 0.663 [-10.377, 100.000], mean action: 0.937 [0.000, 3.000], mean observation: 0.093 [-0.542, 1.000], loss: 4.478249, mean_absolute_error: 35.335865, mean_q: 47.620651
 1197979/1500000: episode: 2190, duration: 3.542s, episode steps: 279, steps per second: 79, episode reward: 196.490, mean reward: 0.704 [-9.822, 100.000], mean action: 1.362 [0.000, 3.000], mean observation: 0.059 [-0.536, 1.000], loss: 3.571402, mean_absolute_error: 35.303070, mean_q: 47.557152
 1198325/1500000: episode: 2191, duration: 5.723s, episode steps: 346, steps per second: 60, episode reward: 178.917, mean reward: 0.517 [-18.094, 100.000], mean action: 1.289 [0.000, 3.000], mean observation: 0.090 [-0.477, 1.000], loss: 4.002796, mean_absolute_error: 35.382450, mean_q: 47.716515
 1198710/1500000: episode: 2192, duration: 9.464s, episode steps: 385, steps per second: 41, episode reward: 199.456, mean reward: 0.518 [-17.407, 100.000], mean action: 1.491 [0.000, 3.000], mean observation: 0.071 [-0.470, 1.000], loss: 4.626474, mean_absolute_error: 35.356148, mean_q: 47.681660
 1198999/1500000: episode: 2193, duration: 3.263s, episode steps: 289, steps per second: 89, episode reward: 216.923, mean reward: 0.751 [-9.156, 100.000], mean action: 1.433 [0.000, 3.000], mean observation: 0.061 [-0.545, 1.000], loss: 5.516142, mean_absolute_error: 35.472832, mean_q: 47.862343
 1199311/1500000: episode: 2194, duration: 6.140s, episode steps: 312, steps per second: 51, episode reward: 217.480, mean reward: 0.697 [-2.612, 100.000], mean action: 1.122 [0.000, 3.000], mean observation: 0.095 [-0.472, 1.000], loss: 3.988255, mean_absolute_error: 35.569836, mean_q: 47.912640
 1199666/1500000: episode: 2195, duration: 7.219s, episode steps: 355, steps per second: 49, episode reward: 216.363, mean reward: 0.609 [-17.740, 100.000], mean action: 1.087 [0.000, 3.000], mean observation: 0.092 [-0.388, 1.000], loss: 2.807170, mean_absolute_error: 35.351925, mean_q: 47.680965
 1200158/1500000: episode: 2196, duration: 7.857s, episode steps: 492, steps per second: 63, episode reward: 213.229, mean reward: 0.433 [-19.621, 100.000], mean action: 1.559 [0.000, 3.000], mean observation: 0.125 [-0.768, 1.000], loss: 4.490066, mean_absolute_error: 35.528187, mean_q: 47.906013
 1200418/1500000: episode: 2197, duration: 6.466s, episode steps: 260, steps per second: 40, episode reward: 183.173, mean reward: 0.705 [-17.794, 100.000], mean action: 1.369 [0.000, 3.000], mean observation: 0.037 [-0.604, 1.000], loss: 6.092697, mean_absolute_error: 35.502254, mean_q: 47.834969
 1200953/1500000: episode: 2198, duration: 7.415s, episode steps: 535, steps per second: 72, episode reward: 222.734, mean reward: 0.416 [-17.570, 100.000], mean action: 0.916 [0.000, 3.000], mean observation: 0.137 [-0.770, 1.000], loss: 5.188464, mean_absolute_error: 35.347057, mean_q: 47.669254
 1201363/1500000: episode: 2199, duration: 9.634s, episode steps: 410, steps per second: 43, episode reward: 193.753, mean reward: 0.473 [-19.417, 100.000], mean action: 1.212 [0.000, 3.000], mean observation: 0.063 [-0.544, 1.000], loss: 3.654187, mean_absolute_error: 35.291386, mean_q: 47.589676
 1201656/1500000: episode: 2200, duration: 3.258s, episode steps: 293, steps per second: 90, episode reward: 237.773, mean reward: 0.812 [-9.509, 100.000], mean action: 1.324 [0.000, 3.000], mean observation: 0.082 [-0.606, 1.000], loss: 6.280032, mean_absolute_error: 35.542507, mean_q: 47.871300
 1202176/1500000: episode: 2201, duration: 11.544s, episode steps: 520, steps per second: 45, episode reward: 168.928, mean reward: 0.325 [-18.306, 100.000], mean action: 0.667 [0.000, 3.000], mean observation: 0.130 [-0.538, 1.000], loss: 4.686691, mean_absolute_error: 35.637997, mean_q: 48.060188
 1202459/1500000: episode: 2202, duration: 5.730s, episode steps: 283, steps per second: 49, episode reward: 191.921, mean reward: 0.678 [-8.539, 100.000], mean action: 1.180 [0.000, 3.000], mean observation: 0.076 [-0.457, 1.000], loss: 4.172083, mean_absolute_error: 35.734360, mean_q: 48.135166
 1202809/1500000: episode: 2203, duration: 9.063s, episode steps: 350, steps per second: 39, episode reward: 255.919, mean reward: 0.731 [-2.479, 100.000], mean action: 1.063 [0.000, 3.000], mean observation: 0.132 [-0.652, 1.000], loss: 4.483300, mean_absolute_error: 35.684368, mean_q: 48.113293
 1203152/1500000: episode: 2204, duration: 8.354s, episode steps: 343, steps per second: 41, episode reward: 199.956, mean reward: 0.583 [-17.367, 100.000], mean action: 1.382 [0.000, 3.000], mean observation: 0.064 [-0.505, 1.000], loss: 3.738426, mean_absolute_error: 35.621651, mean_q: 48.023006
 1203436/1500000: episode: 2205, duration: 7.344s, episode steps: 284, steps per second: 39, episode reward: 244.500, mean reward: 0.861 [-9.683, 100.000], mean action: 1.306 [0.000, 3.000], mean observation: 0.039 [-0.765, 1.000], loss: 6.486230, mean_absolute_error: 35.626625, mean_q: 48.030277
 1203733/1500000: episode: 2206, duration: 7.237s, episode steps: 297, steps per second: 41, episode reward: 195.084, mean reward: 0.657 [-9.093, 100.000], mean action: 1.040 [0.000, 3.000], mean observation: 0.073 [-0.491, 1.000], loss: 3.397996, mean_absolute_error: 35.618721, mean_q: 48.069389
 1204034/1500000: episode: 2207, duration: 7.705s, episode steps: 301, steps per second: 39, episode reward: 252.365, mean reward: 0.838 [-3.774, 100.000], mean action: 1.239 [0.000, 3.000], mean observation: 0.122 [-0.688, 1.000], loss: 3.262712, mean_absolute_error: 35.874836, mean_q: 48.358154
 1204373/1500000: episode: 2208, duration: 8.404s, episode steps: 339, steps per second: 40, episode reward: 159.853, mean reward: 0.472 [-17.877, 100.000], mean action: 1.153 [0.000, 3.000], mean observation: 0.066 [-0.520, 1.000], loss: 3.947571, mean_absolute_error: 35.724293, mean_q: 48.190601
 1204726/1500000: episode: 2209, duration: 8.731s, episode steps: 353, steps per second: 40, episode reward: 217.627, mean reward: 0.617 [-10.811, 100.000], mean action: 1.249 [0.000, 3.000], mean observation: 0.099 [-0.474, 1.006], loss: 5.167883, mean_absolute_error: 35.629322, mean_q: 48.050228
 1205217/1500000: episode: 2210, duration: 12.240s, episode steps: 491, steps per second: 40, episode reward: 214.264, mean reward: 0.436 [-11.455, 100.000], mean action: 1.110 [0.000, 3.000], mean observation: 0.136 [-0.535, 1.000], loss: 4.647738, mean_absolute_error: 35.776852, mean_q: 48.213856
 1205614/1500000: episode: 2211, duration: 10.231s, episode steps: 397, steps per second: 39, episode reward: 216.594, mean reward: 0.546 [-19.609, 100.000], mean action: 0.806 [0.000, 3.000], mean observation: 0.115 [-0.533, 1.000], loss: 3.907619, mean_absolute_error: 35.957554, mean_q: 48.494038
 1205900/1500000: episode: 2212, duration: 7.074s, episode steps: 286, steps per second: 40, episode reward: 157.128, mean reward: 0.549 [-13.099, 100.000], mean action: 2.007 [0.000, 3.000], mean observation: 0.006 [-0.703, 1.000], loss: 3.982471, mean_absolute_error: 35.938126, mean_q: 48.493984
 1206291/1500000: episode: 2213, duration: 10.014s, episode steps: 391, steps per second: 39, episode reward: 175.127, mean reward: 0.448 [-17.181, 100.000], mean action: 1.506 [0.000, 3.000], mean observation: 0.074 [-0.692, 1.000], loss: 4.556409, mean_absolute_error: 36.079685, mean_q: 48.566498
 1206657/1500000: episode: 2214, duration: 8.947s, episode steps: 366, steps per second: 41, episode reward: 212.257, mean reward: 0.580 [-19.226, 100.000], mean action: 1.117 [0.000, 3.000], mean observation: 0.083 [-0.471, 1.000], loss: 3.053314, mean_absolute_error: 35.889721, mean_q: 48.337814
 1206983/1500000: episode: 2215, duration: 7.946s, episode steps: 326, steps per second: 41, episode reward: 196.205, mean reward: 0.602 [-2.954, 100.000], mean action: 1.451 [0.000, 3.000], mean observation: 0.096 [-0.384, 1.000], loss: 3.456504, mean_absolute_error: 35.914402, mean_q: 48.385136
 1207347/1500000: episode: 2216, duration: 9.586s, episode steps: 364, steps per second: 38, episode reward: 216.895, mean reward: 0.596 [-17.415, 100.000], mean action: 1.365 [0.000, 3.000], mean observation: 0.102 [-0.456, 1.000], loss: 5.772242, mean_absolute_error: 35.999279, mean_q: 48.473621
 1208002/1500000: episode: 2217, duration: 16.384s, episode steps: 655, steps per second: 40, episode reward: 191.417, mean reward: 0.292 [-20.093, 100.000], mean action: 0.510 [0.000, 3.000], mean observation: 0.160 [-0.523, 1.000], loss: 3.271295, mean_absolute_error: 36.009243, mean_q: 48.519623
 1208252/1500000: episode: 2218, duration: 6.125s, episode steps: 250, steps per second: 41, episode reward: 176.680, mean reward: 0.707 [-14.228, 100.000], mean action: 1.520 [0.000, 3.000], mean observation: 0.011 [-0.739, 1.000], loss: 3.281552, mean_absolute_error: 35.813080, mean_q: 48.260765
 1208548/1500000: episode: 2219, duration: 7.550s, episode steps: 296, steps per second: 39, episode reward: 183.161, mean reward: 0.619 [-9.156, 100.000], mean action: 0.885 [0.000, 3.000], mean observation: 0.086 [-0.488, 1.000], loss: 3.118196, mean_absolute_error: 35.941078, mean_q: 48.448940
 1208780/1500000: episode: 2220, duration: 6.033s, episode steps: 232, steps per second: 38, episode reward: 218.384, mean reward: 0.941 [-2.862, 100.000], mean action: 1.405 [0.000, 3.000], mean observation: 0.036 [-0.502, 1.000], loss: 4.528473, mean_absolute_error: 35.801956, mean_q: 48.209061
 1209384/1500000: episode: 2221, duration: 16.316s, episode steps: 604, steps per second: 37, episode reward: 208.912, mean reward: 0.346 [-20.272, 100.000], mean action: 1.094 [0.000, 3.000], mean observation: 0.130 [-0.562, 1.000], loss: 3.424860, mean_absolute_error: 35.909355, mean_q: 48.387424
 1209904/1500000: episode: 2222, duration: 14.621s, episode steps: 520, steps per second: 36, episode reward: 197.173, mean reward: 0.379 [-18.711, 100.000], mean action: 1.135 [0.000, 3.000], mean observation: 0.130 [-0.352, 1.000], loss: 4.338137, mean_absolute_error: 35.939545, mean_q: 48.402092
 1210329/1500000: episode: 2223, duration: 11.327s, episode steps: 425, steps per second: 38, episode reward: 194.999, mean reward: 0.459 [-10.119, 100.000], mean action: 1.028 [0.000, 3.000], mean observation: 0.080 [-0.622, 1.000], loss: 3.830531, mean_absolute_error: 35.789452, mean_q: 48.192963
 1210586/1500000: episode: 2224, duration: 6.363s, episode steps: 257, steps per second: 40, episode reward: 140.562, mean reward: 0.547 [-17.418, 100.000], mean action: 2.086 [0.000, 3.000], mean observation: 0.076 [-0.934, 1.000], loss: 4.848453, mean_absolute_error: 35.914860, mean_q: 48.431751
 1210827/1500000: episode: 2225, duration: 6.027s, episode steps: 241, steps per second: 40, episode reward: 188.945, mean reward: 0.784 [-16.028, 100.000], mean action: 2.041 [0.000, 3.000], mean observation: 0.048 [-0.775, 1.000], loss: 3.977596, mean_absolute_error: 35.935055, mean_q: 48.354576
 1211192/1500000: episode: 2226, duration: 9.159s, episode steps: 365, steps per second: 40, episode reward: 218.598, mean reward: 0.599 [-17.759, 100.000], mean action: 1.148 [0.000, 3.000], mean observation: 0.107 [-0.601, 1.000], loss: 5.063396, mean_absolute_error: 35.887989, mean_q: 48.347408
 1211542/1500000: episode: 2227, duration: 8.728s, episode steps: 350, steps per second: 40, episode reward: 222.994, mean reward: 0.637 [-10.148, 100.000], mean action: 1.103 [0.000, 3.000], mean observation: 0.106 [-0.548, 1.000], loss: 3.076329, mean_absolute_error: 36.016022, mean_q: 48.526535
 1211963/1500000: episode: 2228, duration: 10.767s, episode steps: 421, steps per second: 39, episode reward: 124.416, mean reward: 0.296 [-19.330, 100.000], mean action: 2.330 [0.000, 3.000], mean observation: 0.067 [-0.673, 1.000], loss: 4.680270, mean_absolute_error: 35.978065, mean_q: 48.491520
 1212288/1500000: episode: 2229, duration: 7.953s, episode steps: 325, steps per second: 41, episode reward: 201.769, mean reward: 0.621 [-17.599, 100.000], mean action: 1.040 [0.000, 3.000], mean observation: 0.095 [-0.486, 1.000], loss: 3.921423, mean_absolute_error: 36.128670, mean_q: 48.701443
 1212641/1500000: episode: 2230, duration: 8.712s, episode steps: 353, steps per second: 41, episode reward: 249.227, mean reward: 0.706 [-10.281, 100.000], mean action: 1.136 [0.000, 3.000], mean observation: 0.130 [-0.475, 1.000], loss: 3.022281, mean_absolute_error: 36.009529, mean_q: 48.552082
 1212901/1500000: episode: 2231, duration: 6.289s, episode steps: 260, steps per second: 41, episode reward: 211.330, mean reward: 0.813 [-19.030, 100.000], mean action: 1.350 [0.000, 3.000], mean observation: 0.073 [-0.478, 1.000], loss: 3.271129, mean_absolute_error: 36.075535, mean_q: 48.657188
 1213222/1500000: episode: 2232, duration: 8.322s, episode steps: 321, steps per second: 39, episode reward: 189.175, mean reward: 0.589 [-18.888, 100.000], mean action: 1.146 [0.000, 3.000], mean observation: 0.043 [-0.725, 1.000], loss: 3.344225, mean_absolute_error: 35.925640, mean_q: 48.419769
 1213536/1500000: episode: 2233, duration: 7.776s, episode steps: 314, steps per second: 40, episode reward: 245.184, mean reward: 0.781 [-9.761, 100.000], mean action: 1.306 [0.000, 3.000], mean observation: 0.115 [-0.503, 1.000], loss: 3.745791, mean_absolute_error: 36.113434, mean_q: 48.701645
 1213782/1500000: episode: 2234, duration: 6.057s, episode steps: 246, steps per second: 41, episode reward: 170.763, mean reward: 0.694 [-14.306, 100.000], mean action: 1.447 [0.000, 3.000], mean observation: -0.043 [-0.899, 1.000], loss: 3.300756, mean_absolute_error: 35.936817, mean_q: 48.492378
 1213901/1500000: episode: 2235, duration: 3.016s, episode steps: 119, steps per second: 39, episode reward: -20.978, mean reward: -0.176 [-100.000, 12.402], mean action: 1.496 [0.000, 3.000], mean observation: -0.063 [-0.833, 1.000], loss: 5.619860, mean_absolute_error: 35.781796, mean_q: 48.166683
 1214901/1500000: episode: 2236, duration: 27.204s, episode steps: 1000, steps per second: 37, episode reward: 55.542, mean reward: 0.056 [-24.207, 21.701], mean action: 2.582 [0.000, 3.000], mean observation: 0.174 [-0.527, 1.023], loss: 4.439965, mean_absolute_error: 35.810852, mean_q: 48.249210
 1215208/1500000: episode: 2237, duration: 7.808s, episode steps: 307, steps per second: 39, episode reward: 243.340, mean reward: 0.793 [-18.609, 100.000], mean action: 1.502 [0.000, 3.000], mean observation: 0.039 [-0.706, 1.000], loss: 4.545373, mean_absolute_error: 35.889408, mean_q: 48.317005
 1215527/1500000: episode: 2238, duration: 8.136s, episode steps: 319, steps per second: 39, episode reward: 183.344, mean reward: 0.575 [-10.227, 100.000], mean action: 0.984 [0.000, 3.000], mean observation: 0.074 [-0.514, 1.000], loss: 3.359890, mean_absolute_error: 35.833775, mean_q: 48.257298
 1215902/1500000: episode: 2239, duration: 9.595s, episode steps: 375, steps per second: 39, episode reward: 171.080, mean reward: 0.456 [-9.361, 100.000], mean action: 1.269 [0.000, 3.000], mean observation: 0.075 [-0.407, 1.000], loss: 4.778845, mean_absolute_error: 36.012146, mean_q: 48.482891
 1216492/1500000: episode: 2240, duration: 15.342s, episode steps: 590, steps per second: 38, episode reward: 169.576, mean reward: 0.287 [-18.113, 100.000], mean action: 1.912 [0.000, 3.000], mean observation: 0.123 [-0.433, 1.000], loss: 3.238449, mean_absolute_error: 35.913479, mean_q: 48.424316
 1216751/1500000: episode: 2241, duration: 6.364s, episode steps: 259, steps per second: 41, episode reward: 235.562, mean reward: 0.910 [-9.638, 100.000], mean action: 1.301 [0.000, 3.000], mean observation: 0.089 [-0.614, 1.013], loss: 5.743959, mean_absolute_error: 36.089008, mean_q: 48.624397
 1217130/1500000: episode: 2242, duration: 9.222s, episode steps: 379, steps per second: 41, episode reward: 198.457, mean reward: 0.524 [-19.215, 100.000], mean action: 1.040 [0.000, 3.000], mean observation: 0.066 [-1.016, 1.000], loss: 3.944977, mean_absolute_error: 36.050838, mean_q: 48.572445
 1217378/1500000: episode: 2243, duration: 6.138s, episode steps: 248, steps per second: 40, episode reward: 156.216, mean reward: 0.630 [-14.286, 100.000], mean action: 1.887 [0.000, 3.000], mean observation: 0.049 [-0.499, 1.000], loss: 5.318151, mean_absolute_error: 36.154026, mean_q: 48.698051
 1217698/1500000: episode: 2244, duration: 8.172s, episode steps: 320, steps per second: 39, episode reward: 241.069, mean reward: 0.753 [-17.053, 100.000], mean action: 1.962 [0.000, 3.000], mean observation: 0.061 [-0.841, 1.000], loss: 4.630742, mean_absolute_error: 36.100136, mean_q: 48.656101
 1218039/1500000: episode: 2245, duration: 8.925s, episode steps: 341, steps per second: 38, episode reward: 193.264, mean reward: 0.567 [-9.116, 100.000], mean action: 1.651 [0.000, 3.000], mean observation: 0.071 [-0.619, 1.000], loss: 3.663146, mean_absolute_error: 36.184822, mean_q: 48.779812
 1218371/1500000: episode: 2246, duration: 8.280s, episode steps: 332, steps per second: 40, episode reward: 237.989, mean reward: 0.717 [-3.393, 100.000], mean action: 1.630 [0.000, 3.000], mean observation: 0.037 [-0.661, 1.000], loss: 3.691403, mean_absolute_error: 36.265270, mean_q: 48.848969
 1218822/1500000: episode: 2247, duration: 11.405s, episode steps: 451, steps per second: 40, episode reward: 213.393, mean reward: 0.473 [-21.722, 100.000], mean action: 0.969 [0.000, 3.000], mean observation: 0.144 [-0.489, 1.000], loss: 3.862518, mean_absolute_error: 36.203297, mean_q: 48.763096
 1219132/1500000: episode: 2248, duration: 7.954s, episode steps: 310, steps per second: 39, episode reward: 206.954, mean reward: 0.668 [-3.002, 100.000], mean action: 1.552 [0.000, 3.000], mean observation: 0.061 [-0.481, 1.000], loss: 3.450003, mean_absolute_error: 35.979153, mean_q: 48.512497
 1219444/1500000: episode: 2249, duration: 8.046s, episode steps: 312, steps per second: 39, episode reward: 235.835, mean reward: 0.756 [-9.409, 100.000], mean action: 1.420 [0.000, 3.000], mean observation: 0.101 [-0.537, 1.000], loss: 3.010884, mean_absolute_error: 36.192120, mean_q: 48.728752
 1219837/1500000: episode: 2250, duration: 4.688s, episode steps: 393, steps per second: 84, episode reward: 246.865, mean reward: 0.628 [-11.829, 100.000], mean action: 1.290 [0.000, 3.000], mean observation: 0.123 [-0.587, 1.077], loss: 4.127082, mean_absolute_error: 36.217995, mean_q: 48.807640
 1220050/1500000: episode: 2251, duration: 2.542s, episode steps: 213, steps per second: 84, episode reward: 149.165, mean reward: 0.700 [-15.725, 100.000], mean action: 1.343 [0.000, 3.000], mean observation: 0.029 [-0.938, 1.000], loss: 3.441512, mean_absolute_error: 36.199421, mean_q: 48.735458
 1220310/1500000: episode: 2252, duration: 2.954s, episode steps: 260, steps per second: 88, episode reward: 202.894, mean reward: 0.780 [-3.624, 100.000], mean action: 1.888 [0.000, 3.000], mean observation: 0.063 [-0.530, 1.000], loss: 4.175523, mean_absolute_error: 36.192074, mean_q: 48.746201
 1220558/1500000: episode: 2253, duration: 2.826s, episode steps: 248, steps per second: 88, episode reward: 189.404, mean reward: 0.764 [-3.390, 100.000], mean action: 1.254 [0.000, 3.000], mean observation: 0.019 [-0.750, 1.000], loss: 3.607816, mean_absolute_error: 35.836056, mean_q: 48.347916
 1221192/1500000: episode: 2254, duration: 7.629s, episode steps: 634, steps per second: 83, episode reward: 167.329, mean reward: 0.264 [-19.180, 100.000], mean action: 1.825 [0.000, 3.000], mean observation: 0.139 [-0.577, 1.000], loss: 3.294701, mean_absolute_error: 35.949226, mean_q: 48.425053
 1221616/1500000: episode: 2255, duration: 5.221s, episode steps: 424, steps per second: 81, episode reward: 223.898, mean reward: 0.528 [-8.242, 100.000], mean action: 1.243 [0.000, 3.000], mean observation: 0.074 [-0.673, 1.000], loss: 3.135553, mean_absolute_error: 35.856579, mean_q: 48.321243
 1222016/1500000: episode: 2256, duration: 4.683s, episode steps: 400, steps per second: 85, episode reward: 224.362, mean reward: 0.561 [-17.464, 100.000], mean action: 1.038 [0.000, 3.000], mean observation: 0.126 [-0.443, 1.000], loss: 4.950520, mean_absolute_error: 36.003651, mean_q: 48.511612
 1222413/1500000: episode: 2257, duration: 4.636s, episode steps: 397, steps per second: 86, episode reward: 238.279, mean reward: 0.600 [-17.657, 100.000], mean action: 1.474 [0.000, 3.000], mean observation: 0.107 [-0.513, 1.000], loss: 4.322867, mean_absolute_error: 35.996532, mean_q: 48.494095
 1222722/1500000: episode: 2258, duration: 6.682s, episode steps: 309, steps per second: 46, episode reward: 245.961, mean reward: 0.796 [-3.439, 100.000], mean action: 1.417 [0.000, 3.000], mean observation: 0.029 [-0.716, 1.000], loss: 3.832426, mean_absolute_error: 36.076912, mean_q: 48.643078
 1223186/1500000: episode: 2259, duration: 5.664s, episode steps: 464, steps per second: 82, episode reward: 196.564, mean reward: 0.424 [-18.520, 100.000], mean action: 0.866 [0.000, 3.000], mean observation: 0.130 [-0.517, 1.000], loss: 4.308804, mean_absolute_error: 36.132462, mean_q: 48.661465
 1223425/1500000: episode: 2260, duration: 2.839s, episode steps: 239, steps per second: 84, episode reward: 167.710, mean reward: 0.702 [-8.115, 100.000], mean action: 1.406 [0.000, 3.000], mean observation: 0.021 [-0.585, 1.000], loss: 1.897593, mean_absolute_error: 36.408707, mean_q: 49.074402
 1223869/1500000: episode: 2261, duration: 8.661s, episode steps: 444, steps per second: 51, episode reward: 230.753, mean reward: 0.520 [-18.826, 100.000], mean action: 1.324 [0.000, 3.000], mean observation: 0.074 [-0.620, 1.006], loss: 3.587041, mean_absolute_error: 36.210510, mean_q: 48.764446
 1224225/1500000: episode: 2262, duration: 4.200s, episode steps: 356, steps per second: 85, episode reward: 193.792, mean reward: 0.544 [-24.528, 100.000], mean action: 1.287 [0.000, 3.000], mean observation: 0.085 [-0.499, 1.000], loss: 4.209713, mean_absolute_error: 36.284321, mean_q: 48.889538
 1224754/1500000: episode: 2263, duration: 10.613s, episode steps: 529, steps per second: 50, episode reward: 204.458, mean reward: 0.386 [-10.078, 100.000], mean action: 0.911 [0.000, 3.000], mean observation: 0.136 [-0.457, 1.000], loss: 3.321839, mean_absolute_error: 36.291798, mean_q: 48.851940
 1225250/1500000: episode: 2264, duration: 6.023s, episode steps: 496, steps per second: 82, episode reward: 191.443, mean reward: 0.386 [-18.885, 100.000], mean action: 1.823 [0.000, 3.000], mean observation: 0.106 [-0.569, 1.000], loss: 3.670365, mean_absolute_error: 36.079811, mean_q: 48.598324
 1225559/1500000: episode: 2265, duration: 6.989s, episode steps: 309, steps per second: 44, episode reward: 249.470, mean reward: 0.807 [-18.984, 100.000], mean action: 1.401 [0.000, 3.000], mean observation: 0.047 [-0.787, 1.000], loss: 4.213953, mean_absolute_error: 36.269207, mean_q: 48.826836
 1225896/1500000: episode: 2266, duration: 4.282s, episode steps: 337, steps per second: 79, episode reward: 184.585, mean reward: 0.548 [-7.886, 100.000], mean action: 1.234 [0.000, 3.000], mean observation: 0.093 [-0.454, 1.000], loss: 2.779478, mean_absolute_error: 36.352482, mean_q: 48.951225
 1226238/1500000: episode: 2267, duration: 4.585s, episode steps: 342, steps per second: 75, episode reward: 220.177, mean reward: 0.644 [-2.992, 100.000], mean action: 1.462 [0.000, 3.000], mean observation: 0.050 [-0.561, 1.000], loss: 2.853255, mean_absolute_error: 36.249954, mean_q: 48.829540
 1226517/1500000: episode: 2268, duration: 6.680s, episode steps: 279, steps per second: 42, episode reward: 243.437, mean reward: 0.873 [-9.378, 100.000], mean action: 1.172 [0.000, 3.000], mean observation: 0.039 [-0.782, 1.000], loss: 4.874038, mean_absolute_error: 36.542408, mean_q: 49.209145
 1226992/1500000: episode: 2269, duration: 5.755s, episode steps: 475, steps per second: 83, episode reward: 213.829, mean reward: 0.450 [-17.881, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.134 [-0.384, 1.000], loss: 3.250693, mean_absolute_error: 36.220509, mean_q: 48.795544
 1227226/1500000: episode: 2270, duration: 3.906s, episode steps: 234, steps per second: 60, episode reward: 169.764, mean reward: 0.725 [-9.475, 100.000], mean action: 1.103 [0.000, 3.000], mean observation: 0.048 [-0.518, 1.000], loss: 2.544150, mean_absolute_error: 36.043236, mean_q: 48.596786
 1227604/1500000: episode: 2271, duration: 8.078s, episode steps: 378, steps per second: 47, episode reward: 200.816, mean reward: 0.531 [-17.491, 100.000], mean action: 1.164 [0.000, 3.000], mean observation: 0.106 [-0.458, 1.000], loss: 4.301319, mean_absolute_error: 36.250885, mean_q: 48.829601
 1227849/1500000: episode: 2272, duration: 2.780s, episode steps: 245, steps per second: 88, episode reward: 200.296, mean reward: 0.818 [-2.982, 100.000], mean action: 1.429 [0.000, 3.000], mean observation: 0.039 [-0.498, 1.000], loss: 4.095492, mean_absolute_error: 36.283150, mean_q: 48.857956
 1228095/1500000: episode: 2273, duration: 4.104s, episode steps: 246, steps per second: 60, episode reward: 222.703, mean reward: 0.905 [-10.361, 100.000], mean action: 1.268 [0.000, 3.000], mean observation: 0.063 [-0.540, 1.000], loss: 3.424610, mean_absolute_error: 36.182312, mean_q: 48.725769
 1228464/1500000: episode: 2274, duration: 7.345s, episode steps: 369, steps per second: 50, episode reward: 224.936, mean reward: 0.610 [-17.983, 100.000], mean action: 1.228 [0.000, 3.000], mean observation: 0.106 [-0.521, 1.000], loss: 2.856905, mean_absolute_error: 36.304245, mean_q: 48.916779
 1229127/1500000: episode: 2275, duration: 11.191s, episode steps: 663, steps per second: 59, episode reward: 200.698, mean reward: 0.303 [-17.176, 100.000], mean action: 2.526 [0.000, 3.000], mean observation: 0.142 [-0.708, 1.000], loss: 3.046596, mean_absolute_error: 36.411098, mean_q: 49.043316
 1229432/1500000: episode: 2276, duration: 4.763s, episode steps: 305, steps per second: 64, episode reward: 214.854, mean reward: 0.704 [-7.875, 100.000], mean action: 1.315 [0.000, 3.000], mean observation: 0.072 [-0.446, 1.000], loss: 5.475579, mean_absolute_error: 36.532745, mean_q: 49.155155
 1229708/1500000: episode: 2277, duration: 3.199s, episode steps: 276, steps per second: 86, episode reward: 192.797, mean reward: 0.699 [-2.612, 100.000], mean action: 1.011 [0.000, 3.000], mean observation: 0.088 [-0.491, 1.000], loss: 5.116333, mean_absolute_error: 36.392174, mean_q: 49.101379
 1229990/1500000: episode: 2278, duration: 6.210s, episode steps: 282, steps per second: 45, episode reward: 206.775, mean reward: 0.733 [-2.792, 100.000], mean action: 1.238 [0.000, 3.000], mean observation: 0.077 [-0.415, 1.000], loss: 4.090819, mean_absolute_error: 36.670448, mean_q: 49.420254
 1230517/1500000: episode: 2279, duration: 7.644s, episode steps: 527, steps per second: 69, episode reward: 244.380, mean reward: 0.464 [-17.556, 100.000], mean action: 1.008 [0.000, 3.000], mean observation: 0.151 [-0.589, 1.000], loss: 4.740503, mean_absolute_error: 36.669895, mean_q: 49.394226
 1231178/1500000: episode: 2280, duration: 12.782s, episode steps: 661, steps per second: 52, episode reward: 191.422, mean reward: 0.290 [-17.827, 100.000], mean action: 2.325 [0.000, 3.000], mean observation: 0.152 [-0.512, 1.020], loss: 4.117113, mean_absolute_error: 36.728466, mean_q: 49.484512
 1231411/1500000: episode: 2281, duration: 2.640s, episode steps: 233, steps per second: 88, episode reward: 194.986, mean reward: 0.837 [-2.543, 100.000], mean action: 1.399 [0.000, 3.000], mean observation: 0.057 [-0.504, 1.000], loss: 3.440365, mean_absolute_error: 36.804745, mean_q: 49.568520
 1231861/1500000: episode: 2282, duration: 9.983s, episode steps: 450, steps per second: 45, episode reward: 233.675, mean reward: 0.519 [-17.960, 100.000], mean action: 1.160 [0.000, 3.000], mean observation: 0.093 [-0.643, 1.000], loss: 2.863565, mean_absolute_error: 36.696857, mean_q: 49.485386
 1232157/1500000: episode: 2283, duration: 3.379s, episode steps: 296, steps per second: 88, episode reward: 246.458, mean reward: 0.833 [-9.778, 100.000], mean action: 1.152 [0.000, 3.000], mean observation: 0.106 [-0.662, 1.000], loss: 3.946768, mean_absolute_error: 36.727684, mean_q: 49.499790
 1232689/1500000: episode: 2284, duration: 11.430s, episode steps: 532, steps per second: 47, episode reward: 244.441, mean reward: 0.459 [-17.858, 100.000], mean action: 0.799 [0.000, 3.000], mean observation: 0.158 [-0.664, 1.000], loss: 3.656640, mean_absolute_error: 36.623455, mean_q: 49.341908
 1233049/1500000: episode: 2285, duration: 4.141s, episode steps: 360, steps per second: 87, episode reward: 209.085, mean reward: 0.581 [-2.950, 100.000], mean action: 1.089 [0.000, 3.000], mean observation: 0.111 [-0.541, 1.000], loss: 4.967792, mean_absolute_error: 36.845116, mean_q: 49.615200
 1233351/1500000: episode: 2286, duration: 5.616s, episode steps: 302, steps per second: 54, episode reward: 188.108, mean reward: 0.623 [-3.321, 100.000], mean action: 1.129 [0.000, 3.000], mean observation: 0.073 [-0.473, 1.000], loss: 4.279438, mean_absolute_error: 37.048939, mean_q: 49.890556
 1233879/1500000: episode: 2287, duration: 9.063s, episode steps: 528, steps per second: 58, episode reward: 197.852, mean reward: 0.375 [-16.900, 100.000], mean action: 1.907 [0.000, 3.000], mean observation: 0.118 [-0.748, 1.000], loss: 3.400439, mean_absolute_error: 36.953213, mean_q: 49.778629
 1234238/1500000: episode: 2288, duration: 6.412s, episode steps: 359, steps per second: 56, episode reward: 196.346, mean reward: 0.547 [-10.952, 100.000], mean action: 1.493 [0.000, 3.000], mean observation: 0.081 [-0.569, 1.000], loss: 3.408369, mean_absolute_error: 37.108700, mean_q: 49.985611
 1234526/1500000: episode: 2289, duration: 6.010s, episode steps: 288, steps per second: 48, episode reward: 172.441, mean reward: 0.599 [-8.961, 100.000], mean action: 1.240 [0.000, 3.000], mean observation: 0.045 [-0.494, 1.000], loss: 5.313420, mean_absolute_error: 36.945358, mean_q: 49.763260
 1234807/1500000: episode: 2290, duration: 3.202s, episode steps: 281, steps per second: 88, episode reward: 186.973, mean reward: 0.665 [-15.050, 100.000], mean action: 2.174 [0.000, 3.000], mean observation: 0.056 [-0.663, 1.000], loss: 2.849621, mean_absolute_error: 37.035290, mean_q: 49.897629
 1235075/1500000: episode: 2291, duration: 4.727s, episode steps: 268, steps per second: 57, episode reward: 242.565, mean reward: 0.905 [-8.752, 100.000], mean action: 1.474 [0.000, 3.000], mean observation: 0.025 [-0.793, 1.000], loss: 3.118037, mean_absolute_error: 37.027927, mean_q: 49.904305
 1235402/1500000: episode: 2292, duration: 7.293s, episode steps: 327, steps per second: 45, episode reward: 168.200, mean reward: 0.514 [-9.441, 100.000], mean action: 1.153 [0.000, 3.000], mean observation: 0.050 [-0.602, 1.000], loss: 4.988746, mean_absolute_error: 36.826519, mean_q: 49.627480
 1235694/1500000: episode: 2293, duration: 3.403s, episode steps: 292, steps per second: 86, episode reward: 224.707, mean reward: 0.770 [-10.274, 100.000], mean action: 1.305 [0.000, 3.000], mean observation: 0.095 [-0.496, 1.000], loss: 4.628875, mean_absolute_error: 36.817451, mean_q: 49.604473
 1236041/1500000: episode: 2294, duration: 6.949s, episode steps: 347, steps per second: 50, episode reward: 199.639, mean reward: 0.575 [-19.128, 100.000], mean action: 1.271 [0.000, 3.000], mean observation: 0.060 [-0.492, 1.000], loss: 2.847893, mean_absolute_error: 36.800159, mean_q: 49.624985
 1236422/1500000: episode: 2295, duration: 6.624s, episode steps: 381, steps per second: 58, episode reward: 239.011, mean reward: 0.627 [-18.249, 100.000], mean action: 1.381 [0.000, 3.000], mean observation: 0.071 [-0.761, 1.000], loss: 2.806461, mean_absolute_error: 36.753990, mean_q: 49.526047
 1236907/1500000: episode: 2296, duration: 8.945s, episode steps: 485, steps per second: 54, episode reward: 233.586, mean reward: 0.482 [-18.754, 100.000], mean action: 0.951 [0.000, 3.000], mean observation: 0.111 [-0.705, 1.000], loss: 4.029678, mean_absolute_error: 36.830292, mean_q: 49.572430
 1237211/1500000: episode: 2297, duration: 5.520s, episode steps: 304, steps per second: 55, episode reward: 211.661, mean reward: 0.696 [-9.346, 100.000], mean action: 1.122 [0.000, 3.000], mean observation: 0.092 [-0.462, 1.000], loss: 2.825156, mean_absolute_error: 36.858547, mean_q: 49.680656
 1237373/1500000: episode: 2298, duration: 1.834s, episode steps: 162, steps per second: 88, episode reward: -21.830, mean reward: -0.135 [-100.000, 20.182], mean action: 1.815 [0.000, 3.000], mean observation: 0.066 [-0.658, 1.539], loss: 4.261512, mean_absolute_error: 37.009918, mean_q: 49.880798
 1237686/1500000: episode: 2299, duration: 5.405s, episode steps: 313, steps per second: 58, episode reward: 224.719, mean reward: 0.718 [-17.335, 100.000], mean action: 1.144 [0.000, 3.000], mean observation: 0.059 [-0.543, 1.000], loss: 4.825263, mean_absolute_error: 37.065166, mean_q: 49.880695
 1238055/1500000: episode: 2300, duration: 7.930s, episode steps: 369, steps per second: 47, episode reward: 208.390, mean reward: 0.565 [-3.155, 100.000], mean action: 1.382 [0.000, 3.000], mean observation: 0.091 [-0.686, 1.000], loss: 3.943885, mean_absolute_error: 37.020149, mean_q: 49.879402
 1238725/1500000: episode: 2301, duration: 12.139s, episode steps: 670, steps per second: 55, episode reward: 210.832, mean reward: 0.315 [-17.991, 100.000], mean action: 0.672 [0.000, 3.000], mean observation: 0.167 [-0.540, 1.000], loss: 4.333506, mean_absolute_error: 36.855537, mean_q: 49.646652
 1239052/1500000: episode: 2302, duration: 5.060s, episode steps: 327, steps per second: 65, episode reward: 219.530, mean reward: 0.671 [-17.594, 100.000], mean action: 1.239 [0.000, 3.000], mean observation: 0.076 [-0.493, 1.017], loss: 4.717692, mean_absolute_error: 36.962193, mean_q: 49.759590
 1239866/1500000: episode: 2303, duration: 15.005s, episode steps: 814, steps per second: 54, episode reward: 144.315, mean reward: 0.177 [-19.705, 100.000], mean action: 0.834 [0.000, 3.000], mean observation: 0.178 [-0.522, 1.000], loss: 4.646699, mean_absolute_error: 37.153522, mean_q: 50.060047
 1240173/1500000: episode: 2304, duration: 3.974s, episode steps: 307, steps per second: 77, episode reward: 238.695, mean reward: 0.778 [-2.833, 100.000], mean action: 1.609 [0.000, 3.000], mean observation: 0.035 [-0.629, 1.000], loss: 2.767879, mean_absolute_error: 37.025879, mean_q: 49.864647
 1240811/1500000: episode: 2305, duration: 12.623s, episode steps: 638, steps per second: 51, episode reward: 208.116, mean reward: 0.326 [-18.641, 100.000], mean action: 1.469 [0.000, 3.000], mean observation: 0.157 [-0.757, 1.000], loss: 5.062226, mean_absolute_error: 37.101192, mean_q: 49.991829
 1241141/1500000: episode: 2306, duration: 5.513s, episode steps: 330, steps per second: 60, episode reward: 228.022, mean reward: 0.691 [-17.768, 100.000], mean action: 1.164 [0.000, 3.000], mean observation: 0.101 [-0.507, 1.000], loss: 4.304555, mean_absolute_error: 37.229073, mean_q: 50.124355
 1241492/1500000: episode: 2307, duration: 8.262s, episode steps: 351, steps per second: 42, episode reward: 148.067, mean reward: 0.422 [-18.626, 100.000], mean action: 2.154 [0.000, 3.000], mean observation: 0.073 [-0.472, 1.000], loss: 4.340198, mean_absolute_error: 37.156101, mean_q: 49.999176
 1241865/1500000: episode: 2308, duration: 4.477s, episode steps: 373, steps per second: 83, episode reward: 193.682, mean reward: 0.519 [-9.793, 100.000], mean action: 1.748 [0.000, 3.000], mean observation: 0.031 [-0.490, 1.000], loss: 3.467434, mean_absolute_error: 37.267605, mean_q: 50.196339
 1242199/1500000: episode: 2309, duration: 7.997s, episode steps: 334, steps per second: 42, episode reward: 227.620, mean reward: 0.681 [-19.723, 100.000], mean action: 1.371 [0.000, 3.000], mean observation: 0.098 [-0.497, 1.000], loss: 3.937988, mean_absolute_error: 37.366417, mean_q: 50.328655
 1242553/1500000: episode: 2310, duration: 5.559s, episode steps: 354, steps per second: 64, episode reward: 219.131, mean reward: 0.619 [-3.281, 100.000], mean action: 1.347 [0.000, 3.000], mean observation: 0.094 [-0.479, 1.000], loss: 3.773600, mean_absolute_error: 37.170265, mean_q: 50.107376
 1242882/1500000: episode: 2311, duration: 5.328s, episode steps: 329, steps per second: 62, episode reward: 227.805, mean reward: 0.692 [-17.403, 100.000], mean action: 1.571 [0.000, 3.000], mean observation: 0.050 [-0.539, 1.000], loss: 3.133857, mean_absolute_error: 37.266537, mean_q: 50.197151
 1243263/1500000: episode: 2312, duration: 8.627s, episode steps: 381, steps per second: 44, episode reward: 206.700, mean reward: 0.543 [-11.828, 100.000], mean action: 1.270 [0.000, 3.000], mean observation: 0.115 [-0.418, 1.000], loss: 5.269739, mean_absolute_error: 37.293831, mean_q: 50.216110
 1243498/1500000: episode: 2313, duration: 2.721s, episode steps: 235, steps per second: 86, episode reward: -18.796, mean reward: -0.080 [-100.000, 13.885], mean action: 1.694 [0.000, 3.000], mean observation: -0.029 [-0.827, 1.000], loss: 4.767911, mean_absolute_error: 37.365017, mean_q: 50.315487
 1243876/1500000: episode: 2314, duration: 7.386s, episode steps: 378, steps per second: 51, episode reward: 208.950, mean reward: 0.553 [-17.739, 100.000], mean action: 1.040 [0.000, 3.000], mean observation: 0.116 [-0.457, 1.000], loss: 2.972222, mean_absolute_error: 37.282604, mean_q: 50.232670
 1244206/1500000: episode: 2315, duration: 6.455s, episode steps: 330, steps per second: 51, episode reward: 243.129, mean reward: 0.737 [-8.930, 100.000], mean action: 1.179 [0.000, 3.000], mean observation: 0.117 [-0.604, 1.000], loss: 5.186193, mean_absolute_error: 37.145870, mean_q: 50.027805
 1244507/1500000: episode: 2316, duration: 4.029s, episode steps: 301, steps per second: 75, episode reward: 238.183, mean reward: 0.791 [-9.739, 100.000], mean action: 1.535 [0.000, 3.000], mean observation: 0.039 [-0.580, 1.000], loss: 5.253804, mean_absolute_error: 37.281517, mean_q: 50.222954
 1244882/1500000: episode: 2317, duration: 9.472s, episode steps: 375, steps per second: 40, episode reward: 234.337, mean reward: 0.625 [-19.332, 100.000], mean action: 1.347 [0.000, 3.000], mean observation: 0.081 [-0.513, 1.043], loss: 3.629106, mean_absolute_error: 37.214771, mean_q: 50.140999
 1245152/1500000: episode: 2318, duration: 3.314s, episode steps: 270, steps per second: 81, episode reward: 203.876, mean reward: 0.755 [-17.943, 100.000], mean action: 1.400 [0.000, 3.000], mean observation: 0.043 [-0.638, 1.000], loss: 4.241940, mean_absolute_error: 37.120434, mean_q: 50.036884
 1245470/1500000: episode: 2319, duration: 5.219s, episode steps: 318, steps per second: 61, episode reward: 231.293, mean reward: 0.727 [-2.721, 100.000], mean action: 1.110 [0.000, 3.000], mean observation: 0.106 [-0.599, 1.000], loss: 5.257206, mean_absolute_error: 37.118561, mean_q: 50.011860
 1245809/1500000: episode: 2320, duration: 8.230s, episode steps: 339, steps per second: 41, episode reward: 211.078, mean reward: 0.623 [-17.979, 100.000], mean action: 1.147 [0.000, 3.000], mean observation: 0.101 [-0.510, 1.000], loss: 3.733924, mean_absolute_error: 37.127243, mean_q: 50.006275
 1246168/1500000: episode: 2321, duration: 4.120s, episode steps: 359, steps per second: 87, episode reward: 228.715, mean reward: 0.637 [-9.854, 100.000], mean action: 1.279 [0.000, 3.000], mean observation: 0.055 [-0.626, 1.000], loss: 3.930831, mean_absolute_error: 37.354408, mean_q: 50.327908
 1246674/1500000: episode: 2322, duration: 11.697s, episode steps: 506, steps per second: 43, episode reward: 174.223, mean reward: 0.344 [-17.087, 100.000], mean action: 1.621 [0.000, 3.000], mean observation: 0.141 [-0.588, 1.000], loss: 3.917481, mean_absolute_error: 37.277775, mean_q: 50.240227
 1247026/1500000: episode: 2323, duration: 4.171s, episode steps: 352, steps per second: 84, episode reward: 176.293, mean reward: 0.501 [-18.541, 100.000], mean action: 1.060 [0.000, 3.000], mean observation: 0.098 [-0.541, 1.000], loss: 3.030185, mean_absolute_error: 37.187870, mean_q: 50.125221
 1247433/1500000: episode: 2324, duration: 9.298s, episode steps: 407, steps per second: 44, episode reward: 200.280, mean reward: 0.492 [-23.686, 100.000], mean action: 1.108 [0.000, 3.000], mean observation: 0.116 [-0.446, 1.000], loss: 3.515138, mean_absolute_error: 37.098072, mean_q: 49.956341
 1247871/1500000: episode: 2325, duration: 6.511s, episode steps: 438, steps per second: 67, episode reward: 205.839, mean reward: 0.470 [-17.617, 100.000], mean action: 1.158 [0.000, 3.000], mean observation: 0.116 [-0.456, 1.000], loss: 2.983518, mean_absolute_error: 36.970272, mean_q: 49.834641
 1248208/1500000: episode: 2326, duration: 6.693s, episode steps: 337, steps per second: 50, episode reward: 249.652, mean reward: 0.741 [-3.040, 100.000], mean action: 1.134 [0.000, 3.000], mean observation: 0.069 [-0.775, 1.000], loss: 3.102102, mean_absolute_error: 36.959358, mean_q: 49.809673
 1248580/1500000: episode: 2327, duration: 7.597s, episode steps: 372, steps per second: 49, episode reward: 212.203, mean reward: 0.570 [-2.937, 100.000], mean action: 1.097 [0.000, 3.000], mean observation: 0.112 [-0.373, 1.000], loss: 5.754090, mean_absolute_error: 36.903076, mean_q: 49.772697
 1248851/1500000: episode: 2328, duration: 3.027s, episode steps: 271, steps per second: 90, episode reward: 191.842, mean reward: 0.708 [-14.194, 100.000], mean action: 1.771 [0.000, 3.000], mean observation: 0.047 [-0.660, 1.000], loss: 4.593958, mean_absolute_error: 36.991173, mean_q: 49.859604
 1249266/1500000: episode: 2329, duration: 10.769s, episode steps: 415, steps per second: 39, episode reward: 162.517, mean reward: 0.392 [-18.008, 100.000], mean action: 2.304 [0.000, 3.000], mean observation: 0.133 [-0.554, 1.000], loss: 4.698079, mean_absolute_error: 36.929420, mean_q: 49.811752
 1249760/1500000: episode: 2330, duration: 12.248s, episode steps: 494, steps per second: 40, episode reward: 205.114, mean reward: 0.415 [-19.958, 100.000], mean action: 0.830 [0.000, 3.000], mean observation: 0.119 [-0.680, 1.000], loss: 4.608361, mean_absolute_error: 37.045811, mean_q: 49.929070
 1250760/1500000: episode: 2331, duration: 27.526s, episode steps: 1000, steps per second: 36, episode reward: 67.045, mean reward: 0.067 [-17.812, 16.151], mean action: 2.140 [0.000, 3.000], mean observation: 0.175 [-0.628, 1.000], loss: 5.085427, mean_absolute_error: 37.059250, mean_q: 49.966183
 1251145/1500000: episode: 2332, duration: 9.619s, episode steps: 385, steps per second: 40, episode reward: 222.466, mean reward: 0.578 [-9.030, 100.000], mean action: 1.506 [0.000, 3.000], mean observation: 0.095 [-0.601, 1.000], loss: 4.719562, mean_absolute_error: 37.170364, mean_q: 50.063232
 1251477/1500000: episode: 2333, duration: 8.424s, episode steps: 332, steps per second: 39, episode reward: 206.592, mean reward: 0.622 [-9.920, 100.000], mean action: 1.075 [0.000, 3.000], mean observation: 0.098 [-0.512, 1.000], loss: 3.561033, mean_absolute_error: 37.146484, mean_q: 50.043564
 1251962/1500000: episode: 2334, duration: 12.893s, episode steps: 485, steps per second: 38, episode reward: 211.031, mean reward: 0.435 [-19.326, 100.000], mean action: 0.944 [0.000, 3.000], mean observation: 0.130 [-0.546, 1.000], loss: 3.902759, mean_absolute_error: 37.192936, mean_q: 50.125927
 1252191/1500000: episode: 2335, duration: 5.594s, episode steps: 229, steps per second: 41, episode reward: 159.805, mean reward: 0.698 [-15.055, 100.000], mean action: 1.402 [0.000, 3.000], mean observation: 0.012 [-0.589, 1.000], loss: 3.866280, mean_absolute_error: 37.150715, mean_q: 50.060795
 1252470/1500000: episode: 2336, duration: 7.100s, episode steps: 279, steps per second: 39, episode reward: 197.013, mean reward: 0.706 [-14.707, 100.000], mean action: 1.602 [0.000, 3.000], mean observation: 0.030 [-0.708, 1.000], loss: 4.076616, mean_absolute_error: 37.253094, mean_q: 50.219055
 1252840/1500000: episode: 2337, duration: 9.699s, episode steps: 370, steps per second: 38, episode reward: 233.705, mean reward: 0.632 [-19.216, 100.000], mean action: 0.895 [0.000, 3.000], mean observation: 0.114 [-0.564, 1.000], loss: 4.567011, mean_absolute_error: 37.355629, mean_q: 50.351078
 1253095/1500000: episode: 2338, duration: 6.566s, episode steps: 255, steps per second: 39, episode reward: 199.402, mean reward: 0.782 [-9.708, 100.000], mean action: 1.306 [0.000, 3.000], mean observation: 0.062 [-0.557, 1.000], loss: 4.119390, mean_absolute_error: 37.289223, mean_q: 50.290100
 1253475/1500000: episode: 2339, duration: 9.548s, episode steps: 380, steps per second: 40, episode reward: 183.333, mean reward: 0.482 [-11.249, 100.000], mean action: 0.982 [0.000, 3.000], mean observation: 0.061 [-0.523, 1.000], loss: 3.352128, mean_absolute_error: 37.190060, mean_q: 50.164215
 1254475/1500000: episode: 2340, duration: 27.288s, episode steps: 1000, steps per second: 37, episode reward: 60.833, mean reward: 0.061 [-18.121, 22.121], mean action: 2.135 [0.000, 3.000], mean observation: 0.191 [-0.491, 1.000], loss: 4.187220, mean_absolute_error: 37.280228, mean_q: 50.247654
 1254995/1500000: episode: 2341, duration: 13.386s, episode steps: 520, steps per second: 39, episode reward: 159.748, mean reward: 0.307 [-19.724, 100.000], mean action: 1.192 [0.000, 3.000], mean observation: 0.099 [-0.923, 1.000], loss: 3.527342, mean_absolute_error: 37.196266, mean_q: 50.156071
 1255273/1500000: episode: 2342, duration: 7.238s, episode steps: 278, steps per second: 38, episode reward: 183.972, mean reward: 0.662 [-9.584, 100.000], mean action: 1.299 [0.000, 3.000], mean observation: 0.062 [-0.443, 1.000], loss: 3.281678, mean_absolute_error: 37.352558, mean_q: 50.315998
 1255685/1500000: episode: 2343, duration: 10.752s, episode steps: 412, steps per second: 38, episode reward: 91.626, mean reward: 0.222 [-19.017, 100.000], mean action: 2.335 [0.000, 3.000], mean observation: 0.019 [-0.603, 1.000], loss: 2.510665, mean_absolute_error: 37.230244, mean_q: 50.212975
 1255929/1500000: episode: 2344, duration: 5.929s, episode steps: 244, steps per second: 41, episode reward: 182.217, mean reward: 0.747 [-2.817, 100.000], mean action: 1.205 [0.000, 3.000], mean observation: 0.064 [-0.435, 1.000], loss: 3.031306, mean_absolute_error: 37.342796, mean_q: 50.320621
 1256268/1500000: episode: 2345, duration: 8.957s, episode steps: 339, steps per second: 38, episode reward: 194.154, mean reward: 0.573 [-18.471, 100.000], mean action: 1.012 [0.000, 3.000], mean observation: 0.094 [-0.592, 1.004], loss: 4.961484, mean_absolute_error: 37.271690, mean_q: 50.159760
 1256611/1500000: episode: 2346, duration: 8.635s, episode steps: 343, steps per second: 40, episode reward: 182.464, mean reward: 0.532 [-17.357, 100.000], mean action: 1.816 [0.000, 3.000], mean observation: 0.045 [-0.724, 1.000], loss: 5.479180, mean_absolute_error: 37.370182, mean_q: 50.363338
 1256857/1500000: episode: 2347, duration: 5.956s, episode steps: 246, steps per second: 41, episode reward: 182.178, mean reward: 0.741 [-2.981, 100.000], mean action: 1.313 [0.000, 3.000], mean observation: -0.000 [-0.714, 1.000], loss: 2.723294, mean_absolute_error: 37.312298, mean_q: 50.332832
 1257708/1500000: episode: 2348, duration: 23.918s, episode steps: 851, steps per second: 36, episode reward: 167.046, mean reward: 0.196 [-18.955, 100.000], mean action: 1.851 [0.000, 3.000], mean observation: 0.114 [-0.608, 1.000], loss: 2.717216, mean_absolute_error: 37.325893, mean_q: 50.292904
 1258085/1500000: episode: 2349, duration: 9.638s, episode steps: 377, steps per second: 39, episode reward: 202.785, mean reward: 0.538 [-18.667, 100.000], mean action: 1.029 [0.000, 3.000], mean observation: 0.067 [-0.676, 1.000], loss: 3.450819, mean_absolute_error: 37.257259, mean_q: 50.221973
 1258363/1500000: episode: 2350, duration: 6.892s, episode steps: 278, steps per second: 40, episode reward: 156.053, mean reward: 0.561 [-17.701, 100.000], mean action: 1.284 [0.000, 3.000], mean observation: 0.063 [-1.185, 1.000], loss: 4.191101, mean_absolute_error: 37.199783, mean_q: 50.162457
 1258722/1500000: episode: 2351, duration: 8.986s, episode steps: 359, steps per second: 40, episode reward: 192.663, mean reward: 0.537 [-17.490, 100.000], mean action: 1.323 [0.000, 3.000], mean observation: 0.062 [-0.554, 1.000], loss: 3.817768, mean_absolute_error: 37.017746, mean_q: 49.911186
 1259201/1500000: episode: 2352, duration: 12.191s, episode steps: 479, steps per second: 39, episode reward: 213.162, mean reward: 0.445 [-19.415, 100.000], mean action: 0.681 [0.000, 3.000], mean observation: 0.120 [-0.921, 1.000], loss: 3.714082, mean_absolute_error: 37.157375, mean_q: 50.094891
 1260201/1500000: episode: 2353, duration: 26.289s, episode steps: 1000, steps per second: 38, episode reward: 73.453, mean reward: 0.073 [-20.379, 15.165], mean action: 0.844 [0.000, 3.000], mean observation: 0.179 [-0.578, 1.000], loss: 3.700032, mean_absolute_error: 37.117966, mean_q: 50.033321
 1260729/1500000: episode: 2354, duration: 13.848s, episode steps: 528, steps per second: 38, episode reward: 205.111, mean reward: 0.388 [-18.173, 100.000], mean action: 0.761 [0.000, 3.000], mean observation: 0.152 [-0.395, 1.000], loss: 4.394115, mean_absolute_error: 37.034878, mean_q: 49.925404
 1261029/1500000: episode: 2355, duration: 8.384s, episode steps: 300, steps per second: 36, episode reward: 165.137, mean reward: 0.550 [-10.461, 100.000], mean action: 1.043 [0.000, 3.000], mean observation: 0.093 [-0.475, 1.000], loss: 3.379018, mean_absolute_error: 37.198185, mean_q: 50.161076
 1261352/1500000: episode: 2356, duration: 8.755s, episode steps: 323, steps per second: 37, episode reward: 223.327, mean reward: 0.691 [-11.151, 100.000], mean action: 1.211 [0.000, 3.000], mean observation: 0.087 [-0.515, 1.000], loss: 4.052128, mean_absolute_error: 37.161510, mean_q: 50.124573
 1261738/1500000: episode: 2357, duration: 10.791s, episode steps: 386, steps per second: 36, episode reward: 198.302, mean reward: 0.514 [-19.362, 100.000], mean action: 0.984 [0.000, 3.000], mean observation: 0.089 [-0.630, 1.000], loss: 5.565077, mean_absolute_error: 37.241508, mean_q: 50.214024
 1262186/1500000: episode: 2358, duration: 12.975s, episode steps: 448, steps per second: 35, episode reward: 212.001, mean reward: 0.473 [-18.048, 100.000], mean action: 0.980 [0.000, 3.000], mean observation: 0.131 [-0.467, 1.000], loss: 5.330063, mean_absolute_error: 37.362907, mean_q: 50.349567
 1262518/1500000: episode: 2359, duration: 8.903s, episode steps: 332, steps per second: 37, episode reward: 197.330, mean reward: 0.594 [-8.718, 100.000], mean action: 1.389 [0.000, 3.000], mean observation: 0.105 [-0.438, 1.000], loss: 2.950420, mean_absolute_error: 37.059978, mean_q: 49.947701
 1262951/1500000: episode: 2360, duration: 11.473s, episode steps: 433, steps per second: 38, episode reward: 210.989, mean reward: 0.487 [-2.843, 100.000], mean action: 1.178 [0.000, 3.000], mean observation: 0.124 [-0.403, 1.000], loss: 2.898968, mean_absolute_error: 37.421669, mean_q: 50.460007
 1263345/1500000: episode: 2361, duration: 10.293s, episode steps: 394, steps per second: 38, episode reward: 198.389, mean reward: 0.504 [-18.467, 100.000], mean action: 0.972 [0.000, 3.000], mean observation: 0.108 [-0.499, 1.000], loss: 3.720681, mean_absolute_error: 37.383537, mean_q: 50.402233
 1263846/1500000: episode: 2362, duration: 12.943s, episode steps: 501, steps per second: 39, episode reward: 218.902, mean reward: 0.437 [-19.247, 100.000], mean action: 0.958 [0.000, 3.000], mean observation: 0.141 [-0.352, 1.000], loss: 4.483500, mean_absolute_error: 37.380512, mean_q: 50.394436
 1264418/1500000: episode: 2363, duration: 14.596s, episode steps: 572, steps per second: 39, episode reward: 209.763, mean reward: 0.367 [-18.290, 100.000], mean action: 0.857 [0.000, 3.000], mean observation: 0.153 [-0.473, 1.000], loss: 3.528150, mean_absolute_error: 37.380585, mean_q: 50.381260
 1264690/1500000: episode: 2364, duration: 6.789s, episode steps: 272, steps per second: 40, episode reward: 192.733, mean reward: 0.709 [-9.833, 100.000], mean action: 1.412 [0.000, 3.000], mean observation: 0.079 [-0.466, 1.000], loss: 4.333272, mean_absolute_error: 37.516041, mean_q: 50.546223
 1264977/1500000: episode: 2365, duration: 6.897s, episode steps: 287, steps per second: 42, episode reward: 244.254, mean reward: 0.851 [-2.860, 100.000], mean action: 1.498 [0.000, 3.000], mean observation: 0.042 [-0.627, 1.000], loss: 4.693737, mean_absolute_error: 37.421703, mean_q: 50.415432
 1265542/1500000: episode: 2366, duration: 14.950s, episode steps: 565, steps per second: 38, episode reward: 241.089, mean reward: 0.427 [-18.282, 100.000], mean action: 0.922 [0.000, 3.000], mean observation: 0.160 [-0.467, 1.000], loss: 3.039989, mean_absolute_error: 37.284477, mean_q: 50.291248
 1265955/1500000: episode: 2367, duration: 10.540s, episode steps: 413, steps per second: 39, episode reward: 183.211, mean reward: 0.444 [-17.854, 100.000], mean action: 0.990 [0.000, 3.000], mean observation: 0.118 [-0.483, 1.000], loss: 3.948830, mean_absolute_error: 37.467762, mean_q: 50.545410
 1266191/1500000: episode: 2368, duration: 5.668s, episode steps: 236, steps per second: 42, episode reward: 195.588, mean reward: 0.829 [-2.759, 100.000], mean action: 1.182 [0.000, 3.000], mean observation: 0.063 [-0.528, 1.000], loss: 5.246219, mean_absolute_error: 37.227631, mean_q: 50.196007
 1267191/1500000: episode: 2369, duration: 26.420s, episode steps: 1000, steps per second: 38, episode reward: 76.078, mean reward: 0.076 [-21.914, 23.134], mean action: 1.547 [0.000, 3.000], mean observation: 0.180 [-0.710, 1.000], loss: 3.814790, mean_absolute_error: 37.476452, mean_q: 50.505566
 1267714/1500000: episode: 2370, duration: 13.716s, episode steps: 523, steps per second: 38, episode reward: 177.571, mean reward: 0.340 [-17.909, 100.000], mean action: 1.583 [0.000, 3.000], mean observation: 0.123 [-0.513, 1.000], loss: 3.356754, mean_absolute_error: 37.639114, mean_q: 50.703152
 1268138/1500000: episode: 2371, duration: 10.977s, episode steps: 424, steps per second: 39, episode reward: 213.374, mean reward: 0.503 [-20.442, 100.000], mean action: 1.189 [0.000, 3.000], mean observation: 0.080 [-0.679, 1.000], loss: 4.118203, mean_absolute_error: 37.596764, mean_q: 50.662598
 1268449/1500000: episode: 2372, duration: 8.106s, episode steps: 311, steps per second: 38, episode reward: 182.681, mean reward: 0.587 [-8.712, 100.000], mean action: 1.518 [0.000, 3.000], mean observation: 0.051 [-0.459, 1.000], loss: 2.706773, mean_absolute_error: 37.525177, mean_q: 50.576706
 1268859/1500000: episode: 2373, duration: 10.640s, episode steps: 410, steps per second: 39, episode reward: 219.579, mean reward: 0.536 [-17.698, 100.000], mean action: 1.166 [0.000, 3.000], mean observation: 0.113 [-0.467, 1.000], loss: 3.951380, mean_absolute_error: 37.360500, mean_q: 50.340622
 1269100/1500000: episode: 2374, duration: 5.809s, episode steps: 241, steps per second: 41, episode reward: 251.561, mean reward: 1.044 [-8.427, 100.000], mean action: 1.365 [0.000, 3.000], mean observation: 0.024 [-0.809, 1.000], loss: 2.946600, mean_absolute_error: 37.367203, mean_q: 50.353199
 1269471/1500000: episode: 2375, duration: 9.273s, episode steps: 371, steps per second: 40, episode reward: 205.374, mean reward: 0.554 [-22.713, 100.000], mean action: 0.852 [0.000, 3.000], mean observation: 0.118 [-0.464, 1.000], loss: 4.427894, mean_absolute_error: 37.356472, mean_q: 50.290264
 1269842/1500000: episode: 2376, duration: 9.400s, episode steps: 371, steps per second: 39, episode reward: 211.999, mean reward: 0.571 [-19.488, 100.000], mean action: 1.011 [0.000, 3.000], mean observation: 0.115 [-0.480, 1.000], loss: 3.258960, mean_absolute_error: 37.548935, mean_q: 50.557350
 1270352/1500000: episode: 2377, duration: 12.816s, episode steps: 510, steps per second: 40, episode reward: 234.618, mean reward: 0.460 [-9.567, 100.000], mean action: 0.816 [0.000, 3.000], mean observation: 0.114 [-0.506, 1.009], loss: 4.123588, mean_absolute_error: 37.564419, mean_q: 50.592403
 1270595/1500000: episode: 2378, duration: 6.147s, episode steps: 243, steps per second: 40, episode reward: 252.782, mean reward: 1.040 [-8.104, 100.000], mean action: 1.407 [0.000, 3.000], mean observation: 0.023 [-0.795, 1.000], loss: 6.365209, mean_absolute_error: 37.513996, mean_q: 50.577877
 1270951/1500000: episode: 2379, duration: 9.051s, episode steps: 356, steps per second: 39, episode reward: 208.185, mean reward: 0.585 [-17.600, 100.000], mean action: 1.340 [0.000, 3.000], mean observation: 0.060 [-0.520, 1.000], loss: 2.404575, mean_absolute_error: 37.282593, mean_q: 50.319763
 1271325/1500000: episode: 2380, duration: 10.143s, episode steps: 374, steps per second: 37, episode reward: 217.736, mean reward: 0.582 [-17.959, 100.000], mean action: 1.126 [0.000, 3.000], mean observation: 0.105 [-0.389, 1.000], loss: 4.056904, mean_absolute_error: 37.388966, mean_q: 50.364941
 1271701/1500000: episode: 2381, duration: 8.381s, episode steps: 376, steps per second: 45, episode reward: 222.928, mean reward: 0.593 [-9.544, 100.000], mean action: 1.207 [0.000, 3.000], mean observation: 0.095 [-0.467, 1.021], loss: 4.286319, mean_absolute_error: 37.356838, mean_q: 50.394634
 1271964/1500000: episode: 2382, duration: 3.271s, episode steps: 263, steps per second: 80, episode reward: 175.680, mean reward: 0.668 [-8.166, 100.000], mean action: 1.198 [0.000, 3.000], mean observation: 0.065 [-0.461, 1.000], loss: 3.245761, mean_absolute_error: 37.384689, mean_q: 50.439068
 1272275/1500000: episode: 2383, duration: 3.809s, episode steps: 311, steps per second: 82, episode reward: 219.208, mean reward: 0.705 [-11.889, 100.000], mean action: 1.476 [0.000, 3.000], mean observation: 0.037 [-0.516, 1.000], loss: 3.544337, mean_absolute_error: 37.320072, mean_q: 50.294693
 1272563/1500000: episode: 2384, duration: 3.446s, episode steps: 288, steps per second: 84, episode reward: 228.940, mean reward: 0.795 [-2.944, 100.000], mean action: 1.170 [0.000, 3.000], mean observation: 0.079 [-0.554, 1.000], loss: 3.421262, mean_absolute_error: 37.487137, mean_q: 50.510117
 1272795/1500000: episode: 2385, duration: 2.801s, episode steps: 232, steps per second: 83, episode reward: -6.089, mean reward: -0.026 [-100.000, 13.994], mean action: 1.522 [0.000, 3.000], mean observation: 0.067 [-0.532, 1.012], loss: 3.561934, mean_absolute_error: 37.359047, mean_q: 50.388359
 1273142/1500000: episode: 2386, duration: 3.985s, episode steps: 347, steps per second: 87, episode reward: 206.726, mean reward: 0.596 [-17.693, 100.000], mean action: 0.988 [0.000, 3.000], mean observation: 0.098 [-0.440, 1.000], loss: 3.002510, mean_absolute_error: 37.292973, mean_q: 50.268787
 1273560/1500000: episode: 2387, duration: 4.991s, episode steps: 418, steps per second: 84, episode reward: 239.371, mean reward: 0.573 [-17.541, 100.000], mean action: 1.273 [0.000, 3.000], mean observation: 0.127 [-0.533, 1.028], loss: 3.518447, mean_absolute_error: 37.292114, mean_q: 50.244053
 1274009/1500000: episode: 2388, duration: 5.361s, episode steps: 449, steps per second: 84, episode reward: 215.496, mean reward: 0.480 [-13.454, 100.000], mean action: 1.976 [0.000, 3.000], mean observation: 0.114 [-0.745, 1.000], loss: 4.869582, mean_absolute_error: 37.277000, mean_q: 50.265427
 1274456/1500000: episode: 2389, duration: 8.310s, episode steps: 447, steps per second: 54, episode reward: 201.236, mean reward: 0.450 [-20.447, 100.000], mean action: 1.022 [0.000, 3.000], mean observation: 0.110 [-0.532, 1.000], loss: 3.643231, mean_absolute_error: 37.209442, mean_q: 50.147598
 1274716/1500000: episode: 2390, duration: 2.978s, episode steps: 260, steps per second: 87, episode reward: 211.451, mean reward: 0.813 [-10.880, 100.000], mean action: 1.427 [0.000, 3.000], mean observation: 0.024 [-0.672, 1.000], loss: 5.388101, mean_absolute_error: 37.270584, mean_q: 50.259720
 1275067/1500000: episode: 2391, duration: 4.126s, episode steps: 351, steps per second: 85, episode reward: 216.323, mean reward: 0.616 [-18.402, 100.000], mean action: 1.114 [0.000, 3.000], mean observation: 0.074 [-0.543, 1.000], loss: 3.916252, mean_absolute_error: 37.284687, mean_q: 50.268394
 1275428/1500000: episode: 2392, duration: 6.898s, episode steps: 361, steps per second: 52, episode reward: 242.072, mean reward: 0.671 [-9.539, 100.000], mean action: 1.238 [0.000, 3.000], mean observation: 0.132 [-0.499, 1.000], loss: 4.374141, mean_absolute_error: 37.378490, mean_q: 50.402081
 1275658/1500000: episode: 2393, duration: 3.359s, episode steps: 230, steps per second: 68, episode reward: -24.381, mean reward: -0.106 [-100.000, 15.488], mean action: 1.383 [0.000, 3.000], mean observation: 0.067 [-0.950, 1.000], loss: 7.531880, mean_absolute_error: 37.383850, mean_q: 50.415752
 1276362/1500000: episode: 2394, duration: 10.625s, episode steps: 704, steps per second: 66, episode reward: 129.897, mean reward: 0.185 [-19.128, 100.000], mean action: 2.474 [0.000, 3.000], mean observation: 0.100 [-0.643, 1.000], loss: 4.101949, mean_absolute_error: 37.371033, mean_q: 50.408535
 1276684/1500000: episode: 2395, duration: 5.561s, episode steps: 322, steps per second: 58, episode reward: 207.991, mean reward: 0.646 [-9.542, 100.000], mean action: 1.180 [0.000, 3.000], mean observation: 0.098 [-0.485, 1.000], loss: 5.537010, mean_absolute_error: 37.532513, mean_q: 50.610001
 1277116/1500000: episode: 2396, duration: 5.632s, episode steps: 432, steps per second: 77, episode reward: 210.019, mean reward: 0.486 [-19.051, 100.000], mean action: 0.942 [0.000, 3.000], mean observation: 0.129 [-0.374, 1.000], loss: 2.590082, mean_absolute_error: 37.306004, mean_q: 50.346516
 1277573/1500000: episode: 2397, duration: 8.977s, episode steps: 457, steps per second: 51, episode reward: 235.710, mean reward: 0.516 [-18.250, 100.000], mean action: 1.175 [0.000, 3.000], mean observation: 0.137 [-0.542, 1.000], loss: 3.398647, mean_absolute_error: 37.372944, mean_q: 50.413834
 1277834/1500000: episode: 2398, duration: 2.975s, episode steps: 261, steps per second: 88, episode reward: 188.450, mean reward: 0.722 [-2.803, 100.000], mean action: 1.249 [0.000, 3.000], mean observation: 0.068 [-0.446, 1.000], loss: 5.735669, mean_absolute_error: 37.439991, mean_q: 50.534222
 1278153/1500000: episode: 2399, duration: 5.728s, episode steps: 319, steps per second: 56, episode reward: 218.066, mean reward: 0.684 [-3.046, 100.000], mean action: 1.069 [0.000, 3.000], mean observation: 0.097 [-0.528, 1.000], loss: 4.343986, mean_absolute_error: 37.404556, mean_q: 50.474083
 1278724/1500000: episode: 2400, duration: 8.893s, episode steps: 571, steps per second: 64, episode reward: 210.754, mean reward: 0.369 [-18.484, 100.000], mean action: 0.676 [0.000, 3.000], mean observation: 0.149 [-0.666, 1.000], loss: 4.288780, mean_absolute_error: 37.671814, mean_q: 50.814636
 1278990/1500000: episode: 2401, duration: 4.145s, episode steps: 266, steps per second: 64, episode reward: 235.305, mean reward: 0.885 [-2.858, 100.000], mean action: 1.406 [0.000, 3.000], mean observation: 0.031 [-0.691, 1.000], loss: 5.784982, mean_absolute_error: 37.769562, mean_q: 50.940861
 1279300/1500000: episode: 2402, duration: 6.745s, episode steps: 310, steps per second: 46, episode reward: 190.280, mean reward: 0.614 [-10.526, 100.000], mean action: 1.381 [0.000, 3.000], mean observation: 0.077 [-0.469, 1.000], loss: 2.403834, mean_absolute_error: 37.713474, mean_q: 50.916328
 1279651/1500000: episode: 2403, duration: 4.524s, episode steps: 351, steps per second: 78, episode reward: 197.269, mean reward: 0.562 [-17.696, 100.000], mean action: 1.325 [0.000, 3.000], mean observation: 0.102 [-0.421, 1.000], loss: 4.904955, mean_absolute_error: 37.781460, mean_q: 50.952469
 1279930/1500000: episode: 2404, duration: 6.826s, episode steps: 279, steps per second: 41, episode reward: 238.263, mean reward: 0.854 [-3.073, 100.000], mean action: 1.312 [0.000, 3.000], mean observation: 0.047 [-0.652, 1.000], loss: 3.230880, mean_absolute_error: 37.967979, mean_q: 51.206951
 1280313/1500000: episode: 2405, duration: 5.284s, episode steps: 383, steps per second: 72, episode reward: 217.707, mean reward: 0.568 [-17.426, 100.000], mean action: 1.115 [0.000, 3.000], mean observation: 0.111 [-0.446, 1.000], loss: 3.992805, mean_absolute_error: 37.709957, mean_q: 50.856327
 1280713/1500000: episode: 2406, duration: 7.351s, episode steps: 400, steps per second: 54, episode reward: 226.139, mean reward: 0.565 [-19.132, 100.000], mean action: 1.195 [0.000, 3.000], mean observation: 0.117 [-0.638, 1.000], loss: 3.778409, mean_absolute_error: 37.780964, mean_q: 50.926620
 1281213/1500000: episode: 2407, duration: 7.809s, episode steps: 500, steps per second: 64, episode reward: 219.814, mean reward: 0.440 [-17.562, 100.000], mean action: 1.012 [0.000, 3.000], mean observation: 0.144 [-0.540, 1.000], loss: 4.255336, mean_absolute_error: 37.842049, mean_q: 51.035629
 1281510/1500000: episode: 2408, duration: 5.029s, episode steps: 297, steps per second: 59, episode reward: 203.870, mean reward: 0.686 [-3.002, 100.000], mean action: 1.441 [0.000, 3.000], mean observation: 0.061 [-0.452, 1.000], loss: 3.478219, mean_absolute_error: 37.772980, mean_q: 50.930321
 1281890/1500000: episode: 2409, duration: 7.631s, episode steps: 380, steps per second: 50, episode reward: 200.927, mean reward: 0.529 [-17.472, 100.000], mean action: 1.213 [0.000, 3.000], mean observation: 0.109 [-0.487, 1.000], loss: 4.835907, mean_absolute_error: 37.984787, mean_q: 51.216755
 1282392/1500000: episode: 2410, duration: 8.422s, episode steps: 502, steps per second: 60, episode reward: 198.802, mean reward: 0.396 [-19.594, 100.000], mean action: 1.157 [0.000, 3.000], mean observation: 0.118 [-0.450, 1.002], loss: 3.614704, mean_absolute_error: 37.851227, mean_q: 51.082787
 1282648/1500000: episode: 2411, duration: 6.165s, episode steps: 256, steps per second: 42, episode reward: 174.410, mean reward: 0.681 [-22.521, 100.000], mean action: 1.234 [0.000, 3.000], mean observation: 0.056 [-0.444, 1.000], loss: 4.871729, mean_absolute_error: 37.896935, mean_q: 51.119606
 1282996/1500000: episode: 2412, duration: 4.007s, episode steps: 348, steps per second: 87, episode reward: 190.559, mean reward: 0.548 [-17.403, 100.000], mean action: 1.368 [0.000, 3.000], mean observation: 0.071 [-0.464, 1.000], loss: 3.189786, mean_absolute_error: 37.831394, mean_q: 51.049271
 1283352/1500000: episode: 2413, duration: 7.126s, episode steps: 356, steps per second: 50, episode reward: 222.451, mean reward: 0.625 [-19.860, 100.000], mean action: 1.110 [0.000, 3.000], mean observation: 0.099 [-0.464, 1.014], loss: 3.860308, mean_absolute_error: 37.765560, mean_q: 50.949974
 1283711/1500000: episode: 2414, duration: 6.646s, episode steps: 359, steps per second: 54, episode reward: 219.196, mean reward: 0.611 [-19.139, 100.000], mean action: 1.072 [0.000, 3.000], mean observation: 0.104 [-0.700, 1.000], loss: 5.310443, mean_absolute_error: 37.838200, mean_q: 51.055557
 1284094/1500000: episode: 2415, duration: 6.947s, episode steps: 383, steps per second: 55, episode reward: 230.704, mean reward: 0.602 [-18.949, 100.000], mean action: 1.347 [0.000, 3.000], mean observation: 0.109 [-0.630, 1.000], loss: 3.822208, mean_absolute_error: 37.880764, mean_q: 51.090801
 1284355/1500000: episode: 2416, duration: 5.579s, episode steps: 261, steps per second: 47, episode reward: 229.198, mean reward: 0.878 [-9.761, 100.000], mean action: 1.487 [0.000, 3.000], mean observation: 0.077 [-0.501, 1.000], loss: 6.257606, mean_absolute_error: 37.941490, mean_q: 51.186825
 1284695/1500000: episode: 2417, duration: 4.064s, episode steps: 340, steps per second: 84, episode reward: 236.628, mean reward: 0.696 [-10.093, 100.000], mean action: 1.609 [0.000, 3.000], mean observation: 0.044 [-0.616, 1.015], loss: 3.457106, mean_absolute_error: 38.027096, mean_q: 51.299576
 1285044/1500000: episode: 2418, duration: 7.201s, episode steps: 349, steps per second: 48, episode reward: 224.827, mean reward: 0.644 [-3.137, 100.000], mean action: 1.378 [0.000, 3.000], mean observation: 0.057 [-0.662, 1.000], loss: 3.777877, mean_absolute_error: 37.964260, mean_q: 51.215431
 1285358/1500000: episode: 2419, duration: 5.400s, episode steps: 314, steps per second: 58, episode reward: 212.139, mean reward: 0.676 [-20.167, 100.000], mean action: 1.134 [0.000, 3.000], mean observation: 0.039 [-0.564, 1.000], loss: 3.201669, mean_absolute_error: 38.065842, mean_q: 51.353596
 1285690/1500000: episode: 2420, duration: 4.177s, episode steps: 332, steps per second: 79, episode reward: 226.223, mean reward: 0.681 [-8.908, 100.000], mean action: 1.383 [0.000, 3.000], mean observation: 0.053 [-0.568, 1.005], loss: 3.591337, mean_absolute_error: 38.034016, mean_q: 51.320549
 1285938/1500000: episode: 2421, duration: 6.016s, episode steps: 248, steps per second: 41, episode reward: 237.750, mean reward: 0.959 [-12.667, 100.000], mean action: 1.395 [0.000, 3.000], mean observation: 0.028 [-0.715, 1.000], loss: 2.876419, mean_absolute_error: 38.083691, mean_q: 51.395866
 1286274/1500000: episode: 2422, duration: 5.769s, episode steps: 336, steps per second: 58, episode reward: 230.226, mean reward: 0.685 [-17.654, 100.000], mean action: 1.298 [0.000, 3.000], mean observation: 0.052 [-0.554, 1.000], loss: 2.575792, mean_absolute_error: 37.893387, mean_q: 51.148186
 1287274/1500000: episode: 2423, duration: 18.041s, episode steps: 1000, steps per second: 55, episode reward: 66.681, mean reward: 0.067 [-21.069, 22.334], mean action: 2.425 [0.000, 3.000], mean observation: 0.180 [-0.473, 1.000], loss: 3.673184, mean_absolute_error: 37.880180, mean_q: 51.101704
 1287637/1500000: episode: 2424, duration: 8.110s, episode steps: 363, steps per second: 45, episode reward: 180.943, mean reward: 0.498 [-11.352, 100.000], mean action: 1.066 [0.000, 3.000], mean observation: 0.097 [-0.455, 1.000], loss: 3.918154, mean_absolute_error: 37.745567, mean_q: 50.902504
 1288092/1500000: episode: 2425, duration: 6.753s, episode steps: 455, steps per second: 67, episode reward: 204.367, mean reward: 0.449 [-8.443, 100.000], mean action: 0.881 [0.000, 3.000], mean observation: 0.139 [-0.487, 1.000], loss: 3.418525, mean_absolute_error: 37.633060, mean_q: 50.754440
 1288374/1500000: episode: 2426, duration: 4.898s, episode steps: 282, steps per second: 58, episode reward: 209.850, mean reward: 0.744 [-2.683, 100.000], mean action: 1.323 [0.000, 3.000], mean observation: 0.053 [-0.513, 1.000], loss: 3.848882, mean_absolute_error: 37.635319, mean_q: 50.754906
 1288684/1500000: episode: 2427, duration: 7.269s, episode steps: 310, steps per second: 43, episode reward: 208.395, mean reward: 0.672 [-17.698, 100.000], mean action: 1.268 [0.000, 3.000], mean observation: 0.055 [-0.457, 1.000], loss: 3.351087, mean_absolute_error: 37.626305, mean_q: 50.763851
 1289009/1500000: episode: 2428, duration: 3.753s, episode steps: 325, steps per second: 87, episode reward: 181.876, mean reward: 0.560 [-17.365, 100.000], mean action: 1.397 [0.000, 3.000], mean observation: 0.059 [-0.466, 1.000], loss: 2.896235, mean_absolute_error: 37.596424, mean_q: 50.699963
 1289347/1500000: episode: 2429, duration: 6.957s, episode steps: 338, steps per second: 49, episode reward: 215.774, mean reward: 0.638 [-19.324, 100.000], mean action: 1.068 [0.000, 3.000], mean observation: 0.083 [-0.463, 1.000], loss: 3.043684, mean_absolute_error: 37.696644, mean_q: 50.835186
 1289688/1500000: episode: 2430, duration: 6.448s, episode steps: 341, steps per second: 53, episode reward: 174.684, mean reward: 0.512 [-17.168, 100.000], mean action: 2.161 [0.000, 3.000], mean observation: 0.085 [-0.470, 1.000], loss: 3.795022, mean_absolute_error: 37.571861, mean_q: 50.644062
 1290016/1500000: episode: 2431, duration: 3.819s, episode steps: 328, steps per second: 86, episode reward: 219.268, mean reward: 0.669 [-11.288, 100.000], mean action: 1.323 [0.000, 3.000], mean observation: 0.055 [-0.624, 1.000], loss: 3.807086, mean_absolute_error: 37.590351, mean_q: 50.701279
 1290353/1500000: episode: 2432, duration: 8.711s, episode steps: 337, steps per second: 39, episode reward: 235.525, mean reward: 0.699 [-19.892, 100.000], mean action: 1.160 [0.000, 3.000], mean observation: 0.093 [-0.638, 1.000], loss: 3.384492, mean_absolute_error: 37.711979, mean_q: 50.867603
 1290800/1500000: episode: 2433, duration: 5.890s, episode steps: 447, steps per second: 76, episode reward: 249.099, mean reward: 0.557 [-17.332, 100.000], mean action: 1.011 [0.000, 3.000], mean observation: 0.111 [-0.702, 1.000], loss: 4.666290, mean_absolute_error: 37.761230, mean_q: 50.906250
 1291800/1500000: episode: 2434, duration: 18.547s, episode steps: 1000, steps per second: 54, episode reward: 66.803, mean reward: 0.067 [-18.498, 22.217], mean action: 1.860 [0.000, 3.000], mean observation: 0.175 [-0.569, 1.000], loss: 3.471749, mean_absolute_error: 37.727345, mean_q: 50.869759
 1292144/1500000: episode: 2435, duration: 8.916s, episode steps: 344, steps per second: 39, episode reward: 235.874, mean reward: 0.686 [-21.853, 100.000], mean action: 1.134 [0.000, 3.000], mean observation: 0.115 [-0.486, 1.000], loss: 3.202692, mean_absolute_error: 37.867386, mean_q: 51.068592
 1292562/1500000: episode: 2436, duration: 5.233s, episode steps: 418, steps per second: 80, episode reward: 176.720, mean reward: 0.423 [-18.260, 100.000], mean action: 2.103 [0.000, 3.000], mean observation: 0.078 [-0.622, 1.000], loss: 4.153261, mean_absolute_error: 37.844360, mean_q: 51.053444
 1292988/1500000: episode: 2437, duration: 9.630s, episode steps: 426, steps per second: 44, episode reward: 201.648, mean reward: 0.473 [-19.593, 100.000], mean action: 1.204 [0.000, 3.000], mean observation: 0.128 [-0.492, 1.000], loss: 3.032480, mean_absolute_error: 37.812584, mean_q: 50.981277
 1293298/1500000: episode: 2438, duration: 4.680s, episode steps: 310, steps per second: 66, episode reward: 243.311, mean reward: 0.785 [-2.983, 100.000], mean action: 1.377 [0.000, 3.000], mean observation: 0.045 [-0.744, 1.000], loss: 3.422006, mean_absolute_error: 37.731709, mean_q: 50.859802
 1293639/1500000: episode: 2439, duration: 5.053s, episode steps: 341, steps per second: 67, episode reward: 191.510, mean reward: 0.562 [-11.733, 100.000], mean action: 1.317 [0.000, 3.000], mean observation: 0.050 [-0.454, 1.000], loss: 6.262377, mean_absolute_error: 37.750137, mean_q: 50.869663
 1294050/1500000: episode: 2440, duration: 9.504s, episode steps: 411, steps per second: 43, episode reward: 223.017, mean reward: 0.543 [-18.015, 100.000], mean action: 1.056 [0.000, 3.000], mean observation: 0.100 [-0.524, 1.000], loss: 3.253157, mean_absolute_error: 37.911777, mean_q: 51.121494
 1294539/1500000: episode: 2441, duration: 7.061s, episode steps: 489, steps per second: 69, episode reward: 199.756, mean reward: 0.408 [-17.503, 100.000], mean action: 1.049 [0.000, 3.000], mean observation: 0.127 [-0.499, 1.000], loss: 4.747212, mean_absolute_error: 37.891933, mean_q: 51.090851
 1295166/1500000: episode: 2442, duration: 12.107s, episode steps: 627, steps per second: 52, episode reward: 162.630, mean reward: 0.259 [-18.781, 100.000], mean action: 1.265 [0.000, 3.000], mean observation: 0.156 [-0.481, 1.000], loss: 4.424616, mean_absolute_error: 37.993694, mean_q: 51.177956
 1295518/1500000: episode: 2443, duration: 6.802s, episode steps: 352, steps per second: 52, episode reward: 242.539, mean reward: 0.689 [-17.547, 100.000], mean action: 1.074 [0.000, 3.000], mean observation: 0.083 [-0.717, 1.000], loss: 3.544339, mean_absolute_error: 38.072594, mean_q: 51.313282
 1295867/1500000: episode: 2444, duration: 8.038s, episode steps: 349, steps per second: 43, episode reward: 229.336, mean reward: 0.657 [-8.613, 100.000], mean action: 1.192 [0.000, 3.000], mean observation: 0.117 [-0.494, 1.000], loss: 3.370581, mean_absolute_error: 38.070168, mean_q: 51.336533
 1296867/1500000: episode: 2445, duration: 18.450s, episode steps: 1000, steps per second: 54, episode reward: 101.222, mean reward: 0.101 [-22.588, 22.006], mean action: 0.852 [0.000, 3.000], mean observation: 0.186 [-0.684, 1.009], loss: 3.532014, mean_absolute_error: 37.970047, mean_q: 51.203133
 1297293/1500000: episode: 2446, duration: 8.632s, episode steps: 426, steps per second: 49, episode reward: 240.783, mean reward: 0.565 [-11.351, 100.000], mean action: 1.082 [0.000, 3.000], mean observation: 0.130 [-0.659, 1.000], loss: 3.602109, mean_absolute_error: 38.073021, mean_q: 51.334522
 1297622/1500000: episode: 2447, duration: 6.237s, episode steps: 329, steps per second: 53, episode reward: 228.817, mean reward: 0.695 [-13.057, 100.000], mean action: 1.347 [0.000, 3.000], mean observation: 0.102 [-0.546, 1.000], loss: 3.495172, mean_absolute_error: 37.969112, mean_q: 51.201618
 1297952/1500000: episode: 2448, duration: 4.133s, episode steps: 330, steps per second: 80, episode reward: 214.263, mean reward: 0.649 [-9.482, 100.000], mean action: 1.485 [0.000, 3.000], mean observation: 0.096 [-0.437, 1.000], loss: 3.567189, mean_absolute_error: 38.070175, mean_q: 51.318764
 1298283/1500000: episode: 2449, duration: 8.141s, episode steps: 331, steps per second: 41, episode reward: 209.225, mean reward: 0.632 [-18.681, 100.000], mean action: 1.405 [0.000, 3.000], mean observation: 0.057 [-0.460, 1.000], loss: 4.037343, mean_absolute_error: 38.243771, mean_q: 51.559395
 1298688/1500000: episode: 2450, duration: 6.126s, episode steps: 405, steps per second: 66, episode reward: 243.003, mean reward: 0.600 [-17.636, 100.000], mean action: 1.121 [0.000, 3.000], mean observation: 0.129 [-0.415, 1.000], loss: 3.565214, mean_absolute_error: 38.129745, mean_q: 51.417854
 1299071/1500000: episode: 2451, duration: 8.083s, episode steps: 383, steps per second: 47, episode reward: 215.970, mean reward: 0.564 [-9.994, 100.000], mean action: 0.943 [0.000, 3.000], mean observation: 0.098 [-0.489, 1.017], loss: 4.692634, mean_absolute_error: 38.293667, mean_q: 51.624828
 1299441/1500000: episode: 2452, duration: 7.293s, episode steps: 370, steps per second: 51, episode reward: 201.107, mean reward: 0.544 [-17.421, 100.000], mean action: 1.097 [0.000, 3.000], mean observation: 0.114 [-0.449, 1.000], loss: 4.182034, mean_absolute_error: 38.381889, mean_q: 51.746674
 1299756/1500000: episode: 2453, duration: 3.995s, episode steps: 315, steps per second: 79, episode reward: 180.115, mean reward: 0.572 [-11.824, 100.000], mean action: 1.254 [0.000, 3.000], mean observation: 0.043 [-0.593, 1.000], loss: 3.443574, mean_absolute_error: 38.377991, mean_q: 51.726295
 1300176/1500000: episode: 2454, duration: 10.742s, episode steps: 420, steps per second: 39, episode reward: 165.252, mean reward: 0.393 [-17.283, 100.000], mean action: 2.460 [0.000, 3.000], mean observation: 0.122 [-0.608, 1.000], loss: 3.775498, mean_absolute_error: 38.462330, mean_q: 51.824833
 1300510/1500000: episode: 2455, duration: 3.863s, episode steps: 334, steps per second: 86, episode reward: 232.034, mean reward: 0.695 [-18.976, 100.000], mean action: 1.180 [0.000, 3.000], mean observation: 0.113 [-0.546, 1.000], loss: 4.360994, mean_absolute_error: 38.345295, mean_q: 51.705372
 1300789/1500000: episode: 2456, duration: 5.681s, episode steps: 279, steps per second: 49, episode reward: 198.713, mean reward: 0.712 [-2.761, 100.000], mean action: 1.111 [0.000, 3.000], mean observation: 0.055 [-0.475, 1.000], loss: 2.550167, mean_absolute_error: 38.197399, mean_q: 51.546898
 1301188/1500000: episode: 2457, duration: 8.325s, episode steps: 399, steps per second: 48, episode reward: 208.322, mean reward: 0.522 [-2.764, 100.000], mean action: 1.163 [0.000, 3.000], mean observation: 0.114 [-0.464, 1.000], loss: 3.705022, mean_absolute_error: 38.181526, mean_q: 51.500557
 1301555/1500000: episode: 2458, duration: 4.756s, episode steps: 367, steps per second: 77, episode reward: 231.088, mean reward: 0.630 [-18.531, 100.000], mean action: 1.185 [0.000, 3.000], mean observation: 0.072 [-0.660, 1.000], loss: 3.196672, mean_absolute_error: 38.068310, mean_q: 51.344715
 1301870/1500000: episode: 2459, duration: 7.866s, episode steps: 315, steps per second: 40, episode reward: 241.431, mean reward: 0.766 [-19.290, 100.000], mean action: 1.213 [0.000, 3.000], mean observation: 0.097 [-0.651, 1.000], loss: 5.362117, mean_absolute_error: 38.150837, mean_q: 51.402534
 1302246/1500000: episode: 2460, duration: 5.669s, episode steps: 376, steps per second: 66, episode reward: 216.847, mean reward: 0.577 [-11.141, 100.000], mean action: 1.332 [0.000, 3.000], mean observation: 0.105 [-0.492, 1.000], loss: 3.301438, mean_absolute_error: 38.213394, mean_q: 51.526272
 1302526/1500000: episode: 2461, duration: 4.664s, episode steps: 280, steps per second: 60, episode reward: 242.279, mean reward: 0.865 [-10.577, 100.000], mean action: 1.364 [0.000, 3.000], mean observation: 0.054 [-0.663, 1.000], loss: 4.820539, mean_absolute_error: 38.254955, mean_q: 51.564133
 1302886/1500000: episode: 2462, duration: 8.764s, episode steps: 360, steps per second: 41, episode reward: 228.678, mean reward: 0.635 [-17.645, 100.000], mean action: 1.206 [0.000, 3.000], mean observation: 0.079 [-0.553, 1.000], loss: 3.107735, mean_absolute_error: 38.048157, mean_q: 51.321201
 1303108/1500000: episode: 2463, duration: 2.638s, episode steps: 222, steps per second: 84, episode reward: 254.156, mean reward: 1.145 [-10.882, 100.000], mean action: 1.622 [0.000, 3.000], mean observation: 0.008 [-0.749, 1.000], loss: 3.704129, mean_absolute_error: 38.218212, mean_q: 51.518150
 1303480/1500000: episode: 2464, duration: 6.523s, episode steps: 372, steps per second: 57, episode reward: 210.496, mean reward: 0.566 [-10.158, 100.000], mean action: 0.952 [0.000, 3.000], mean observation: 0.123 [-0.523, 1.000], loss: 5.020702, mean_absolute_error: 38.289703, mean_q: 51.609558
 1303849/1500000: episode: 2465, duration: 8.513s, episode steps: 369, steps per second: 43, episode reward: 171.921, mean reward: 0.466 [-17.363, 100.000], mean action: 0.743 [0.000, 3.000], mean observation: 0.102 [-0.540, 1.000], loss: 2.964945, mean_absolute_error: 38.412018, mean_q: 51.801140
 1304344/1500000: episode: 2466, duration: 7.135s, episode steps: 495, steps per second: 69, episode reward: 198.940, mean reward: 0.402 [-17.629, 100.000], mean action: 0.859 [0.000, 3.000], mean observation: 0.149 [-0.459, 1.000], loss: 5.005872, mean_absolute_error: 38.270927, mean_q: 51.582561
 1304836/1500000: episode: 2467, duration: 10.796s, episode steps: 492, steps per second: 46, episode reward: 186.994, mean reward: 0.380 [-17.702, 100.000], mean action: 0.815 [0.000, 3.000], mean observation: 0.143 [-0.458, 1.000], loss: 3.337488, mean_absolute_error: 38.326054, mean_q: 51.665115
 1305007/1500000: episode: 2468, duration: 2.074s, episode steps: 171, steps per second: 82, episode reward: -5.186, mean reward: -0.030 [-100.000, 11.775], mean action: 1.854 [0.000, 3.000], mean observation: 0.068 [-0.580, 1.000], loss: 2.660440, mean_absolute_error: 38.201675, mean_q: 51.505287
 1305300/1500000: episode: 2469, duration: 5.420s, episode steps: 293, steps per second: 54, episode reward: 216.639, mean reward: 0.739 [-10.781, 100.000], mean action: 0.945 [0.000, 3.000], mean observation: 0.077 [-0.513, 1.007], loss: 4.960246, mean_absolute_error: 38.273510, mean_q: 51.591503
 1305678/1500000: episode: 2470, duration: 8.643s, episode steps: 378, steps per second: 44, episode reward: 234.317, mean reward: 0.620 [-22.114, 100.000], mean action: 0.974 [0.000, 3.000], mean observation: 0.070 [-0.817, 1.000], loss: 3.464561, mean_absolute_error: 38.433197, mean_q: 51.808895
 1305921/1500000: episode: 2471, duration: 2.841s, episode steps: 243, steps per second: 86, episode reward: 239.820, mean reward: 0.987 [-3.314, 100.000], mean action: 1.379 [0.000, 3.000], mean observation: 0.031 [-0.650, 1.000], loss: 1.929680, mean_absolute_error: 38.336971, mean_q: 51.713428
 1306337/1500000: episode: 2472, duration: 8.132s, episode steps: 416, steps per second: 51, episode reward: 240.271, mean reward: 0.578 [-17.598, 100.000], mean action: 0.990 [0.000, 3.000], mean observation: 0.128 [-0.445, 1.000], loss: 4.553721, mean_absolute_error: 38.301327, mean_q: 51.626614
 1306638/1500000: episode: 2473, duration: 6.341s, episode steps: 301, steps per second: 47, episode reward: 177.063, mean reward: 0.588 [-17.344, 100.000], mean action: 0.940 [0.000, 3.000], mean observation: 0.083 [-0.496, 1.000], loss: 4.069232, mean_absolute_error: 38.201283, mean_q: 51.519737
 1306920/1500000: episode: 2474, duration: 3.331s, episode steps: 282, steps per second: 85, episode reward: 171.729, mean reward: 0.609 [-9.403, 100.000], mean action: 1.284 [0.000, 3.000], mean observation: 0.056 [-0.491, 1.000], loss: 4.905407, mean_absolute_error: 38.059780, mean_q: 51.316074
 1307238/1500000: episode: 2475, duration: 6.544s, episode steps: 318, steps per second: 49, episode reward: 233.024, mean reward: 0.733 [-9.224, 100.000], mean action: 1.274 [0.000, 3.000], mean observation: 0.073 [-0.625, 1.000], loss: 3.462466, mean_absolute_error: 38.318985, mean_q: 51.637707
 1307695/1500000: episode: 2476, duration: 9.103s, episode steps: 457, steps per second: 50, episode reward: 213.588, mean reward: 0.467 [-17.455, 100.000], mean action: 0.906 [0.000, 3.000], mean observation: 0.134 [-0.498, 1.001], loss: 4.384631, mean_absolute_error: 38.393368, mean_q: 51.739380
 1307941/1500000: episode: 2477, duration: 2.788s, episode steps: 246, steps per second: 88, episode reward: 178.304, mean reward: 0.725 [-13.951, 100.000], mean action: 1.402 [0.000, 3.000], mean observation: -0.046 [-0.718, 1.000], loss: 3.962432, mean_absolute_error: 38.398319, mean_q: 51.757923
 1308246/1500000: episode: 2478, duration: 7.214s, episode steps: 305, steps per second: 42, episode reward: 228.038, mean reward: 0.748 [-2.682, 100.000], mean action: 1.056 [0.000, 3.000], mean observation: 0.108 [-0.619, 1.000], loss: 4.378702, mean_absolute_error: 38.434879, mean_q: 51.773434
 1308511/1500000: episode: 2479, duration: 5.863s, episode steps: 265, steps per second: 45, episode reward: 227.930, mean reward: 0.860 [-19.167, 100.000], mean action: 1.332 [0.000, 3.000], mean observation: 0.058 [-0.647, 1.000], loss: 3.944140, mean_absolute_error: 38.530159, mean_q: 51.929317
 1308786/1500000: episode: 2480, duration: 3.194s, episode steps: 275, steps per second: 86, episode reward: 222.246, mean reward: 0.808 [-13.534, 100.000], mean action: 1.411 [0.000, 3.000], mean observation: 0.049 [-0.690, 1.000], loss: 2.868283, mean_absolute_error: 38.496067, mean_q: 51.898914
 1309060/1500000: episode: 2481, duration: 5.219s, episode steps: 274, steps per second: 53, episode reward: 229.415, mean reward: 0.837 [-2.910, 100.000], mean action: 1.321 [0.000, 3.000], mean observation: 0.090 [-0.492, 1.000], loss: 2.210814, mean_absolute_error: 38.455536, mean_q: 51.867607
 1309382/1500000: episode: 2482, duration: 8.038s, episode steps: 322, steps per second: 40, episode reward: 187.622, mean reward: 0.583 [-17.582, 100.000], mean action: 1.031 [0.000, 3.000], mean observation: 0.093 [-0.529, 1.000], loss: 2.813574, mean_absolute_error: 38.481186, mean_q: 51.891571
 1309594/1500000: episode: 2483, duration: 2.701s, episode steps: 212, steps per second: 78, episode reward: 254.552, mean reward: 1.201 [-12.092, 100.000], mean action: 1.533 [0.000, 3.000], mean observation: 0.022 [-0.704, 1.000], loss: 3.488475, mean_absolute_error: 38.769680, mean_q: 52.266243
 1309865/1500000: episode: 2484, duration: 3.659s, episode steps: 271, steps per second: 74, episode reward: 179.808, mean reward: 0.663 [-9.374, 100.000], mean action: 1.351 [0.000, 3.000], mean observation: 0.034 [-0.465, 1.000], loss: 4.829092, mean_absolute_error: 38.578068, mean_q: 52.012871
 1310215/1500000: episode: 2485, duration: 8.931s, episode steps: 350, steps per second: 39, episode reward: 255.987, mean reward: 0.731 [-17.357, 100.000], mean action: 0.920 [0.000, 3.000], mean observation: 0.096 [-0.876, 1.000], loss: 5.182993, mean_absolute_error: 38.721416, mean_q: 52.210167
 1310566/1500000: episode: 2486, duration: 5.397s, episode steps: 351, steps per second: 65, episode reward: 222.370, mean reward: 0.634 [-18.576, 100.000], mean action: 1.046 [0.000, 3.000], mean observation: 0.086 [-0.539, 1.000], loss: 2.914073, mean_absolute_error: 38.442448, mean_q: 51.852386
 1310859/1500000: episode: 2487, duration: 4.185s, episode steps: 293, steps per second: 70, episode reward: 250.466, mean reward: 0.855 [-10.168, 100.000], mean action: 1.078 [0.000, 3.000], mean observation: 0.067 [-0.819, 1.000], loss: 3.492155, mean_absolute_error: 38.537640, mean_q: 51.966278
 1311134/1500000: episode: 2488, duration: 6.948s, episode steps: 275, steps per second: 40, episode reward: 225.334, mean reward: 0.819 [-9.498, 100.000], mean action: 1.262 [0.000, 3.000], mean observation: 0.086 [-0.609, 1.000], loss: 3.338741, mean_absolute_error: 38.778961, mean_q: 52.276043
 1311453/1500000: episode: 2489, duration: 5.576s, episode steps: 319, steps per second: 57, episode reward: 187.711, mean reward: 0.588 [-18.253, 100.000], mean action: 1.194 [0.000, 3.000], mean observation: 0.099 [-0.615, 1.000], loss: 6.382607, mean_absolute_error: 38.692562, mean_q: 52.135067
 1311708/1500000: episode: 2490, duration: 2.798s, episode steps: 255, steps per second: 91, episode reward: 183.914, mean reward: 0.721 [-10.255, 100.000], mean action: 1.255 [0.000, 3.000], mean observation: 0.051 [-0.500, 1.000], loss: 4.188238, mean_absolute_error: 38.843956, mean_q: 52.363091
 1312089/1500000: episode: 2491, duration: 9.737s, episode steps: 381, steps per second: 39, episode reward: 184.264, mean reward: 0.484 [-18.963, 100.000], mean action: 0.942 [0.000, 3.000], mean observation: 0.128 [-0.511, 1.000], loss: 3.646309, mean_absolute_error: 38.836082, mean_q: 52.361824
 1312396/1500000: episode: 2492, duration: 5.955s, episode steps: 307, steps per second: 52, episode reward: 198.117, mean reward: 0.645 [-17.742, 100.000], mean action: 1.459 [0.000, 3.000], mean observation: 0.048 [-0.646, 1.000], loss: 3.456099, mean_absolute_error: 38.769791, mean_q: 52.258251
 1312667/1500000: episode: 2493, duration: 4.077s, episode steps: 271, steps per second: 66, episode reward: 244.591, mean reward: 0.903 [-19.414, 100.000], mean action: 1.059 [0.000, 3.000], mean observation: 0.093 [-0.725, 1.000], loss: 3.734393, mean_absolute_error: 38.903557, mean_q: 52.441105
 1313050/1500000: episode: 2494, duration: 9.518s, episode steps: 383, steps per second: 40, episode reward: 218.413, mean reward: 0.570 [-17.759, 100.000], mean action: 1.003 [0.000, 3.000], mean observation: 0.123 [-0.534, 1.006], loss: 3.909164, mean_absolute_error: 38.907276, mean_q: 52.451778
 1313383/1500000: episode: 2495, duration: 4.491s, episode steps: 333, steps per second: 74, episode reward: 238.446, mean reward: 0.716 [-17.808, 100.000], mean action: 1.168 [0.000, 3.000], mean observation: 0.116 [-0.511, 1.000], loss: 6.439214, mean_absolute_error: 38.867569, mean_q: 52.399498
 1313726/1500000: episode: 2496, duration: 6.244s, episode steps: 343, steps per second: 55, episode reward: 207.777, mean reward: 0.606 [-17.330, 100.000], mean action: 1.347 [0.000, 3.000], mean observation: 0.072 [-0.561, 1.000], loss: 3.801925, mean_absolute_error: 38.781681, mean_q: 52.298180
 1314128/1500000: episode: 2497, duration: 9.265s, episode steps: 402, steps per second: 43, episode reward: 252.853, mean reward: 0.629 [-17.412, 100.000], mean action: 0.856 [0.000, 3.000], mean observation: 0.126 [-0.749, 1.000], loss: 3.413498, mean_absolute_error: 39.044430, mean_q: 52.636917
 1314548/1500000: episode: 2498, duration: 5.758s, episode steps: 420, steps per second: 73, episode reward: 201.878, mean reward: 0.481 [-20.540, 100.000], mean action: 1.200 [0.000, 3.000], mean observation: 0.080 [-0.522, 1.000], loss: 3.339119, mean_absolute_error: 39.125694, mean_q: 52.736385
 1314892/1500000: episode: 2499, duration: 9.586s, episode steps: 344, steps per second: 36, episode reward: 185.880, mean reward: 0.540 [-9.301, 100.000], mean action: 1.038 [0.000, 3.000], mean observation: 0.107 [-0.627, 1.000], loss: 4.291929, mean_absolute_error: 39.048756, mean_q: 52.640137
 1315171/1500000: episode: 2500, duration: 5.033s, episode steps: 279, steps per second: 55, episode reward: 245.944, mean reward: 0.882 [-18.914, 100.000], mean action: 1.401 [0.000, 3.000], mean observation: 0.053 [-0.673, 1.000], loss: 3.978033, mean_absolute_error: 38.979084, mean_q: 52.515888
 1315495/1500000: episode: 2501, duration: 4.408s, episode steps: 324, steps per second: 74, episode reward: 205.483, mean reward: 0.634 [-9.418, 100.000], mean action: 1.194 [0.000, 3.000], mean observation: 0.073 [-0.588, 1.009], loss: 4.583089, mean_absolute_error: 39.138882, mean_q: 52.762501
 1315802/1500000: episode: 2502, duration: 7.655s, episode steps: 307, steps per second: 40, episode reward: 195.282, mean reward: 0.636 [-8.166, 100.000], mean action: 1.195 [0.000, 3.000], mean observation: 0.087 [-0.488, 1.000], loss: 3.509856, mean_absolute_error: 39.121174, mean_q: 52.723045
 1316141/1500000: episode: 2503, duration: 6.420s, episode steps: 339, steps per second: 53, episode reward: 218.767, mean reward: 0.645 [-19.981, 100.000], mean action: 1.389 [0.000, 3.000], mean observation: 0.105 [-0.410, 1.000], loss: 4.464786, mean_absolute_error: 39.215420, mean_q: 52.819839
 1316452/1500000: episode: 2504, duration: 5.418s, episode steps: 311, steps per second: 57, episode reward: 186.626, mean reward: 0.600 [-18.800, 100.000], mean action: 1.309 [0.000, 3.000], mean observation: 0.074 [-0.499, 1.000], loss: 4.688660, mean_absolute_error: 39.277115, mean_q: 52.929234
 1316825/1500000: episode: 2505, duration: 9.509s, episode steps: 373, steps per second: 39, episode reward: 228.021, mean reward: 0.611 [-17.694, 100.000], mean action: 1.008 [0.000, 3.000], mean observation: 0.076 [-0.695, 1.000], loss: 3.435842, mean_absolute_error: 39.422539, mean_q: 53.117901
 1317274/1500000: episode: 2506, duration: 5.455s, episode steps: 449, steps per second: 82, episode reward: 243.903, mean reward: 0.543 [-19.078, 100.000], mean action: 1.187 [0.000, 3.000], mean observation: 0.138 [-0.561, 1.000], loss: 3.537943, mean_absolute_error: 39.439148, mean_q: 53.147381
 1317625/1500000: episode: 2507, duration: 8.493s, episode steps: 351, steps per second: 41, episode reward: 215.993, mean reward: 0.615 [-17.371, 100.000], mean action: 1.094 [0.000, 3.000], mean observation: 0.107 [-0.613, 1.000], loss: 3.447335, mean_absolute_error: 39.480228, mean_q: 53.223732
 1317866/1500000: episode: 2508, duration: 5.251s, episode steps: 241, steps per second: 46, episode reward: 174.024, mean reward: 0.722 [-18.289, 100.000], mean action: 1.116 [0.000, 3.000], mean observation: 0.052 [-0.494, 1.000], loss: 6.736904, mean_absolute_error: 39.404816, mean_q: 53.083416
 1318866/1500000: episode: 2509, duration: 18.644s, episode steps: 1000, steps per second: 54, episode reward: 97.281, mean reward: 0.097 [-20.296, 22.535], mean action: 1.349 [0.000, 3.000], mean observation: 0.169 [-0.668, 1.000], loss: 4.893893, mean_absolute_error: 39.430462, mean_q: 53.150658
 1319301/1500000: episode: 2510, duration: 7.782s, episode steps: 435, steps per second: 56, episode reward: 186.236, mean reward: 0.428 [-19.120, 100.000], mean action: 0.940 [0.000, 3.000], mean observation: 0.128 [-0.447, 1.000], loss: 3.874406, mean_absolute_error: 39.151928, mean_q: 52.801395
 1319577/1500000: episode: 2511, duration: 6.835s, episode steps: 276, steps per second: 40, episode reward: 215.477, mean reward: 0.781 [-2.839, 100.000], mean action: 1.319 [0.000, 3.000], mean observation: 0.042 [-0.526, 1.000], loss: 5.451529, mean_absolute_error: 39.058899, mean_q: 52.690746
 1319981/1500000: episode: 2512, duration: 5.415s, episode steps: 404, steps per second: 75, episode reward: 171.578, mean reward: 0.425 [-17.386, 100.000], mean action: 1.022 [0.000, 3.000], mean observation: 0.107 [-0.436, 1.000], loss: 3.958115, mean_absolute_error: 39.135712, mean_q: 52.756245
 1320243/1500000: episode: 2513, duration: 5.471s, episode steps: 262, steps per second: 48, episode reward: 226.408, mean reward: 0.864 [-11.626, 100.000], mean action: 1.187 [0.000, 3.000], mean observation: 0.069 [-0.701, 1.000], loss: 5.200041, mean_absolute_error: 38.983681, mean_q: 52.575157
 1320493/1500000: episode: 2514, duration: 6.432s, episode steps: 250, steps per second: 39, episode reward: 208.727, mean reward: 0.835 [-2.706, 100.000], mean action: 1.080 [0.000, 3.000], mean observation: 0.070 [-0.606, 1.000], loss: 3.549714, mean_absolute_error: 38.934380, mean_q: 52.530972
 1320899/1500000: episode: 2515, duration: 6.218s, episode steps: 406, steps per second: 65, episode reward: 205.618, mean reward: 0.506 [-19.594, 100.000], mean action: 1.086 [0.000, 3.000], mean observation: 0.064 [-0.545, 1.000], loss: 3.484345, mean_absolute_error: 38.761143, mean_q: 52.260178
 1321561/1500000: episode: 2516, duration: 14.622s, episode steps: 662, steps per second: 45, episode reward: 194.652, mean reward: 0.294 [-19.061, 100.000], mean action: 0.769 [0.000, 3.000], mean observation: 0.144 [-0.506, 1.000], loss: 3.700009, mean_absolute_error: 38.808884, mean_q: 52.325871
 1322015/1500000: episode: 2517, duration: 7.582s, episode steps: 454, steps per second: 60, episode reward: 175.895, mean reward: 0.387 [-17.740, 100.000], mean action: 0.866 [0.000, 3.000], mean observation: 0.104 [-0.461, 1.000], loss: 3.836175, mean_absolute_error: 38.889008, mean_q: 52.439476
 1322390/1500000: episode: 2518, duration: 9.171s, episode steps: 375, steps per second: 41, episode reward: 230.430, mean reward: 0.614 [-2.958, 100.000], mean action: 1.227 [0.000, 3.000], mean observation: 0.114 [-0.531, 1.000], loss: 3.197473, mean_absolute_error: 38.862946, mean_q: 52.398624
 1322893/1500000: episode: 2519, duration: 7.228s, episode steps: 503, steps per second: 70, episode reward: 248.576, mean reward: 0.494 [-18.706, 100.000], mean action: 0.875 [0.000, 3.000], mean observation: 0.131 [-0.765, 1.000], loss: 3.284379, mean_absolute_error: 38.789875, mean_q: 52.295918
 1323412/1500000: episode: 2520, duration: 11.719s, episode steps: 519, steps per second: 44, episode reward: 218.562, mean reward: 0.421 [-17.765, 100.000], mean action: 0.884 [0.000, 3.000], mean observation: 0.144 [-0.582, 1.000], loss: 2.867922, mean_absolute_error: 38.850613, mean_q: 52.381939
 1323772/1500000: episode: 2521, duration: 4.294s, episode steps: 360, steps per second: 84, episode reward: 237.279, mean reward: 0.659 [-17.581, 100.000], mean action: 1.275 [0.000, 3.000], mean observation: 0.057 [-0.584, 1.000], loss: 3.834652, mean_absolute_error: 38.775623, mean_q: 52.284927
 1324264/1500000: episode: 2522, duration: 12.456s, episode steps: 492, steps per second: 39, episode reward: 248.896, mean reward: 0.506 [-17.730, 100.000], mean action: 1.098 [0.000, 3.000], mean observation: 0.101 [-1.207, 1.000], loss: 3.876736, mean_absolute_error: 38.760162, mean_q: 52.276348
 1324643/1500000: episode: 2523, duration: 4.398s, episode steps: 379, steps per second: 86, episode reward: 230.118, mean reward: 0.607 [-17.880, 100.000], mean action: 0.942 [0.000, 3.000], mean observation: 0.110 [-0.729, 1.000], loss: 4.663545, mean_absolute_error: 38.614265, mean_q: 52.061676
 1325003/1500000: episode: 2524, duration: 8.320s, episode steps: 360, steps per second: 43, episode reward: 213.638, mean reward: 0.593 [-17.424, 100.000], mean action: 0.986 [0.000, 3.000], mean observation: 0.089 [-0.549, 1.000], loss: 2.096222, mean_absolute_error: 38.885227, mean_q: 52.421700
 1325400/1500000: episode: 2525, duration: 7.765s, episode steps: 397, steps per second: 51, episode reward: 229.273, mean reward: 0.578 [-9.525, 100.000], mean action: 1.355 [0.000, 3.000], mean observation: 0.100 [-0.581, 1.085], loss: 5.544354, mean_absolute_error: 38.669121, mean_q: 52.147659
 1325748/1500000: episode: 2526, duration: 4.984s, episode steps: 348, steps per second: 70, episode reward: 214.976, mean reward: 0.618 [-10.167, 100.000], mean action: 0.968 [0.000, 3.000], mean observation: 0.092 [-0.476, 1.000], loss: 3.727280, mean_absolute_error: 38.738571, mean_q: 52.244347
 1326024/1500000: episode: 2527, duration: 6.721s, episode steps: 276, steps per second: 41, episode reward: 237.564, mean reward: 0.861 [-7.826, 100.000], mean action: 1.529 [0.000, 3.000], mean observation: 0.027 [-0.717, 1.000], loss: 1.783344, mean_absolute_error: 38.339314, mean_q: 51.731075
 1326351/1500000: episode: 2528, duration: 6.450s, episode steps: 327, steps per second: 51, episode reward: 220.590, mean reward: 0.675 [-7.623, 100.000], mean action: 1.352 [0.000, 3.000], mean observation: 0.102 [-0.495, 1.000], loss: 3.745246, mean_absolute_error: 38.643921, mean_q: 52.086025
 1326703/1500000: episode: 2529, duration: 4.691s, episode steps: 352, steps per second: 75, episode reward: 213.701, mean reward: 0.607 [-12.043, 100.000], mean action: 1.457 [0.000, 3.000], mean observation: 0.121 [-0.647, 1.000], loss: 4.413318, mean_absolute_error: 38.447498, mean_q: 51.870472
 1327199/1500000: episode: 2530, duration: 12.322s, episode steps: 496, steps per second: 40, episode reward: 196.546, mean reward: 0.396 [-17.651, 100.000], mean action: 0.863 [0.000, 3.000], mean observation: 0.123 [-0.945, 1.000], loss: 3.170569, mean_absolute_error: 38.356239, mean_q: 51.758545
 1327547/1500000: episode: 2531, duration: 4.039s, episode steps: 348, steps per second: 86, episode reward: 251.490, mean reward: 0.723 [-9.582, 100.000], mean action: 1.224 [0.000, 3.000], mean observation: 0.101 [-0.699, 1.000], loss: 3.798211, mean_absolute_error: 38.475857, mean_q: 51.852123
 1327834/1500000: episode: 2532, duration: 6.465s, episode steps: 287, steps per second: 44, episode reward: 226.040, mean reward: 0.788 [-2.875, 100.000], mean action: 1.174 [0.000, 3.000], mean observation: 0.101 [-0.596, 1.000], loss: 4.409473, mean_absolute_error: 38.319645, mean_q: 51.679306
 1328372/1500000: episode: 2533, duration: 14.359s, episode steps: 538, steps per second: 37, episode reward: 190.538, mean reward: 0.354 [-18.153, 100.000], mean action: 1.009 [0.000, 3.000], mean observation: 0.135 [-0.489, 1.000], loss: 5.174039, mean_absolute_error: 38.401241, mean_q: 51.782711
 1328835/1500000: episode: 2534, duration: 12.368s, episode steps: 463, steps per second: 37, episode reward: 193.449, mean reward: 0.418 [-17.748, 100.000], mean action: 1.076 [0.000, 3.000], mean observation: 0.119 [-0.529, 1.000], loss: 3.189616, mean_absolute_error: 38.304939, mean_q: 51.676750
 1329102/1500000: episode: 2535, duration: 6.590s, episode steps: 267, steps per second: 41, episode reward: 234.262, mean reward: 0.877 [-5.499, 100.000], mean action: 1.382 [0.000, 3.000], mean observation: 0.035 [-0.569, 1.000], loss: 4.143391, mean_absolute_error: 38.560673, mean_q: 52.000534
 1329368/1500000: episode: 2536, duration: 6.754s, episode steps: 266, steps per second: 39, episode reward: 189.135, mean reward: 0.711 [-17.921, 100.000], mean action: 1.184 [0.000, 3.000], mean observation: 0.047 [-0.472, 1.000], loss: 5.015106, mean_absolute_error: 38.488647, mean_q: 51.902676
 1329663/1500000: episode: 2537, duration: 7.558s, episode steps: 295, steps per second: 39, episode reward: 229.530, mean reward: 0.778 [-10.315, 100.000], mean action: 1.285 [0.000, 3.000], mean observation: 0.043 [-0.503, 1.008], loss: 4.329697, mean_absolute_error: 38.545837, mean_q: 51.992764
 1329981/1500000: episode: 2538, duration: 8.301s, episode steps: 318, steps per second: 38, episode reward: 211.046, mean reward: 0.664 [-12.019, 100.000], mean action: 1.365 [0.000, 3.000], mean observation: 0.067 [-0.509, 1.000], loss: 4.170462, mean_absolute_error: 38.540668, mean_q: 51.987106
 1330342/1500000: episode: 2539, duration: 9.278s, episode steps: 361, steps per second: 39, episode reward: 214.973, mean reward: 0.595 [-9.309, 100.000], mean action: 1.172 [0.000, 3.000], mean observation: 0.090 [-0.482, 1.001], loss: 4.525077, mean_absolute_error: 38.765259, mean_q: 52.315601
 1330625/1500000: episode: 2540, duration: 6.743s, episode steps: 283, steps per second: 42, episode reward: 226.791, mean reward: 0.801 [-2.618, 100.000], mean action: 1.329 [0.000, 3.000], mean observation: 0.084 [-0.534, 1.000], loss: 5.203749, mean_absolute_error: 38.834709, mean_q: 52.369808
 1331022/1500000: episode: 2541, duration: 9.993s, episode steps: 397, steps per second: 40, episode reward: 198.130, mean reward: 0.499 [-18.137, 100.000], mean action: 1.020 [0.000, 3.000], mean observation: 0.120 [-0.561, 1.000], loss: 4.410935, mean_absolute_error: 38.912224, mean_q: 52.462734
 1331393/1500000: episode: 2542, duration: 9.601s, episode steps: 371, steps per second: 39, episode reward: 220.220, mean reward: 0.594 [-9.393, 100.000], mean action: 1.461 [0.000, 3.000], mean observation: 0.094 [-0.591, 1.000], loss: 5.917900, mean_absolute_error: 38.843288, mean_q: 52.372562
 1331702/1500000: episode: 2543, duration: 8.293s, episode steps: 309, steps per second: 37, episode reward: 222.536, mean reward: 0.720 [-11.472, 100.000], mean action: 1.159 [0.000, 3.000], mean observation: 0.067 [-0.557, 1.000], loss: 3.537843, mean_absolute_error: 39.092484, mean_q: 52.737450
 1332006/1500000: episode: 2544, duration: 7.613s, episode steps: 304, steps per second: 40, episode reward: 250.084, mean reward: 0.823 [-3.131, 100.000], mean action: 1.326 [0.000, 3.000], mean observation: 0.048 [-0.703, 1.000], loss: 4.646378, mean_absolute_error: 39.330379, mean_q: 53.032635
 1332324/1500000: episode: 2545, duration: 7.859s, episode steps: 318, steps per second: 40, episode reward: 208.447, mean reward: 0.655 [-11.236, 100.000], mean action: 1.035 [0.000, 3.000], mean observation: 0.092 [-0.591, 1.000], loss: 4.456923, mean_absolute_error: 39.063801, mean_q: 52.713123
 1332705/1500000: episode: 2546, duration: 9.834s, episode steps: 381, steps per second: 39, episode reward: 216.698, mean reward: 0.569 [-17.335, 100.000], mean action: 1.194 [0.000, 3.000], mean observation: 0.104 [-0.435, 1.000], loss: 3.326298, mean_absolute_error: 39.225010, mean_q: 52.902164
 1333078/1500000: episode: 2547, duration: 9.066s, episode steps: 373, steps per second: 41, episode reward: 226.003, mean reward: 0.606 [-17.712, 100.000], mean action: 1.193 [0.000, 3.000], mean observation: 0.110 [-0.580, 1.000], loss: 3.635718, mean_absolute_error: 39.344509, mean_q: 53.044918
 1333374/1500000: episode: 2548, duration: 7.665s, episode steps: 296, steps per second: 39, episode reward: 175.515, mean reward: 0.593 [-8.931, 100.000], mean action: 1.405 [0.000, 3.000], mean observation: 0.048 [-0.486, 1.000], loss: 4.540396, mean_absolute_error: 39.399296, mean_q: 53.133656
 1333821/1500000: episode: 2549, duration: 11.754s, episode steps: 447, steps per second: 38, episode reward: 228.175, mean reward: 0.510 [-17.582, 100.000], mean action: 1.049 [0.000, 3.000], mean observation: 0.119 [-0.637, 1.000], loss: 3.365718, mean_absolute_error: 39.021091, mean_q: 52.625568
 1334317/1500000: episode: 2550, duration: 13.103s, episode steps: 496, steps per second: 38, episode reward: 207.416, mean reward: 0.418 [-19.734, 100.000], mean action: 0.917 [0.000, 3.000], mean observation: 0.141 [-0.587, 1.000], loss: 3.201485, mean_absolute_error: 39.304424, mean_q: 52.992172
 1334708/1500000: episode: 2551, duration: 10.178s, episode steps: 391, steps per second: 38, episode reward: 190.208, mean reward: 0.486 [-9.624, 100.000], mean action: 1.251 [0.000, 3.000], mean observation: 0.081 [-0.594, 1.000], loss: 3.334031, mean_absolute_error: 39.414577, mean_q: 53.122578
 1335014/1500000: episode: 2552, duration: 7.805s, episode steps: 306, steps per second: 39, episode reward: 251.199, mean reward: 0.821 [-17.644, 100.000], mean action: 1.350 [0.000, 3.000], mean observation: 0.052 [-0.708, 1.000], loss: 7.070161, mean_absolute_error: 39.570721, mean_q: 53.326862
 1335448/1500000: episode: 2553, duration: 11.010s, episode steps: 434, steps per second: 39, episode reward: 217.326, mean reward: 0.501 [-17.402, 100.000], mean action: 0.993 [0.000, 3.000], mean observation: 0.131 [-0.425, 1.000], loss: 3.513670, mean_absolute_error: 39.281796, mean_q: 52.977589
 1335846/1500000: episode: 2554, duration: 10.052s, episode steps: 398, steps per second: 40, episode reward: 234.662, mean reward: 0.590 [-20.131, 100.000], mean action: 1.018 [0.000, 3.000], mean observation: 0.105 [-0.682, 1.000], loss: 3.781045, mean_absolute_error: 39.609940, mean_q: 53.385002
 1336203/1500000: episode: 2555, duration: 9.226s, episode steps: 357, steps per second: 39, episode reward: 182.517, mean reward: 0.511 [-9.758, 100.000], mean action: 1.003 [0.000, 3.000], mean observation: 0.074 [-0.513, 1.000], loss: 4.451394, mean_absolute_error: 39.491554, mean_q: 53.256592
 1336491/1500000: episode: 2556, duration: 6.914s, episode steps: 288, steps per second: 42, episode reward: 237.093, mean reward: 0.823 [-7.172, 100.000], mean action: 1.465 [0.000, 3.000], mean observation: 0.027 [-0.676, 1.000], loss: 5.914554, mean_absolute_error: 39.696812, mean_q: 53.481976
 1336754/1500000: episode: 2557, duration: 6.681s, episode steps: 263, steps per second: 39, episode reward: 202.047, mean reward: 0.768 [-9.818, 100.000], mean action: 1.395 [0.000, 3.000], mean observation: 0.065 [-0.561, 1.000], loss: 2.777668, mean_absolute_error: 39.470924, mean_q: 53.190872
 1337178/1500000: episode: 2558, duration: 10.853s, episode steps: 424, steps per second: 39, episode reward: 205.534, mean reward: 0.485 [-18.229, 100.000], mean action: 1.215 [0.000, 3.000], mean observation: 0.115 [-0.526, 1.000], loss: 4.482957, mean_absolute_error: 39.566765, mean_q: 53.328640
 1337515/1500000: episode: 2559, duration: 8.511s, episode steps: 337, steps per second: 40, episode reward: 193.185, mean reward: 0.573 [-9.578, 100.000], mean action: 1.389 [0.000, 3.000], mean observation: 0.052 [-0.464, 1.000], loss: 3.632118, mean_absolute_error: 39.398315, mean_q: 53.113628
 1337830/1500000: episode: 2560, duration: 8.004s, episode steps: 315, steps per second: 39, episode reward: 222.806, mean reward: 0.707 [-13.670, 100.000], mean action: 1.419 [0.000, 3.000], mean observation: 0.056 [-0.540, 1.000], loss: 5.050002, mean_absolute_error: 39.476032, mean_q: 53.176903
 1338263/1500000: episode: 2561, duration: 11.287s, episode steps: 433, steps per second: 38, episode reward: 207.186, mean reward: 0.478 [-18.500, 100.000], mean action: 1.046 [0.000, 3.000], mean observation: 0.103 [-0.549, 1.000], loss: 4.393090, mean_absolute_error: 39.315804, mean_q: 52.971039
 1339180/1500000: episode: 2562, duration: 24.412s, episode steps: 917, steps per second: 38, episode reward: 131.816, mean reward: 0.144 [-20.128, 100.000], mean action: 1.966 [0.000, 3.000], mean observation: 0.168 [-0.397, 1.000], loss: 4.028981, mean_absolute_error: 39.470875, mean_q: 53.196140
 1339582/1500000: episode: 2563, duration: 10.104s, episode steps: 402, steps per second: 40, episode reward: 216.735, mean reward: 0.539 [-17.594, 100.000], mean action: 0.990 [0.000, 3.000], mean observation: 0.100 [-0.557, 1.000], loss: 3.882055, mean_absolute_error: 39.423862, mean_q: 53.162312
 1339969/1500000: episode: 2564, duration: 10.243s, episode steps: 387, steps per second: 38, episode reward: 176.516, mean reward: 0.456 [-18.211, 100.000], mean action: 1.127 [0.000, 3.000], mean observation: 0.105 [-0.528, 1.000], loss: 3.379770, mean_absolute_error: 39.460155, mean_q: 53.194828
 1340396/1500000: episode: 2565, duration: 11.141s, episode steps: 427, steps per second: 38, episode reward: 211.667, mean reward: 0.496 [-18.035, 100.000], mean action: 1.143 [0.000, 3.000], mean observation: 0.121 [-0.551, 1.000], loss: 3.548556, mean_absolute_error: 39.309715, mean_q: 53.012897
 1340787/1500000: episode: 2566, duration: 10.130s, episode steps: 391, steps per second: 39, episode reward: 228.819, mean reward: 0.585 [-24.528, 100.000], mean action: 1.545 [0.000, 3.000], mean observation: 0.087 [-0.510, 1.056], loss: 3.809089, mean_absolute_error: 39.584435, mean_q: 53.349579
 1341142/1500000: episode: 2567, duration: 9.069s, episode steps: 355, steps per second: 39, episode reward: 218.061, mean reward: 0.614 [-2.851, 100.000], mean action: 1.414 [0.000, 3.000], mean observation: 0.091 [-0.388, 1.000], loss: 2.845486, mean_absolute_error: 39.445107, mean_q: 53.179298
 1341621/1500000: episode: 2568, duration: 13.593s, episode steps: 479, steps per second: 35, episode reward: 214.827, mean reward: 0.448 [-18.272, 100.000], mean action: 1.038 [0.000, 3.000], mean observation: 0.140 [-0.585, 1.000], loss: 3.617957, mean_absolute_error: 39.500828, mean_q: 53.269100
 1342148/1500000: episode: 2569, duration: 14.272s, episode steps: 527, steps per second: 37, episode reward: 225.255, mean reward: 0.427 [-17.900, 100.000], mean action: 1.027 [0.000, 3.000], mean observation: 0.129 [-0.473, 1.000], loss: 4.224422, mean_absolute_error: 39.375484, mean_q: 53.108791
 1342489/1500000: episode: 2570, duration: 8.496s, episode steps: 341, steps per second: 40, episode reward: 221.299, mean reward: 0.649 [-2.773, 100.000], mean action: 1.320 [0.000, 3.000], mean observation: 0.081 [-0.383, 1.000], loss: 3.974189, mean_absolute_error: 39.438042, mean_q: 53.193077
 1342783/1500000: episode: 2571, duration: 7.034s, episode steps: 294, steps per second: 42, episode reward: 272.413, mean reward: 0.927 [-17.699, 100.000], mean action: 0.929 [0.000, 3.000], mean observation: 0.096 [-0.860, 1.000], loss: 3.357245, mean_absolute_error: 39.559685, mean_q: 53.381691
 1343132/1500000: episode: 2572, duration: 8.967s, episode steps: 349, steps per second: 39, episode reward: 203.050, mean reward: 0.582 [-2.802, 100.000], mean action: 1.232 [0.000, 3.000], mean observation: 0.094 [-0.594, 1.000], loss: 3.468256, mean_absolute_error: 39.598816, mean_q: 53.398605
 1343445/1500000: episode: 2573, duration: 7.795s, episode steps: 313, steps per second: 40, episode reward: 218.320, mean reward: 0.698 [-8.259, 100.000], mean action: 1.466 [0.000, 3.000], mean observation: 0.055 [-0.543, 1.000], loss: 4.327388, mean_absolute_error: 39.619289, mean_q: 53.401894
 1343727/1500000: episode: 2574, duration: 7.117s, episode steps: 282, steps per second: 40, episode reward: 193.444, mean reward: 0.686 [-9.981, 100.000], mean action: 1.301 [0.000, 3.000], mean observation: 0.072 [-0.500, 1.000], loss: 3.285574, mean_absolute_error: 39.639481, mean_q: 53.425678
 1343984/1500000: episode: 2575, duration: 6.291s, episode steps: 257, steps per second: 41, episode reward: 242.222, mean reward: 0.942 [-9.528, 100.000], mean action: 1.307 [0.000, 3.000], mean observation: 0.106 [-0.670, 1.000], loss: 5.549387, mean_absolute_error: 39.654060, mean_q: 53.435295
 1344637/1500000: episode: 2576, duration: 17.719s, episode steps: 653, steps per second: 37, episode reward: 199.979, mean reward: 0.306 [-20.888, 100.000], mean action: 0.677 [0.000, 3.000], mean observation: 0.173 [-0.443, 1.000], loss: 3.321603, mean_absolute_error: 39.730232, mean_q: 53.552700
 1344919/1500000: episode: 2577, duration: 7.135s, episode steps: 282, steps per second: 40, episode reward: 210.121, mean reward: 0.745 [-10.162, 100.000], mean action: 1.422 [0.000, 3.000], mean observation: 0.055 [-0.532, 1.000], loss: 2.614391, mean_absolute_error: 39.730923, mean_q: 53.566277
 1345252/1500000: episode: 2578, duration: 8.526s, episode steps: 333, steps per second: 39, episode reward: 202.977, mean reward: 0.610 [-10.550, 100.000], mean action: 1.354 [0.000, 3.000], mean observation: 0.049 [-0.451, 1.000], loss: 2.980318, mean_absolute_error: 39.551319, mean_q: 53.304371
 1345561/1500000: episode: 2579, duration: 7.738s, episode steps: 309, steps per second: 40, episode reward: 256.720, mean reward: 0.831 [-18.150, 100.000], mean action: 1.256 [0.000, 3.000], mean observation: 0.061 [-0.775, 1.000], loss: 4.757187, mean_absolute_error: 39.741436, mean_q: 53.531933
 1345824/1500000: episode: 2580, duration: 7.016s, episode steps: 263, steps per second: 37, episode reward: 197.657, mean reward: 0.752 [-9.446, 100.000], mean action: 1.186 [0.000, 3.000], mean observation: 0.073 [-0.541, 1.000], loss: 3.447078, mean_absolute_error: 39.780163, mean_q: 53.606434
 1346187/1500000: episode: 2581, duration: 9.986s, episode steps: 363, steps per second: 36, episode reward: 228.636, mean reward: 0.630 [-17.429, 100.000], mean action: 1.185 [0.000, 3.000], mean observation: 0.114 [-0.521, 1.000], loss: 4.269074, mean_absolute_error: 39.631523, mean_q: 53.431259
 1346418/1500000: episode: 2582, duration: 6.600s, episode steps: 231, steps per second: 35, episode reward: 244.899, mean reward: 1.060 [-4.305, 100.000], mean action: 1.584 [0.000, 3.000], mean observation: 0.031 [-0.806, 1.000], loss: 2.770194, mean_absolute_error: 39.699142, mean_q: 53.523140
 1346725/1500000: episode: 2583, duration: 8.011s, episode steps: 307, steps per second: 38, episode reward: 177.450, mean reward: 0.578 [-10.815, 100.000], mean action: 1.121 [0.000, 3.000], mean observation: 0.087 [-0.561, 1.000], loss: 3.720639, mean_absolute_error: 39.705433, mean_q: 53.535767
 1347049/1500000: episode: 2584, duration: 9.013s, episode steps: 324, steps per second: 36, episode reward: 210.844, mean reward: 0.651 [-7.087, 100.000], mean action: 1.373 [0.000, 3.000], mean observation: 0.067 [-0.471, 1.000], loss: 3.859355, mean_absolute_error: 39.726109, mean_q: 53.556606
 1347367/1500000: episode: 2585, duration: 7.750s, episode steps: 318, steps per second: 41, episode reward: 227.296, mean reward: 0.715 [-2.688, 100.000], mean action: 1.292 [0.000, 3.000], mean observation: 0.088 [-0.525, 1.018], loss: 3.691884, mean_absolute_error: 39.876320, mean_q: 53.725597
 1347591/1500000: episode: 2586, duration: 5.530s, episode steps: 224, steps per second: 41, episode reward: 258.564, mean reward: 1.154 [-11.108, 100.000], mean action: 1.576 [0.000, 3.000], mean observation: 0.031 [-0.821, 1.000], loss: 4.307132, mean_absolute_error: 39.663555, mean_q: 53.428917
 1348013/1500000: episode: 2587, duration: 11.093s, episode steps: 422, steps per second: 38, episode reward: 189.567, mean reward: 0.449 [-10.415, 100.000], mean action: 1.194 [0.000, 3.000], mean observation: 0.083 [-0.445, 1.000], loss: 3.221278, mean_absolute_error: 39.725155, mean_q: 53.520512
 1348426/1500000: episode: 2588, duration: 10.240s, episode steps: 413, steps per second: 40, episode reward: 191.716, mean reward: 0.464 [-17.384, 100.000], mean action: 1.305 [0.000, 3.000], mean observation: 0.122 [-0.615, 1.000], loss: 4.586257, mean_absolute_error: 39.809189, mean_q: 53.631931
 1348685/1500000: episode: 2589, duration: 6.556s, episode steps: 259, steps per second: 40, episode reward: 205.254, mean reward: 0.792 [-10.699, 100.000], mean action: 1.201 [0.000, 3.000], mean observation: 0.046 [-0.532, 1.000], loss: 3.718934, mean_absolute_error: 39.772697, mean_q: 53.602428
 1348969/1500000: episode: 2590, duration: 6.991s, episode steps: 284, steps per second: 41, episode reward: 229.812, mean reward: 0.809 [-6.329, 100.000], mean action: 1.377 [0.000, 3.000], mean observation: 0.078 [-0.533, 1.000], loss: 2.897247, mean_absolute_error: 39.943565, mean_q: 53.831799
 1349287/1500000: episode: 2591, duration: 7.790s, episode steps: 318, steps per second: 41, episode reward: 247.634, mean reward: 0.779 [-17.372, 100.000], mean action: 1.340 [0.000, 3.000], mean observation: 0.053 [-0.783, 1.000], loss: 5.895821, mean_absolute_error: 39.821575, mean_q: 53.646988
 1349636/1500000: episode: 2592, duration: 8.884s, episode steps: 349, steps per second: 39, episode reward: 224.084, mean reward: 0.642 [-2.694, 100.000], mean action: 0.923 [0.000, 3.000], mean observation: 0.117 [-0.670, 1.000], loss: 2.835124, mean_absolute_error: 39.889915, mean_q: 53.760033
 1349923/1500000: episode: 2593, duration: 7.224s, episode steps: 287, steps per second: 40, episode reward: 228.107, mean reward: 0.795 [-9.503, 100.000], mean action: 1.387 [0.000, 3.000], mean observation: 0.053 [-0.618, 1.000], loss: 4.332947, mean_absolute_error: 39.908142, mean_q: 53.772038
 1350170/1500000: episode: 2594, duration: 6.061s, episode steps: 247, steps per second: 41, episode reward: 221.446, mean reward: 0.897 [-8.518, 100.000], mean action: 1.478 [0.000, 3.000], mean observation: 0.030 [-0.544, 1.000], loss: 2.033500, mean_absolute_error: 39.681633, mean_q: 53.506664
 1350531/1500000: episode: 2595, duration: 9.129s, episode steps: 361, steps per second: 40, episode reward: 170.565, mean reward: 0.472 [-18.467, 100.000], mean action: 1.006 [0.000, 3.000], mean observation: 0.092 [-0.479, 1.000], loss: 4.369456, mean_absolute_error: 39.782406, mean_q: 53.597523
 1350872/1500000: episode: 2596, duration: 8.927s, episode steps: 341, steps per second: 38, episode reward: 233.156, mean reward: 0.684 [-12.722, 100.000], mean action: 1.114 [0.000, 3.000], mean observation: 0.062 [-0.649, 1.000], loss: 5.663177, mean_absolute_error: 39.661083, mean_q: 53.455238
 1351157/1500000: episode: 2597, duration: 6.861s, episode steps: 285, steps per second: 42, episode reward: 205.521, mean reward: 0.721 [-2.880, 100.000], mean action: 1.032 [0.000, 3.000], mean observation: 0.085 [-0.561, 1.000], loss: 6.373994, mean_absolute_error: 39.621662, mean_q: 53.383389
 1351430/1500000: episode: 2598, duration: 6.645s, episode steps: 273, steps per second: 41, episode reward: 227.247, mean reward: 0.832 [-2.801, 100.000], mean action: 1.374 [0.000, 3.000], mean observation: 0.051 [-0.478, 1.000], loss: 3.284565, mean_absolute_error: 39.851604, mean_q: 53.699589
 1352430/1500000: episode: 2599, duration: 27.537s, episode steps: 1000, steps per second: 36, episode reward: 37.574, mean reward: 0.038 [-22.569, 14.093], mean action: 2.283 [0.000, 3.000], mean observation: 0.175 [-0.468, 1.000], loss: 4.372428, mean_absolute_error: 39.719994, mean_q: 53.489216
 1352811/1500000: episode: 2600, duration: 9.215s, episode steps: 381, steps per second: 41, episode reward: 217.206, mean reward: 0.570 [-8.575, 100.000], mean action: 1.047 [0.000, 3.000], mean observation: 0.083 [-0.531, 1.000], loss: 3.862545, mean_absolute_error: 39.631161, mean_q: 53.429825
 1353068/1500000: episode: 2601, duration: 6.397s, episode steps: 257, steps per second: 40, episode reward: 208.201, mean reward: 0.810 [-9.835, 100.000], mean action: 1.529 [0.000, 3.000], mean observation: 0.061 [-0.549, 1.000], loss: 3.769609, mean_absolute_error: 39.634560, mean_q: 53.387257
 1353358/1500000: episode: 2602, duration: 7.841s, episode steps: 290, steps per second: 37, episode reward: 258.397, mean reward: 0.891 [-9.801, 100.000], mean action: 1.269 [0.000, 3.000], mean observation: 0.041 [-0.701, 1.008], loss: 3.956520, mean_absolute_error: 39.756573, mean_q: 53.557476
 1353675/1500000: episode: 2603, duration: 7.774s, episode steps: 317, steps per second: 41, episode reward: 212.966, mean reward: 0.672 [-17.571, 100.000], mean action: 1.091 [0.000, 3.000], mean observation: 0.074 [-0.472, 1.000], loss: 2.297832, mean_absolute_error: 39.871143, mean_q: 53.726059
 1354001/1500000: episode: 2604, duration: 7.785s, episode steps: 326, steps per second: 42, episode reward: 224.335, mean reward: 0.688 [-13.820, 100.000], mean action: 1.365 [0.000, 3.000], mean observation: 0.090 [-0.512, 1.000], loss: 4.444784, mean_absolute_error: 39.626785, mean_q: 53.380386
 1354281/1500000: episode: 2605, duration: 7.400s, episode steps: 280, steps per second: 38, episode reward: 226.287, mean reward: 0.808 [-21.615, 100.000], mean action: 1.268 [0.000, 3.000], mean observation: 0.072 [-0.498, 1.000], loss: 3.706681, mean_absolute_error: 39.872490, mean_q: 53.717606
 1354645/1500000: episode: 2606, duration: 9.155s, episode steps: 364, steps per second: 40, episode reward: 187.558, mean reward: 0.515 [-17.808, 100.000], mean action: 1.140 [0.000, 3.000], mean observation: 0.116 [-0.544, 1.000], loss: 3.795276, mean_absolute_error: 39.726089, mean_q: 53.517670
 1355041/1500000: episode: 2607, duration: 9.965s, episode steps: 396, steps per second: 40, episode reward: 222.015, mean reward: 0.561 [-11.126, 100.000], mean action: 1.116 [0.000, 3.000], mean observation: 0.089 [-0.493, 1.000], loss: 2.720992, mean_absolute_error: 39.622143, mean_q: 53.395824
 1355318/1500000: episode: 2608, duration: 6.873s, episode steps: 277, steps per second: 40, episode reward: 222.161, mean reward: 0.802 [-8.822, 100.000], mean action: 1.422 [0.000, 3.000], mean observation: 0.047 [-0.522, 1.000], loss: 3.720885, mean_absolute_error: 39.453850, mean_q: 53.185787
 1355429/1500000: episode: 2609, duration: 3.355s, episode steps: 111, steps per second: 33, episode reward: -32.242, mean reward: -0.290 [-100.000, 13.291], mean action: 1.820 [0.000, 3.000], mean observation: -0.083 [-0.865, 1.145], loss: 2.207985, mean_absolute_error: 39.449242, mean_q: 53.156284
 1355709/1500000: episode: 2610, duration: 7.351s, episode steps: 280, steps per second: 38, episode reward: 220.372, mean reward: 0.787 [-9.689, 100.000], mean action: 1.286 [0.000, 3.000], mean observation: 0.058 [-0.514, 1.000], loss: 4.725679, mean_absolute_error: 39.617943, mean_q: 53.376869
 1356092/1500000: episode: 2611, duration: 9.496s, episode steps: 383, steps per second: 40, episode reward: 234.739, mean reward: 0.613 [-18.904, 100.000], mean action: 1.023 [0.000, 3.000], mean observation: 0.108 [-0.595, 1.000], loss: 3.701654, mean_absolute_error: 39.642151, mean_q: 53.412186
 1356361/1500000: episode: 2612, duration: 6.889s, episode steps: 269, steps per second: 39, episode reward: 209.661, mean reward: 0.779 [-19.526, 100.000], mean action: 1.067 [0.000, 3.000], mean observation: 0.085 [-0.571, 1.000], loss: 4.579810, mean_absolute_error: 39.510986, mean_q: 53.222431
 1356655/1500000: episode: 2613, duration: 7.369s, episode steps: 294, steps per second: 40, episode reward: 203.816, mean reward: 0.693 [-18.941, 100.000], mean action: 1.259 [0.000, 3.000], mean observation: 0.075 [-0.526, 1.000], loss: 3.810877, mean_absolute_error: 39.632446, mean_q: 53.384727
 1357006/1500000: episode: 2614, duration: 8.795s, episode steps: 351, steps per second: 40, episode reward: 204.760, mean reward: 0.583 [-20.246, 100.000], mean action: 1.077 [0.000, 3.000], mean observation: 0.106 [-0.497, 1.000], loss: 4.908996, mean_absolute_error: 39.883045, mean_q: 53.691319
 1357495/1500000: episode: 2615, duration: 12.470s, episode steps: 489, steps per second: 39, episode reward: 241.247, mean reward: 0.493 [-9.812, 100.000], mean action: 0.914 [0.000, 3.000], mean observation: 0.141 [-0.674, 1.105], loss: 3.923793, mean_absolute_error: 39.817387, mean_q: 53.661808
 1357777/1500000: episode: 2616, duration: 7.084s, episode steps: 282, steps per second: 40, episode reward: 218.746, mean reward: 0.776 [-10.248, 100.000], mean action: 1.387 [0.000, 3.000], mean observation: 0.046 [-0.546, 1.000], loss: 2.952223, mean_absolute_error: 39.806438, mean_q: 53.634289
 1358006/1500000: episode: 2617, duration: 5.443s, episode steps: 229, steps per second: 42, episode reward: 233.558, mean reward: 1.020 [-13.006, 100.000], mean action: 1.476 [0.000, 3.000], mean observation: 0.021 [-0.663, 1.000], loss: 4.295750, mean_absolute_error: 39.505169, mean_q: 53.236462
 1358399/1500000: episode: 2618, duration: 4.581s, episode steps: 393, steps per second: 86, episode reward: 224.361, mean reward: 0.571 [-18.960, 100.000], mean action: 0.939 [0.000, 3.000], mean observation: 0.124 [-0.591, 1.000], loss: 3.379306, mean_absolute_error: 39.807667, mean_q: 53.635685
 1358707/1500000: episode: 2619, duration: 3.726s, episode steps: 308, steps per second: 83, episode reward: 180.793, mean reward: 0.587 [-11.633, 100.000], mean action: 1.256 [0.000, 3.000], mean observation: 0.080 [-0.478, 1.000], loss: 3.074534, mean_absolute_error: 39.702457, mean_q: 53.484833
 1358982/1500000: episode: 2620, duration: 3.115s, episode steps: 275, steps per second: 88, episode reward: 211.902, mean reward: 0.771 [-2.775, 100.000], mean action: 1.255 [0.000, 3.000], mean observation: 0.074 [-0.537, 1.000], loss: 2.290100, mean_absolute_error: 39.618774, mean_q: 53.375553
 1359320/1500000: episode: 2621, duration: 3.888s, episode steps: 338, steps per second: 87, episode reward: 236.934, mean reward: 0.701 [-17.596, 100.000], mean action: 1.178 [0.000, 3.000], mean observation: 0.071 [-0.519, 1.000], loss: 3.715142, mean_absolute_error: 39.525345, mean_q: 53.246742
 1359789/1500000: episode: 2622, duration: 6.145s, episode steps: 469, steps per second: 76, episode reward: 226.983, mean reward: 0.484 [-17.950, 100.000], mean action: 0.804 [0.000, 3.000], mean observation: 0.139 [-0.533, 1.000], loss: 4.352535, mean_absolute_error: 39.382446, mean_q: 53.049633
 1360104/1500000: episode: 2623, duration: 4.456s, episode steps: 315, steps per second: 71, episode reward: 196.644, mean reward: 0.624 [-11.844, 100.000], mean action: 0.956 [0.000, 3.000], mean observation: 0.101 [-0.531, 1.000], loss: 4.316697, mean_absolute_error: 39.670982, mean_q: 53.421917
 1360436/1500000: episode: 2624, duration: 6.440s, episode steps: 332, steps per second: 52, episode reward: 215.026, mean reward: 0.648 [-9.565, 100.000], mean action: 1.274 [0.000, 3.000], mean observation: 0.069 [-0.662, 1.000], loss: 5.616593, mean_absolute_error: 39.723095, mean_q: 53.462868
 1361219/1500000: episode: 2625, duration: 12.915s, episode steps: 783, steps per second: 61, episode reward: 185.695, mean reward: 0.237 [-19.720, 100.000], mean action: 0.732 [0.000, 3.000], mean observation: 0.182 [-0.518, 1.000], loss: 4.808130, mean_absolute_error: 39.993694, mean_q: 53.880638
 1361651/1500000: episode: 2626, duration: 5.095s, episode steps: 432, steps per second: 85, episode reward: 168.810, mean reward: 0.391 [-18.741, 100.000], mean action: 0.993 [0.000, 3.000], mean observation: 0.099 [-0.500, 1.000], loss: 3.521307, mean_absolute_error: 39.823929, mean_q: 53.697845
 1362131/1500000: episode: 2627, duration: 9.141s, episode steps: 480, steps per second: 53, episode reward: 209.214, mean reward: 0.436 [-10.476, 100.000], mean action: 1.004 [0.000, 3.000], mean observation: 0.138 [-0.483, 1.000], loss: 4.686598, mean_absolute_error: 40.004932, mean_q: 53.900112
 1362428/1500000: episode: 2628, duration: 3.461s, episode steps: 297, steps per second: 86, episode reward: 248.581, mean reward: 0.837 [-19.509, 100.000], mean action: 1.259 [0.000, 3.000], mean observation: 0.096 [-0.687, 1.000], loss: 5.530776, mean_absolute_error: 39.934010, mean_q: 53.823917
 1362844/1500000: episode: 2629, duration: 5.384s, episode steps: 416, steps per second: 77, episode reward: 195.513, mean reward: 0.470 [-18.888, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.123 [-0.541, 1.000], loss: 4.597638, mean_absolute_error: 40.094296, mean_q: 54.029366
 1363123/1500000: episode: 2630, duration: 6.352s, episode steps: 279, steps per second: 44, episode reward: 239.354, mean reward: 0.858 [-8.528, 100.000], mean action: 1.280 [0.000, 3.000], mean observation: 0.048 [-0.577, 1.000], loss: 4.092716, mean_absolute_error: 40.108845, mean_q: 54.110268
 1363456/1500000: episode: 2631, duration: 3.857s, episode steps: 333, steps per second: 86, episode reward: 209.698, mean reward: 0.630 [-2.745, 100.000], mean action: 1.024 [0.000, 3.000], mean observation: 0.107 [-0.441, 1.000], loss: 3.680116, mean_absolute_error: 40.080162, mean_q: 54.058636
 1363939/1500000: episode: 2632, duration: 9.313s, episode steps: 483, steps per second: 52, episode reward: 223.620, mean reward: 0.463 [-17.213, 100.000], mean action: 1.892 [0.000, 3.000], mean observation: 0.144 [-0.828, 1.000], loss: 3.425879, mean_absolute_error: 40.043789, mean_q: 53.998142
 1364238/1500000: episode: 2633, duration: 3.932s, episode steps: 299, steps per second: 76, episode reward: 200.992, mean reward: 0.672 [-8.222, 100.000], mean action: 1.425 [0.000, 3.000], mean observation: 0.075 [-0.548, 1.000], loss: 5.417365, mean_absolute_error: 40.029625, mean_q: 53.940830
 1364736/1500000: episode: 2634, duration: 8.409s, episode steps: 498, steps per second: 59, episode reward: 202.178, mean reward: 0.406 [-17.802, 100.000], mean action: 1.502 [0.000, 3.000], mean observation: 0.119 [-0.551, 1.000], loss: 3.329050, mean_absolute_error: 40.104362, mean_q: 54.035915
 1365063/1500000: episode: 2635, duration: 5.556s, episode steps: 327, steps per second: 59, episode reward: 238.360, mean reward: 0.729 [-9.494, 100.000], mean action: 1.398 [0.000, 3.000], mean observation: 0.094 [-0.550, 1.000], loss: 4.340548, mean_absolute_error: 39.875088, mean_q: 53.731567
 1365293/1500000: episode: 2636, duration: 2.610s, episode steps: 230, steps per second: 88, episode reward: 252.286, mean reward: 1.097 [-3.500, 100.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.014 [-0.742, 1.000], loss: 4.599967, mean_absolute_error: 39.917137, mean_q: 53.792244
 1365610/1500000: episode: 2637, duration: 6.383s, episode steps: 317, steps per second: 50, episode reward: 194.622, mean reward: 0.614 [-19.305, 100.000], mean action: 0.912 [0.000, 3.000], mean observation: 0.067 [-0.507, 1.000], loss: 3.506395, mean_absolute_error: 39.861710, mean_q: 53.714214
 1366029/1500000: episode: 2638, duration: 6.701s, episode steps: 419, steps per second: 63, episode reward: 231.033, mean reward: 0.551 [-17.636, 100.000], mean action: 1.012 [0.000, 3.000], mean observation: 0.136 [-0.467, 1.000], loss: 4.193904, mean_absolute_error: 39.846443, mean_q: 53.712467
 1366322/1500000: episode: 2639, duration: 3.459s, episode steps: 293, steps per second: 85, episode reward: 161.727, mean reward: 0.552 [-13.311, 100.000], mean action: 1.215 [0.000, 3.000], mean observation: 0.051 [-0.739, 1.000], loss: 4.324480, mean_absolute_error: 39.777885, mean_q: 53.647659
 1366594/1500000: episode: 2640, duration: 6.662s, episode steps: 272, steps per second: 41, episode reward: 229.944, mean reward: 0.845 [-2.910, 100.000], mean action: 1.485 [0.000, 3.000], mean observation: 0.036 [-0.514, 1.017], loss: 2.506567, mean_absolute_error: 39.701775, mean_q: 53.541805
 1366856/1500000: episode: 2641, duration: 4.141s, episode steps: 262, steps per second: 63, episode reward: 203.766, mean reward: 0.778 [-2.754, 100.000], mean action: 1.260 [0.000, 3.000], mean observation: 0.068 [-0.452, 1.000], loss: 3.097361, mean_absolute_error: 39.842201, mean_q: 53.685482
 1367199/1500000: episode: 2642, duration: 5.361s, episode steps: 343, steps per second: 64, episode reward: 218.842, mean reward: 0.638 [-19.032, 100.000], mean action: 1.379 [0.000, 3.000], mean observation: 0.078 [-0.597, 1.000], loss: 3.707487, mean_absolute_error: 39.696625, mean_q: 53.483360
 1367434/1500000: episode: 2643, duration: 5.777s, episode steps: 235, steps per second: 41, episode reward: 181.446, mean reward: 0.772 [-10.721, 100.000], mean action: 1.421 [0.000, 3.000], mean observation: 0.043 [-0.497, 1.000], loss: 2.919426, mean_absolute_error: 39.760284, mean_q: 53.609768
 1367777/1500000: episode: 2644, duration: 4.342s, episode steps: 343, steps per second: 79, episode reward: 226.686, mean reward: 0.661 [-19.120, 100.000], mean action: 1.198 [0.000, 3.000], mean observation: 0.078 [-0.691, 1.000], loss: 5.311272, mean_absolute_error: 39.724560, mean_q: 53.545998
 1368144/1500000: episode: 2645, duration: 6.622s, episode steps: 367, steps per second: 55, episode reward: 217.464, mean reward: 0.593 [-9.672, 100.000], mean action: 1.074 [0.000, 3.000], mean observation: 0.073 [-0.526, 1.000], loss: 5.521944, mean_absolute_error: 39.716713, mean_q: 53.533703
 1368692/1500000: episode: 2646, duration: 8.797s, episode steps: 548, steps per second: 62, episode reward: 237.646, mean reward: 0.434 [-19.916, 100.000], mean action: 0.836 [0.000, 3.000], mean observation: 0.163 [-0.697, 1.000], loss: 2.882723, mean_absolute_error: 39.728168, mean_q: 53.560684
 1369129/1500000: episode: 2647, duration: 9.941s, episode steps: 437, steps per second: 44, episode reward: 215.123, mean reward: 0.492 [-18.216, 100.000], mean action: 1.208 [0.000, 3.000], mean observation: 0.131 [-0.442, 1.000], loss: 2.666953, mean_absolute_error: 39.594952, mean_q: 53.376743
 1369507/1500000: episode: 2648, duration: 5.032s, episode steps: 378, steps per second: 75, episode reward: 238.027, mean reward: 0.630 [-17.915, 100.000], mean action: 1.304 [0.000, 3.000], mean observation: 0.117 [-0.556, 1.000], loss: 3.809007, mean_absolute_error: 39.589130, mean_q: 53.339264
 1369836/1500000: episode: 2649, duration: 6.167s, episode steps: 329, steps per second: 53, episode reward: 211.150, mean reward: 0.642 [-12.140, 100.000], mean action: 1.207 [0.000, 3.000], mean observation: 0.107 [-0.542, 1.000], loss: 3.491814, mean_absolute_error: 39.748821, mean_q: 53.524044
 1370342/1500000: episode: 2650, duration: 8.646s, episode steps: 506, steps per second: 59, episode reward: 225.534, mean reward: 0.446 [-18.200, 100.000], mean action: 0.862 [0.000, 3.000], mean observation: 0.130 [-0.568, 1.000], loss: 4.660971, mean_absolute_error: 39.629192, mean_q: 53.378437
 1370565/1500000: episode: 2651, duration: 3.095s, episode steps: 223, steps per second: 72, episode reward: 240.968, mean reward: 1.081 [-18.559, 100.000], mean action: 1.269 [0.000, 3.000], mean observation: 0.046 [-0.817, 1.000], loss: 4.303485, mean_absolute_error: 39.767368, mean_q: 53.592026
 1370886/1500000: episode: 2652, duration: 8.031s, episode steps: 321, steps per second: 40, episode reward: 219.109, mean reward: 0.683 [-17.345, 100.000], mean action: 1.059 [0.000, 3.000], mean observation: 0.091 [-0.475, 1.000], loss: 3.621604, mean_absolute_error: 39.616013, mean_q: 53.378006
 1371159/1500000: episode: 2653, duration: 3.629s, episode steps: 273, steps per second: 75, episode reward: 186.906, mean reward: 0.685 [-9.719, 100.000], mean action: 1.048 [0.000, 3.000], mean observation: 0.071 [-0.522, 1.000], loss: 3.061095, mean_absolute_error: 39.696007, mean_q: 53.499386
 1371621/1500000: episode: 2654, duration: 8.594s, episode steps: 462, steps per second: 54, episode reward: 209.864, mean reward: 0.454 [-19.177, 100.000], mean action: 0.903 [0.000, 3.000], mean observation: 0.130 [-0.536, 1.000], loss: 4.811334, mean_absolute_error: 39.791775, mean_q: 53.590973
 1371973/1500000: episode: 2655, duration: 6.092s, episode steps: 352, steps per second: 58, episode reward: 225.926, mean reward: 0.642 [-17.728, 100.000], mean action: 0.926 [0.000, 3.000], mean observation: 0.118 [-0.718, 1.000], loss: 5.062514, mean_absolute_error: 39.656521, mean_q: 53.437634
 1372319/1500000: episode: 2656, duration: 4.418s, episode steps: 346, steps per second: 78, episode reward: 234.550, mean reward: 0.678 [-10.801, 100.000], mean action: 1.078 [0.000, 3.000], mean observation: 0.085 [-0.603, 1.000], loss: 3.033829, mean_absolute_error: 39.675735, mean_q: 53.483856
 1372699/1500000: episode: 2657, duration: 9.484s, episode steps: 380, steps per second: 40, episode reward: 223.680, mean reward: 0.589 [-18.514, 100.000], mean action: 0.916 [0.000, 3.000], mean observation: 0.112 [-0.650, 1.000], loss: 4.735941, mean_absolute_error: 39.699665, mean_q: 53.462429
 1373016/1500000: episode: 2658, duration: 3.835s, episode steps: 317, steps per second: 83, episode reward: 205.744, mean reward: 0.649 [-17.430, 100.000], mean action: 1.256 [0.000, 3.000], mean observation: 0.084 [-0.606, 1.000], loss: 2.735696, mean_absolute_error: 39.763050, mean_q: 53.586048
 1373416/1500000: episode: 2659, duration: 8.427s, episode steps: 400, steps per second: 47, episode reward: 213.196, mean reward: 0.533 [-9.144, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.135 [-0.540, 1.000], loss: 3.261205, mean_absolute_error: 39.835327, mean_q: 53.672382
 1373745/1500000: episode: 2660, duration: 5.553s, episode steps: 329, steps per second: 59, episode reward: 227.370, mean reward: 0.691 [-10.716, 100.000], mean action: 1.146 [0.000, 3.000], mean observation: 0.108 [-0.609, 1.000], loss: 5.352487, mean_absolute_error: 39.837029, mean_q: 53.638115
 1374088/1500000: episode: 2661, duration: 4.982s, episode steps: 343, steps per second: 69, episode reward: 226.945, mean reward: 0.662 [-19.709, 100.000], mean action: 1.580 [0.000, 3.000], mean observation: 0.095 [-0.464, 1.000], loss: 4.174747, mean_absolute_error: 39.883530, mean_q: 53.715343
 1374521/1500000: episode: 2662, duration: 9.707s, episode steps: 433, steps per second: 45, episode reward: 208.629, mean reward: 0.482 [-20.912, 100.000], mean action: 1.039 [0.000, 3.000], mean observation: 0.075 [-0.538, 1.000], loss: 3.030839, mean_absolute_error: 39.868259, mean_q: 53.711632
 1374818/1500000: episode: 2663, duration: 3.412s, episode steps: 297, steps per second: 87, episode reward: 198.169, mean reward: 0.667 [-9.366, 100.000], mean action: 0.953 [0.000, 3.000], mean observation: 0.086 [-1.113, 1.000], loss: 3.212199, mean_absolute_error: 39.764778, mean_q: 53.580669
 1375273/1500000: episode: 2664, duration: 10.599s, episode steps: 455, steps per second: 43, episode reward: 196.043, mean reward: 0.431 [-19.766, 100.000], mean action: 0.982 [0.000, 3.000], mean observation: 0.136 [-0.555, 1.000], loss: 2.401554, mean_absolute_error: 39.662289, mean_q: 53.461025
 1375581/1500000: episode: 2665, duration: 3.966s, episode steps: 308, steps per second: 78, episode reward: 229.596, mean reward: 0.745 [-3.077, 100.000], mean action: 1.166 [0.000, 3.000], mean observation: 0.105 [-0.712, 1.000], loss: 3.545964, mean_absolute_error: 39.746227, mean_q: 53.548023
 1375872/1500000: episode: 2666, duration: 4.423s, episode steps: 291, steps per second: 66, episode reward: 248.007, mean reward: 0.852 [-11.406, 100.000], mean action: 1.323 [0.000, 3.000], mean observation: 0.059 [-0.798, 1.000], loss: 3.419384, mean_absolute_error: 39.657181, mean_q: 53.453072
 1376141/1500000: episode: 2667, duration: 6.723s, episode steps: 269, steps per second: 40, episode reward: 179.114, mean reward: 0.666 [-2.816, 100.000], mean action: 1.379 [0.000, 3.000], mean observation: 0.020 [-0.454, 1.000], loss: 5.111227, mean_absolute_error: 39.628899, mean_q: 53.386959
 1376538/1500000: episode: 2668, duration: 5.484s, episode steps: 397, steps per second: 72, episode reward: 218.988, mean reward: 0.552 [-8.328, 100.000], mean action: 1.126 [0.000, 3.000], mean observation: 0.107 [-0.451, 1.018], loss: 2.996293, mean_absolute_error: 39.466305, mean_q: 53.195282
 1376921/1500000: episode: 2669, duration: 7.732s, episode steps: 383, steps per second: 50, episode reward: 213.484, mean reward: 0.557 [-9.661, 100.000], mean action: 1.243 [0.000, 3.000], mean observation: 0.115 [-0.610, 1.000], loss: 5.021137, mean_absolute_error: 39.612663, mean_q: 53.384159
 1377219/1500000: episode: 2670, duration: 5.910s, episode steps: 298, steps per second: 50, episode reward: 236.548, mean reward: 0.794 [-3.055, 100.000], mean action: 1.523 [0.000, 3.000], mean observation: 0.083 [-0.672, 1.000], loss: 4.583550, mean_absolute_error: 39.439823, mean_q: 53.155014
 1377536/1500000: episode: 2671, duration: 3.645s, episode steps: 317, steps per second: 87, episode reward: 204.248, mean reward: 0.644 [-2.980, 100.000], mean action: 1.278 [0.000, 3.000], mean observation: 0.102 [-0.545, 1.000], loss: 5.330429, mean_absolute_error: 39.615505, mean_q: 53.366325
 1377775/1500000: episode: 2672, duration: 5.110s, episode steps: 239, steps per second: 47, episode reward: 222.544, mean reward: 0.931 [-3.940, 100.000], mean action: 1.603 [0.000, 3.000], mean observation: 0.035 [-0.494, 1.000], loss: 4.708866, mean_absolute_error: 39.737000, mean_q: 53.491207
 1378078/1500000: episode: 2673, duration: 7.143s, episode steps: 303, steps per second: 42, episode reward: 220.683, mean reward: 0.728 [-17.542, 100.000], mean action: 1.459 [0.000, 3.000], mean observation: 0.063 [-0.563, 1.000], loss: 2.975320, mean_absolute_error: 39.546017, mean_q: 53.277710
 1378375/1500000: episode: 2674, duration: 3.315s, episode steps: 297, steps per second: 90, episode reward: 210.796, mean reward: 0.710 [-3.105, 100.000], mean action: 1.138 [0.000, 3.000], mean observation: 0.080 [-0.538, 1.000], loss: 4.387081, mean_absolute_error: 39.646996, mean_q: 53.390087
 1378687/1500000: episode: 2675, duration: 5.969s, episode steps: 312, steps per second: 52, episode reward: 224.770, mean reward: 0.720 [-19.569, 100.000], mean action: 1.263 [0.000, 3.000], mean observation: 0.050 [-0.623, 1.000], loss: 4.820782, mean_absolute_error: 39.424500, mean_q: 53.101482
 1378992/1500000: episode: 2676, duration: 7.104s, episode steps: 305, steps per second: 43, episode reward: 204.814, mean reward: 0.672 [-9.891, 100.000], mean action: 1.282 [0.000, 3.000], mean observation: 0.078 [-0.495, 1.000], loss: 4.047505, mean_absolute_error: 39.657154, mean_q: 53.425354
 1379313/1500000: episode: 2677, duration: 3.797s, episode steps: 321, steps per second: 85, episode reward: 255.173, mean reward: 0.795 [-19.462, 100.000], mean action: 1.417 [0.000, 3.000], mean observation: 0.105 [-0.760, 1.000], loss: 4.006244, mean_absolute_error: 39.671783, mean_q: 53.413551
 1379556/1500000: episode: 2678, duration: 5.424s, episode steps: 243, steps per second: 45, episode reward: 237.678, mean reward: 0.978 [-11.080, 100.000], mean action: 1.350 [0.000, 3.000], mean observation: 0.033 [-0.677, 1.000], loss: 3.545542, mean_absolute_error: 39.527126, mean_q: 53.250042
 1379984/1500000: episode: 2679, duration: 9.187s, episode steps: 428, steps per second: 47, episode reward: 238.124, mean reward: 0.556 [-17.529, 100.000], mean action: 1.061 [0.000, 3.000], mean observation: 0.135 [-0.568, 1.000], loss: 5.067153, mean_absolute_error: 39.751770, mean_q: 53.531227
 1380266/1500000: episode: 2680, duration: 3.235s, episode steps: 282, steps per second: 87, episode reward: 202.745, mean reward: 0.719 [-2.783, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.076 [-0.484, 1.000], loss: 4.843048, mean_absolute_error: 39.663078, mean_q: 53.414551
 1380616/1500000: episode: 2681, duration: 8.435s, episode steps: 350, steps per second: 41, episode reward: 246.565, mean reward: 0.704 [-10.222, 100.000], mean action: 1.257 [0.000, 3.000], mean observation: 0.104 [-0.638, 1.008], loss: 3.665516, mean_absolute_error: 39.568104, mean_q: 53.329514
 1381009/1500000: episode: 2682, duration: 6.396s, episode steps: 393, steps per second: 61, episode reward: 251.289, mean reward: 0.639 [-18.040, 100.000], mean action: 1.219 [0.000, 3.000], mean observation: 0.138 [-0.556, 1.000], loss: 3.696834, mean_absolute_error: 39.769634, mean_q: 53.559696
 1381290/1500000: episode: 2683, duration: 4.317s, episode steps: 281, steps per second: 65, episode reward: 211.345, mean reward: 0.752 [-9.898, 100.000], mean action: 1.263 [0.000, 3.000], mean observation: 0.072 [-0.541, 1.000], loss: 4.830525, mean_absolute_error: 39.582775, mean_q: 53.304096
 1381582/1500000: episode: 2684, duration: 7.453s, episode steps: 292, steps per second: 39, episode reward: 199.698, mean reward: 0.684 [-2.872, 100.000], mean action: 1.048 [0.000, 3.000], mean observation: 0.077 [-0.521, 1.000], loss: 4.245566, mean_absolute_error: 39.611843, mean_q: 53.366871
 1381913/1500000: episode: 2685, duration: 5.014s, episode steps: 331, steps per second: 66, episode reward: 212.087, mean reward: 0.641 [-9.332, 100.000], mean action: 1.184 [0.000, 3.000], mean observation: 0.091 [-0.629, 1.000], loss: 3.835088, mean_absolute_error: 39.492039, mean_q: 53.227650
 1382159/1500000: episode: 2686, duration: 2.983s, episode steps: 246, steps per second: 82, episode reward: 189.376, mean reward: 0.770 [-11.035, 100.000], mean action: 1.557 [0.000, 3.000], mean observation: 0.043 [-0.478, 1.000], loss: 4.062182, mean_absolute_error: 39.461071, mean_q: 53.151344
 1382536/1500000: episode: 2687, duration: 9.611s, episode steps: 377, steps per second: 39, episode reward: 207.104, mean reward: 0.549 [-3.156, 100.000], mean action: 1.284 [0.000, 3.000], mean observation: 0.096 [-0.456, 1.000], loss: 3.637403, mean_absolute_error: 39.690918, mean_q: 53.461517
 1382781/1500000: episode: 2688, duration: 3.709s, episode steps: 245, steps per second: 66, episode reward: 251.987, mean reward: 1.029 [-3.628, 100.000], mean action: 1.420 [0.000, 3.000], mean observation: 0.037 [-0.813, 1.000], loss: 4.232039, mean_absolute_error: 39.724541, mean_q: 53.499268
 1383250/1500000: episode: 2689, duration: 8.631s, episode steps: 469, steps per second: 54, episode reward: 214.843, mean reward: 0.458 [-20.127, 100.000], mean action: 1.100 [0.000, 3.000], mean observation: 0.125 [-0.455, 1.000], loss: 4.503493, mean_absolute_error: 39.721722, mean_q: 53.500076
 1383633/1500000: episode: 2690, duration: 7.984s, episode steps: 383, steps per second: 48, episode reward: 251.165, mean reward: 0.656 [-17.630, 100.000], mean action: 1.180 [0.000, 3.000], mean observation: 0.078 [-0.702, 1.031], loss: 3.236577, mean_absolute_error: 39.732132, mean_q: 53.522827
 1383931/1500000: episode: 2691, duration: 3.453s, episode steps: 298, steps per second: 86, episode reward: 188.242, mean reward: 0.632 [-10.861, 100.000], mean action: 1.141 [0.000, 3.000], mean observation: 0.048 [-0.524, 1.000], loss: 6.355523, mean_absolute_error: 39.673855, mean_q: 53.419941
 1384425/1500000: episode: 2692, duration: 12.571s, episode steps: 494, steps per second: 39, episode reward: 241.707, mean reward: 0.489 [-17.669, 100.000], mean action: 0.832 [0.000, 3.000], mean observation: 0.139 [-0.670, 1.000], loss: 4.587496, mean_absolute_error: 39.879871, mean_q: 53.732521
 1384703/1500000: episode: 2693, duration: 3.205s, episode steps: 278, steps per second: 87, episode reward: 176.280, mean reward: 0.634 [-8.954, 100.000], mean action: 1.209 [0.000, 3.000], mean observation: 0.052 [-0.513, 1.000], loss: 2.686767, mean_absolute_error: 39.778179, mean_q: 53.630611
 1385135/1500000: episode: 2694, duration: 9.423s, episode steps: 432, steps per second: 46, episode reward: 183.975, mean reward: 0.426 [-9.344, 100.000], mean action: 1.113 [0.000, 3.000], mean observation: 0.069 [-0.620, 1.000], loss: 5.352264, mean_absolute_error: 39.976475, mean_q: 53.856174
 1385445/1500000: episode: 2695, duration: 6.133s, episode steps: 310, steps per second: 51, episode reward: 243.083, mean reward: 0.784 [-3.218, 100.000], mean action: 1.235 [0.000, 3.000], mean observation: 0.055 [-0.616, 1.000], loss: 3.201114, mean_absolute_error: 39.888191, mean_q: 53.780186
 1385769/1500000: episode: 2696, duration: 5.095s, episode steps: 324, steps per second: 64, episode reward: 207.590, mean reward: 0.641 [-2.611, 100.000], mean action: 1.099 [0.000, 3.000], mean observation: 0.084 [-0.511, 1.000], loss: 5.087247, mean_absolute_error: 40.070484, mean_q: 53.956825
 1386038/1500000: episode: 2697, duration: 6.894s, episode steps: 269, steps per second: 39, episode reward: 239.239, mean reward: 0.889 [-9.658, 100.000], mean action: 1.383 [0.000, 3.000], mean observation: 0.089 [-0.719, 1.000], loss: 4.519353, mean_absolute_error: 40.178974, mean_q: 54.142818
 1386321/1500000: episode: 2698, duration: 4.658s, episode steps: 283, steps per second: 61, episode reward: 235.546, mean reward: 0.832 [-9.350, 100.000], mean action: 1.042 [0.000, 3.000], mean observation: 0.074 [-1.557, 1.000], loss: 3.723223, mean_absolute_error: 40.031357, mean_q: 53.952663
 1386744/1500000: episode: 2699, duration: 6.958s, episode steps: 423, steps per second: 61, episode reward: 212.164, mean reward: 0.502 [-11.611, 100.000], mean action: 1.461 [0.000, 3.000], mean observation: 0.113 [-0.736, 1.000], loss: 3.327134, mean_absolute_error: 40.209717, mean_q: 54.199467
 1387266/1500000: episode: 2700, duration: 10.866s, episode steps: 522, steps per second: 48, episode reward: 249.344, mean reward: 0.478 [-22.979, 100.000], mean action: 0.807 [0.000, 3.000], mean observation: 0.137 [-0.747, 1.000], loss: 4.059295, mean_absolute_error: 40.285751, mean_q: 54.274635
 1387620/1500000: episode: 2701, duration: 5.660s, episode steps: 354, steps per second: 63, episode reward: 222.336, mean reward: 0.628 [-9.404, 100.000], mean action: 1.071 [0.000, 3.000], mean observation: 0.051 [-0.666, 1.000], loss: 3.240899, mean_absolute_error: 40.433975, mean_q: 54.460640
 1387923/1500000: episode: 2702, duration: 8.271s, episode steps: 303, steps per second: 37, episode reward: 220.478, mean reward: 0.728 [-3.193, 100.000], mean action: 1.261 [0.000, 3.000], mean observation: 0.076 [-0.564, 1.021], loss: 4.026529, mean_absolute_error: 40.518204, mean_q: 54.547634
 1388460/1500000: episode: 2703, duration: 8.056s, episode steps: 537, steps per second: 67, episode reward: 193.324, mean reward: 0.360 [-18.233, 100.000], mean action: 0.737 [0.000, 3.000], mean observation: 0.158 [-0.584, 1.000], loss: 4.879594, mean_absolute_error: 40.324306, mean_q: 54.317955
 1389063/1500000: episode: 2704, duration: 12.955s, episode steps: 603, steps per second: 47, episode reward: 237.426, mean reward: 0.394 [-18.222, 100.000], mean action: 0.701 [0.000, 3.000], mean observation: 0.148 [-0.624, 1.000], loss: 3.707651, mean_absolute_error: 40.112347, mean_q: 54.057594
 1389369/1500000: episode: 2705, duration: 4.294s, episode steps: 306, steps per second: 71, episode reward: 200.445, mean reward: 0.655 [-10.090, 100.000], mean action: 1.458 [0.000, 3.000], mean observation: 0.081 [-0.534, 1.000], loss: 4.546168, mean_absolute_error: 40.129768, mean_q: 54.059559
 1389754/1500000: episode: 2706, duration: 9.817s, episode steps: 385, steps per second: 39, episode reward: 227.057, mean reward: 0.590 [-17.700, 100.000], mean action: 0.919 [0.000, 3.000], mean observation: 0.131 [-0.679, 1.000], loss: 3.524404, mean_absolute_error: 40.222446, mean_q: 54.178242
 1390097/1500000: episode: 2707, duration: 4.844s, episode steps: 343, steps per second: 71, episode reward: 233.631, mean reward: 0.681 [-9.544, 100.000], mean action: 1.542 [0.000, 3.000], mean observation: 0.098 [-0.487, 1.000], loss: 4.626219, mean_absolute_error: 40.150517, mean_q: 54.059437
 1390457/1500000: episode: 2708, duration: 7.182s, episode steps: 360, steps per second: 50, episode reward: 178.134, mean reward: 0.495 [-17.589, 100.000], mean action: 1.136 [0.000, 3.000], mean observation: 0.110 [-0.512, 1.000], loss: 4.085094, mean_absolute_error: 40.070488, mean_q: 53.962234
 1390683/1500000: episode: 2709, duration: 5.650s, episode steps: 226, steps per second: 40, episode reward: 177.749, mean reward: 0.787 [-10.929, 100.000], mean action: 1.106 [0.000, 3.000], mean observation: 0.044 [-0.465, 1.000], loss: 3.682945, mean_absolute_error: 40.229805, mean_q: 54.157478
 1390990/1500000: episode: 2710, duration: 4.556s, episode steps: 307, steps per second: 67, episode reward: 214.962, mean reward: 0.700 [-2.977, 100.000], mean action: 1.140 [0.000, 3.000], mean observation: 0.098 [-0.407, 1.000], loss: 3.649185, mean_absolute_error: 40.163033, mean_q: 54.073429
 1391273/1500000: episode: 2711, duration: 4.838s, episode steps: 283, steps per second: 59, episode reward: 254.459, mean reward: 0.899 [-18.058, 100.000], mean action: 1.085 [0.000, 3.000], mean observation: 0.151 [-0.589, 1.315], loss: 4.975204, mean_absolute_error: 40.295162, mean_q: 54.231998
 1391481/1500000: episode: 2712, duration: 5.175s, episode steps: 208, steps per second: 40, episode reward: 248.594, mean reward: 1.195 [-10.061, 100.000], mean action: 1.433 [0.000, 3.000], mean observation: 0.011 [-0.819, 1.000], loss: 3.218724, mean_absolute_error: 40.331741, mean_q: 54.301682
 1391728/1500000: episode: 2713, duration: 5.371s, episode steps: 247, steps per second: 46, episode reward: 231.228, mean reward: 0.936 [-3.009, 100.000], mean action: 1.316 [0.000, 3.000], mean observation: 0.063 [-0.565, 1.000], loss: 3.600584, mean_absolute_error: 40.402557, mean_q: 54.395607
 1391973/1500000: episode: 2714, duration: 2.762s, episode steps: 245, steps per second: 89, episode reward: 208.578, mean reward: 0.851 [-9.475, 100.000], mean action: 1.424 [0.000, 3.000], mean observation: 0.051 [-0.479, 1.000], loss: 3.711056, mean_absolute_error: 40.233719, mean_q: 54.171597
 1392308/1500000: episode: 2715, duration: 6.635s, episode steps: 335, steps per second: 50, episode reward: 211.967, mean reward: 0.633 [-11.227, 100.000], mean action: 1.134 [0.000, 3.000], mean observation: 0.106 [-0.548, 1.000], loss: 3.295182, mean_absolute_error: 40.213955, mean_q: 54.163971
 1392568/1500000: episode: 2716, duration: 6.423s, episode steps: 260, steps per second: 40, episode reward: 250.876, mean reward: 0.965 [-11.179, 100.000], mean action: 1.404 [0.000, 3.000], mean observation: 0.076 [-0.575, 1.000], loss: 3.605008, mean_absolute_error: 40.074074, mean_q: 54.006790
 1392920/1500000: episode: 2717, duration: 7.183s, episode steps: 352, steps per second: 49, episode reward: 210.776, mean reward: 0.599 [-10.084, 100.000], mean action: 0.986 [0.000, 3.000], mean observation: 0.105 [-0.490, 1.000], loss: 3.875700, mean_absolute_error: 40.367023, mean_q: 54.370785
 1393268/1500000: episode: 2718, duration: 9.821s, episode steps: 348, steps per second: 35, episode reward: 224.508, mean reward: 0.645 [-8.693, 100.000], mean action: 1.316 [0.000, 3.000], mean observation: 0.095 [-0.569, 1.000], loss: 4.083066, mean_absolute_error: 39.999874, mean_q: 53.886909
 1393533/1500000: episode: 2719, duration: 6.860s, episode steps: 265, steps per second: 39, episode reward: 245.003, mean reward: 0.925 [-8.979, 100.000], mean action: 1.291 [0.000, 3.000], mean observation: 0.049 [-0.750, 1.000], loss: 4.607543, mean_absolute_error: 40.260166, mean_q: 54.260914
 1393849/1500000: episode: 2720, duration: 7.924s, episode steps: 316, steps per second: 40, episode reward: 206.333, mean reward: 0.653 [-17.767, 100.000], mean action: 1.009 [0.000, 3.000], mean observation: 0.098 [-0.585, 1.000], loss: 3.475331, mean_absolute_error: 40.231777, mean_q: 54.228786
 1394113/1500000: episode: 2721, duration: 6.640s, episode steps: 264, steps per second: 40, episode reward: 232.154, mean reward: 0.879 [-3.189, 100.000], mean action: 1.208 [0.000, 3.000], mean observation: 0.033 [-0.599, 1.000], loss: 4.377794, mean_absolute_error: 40.242249, mean_q: 54.275574
 1394391/1500000: episode: 2722, duration: 6.883s, episode steps: 278, steps per second: 40, episode reward: 239.044, mean reward: 0.860 [-3.578, 100.000], mean action: 1.367 [0.000, 3.000], mean observation: 0.025 [-0.732, 1.000], loss: 3.320751, mean_absolute_error: 39.973137, mean_q: 53.893009
 1394763/1500000: episode: 2723, duration: 9.217s, episode steps: 372, steps per second: 40, episode reward: 214.166, mean reward: 0.576 [-20.263, 100.000], mean action: 1.019 [0.000, 3.000], mean observation: 0.097 [-0.565, 1.000], loss: 5.138527, mean_absolute_error: 40.067085, mean_q: 54.026424
 1395087/1500000: episode: 2724, duration: 8.207s, episode steps: 324, steps per second: 39, episode reward: 200.843, mean reward: 0.620 [-19.202, 100.000], mean action: 1.068 [0.000, 3.000], mean observation: 0.092 [-0.489, 1.000], loss: 2.634994, mean_absolute_error: 40.137058, mean_q: 54.138058
 1395355/1500000: episode: 2725, duration: 7.037s, episode steps: 268, steps per second: 38, episode reward: 191.529, mean reward: 0.715 [-9.403, 100.000], mean action: 1.187 [0.000, 3.000], mean observation: 0.020 [-0.612, 1.000], loss: 4.751483, mean_absolute_error: 40.084831, mean_q: 54.042679
 1395646/1500000: episode: 2726, duration: 7.164s, episode steps: 291, steps per second: 41, episode reward: 251.998, mean reward: 0.866 [-11.567, 100.000], mean action: 1.282 [0.000, 3.000], mean observation: 0.063 [-0.763, 1.000], loss: 3.495804, mean_absolute_error: 40.071651, mean_q: 54.006863
 1395962/1500000: episode: 2727, duration: 8.121s, episode steps: 316, steps per second: 39, episode reward: 205.138, mean reward: 0.649 [-3.071, 100.000], mean action: 1.215 [0.000, 3.000], mean observation: 0.092 [-0.459, 1.000], loss: 3.760156, mean_absolute_error: 40.088909, mean_q: 54.033600
 1396490/1500000: episode: 2728, duration: 14.317s, episode steps: 528, steps per second: 37, episode reward: 221.916, mean reward: 0.420 [-17.967, 100.000], mean action: 1.062 [0.000, 3.000], mean observation: 0.145 [-0.449, 1.000], loss: 3.642252, mean_absolute_error: 40.091766, mean_q: 54.021927
 1396922/1500000: episode: 2729, duration: 11.108s, episode steps: 432, steps per second: 39, episode reward: 208.143, mean reward: 0.482 [-9.613, 100.000], mean action: 1.197 [0.000, 3.000], mean observation: 0.067 [-0.547, 1.000], loss: 4.087566, mean_absolute_error: 40.256004, mean_q: 54.231956
 1397177/1500000: episode: 2730, duration: 6.131s, episode steps: 255, steps per second: 42, episode reward: 183.983, mean reward: 0.722 [-13.515, 100.000], mean action: 1.373 [0.000, 3.000], mean observation: 0.049 [-0.525, 1.000], loss: 4.487832, mean_absolute_error: 40.087921, mean_q: 54.019547
 1397438/1500000: episode: 2731, duration: 6.515s, episode steps: 261, steps per second: 40, episode reward: 170.907, mean reward: 0.655 [-3.329, 100.000], mean action: 1.908 [0.000, 3.000], mean observation: 0.065 [-0.543, 1.000], loss: 3.663595, mean_absolute_error: 40.296688, mean_q: 54.306828
 1397850/1500000: episode: 2732, duration: 10.902s, episode steps: 412, steps per second: 38, episode reward: 214.913, mean reward: 0.522 [-13.574, 100.000], mean action: 0.973 [0.000, 3.000], mean observation: 0.098 [-0.478, 1.003], loss: 4.404366, mean_absolute_error: 40.196033, mean_q: 54.171207
 1398360/1500000: episode: 2733, duration: 13.101s, episode steps: 510, steps per second: 39, episode reward: 215.990, mean reward: 0.424 [-18.027, 100.000], mean action: 1.149 [0.000, 3.000], mean observation: 0.132 [-0.672, 1.004], loss: 5.636322, mean_absolute_error: 40.302944, mean_q: 54.302811
 1398823/1500000: episode: 2734, duration: 12.225s, episode steps: 463, steps per second: 38, episode reward: 247.002, mean reward: 0.533 [-17.960, 100.000], mean action: 1.052 [0.000, 3.000], mean observation: 0.139 [-0.678, 1.017], loss: 4.114551, mean_absolute_error: 40.270073, mean_q: 54.278111
 1399181/1500000: episode: 2735, duration: 10.325s, episode steps: 358, steps per second: 35, episode reward: 201.712, mean reward: 0.563 [-17.505, 100.000], mean action: 1.182 [0.000, 3.000], mean observation: 0.119 [-0.502, 1.000], loss: 4.785384, mean_absolute_error: 40.305271, mean_q: 54.285221
 1399532/1500000: episode: 2736, duration: 9.068s, episode steps: 351, steps per second: 39, episode reward: 168.211, mean reward: 0.479 [-13.755, 100.000], mean action: 1.473 [0.000, 3.000], mean observation: 0.060 [-0.851, 1.000], loss: 2.499992, mean_absolute_error: 40.386314, mean_q: 54.415321
 1399987/1500000: episode: 2737, duration: 11.558s, episode steps: 455, steps per second: 39, episode reward: 240.544, mean reward: 0.529 [-19.471, 100.000], mean action: 0.820 [0.000, 3.000], mean observation: 0.148 [-0.698, 1.000], loss: 3.599361, mean_absolute_error: 40.377148, mean_q: 54.387405
 1400194/1500000: episode: 2738, duration: 5.249s, episode steps: 207, steps per second: 39, episode reward: 254.911, mean reward: 1.231 [-3.573, 100.000], mean action: 1.498 [0.000, 3.000], mean observation: 0.004 [-0.762, 1.000], loss: 3.476131, mean_absolute_error: 40.401268, mean_q: 54.434715
 1400482/1500000: episode: 2739, duration: 7.351s, episode steps: 288, steps per second: 39, episode reward: 228.592, mean reward: 0.794 [-11.975, 100.000], mean action: 1.330 [0.000, 3.000], mean observation: 0.054 [-0.534, 1.000], loss: 3.283270, mean_absolute_error: 40.466022, mean_q: 54.540276
 1400727/1500000: episode: 2740, duration: 6.206s, episode steps: 245, steps per second: 39, episode reward: 237.421, mean reward: 0.969 [-8.895, 100.000], mean action: 1.298 [0.000, 3.000], mean observation: 0.043 [-0.738, 1.000], loss: 3.546167, mean_absolute_error: 40.271461, mean_q: 54.254066
 1401085/1500000: episode: 2741, duration: 9.212s, episode steps: 358, steps per second: 39, episode reward: 220.292, mean reward: 0.615 [-18.271, 100.000], mean action: 1.399 [0.000, 3.000], mean observation: 0.070 [-0.498, 1.000], loss: 2.561871, mean_absolute_error: 40.397434, mean_q: 54.428345
 1401417/1500000: episode: 2742, duration: 8.103s, episode steps: 332, steps per second: 41, episode reward: 213.992, mean reward: 0.645 [-2.805, 100.000], mean action: 1.169 [0.000, 3.000], mean observation: 0.086 [-0.448, 1.000], loss: 4.901433, mean_absolute_error: 40.373165, mean_q: 54.386982
 1401683/1500000: episode: 2743, duration: 6.955s, episode steps: 266, steps per second: 38, episode reward: 244.405, mean reward: 0.919 [-9.180, 100.000], mean action: 1.346 [0.000, 3.000], mean observation: 0.035 [-0.630, 1.000], loss: 2.396136, mean_absolute_error: 40.338947, mean_q: 54.375137
 1402027/1500000: episode: 2744, duration: 8.633s, episode steps: 344, steps per second: 40, episode reward: 210.105, mean reward: 0.611 [-17.531, 100.000], mean action: 1.174 [0.000, 3.000], mean observation: 0.079 [-0.508, 1.000], loss: 3.204856, mean_absolute_error: 40.407864, mean_q: 54.430382
 1402363/1500000: episode: 2745, duration: 8.480s, episode steps: 336, steps per second: 40, episode reward: 211.548, mean reward: 0.630 [-9.577, 100.000], mean action: 1.432 [0.000, 3.000], mean observation: 0.089 [-0.464, 1.010], loss: 4.729158, mean_absolute_error: 40.259251, mean_q: 54.237419
 1402682/1500000: episode: 2746, duration: 7.515s, episode steps: 319, steps per second: 42, episode reward: 224.913, mean reward: 0.705 [-9.529, 100.000], mean action: 1.266 [0.000, 3.000], mean observation: 0.085 [-0.581, 1.001], loss: 6.662912, mean_absolute_error: 40.426975, mean_q: 54.423016
 1403053/1500000: episode: 2747, duration: 9.691s, episode steps: 371, steps per second: 38, episode reward: 195.643, mean reward: 0.527 [-17.442, 100.000], mean action: 1.121 [0.000, 3.000], mean observation: 0.121 [-0.545, 1.000], loss: 3.612312, mean_absolute_error: 40.101898, mean_q: 54.080769
 1403458/1500000: episode: 2748, duration: 10.384s, episode steps: 405, steps per second: 39, episode reward: 227.735, mean reward: 0.562 [-17.902, 100.000], mean action: 1.299 [0.000, 3.000], mean observation: 0.110 [-0.515, 1.000], loss: 4.442641, mean_absolute_error: 40.508854, mean_q: 54.559578
 1403905/1500000: episode: 2749, duration: 11.394s, episode steps: 447, steps per second: 39, episode reward: 237.454, mean reward: 0.531 [-17.436, 100.000], mean action: 1.074 [0.000, 3.000], mean observation: 0.141 [-0.719, 1.000], loss: 4.230619, mean_absolute_error: 40.576630, mean_q: 54.661003
 1404235/1500000: episode: 2750, duration: 8.453s, episode steps: 330, steps per second: 39, episode reward: 213.527, mean reward: 0.647 [-2.642, 100.000], mean action: 1.203 [0.000, 3.000], mean observation: 0.070 [-0.433, 1.000], loss: 4.323331, mean_absolute_error: 40.373672, mean_q: 54.393509
 1404671/1500000: episode: 2751, duration: 11.532s, episode steps: 436, steps per second: 38, episode reward: 205.054, mean reward: 0.470 [-18.446, 100.000], mean action: 0.929 [0.000, 3.000], mean observation: 0.106 [-0.526, 1.000], loss: 3.631514, mean_absolute_error: 40.422626, mean_q: 54.452499
 1404998/1500000: episode: 2752, duration: 7.917s, episode steps: 327, steps per second: 41, episode reward: 211.070, mean reward: 0.645 [-8.684, 100.000], mean action: 1.196 [0.000, 3.000], mean observation: 0.076 [-0.634, 1.000], loss: 3.887955, mean_absolute_error: 40.511890, mean_q: 54.572670
 1405370/1500000: episode: 2753, duration: 9.558s, episode steps: 372, steps per second: 39, episode reward: 235.058, mean reward: 0.632 [-18.696, 100.000], mean action: 1.196 [0.000, 3.000], mean observation: 0.113 [-0.582, 1.016], loss: 4.486860, mean_absolute_error: 40.584217, mean_q: 54.674103
 1405791/1500000: episode: 2754, duration: 10.657s, episode steps: 421, steps per second: 40, episode reward: 183.364, mean reward: 0.436 [-18.064, 100.000], mean action: 0.900 [0.000, 3.000], mean observation: 0.131 [-0.463, 1.000], loss: 3.391411, mean_absolute_error: 40.367256, mean_q: 54.373196
 1406205/1500000: episode: 2755, duration: 10.805s, episode steps: 414, steps per second: 38, episode reward: 136.748, mean reward: 0.330 [-18.384, 100.000], mean action: 1.273 [0.000, 3.000], mean observation: 0.099 [-0.773, 1.000], loss: 4.538351, mean_absolute_error: 40.478760, mean_q: 54.505932
 1406482/1500000: episode: 2756, duration: 6.862s, episode steps: 277, steps per second: 40, episode reward: 249.895, mean reward: 0.902 [-10.141, 100.000], mean action: 1.365 [0.000, 3.000], mean observation: 0.040 [-0.719, 1.000], loss: 4.195259, mean_absolute_error: 40.491596, mean_q: 54.527691
 1406799/1500000: episode: 2757, duration: 8.195s, episode steps: 317, steps per second: 39, episode reward: 139.377, mean reward: 0.440 [-15.224, 100.000], mean action: 2.114 [0.000, 3.000], mean observation: 0.043 [-0.578, 1.000], loss: 5.332405, mean_absolute_error: 40.473335, mean_q: 54.510906
 1407378/1500000: episode: 2758, duration: 14.520s, episode steps: 579, steps per second: 40, episode reward: 218.511, mean reward: 0.377 [-19.814, 100.000], mean action: 0.767 [0.000, 3.000], mean observation: 0.152 [-0.512, 1.000], loss: 3.929325, mean_absolute_error: 40.579723, mean_q: 54.713612
 1407720/1500000: episode: 2759, duration: 8.883s, episode steps: 342, steps per second: 39, episode reward: 193.589, mean reward: 0.566 [-9.776, 100.000], mean action: 1.275 [0.000, 3.000], mean observation: 0.071 [-0.441, 1.000], loss: 2.584429, mean_absolute_error: 40.289577, mean_q: 54.331848
 1408006/1500000: episode: 2760, duration: 7.407s, episode steps: 286, steps per second: 39, episode reward: 239.315, mean reward: 0.837 [-2.848, 100.000], mean action: 1.353 [0.000, 3.000], mean observation: 0.092 [-0.444, 1.000], loss: 4.322405, mean_absolute_error: 40.371761, mean_q: 54.447628
 1408326/1500000: episode: 2761, duration: 8.003s, episode steps: 320, steps per second: 40, episode reward: 222.181, mean reward: 0.694 [-2.743, 100.000], mean action: 0.978 [0.000, 3.000], mean observation: 0.083 [-0.561, 1.000], loss: 4.290058, mean_absolute_error: 40.426239, mean_q: 54.515480
 1408669/1500000: episode: 2762, duration: 8.684s, episode steps: 343, steps per second: 39, episode reward: 201.687, mean reward: 0.588 [-17.375, 100.000], mean action: 1.382 [0.000, 3.000], mean observation: 0.092 [-0.483, 1.000], loss: 2.283749, mean_absolute_error: 40.439682, mean_q: 54.520702
 1408964/1500000: episode: 2763, duration: 7.018s, episode steps: 295, steps per second: 42, episode reward: 260.925, mean reward: 0.884 [-17.861, 100.000], mean action: 1.180 [0.000, 3.000], mean observation: 0.106 [-0.691, 1.000], loss: 4.141052, mean_absolute_error: 40.448849, mean_q: 54.485092
 1409218/1500000: episode: 2764, duration: 6.392s, episode steps: 254, steps per second: 40, episode reward: 242.208, mean reward: 0.954 [-9.587, 100.000], mean action: 1.421 [0.000, 3.000], mean observation: 0.082 [-0.644, 1.000], loss: 3.666041, mean_absolute_error: 40.344772, mean_q: 54.389160
 1409527/1500000: episode: 2765, duration: 7.708s, episode steps: 309, steps per second: 40, episode reward: 258.198, mean reward: 0.836 [-19.665, 100.000], mean action: 1.246 [0.000, 3.000], mean observation: 0.066 [-0.707, 1.000], loss: 5.080201, mean_absolute_error: 40.302479, mean_q: 54.335571
 1409630/1500000: episode: 2766, duration: 2.615s, episode steps: 103, steps per second: 39, episode reward: -6.936, mean reward: -0.067 [-100.000, 18.806], mean action: 1.913 [0.000, 3.000], mean observation: -0.080 [-0.838, 1.000], loss: 4.307973, mean_absolute_error: 40.623344, mean_q: 54.758228
 1410089/1500000: episode: 2767, duration: 11.585s, episode steps: 459, steps per second: 40, episode reward: 195.987, mean reward: 0.427 [-3.037, 100.000], mean action: 1.501 [0.000, 3.000], mean observation: 0.121 [-0.513, 1.000], loss: 3.994923, mean_absolute_error: 40.588413, mean_q: 54.729092
 1410434/1500000: episode: 2768, duration: 8.714s, episode steps: 345, steps per second: 40, episode reward: 213.986, mean reward: 0.620 [-17.360, 100.000], mean action: 1.490 [0.000, 3.000], mean observation: 0.064 [-0.507, 1.000], loss: 3.277700, mean_absolute_error: 40.483078, mean_q: 54.564278
 1410770/1500000: episode: 2769, duration: 9.605s, episode steps: 336, steps per second: 35, episode reward: 227.384, mean reward: 0.677 [-17.440, 100.000], mean action: 1.226 [0.000, 3.000], mean observation: 0.111 [-0.681, 1.000], loss: 3.446138, mean_absolute_error: 40.359230, mean_q: 54.387726
 1411083/1500000: episode: 2770, duration: 8.240s, episode steps: 313, steps per second: 38, episode reward: 211.771, mean reward: 0.677 [-10.620, 100.000], mean action: 1.351 [0.000, 3.000], mean observation: 0.085 [-0.549, 1.000], loss: 3.401220, mean_absolute_error: 40.583977, mean_q: 54.685764
 1411332/1500000: episode: 2771, duration: 6.398s, episode steps: 249, steps per second: 39, episode reward: 196.397, mean reward: 0.789 [-2.782, 100.000], mean action: 1.124 [0.000, 3.000], mean observation: 0.055 [-0.470, 1.000], loss: 2.187066, mean_absolute_error: 40.234005, mean_q: 54.255730
 1411605/1500000: episode: 2772, duration: 6.953s, episode steps: 273, steps per second: 39, episode reward: 198.026, mean reward: 0.725 [-12.576, 100.000], mean action: 1.209 [0.000, 3.000], mean observation: 0.057 [-0.528, 1.000], loss: 4.025912, mean_absolute_error: 40.361038, mean_q: 54.391232
 1411897/1500000: episode: 2773, duration: 7.155s, episode steps: 292, steps per second: 41, episode reward: 240.691, mean reward: 0.824 [-17.941, 100.000], mean action: 1.387 [0.000, 3.000], mean observation: 0.049 [-0.702, 1.000], loss: 4.127553, mean_absolute_error: 40.239487, mean_q: 54.182102
 1412247/1500000: episode: 2774, duration: 9.118s, episode steps: 350, steps per second: 38, episode reward: 223.564, mean reward: 0.639 [-9.177, 100.000], mean action: 1.283 [0.000, 3.000], mean observation: 0.099 [-0.558, 1.000], loss: 3.367897, mean_absolute_error: 40.492165, mean_q: 54.550793
 1412577/1500000: episode: 2775, duration: 8.447s, episode steps: 330, steps per second: 39, episode reward: 163.785, mean reward: 0.496 [-13.914, 100.000], mean action: 1.933 [0.000, 3.000], mean observation: 0.074 [-0.416, 1.000], loss: 3.544340, mean_absolute_error: 40.326645, mean_q: 54.321495
 1412859/1500000: episode: 2776, duration: 7.059s, episode steps: 282, steps per second: 40, episode reward: 210.641, mean reward: 0.747 [-2.942, 100.000], mean action: 1.301 [0.000, 3.000], mean observation: 0.035 [-0.538, 1.000], loss: 4.044001, mean_absolute_error: 40.527962, mean_q: 54.591743
 1413194/1500000: episode: 2777, duration: 8.286s, episode steps: 335, steps per second: 40, episode reward: 246.224, mean reward: 0.735 [-2.807, 100.000], mean action: 1.287 [0.000, 3.000], mean observation: 0.066 [-0.566, 1.016], loss: 3.771350, mean_absolute_error: 40.181683, mean_q: 54.137245
 1413520/1500000: episode: 2778, duration: 8.208s, episode steps: 326, steps per second: 40, episode reward: 224.825, mean reward: 0.690 [-9.002, 100.000], mean action: 1.282 [0.000, 3.000], mean observation: 0.066 [-0.511, 1.012], loss: 4.420265, mean_absolute_error: 40.252686, mean_q: 54.215717
 1413849/1500000: episode: 2779, duration: 8.462s, episode steps: 329, steps per second: 39, episode reward: 185.065, mean reward: 0.563 [-7.366, 100.000], mean action: 1.544 [0.000, 3.000], mean observation: 0.076 [-0.466, 1.000], loss: 3.241457, mean_absolute_error: 40.364758, mean_q: 54.351871
 1414144/1500000: episode: 2780, duration: 8.094s, episode steps: 295, steps per second: 36, episode reward: 214.539, mean reward: 0.727 [-17.708, 100.000], mean action: 1.471 [0.000, 3.000], mean observation: 0.077 [-0.643, 1.000], loss: 2.847832, mean_absolute_error: 40.379250, mean_q: 54.382473
 1414531/1500000: episode: 2781, duration: 9.670s, episode steps: 387, steps per second: 40, episode reward: 219.029, mean reward: 0.566 [-9.520, 100.000], mean action: 1.189 [0.000, 3.000], mean observation: 0.104 [-0.612, 1.000], loss: 4.220065, mean_absolute_error: 40.258881, mean_q: 54.213764
 1414852/1500000: episode: 2782, duration: 8.087s, episode steps: 321, steps per second: 40, episode reward: 208.040, mean reward: 0.648 [-3.364, 100.000], mean action: 1.178 [0.000, 3.000], mean observation: 0.055 [-0.521, 1.000], loss: 4.508572, mean_absolute_error: 40.254986, mean_q: 54.199722
 1415095/1500000: episode: 2783, duration: 3.982s, episode steps: 243, steps per second: 61, episode reward: 212.870, mean reward: 0.876 [-10.620, 100.000], mean action: 1.247 [0.000, 3.000], mean observation: 0.062 [-0.552, 1.000], loss: 4.180781, mean_absolute_error: 40.224701, mean_q: 54.211113
 1415323/1500000: episode: 2784, duration: 2.572s, episode steps: 228, steps per second: 89, episode reward: 245.592, mean reward: 1.077 [-10.530, 100.000], mean action: 1.539 [0.000, 3.000], mean observation: 0.016 [-0.720, 1.000], loss: 3.817936, mean_absolute_error: 40.260914, mean_q: 54.229343
 1415654/1500000: episode: 2785, duration: 3.930s, episode steps: 331, steps per second: 84, episode reward: 224.200, mean reward: 0.677 [-9.135, 100.000], mean action: 1.302 [0.000, 3.000], mean observation: 0.084 [-0.486, 1.000], loss: 3.966579, mean_absolute_error: 40.280342, mean_q: 54.276039
 1415887/1500000: episode: 2786, duration: 2.603s, episode steps: 233, steps per second: 89, episode reward: 215.061, mean reward: 0.923 [-3.024, 100.000], mean action: 1.395 [0.000, 3.000], mean observation: 0.030 [-0.603, 1.000], loss: 6.175936, mean_absolute_error: 40.389977, mean_q: 54.368462
 1416887/1500000: episode: 2787, duration: 12.503s, episode steps: 1000, steps per second: 80, episode reward: 110.512, mean reward: 0.111 [-19.658, 21.828], mean action: 1.751 [0.000, 3.000], mean observation: 0.184 [-0.853, 1.000], loss: 2.990028, mean_absolute_error: 40.186718, mean_q: 54.138302
 1417286/1500000: episode: 2788, duration: 5.902s, episode steps: 399, steps per second: 68, episode reward: 247.649, mean reward: 0.621 [-17.363, 100.000], mean action: 1.221 [0.000, 3.000], mean observation: 0.127 [-0.646, 1.033], loss: 4.341925, mean_absolute_error: 40.134613, mean_q: 54.085812
 1417507/1500000: episode: 2789, duration: 4.437s, episode steps: 221, steps per second: 50, episode reward: 198.146, mean reward: 0.897 [-2.769, 100.000], mean action: 1.208 [0.000, 3.000], mean observation: 0.062 [-0.491, 1.000], loss: 6.210186, mean_absolute_error: 39.990959, mean_q: 53.900551
 1417795/1500000: episode: 2790, duration: 3.317s, episode steps: 288, steps per second: 87, episode reward: 211.433, mean reward: 0.734 [-2.760, 100.000], mean action: 0.979 [0.000, 3.000], mean observation: 0.078 [-0.587, 1.000], loss: 7.040583, mean_absolute_error: 40.249561, mean_q: 54.269539
 1418106/1500000: episode: 2791, duration: 3.655s, episode steps: 311, steps per second: 85, episode reward: 182.075, mean reward: 0.585 [-9.364, 100.000], mean action: 1.421 [0.000, 3.000], mean observation: 0.065 [-0.535, 1.000], loss: 3.258954, mean_absolute_error: 40.350872, mean_q: 54.437225
 1418789/1500000: episode: 2792, duration: 12.397s, episode steps: 683, steps per second: 55, episode reward: 212.310, mean reward: 0.311 [-19.959, 100.000], mean action: 0.884 [0.000, 3.000], mean observation: 0.136 [-0.563, 1.045], loss: 3.637918, mean_absolute_error: 40.150959, mean_q: 54.136971
 1419113/1500000: episode: 2793, duration: 4.595s, episode steps: 324, steps per second: 71, episode reward: 222.251, mean reward: 0.686 [-17.680, 100.000], mean action: 1.006 [0.000, 3.000], mean observation: 0.083 [-0.527, 1.013], loss: 4.262526, mean_absolute_error: 40.146950, mean_q: 54.093815
 1419440/1500000: episode: 2794, duration: 7.015s, episode steps: 327, steps per second: 47, episode reward: 179.691, mean reward: 0.550 [-21.483, 100.000], mean action: 1.257 [0.000, 3.000], mean observation: 0.086 [-0.484, 1.000], loss: 3.731398, mean_absolute_error: 40.523182, mean_q: 54.586422
 1419779/1500000: episode: 2795, duration: 4.033s, episode steps: 339, steps per second: 84, episode reward: 255.829, mean reward: 0.755 [-9.938, 100.000], mean action: 1.189 [0.000, 3.000], mean observation: 0.121 [-0.706, 1.000], loss: 2.374786, mean_absolute_error: 40.386223, mean_q: 54.436741
 1420074/1500000: episode: 2796, duration: 5.397s, episode steps: 295, steps per second: 55, episode reward: 232.063, mean reward: 0.787 [-17.466, 100.000], mean action: 1.264 [0.000, 3.000], mean observation: 0.041 [-0.554, 1.000], loss: 5.202651, mean_absolute_error: 40.502609, mean_q: 54.522369
 1420416/1500000: episode: 2797, duration: 6.217s, episode steps: 342, steps per second: 55, episode reward: 194.609, mean reward: 0.569 [-9.700, 100.000], mean action: 1.029 [0.000, 3.000], mean observation: 0.079 [-0.488, 1.000], loss: 2.989208, mean_absolute_error: 40.532665, mean_q: 54.591625
 1420737/1500000: episode: 2798, duration: 3.742s, episode steps: 321, steps per second: 86, episode reward: 220.387, mean reward: 0.687 [-19.595, 100.000], mean action: 1.212 [0.000, 3.000], mean observation: 0.073 [-0.579, 1.000], loss: 2.838870, mean_absolute_error: 40.587948, mean_q: 54.671249
 1421140/1500000: episode: 2799, duration: 9.164s, episode steps: 403, steps per second: 44, episode reward: 214.094, mean reward: 0.531 [-18.338, 100.000], mean action: 1.072 [0.000, 3.000], mean observation: 0.106 [-0.606, 1.000], loss: 4.506167, mean_absolute_error: 40.526596, mean_q: 54.545364
 1421570/1500000: episode: 2800, duration: 5.127s, episode steps: 430, steps per second: 84, episode reward: 191.865, mean reward: 0.446 [-18.228, 100.000], mean action: 0.795 [0.000, 3.000], mean observation: 0.125 [-0.540, 1.000], loss: 3.228281, mean_absolute_error: 40.602280, mean_q: 54.697830
 1421859/1500000: episode: 2801, duration: 6.103s, episode steps: 289, steps per second: 47, episode reward: 224.922, mean reward: 0.778 [-17.663, 100.000], mean action: 1.114 [0.000, 3.000], mean observation: 0.063 [-0.647, 1.000], loss: 2.325679, mean_absolute_error: 40.414627, mean_q: 54.456604
 1422216/1500000: episode: 2802, duration: 6.276s, episode steps: 357, steps per second: 57, episode reward: 229.281, mean reward: 0.642 [-17.793, 100.000], mean action: 1.277 [0.000, 3.000], mean observation: 0.085 [-0.520, 1.000], loss: 5.428199, mean_absolute_error: 40.596157, mean_q: 54.657814
 1422561/1500000: episode: 2803, duration: 4.558s, episode steps: 345, steps per second: 76, episode reward: 197.885, mean reward: 0.574 [-8.768, 100.000], mean action: 1.539 [0.000, 3.000], mean observation: 0.070 [-0.652, 1.000], loss: 3.407502, mean_absolute_error: 40.534733, mean_q: 54.630146
 1422869/1500000: episode: 2804, duration: 7.604s, episode steps: 308, steps per second: 41, episode reward: 185.204, mean reward: 0.601 [-19.344, 100.000], mean action: 1.149 [0.000, 3.000], mean observation: 0.083 [-0.549, 1.000], loss: 4.471191, mean_absolute_error: 40.837048, mean_q: 54.980709
 1423254/1500000: episode: 2805, duration: 5.056s, episode steps: 385, steps per second: 76, episode reward: 234.901, mean reward: 0.610 [-11.535, 100.000], mean action: 2.501 [0.000, 3.000], mean observation: 0.126 [-0.787, 1.000], loss: 2.893346, mean_absolute_error: 40.898193, mean_q: 55.089256
 1423493/1500000: episode: 2806, duration: 4.049s, episode steps: 239, steps per second: 59, episode reward: 190.874, mean reward: 0.799 [-9.158, 100.000], mean action: 1.297 [0.000, 3.000], mean observation: 0.048 [-0.547, 1.000], loss: 3.886798, mean_absolute_error: 40.513866, mean_q: 54.583504
 1423997/1500000: episode: 2807, duration: 10.288s, episode steps: 504, steps per second: 49, episode reward: 154.787, mean reward: 0.307 [-17.594, 100.000], mean action: 1.292 [0.000, 3.000], mean observation: 0.079 [-0.660, 1.000], loss: 4.507334, mean_absolute_error: 40.674480, mean_q: 54.779686
 1424321/1500000: episode: 2808, duration: 4.435s, episode steps: 324, steps per second: 73, episode reward: 217.416, mean reward: 0.671 [-18.120, 100.000], mean action: 1.062 [0.000, 3.000], mean observation: 0.089 [-0.478, 1.000], loss: 3.691230, mean_absolute_error: 40.598728, mean_q: 54.701546
 1424550/1500000: episode: 2809, duration: 5.718s, episode steps: 229, steps per second: 40, episode reward: 224.371, mean reward: 0.980 [-3.217, 100.000], mean action: 1.345 [0.000, 3.000], mean observation: 0.016 [-0.781, 1.000], loss: 5.032284, mean_absolute_error: 40.438541, mean_q: 54.480309
 1424902/1500000: episode: 2810, duration: 5.605s, episode steps: 352, steps per second: 63, episode reward: 212.648, mean reward: 0.604 [-18.662, 100.000], mean action: 0.909 [0.000, 3.000], mean observation: 0.077 [-0.490, 1.004], loss: 4.159222, mean_absolute_error: 40.526821, mean_q: 54.618820
 1425219/1500000: episode: 2811, duration: 4.808s, episode steps: 317, steps per second: 66, episode reward: 235.325, mean reward: 0.742 [-10.281, 100.000], mean action: 1.397 [0.000, 3.000], mean observation: 0.084 [-0.532, 1.000], loss: 4.721209, mean_absolute_error: 40.644936, mean_q: 54.749729
 1425470/1500000: episode: 2812, duration: 6.682s, episode steps: 251, steps per second: 38, episode reward: 183.929, mean reward: 0.733 [-10.715, 100.000], mean action: 1.474 [0.000, 3.000], mean observation: 0.017 [-0.445, 1.000], loss: 2.109110, mean_absolute_error: 40.529423, mean_q: 54.604519
 1425916/1500000: episode: 2813, duration: 6.244s, episode steps: 446, steps per second: 71, episode reward: 179.096, mean reward: 0.402 [-19.743, 100.000], mean action: 0.836 [0.000, 3.000], mean observation: 0.133 [-0.528, 1.000], loss: 4.311911, mean_absolute_error: 40.591496, mean_q: 54.672768
 1426305/1500000: episode: 2814, duration: 8.155s, episode steps: 389, steps per second: 48, episode reward: 220.590, mean reward: 0.567 [-8.974, 100.000], mean action: 1.049 [0.000, 3.000], mean observation: 0.109 [-0.655, 1.000], loss: 3.241155, mean_absolute_error: 40.530106, mean_q: 54.605991
 1426644/1500000: episode: 2815, duration: 6.336s, episode steps: 339, steps per second: 54, episode reward: 216.665, mean reward: 0.639 [-3.106, 100.000], mean action: 1.407 [0.000, 3.000], mean observation: 0.079 [-0.515, 1.003], loss: 4.735219, mean_absolute_error: 40.610931, mean_q: 54.678024
 1427167/1500000: episode: 2816, duration: 10.310s, episode steps: 523, steps per second: 51, episode reward: 198.365, mean reward: 0.379 [-17.553, 100.000], mean action: 0.721 [0.000, 3.000], mean observation: 0.142 [-0.459, 1.000], loss: 6.501090, mean_absolute_error: 40.486042, mean_q: 54.504883
 1427513/1500000: episode: 2817, duration: 5.710s, episode steps: 346, steps per second: 61, episode reward: 233.250, mean reward: 0.674 [-3.002, 100.000], mean action: 1.353 [0.000, 3.000], mean observation: 0.107 [-0.591, 1.000], loss: 3.386676, mean_absolute_error: 40.421562, mean_q: 54.434769
 1427975/1500000: episode: 2818, duration: 9.057s, episode steps: 462, steps per second: 51, episode reward: 221.116, mean reward: 0.479 [-17.734, 100.000], mean action: 1.082 [0.000, 3.000], mean observation: 0.106 [-0.442, 1.029], loss: 3.616611, mean_absolute_error: 40.685505, mean_q: 54.776531
 1428344/1500000: episode: 2819, duration: 7.015s, episode steps: 369, steps per second: 53, episode reward: 226.806, mean reward: 0.615 [-18.701, 100.000], mean action: 1.108 [0.000, 3.000], mean observation: 0.075 [-0.762, 1.000], loss: 4.457641, mean_absolute_error: 40.727879, mean_q: 54.839008
 1428598/1500000: episode: 2820, duration: 2.862s, episode steps: 254, steps per second: 89, episode reward: 240.295, mean reward: 0.946 [-9.201, 100.000], mean action: 1.343 [0.000, 3.000], mean observation: 0.028 [-0.709, 1.000], loss: 2.494915, mean_absolute_error: 40.577602, mean_q: 54.661510
 1428895/1500000: episode: 2821, duration: 6.975s, episode steps: 297, steps per second: 43, episode reward: 195.102, mean reward: 0.657 [-9.529, 100.000], mean action: 1.290 [0.000, 3.000], mean observation: 0.083 [-0.535, 1.000], loss: 3.477668, mean_absolute_error: 40.474468, mean_q: 54.500465
 1429143/1500000: episode: 2822, duration: 5.423s, episode steps: 248, steps per second: 46, episode reward: 184.438, mean reward: 0.744 [-2.668, 100.000], mean action: 1.210 [0.000, 3.000], mean observation: 0.039 [-0.564, 1.000], loss: 2.746758, mean_absolute_error: 40.549961, mean_q: 54.605785
 1429587/1500000: episode: 2823, duration: 6.205s, episode steps: 444, steps per second: 72, episode reward: 243.441, mean reward: 0.548 [-18.800, 100.000], mean action: 1.151 [0.000, 3.000], mean observation: 0.135 [-0.721, 1.000], loss: 3.385018, mean_absolute_error: 40.633396, mean_q: 54.702942
 1429879/1500000: episode: 2824, duration: 7.333s, episode steps: 292, steps per second: 40, episode reward: 202.727, mean reward: 0.694 [-3.026, 100.000], mean action: 1.466 [0.000, 3.000], mean observation: 0.080 [-0.491, 1.000], loss: 2.438499, mean_absolute_error: 40.292446, mean_q: 54.279690
 1430213/1500000: episode: 2825, duration: 5.377s, episode steps: 334, steps per second: 62, episode reward: 195.789, mean reward: 0.586 [-19.629, 100.000], mean action: 1.072 [0.000, 3.000], mean observation: 0.076 [-0.454, 1.000], loss: 3.363583, mean_absolute_error: 40.403923, mean_q: 54.401917
 1430651/1500000: episode: 2826, duration: 8.663s, episode steps: 438, steps per second: 51, episode reward: 198.946, mean reward: 0.454 [-17.678, 100.000], mean action: 1.130 [0.000, 3.000], mean observation: 0.119 [-0.442, 1.000], loss: 4.919569, mean_absolute_error: 40.447311, mean_q: 54.444752
 1430958/1500000: episode: 2827, duration: 6.330s, episode steps: 307, steps per second: 48, episode reward: 207.165, mean reward: 0.675 [-9.620, 100.000], mean action: 1.189 [0.000, 3.000], mean observation: 0.081 [-0.523, 1.000], loss: 3.020410, mean_absolute_error: 40.143116, mean_q: 54.073563
 1431378/1500000: episode: 2828, duration: 6.459s, episode steps: 420, steps per second: 65, episode reward: 200.632, mean reward: 0.478 [-17.969, 100.000], mean action: 1.155 [0.000, 3.000], mean observation: 0.115 [-0.438, 1.000], loss: 4.890668, mean_absolute_error: 40.167179, mean_q: 54.069260
 1431742/1500000: episode: 2829, duration: 9.216s, episode steps: 364, steps per second: 39, episode reward: 239.703, mean reward: 0.659 [-17.772, 100.000], mean action: 1.126 [0.000, 3.000], mean observation: 0.090 [-1.036, 1.000], loss: 5.230758, mean_absolute_error: 40.092190, mean_q: 53.997028
 1432018/1500000: episode: 2830, duration: 3.282s, episode steps: 276, steps per second: 84, episode reward: 230.867, mean reward: 0.836 [-18.777, 100.000], mean action: 1.493 [0.000, 3.000], mean observation: 0.051 [-0.688, 1.000], loss: 3.885376, mean_absolute_error: 40.103580, mean_q: 53.990780
 1432318/1500000: episode: 2831, duration: 5.442s, episode steps: 300, steps per second: 55, episode reward: 208.717, mean reward: 0.696 [-10.540, 100.000], mean action: 1.437 [0.000, 3.000], mean observation: 0.044 [-0.502, 1.000], loss: 3.062181, mean_absolute_error: 40.204082, mean_q: 54.113789
 1432931/1500000: episode: 2832, duration: 12.336s, episode steps: 613, steps per second: 50, episode reward: 180.882, mean reward: 0.295 [-20.672, 100.000], mean action: 1.873 [0.000, 3.000], mean observation: 0.130 [-0.461, 1.006], loss: 4.084166, mean_absolute_error: 40.226822, mean_q: 54.141548
 1433161/1500000: episode: 2833, duration: 4.122s, episode steps: 230, steps per second: 56, episode reward: 249.171, mean reward: 1.083 [-9.030, 100.000], mean action: 1.296 [0.000, 3.000], mean observation: 0.133 [-0.651, 1.000], loss: 5.384972, mean_absolute_error: 40.168991, mean_q: 54.046215
 1433489/1500000: episode: 2834, duration: 8.247s, episode steps: 328, steps per second: 40, episode reward: 236.102, mean reward: 0.720 [-19.287, 100.000], mean action: 1.247 [0.000, 3.000], mean observation: 0.108 [-0.695, 1.000], loss: 3.176474, mean_absolute_error: 40.197254, mean_q: 54.096905
 1433848/1500000: episode: 2835, duration: 4.954s, episode steps: 359, steps per second: 72, episode reward: 258.324, mean reward: 0.720 [-10.393, 100.000], mean action: 0.989 [0.000, 3.000], mean observation: 0.137 [-0.754, 1.000], loss: 3.240838, mean_absolute_error: 40.231350, mean_q: 54.163147
 1434162/1500000: episode: 2836, duration: 6.192s, episode steps: 314, steps per second: 51, episode reward: 247.636, mean reward: 0.789 [-17.793, 100.000], mean action: 1.229 [0.000, 3.000], mean observation: 0.051 [-0.719, 1.000], loss: 4.092242, mean_absolute_error: 40.108479, mean_q: 54.008778
 1434470/1500000: episode: 2837, duration: 8.185s, episode steps: 308, steps per second: 38, episode reward: 249.206, mean reward: 0.809 [-10.455, 100.000], mean action: 1.198 [0.000, 3.000], mean observation: 0.066 [-0.744, 1.000], loss: 3.498290, mean_absolute_error: 40.194607, mean_q: 54.132065
 1434789/1500000: episode: 2838, duration: 3.950s, episode steps: 319, steps per second: 81, episode reward: 216.002, mean reward: 0.677 [-3.102, 100.000], mean action: 1.245 [0.000, 3.000], mean observation: 0.087 [-0.540, 1.000], loss: 4.830793, mean_absolute_error: 40.225220, mean_q: 54.163818
 1435106/1500000: episode: 2839, duration: 6.861s, episode steps: 317, steps per second: 46, episode reward: 169.813, mean reward: 0.536 [-13.052, 100.000], mean action: 1.240 [0.000, 3.000], mean observation: 0.048 [-0.672, 1.000], loss: 2.837362, mean_absolute_error: 40.289314, mean_q: 54.257172
 1435551/1500000: episode: 2840, duration: 8.503s, episode steps: 445, steps per second: 52, episode reward: 159.033, mean reward: 0.357 [-17.966, 100.000], mean action: 2.135 [0.000, 3.000], mean observation: 0.136 [-0.505, 1.000], loss: 4.526089, mean_absolute_error: 40.358639, mean_q: 54.338467
 1435814/1500000: episode: 2841, duration: 3.258s, episode steps: 263, steps per second: 81, episode reward: 242.931, mean reward: 0.924 [-18.552, 100.000], mean action: 1.354 [0.000, 3.000], mean observation: 0.055 [-0.787, 1.000], loss: 2.311467, mean_absolute_error: 40.212883, mean_q: 54.164246
 1436112/1500000: episode: 2842, duration: 7.586s, episode steps: 298, steps per second: 39, episode reward: 239.338, mean reward: 0.803 [-9.378, 100.000], mean action: 1.440 [0.000, 3.000], mean observation: 0.045 [-0.629, 1.000], loss: 5.445308, mean_absolute_error: 40.396275, mean_q: 54.353210
 1436411/1500000: episode: 2843, duration: 6.011s, episode steps: 299, steps per second: 50, episode reward: 237.614, mean reward: 0.795 [-3.324, 100.000], mean action: 1.167 [0.000, 3.000], mean observation: 0.115 [-0.498, 1.000], loss: 3.163628, mean_absolute_error: 40.153049, mean_q: 54.058071
 1436877/1500000: episode: 2844, duration: 7.542s, episode steps: 466, steps per second: 62, episode reward: 244.231, mean reward: 0.524 [-17.438, 100.000], mean action: 0.957 [0.000, 3.000], mean observation: 0.137 [-0.750, 1.000], loss: 4.678919, mean_absolute_error: 40.270222, mean_q: 54.206635
 1437347/1500000: episode: 2845, duration: 10.194s, episode steps: 470, steps per second: 46, episode reward: 233.442, mean reward: 0.497 [-18.351, 100.000], mean action: 0.996 [0.000, 3.000], mean observation: 0.146 [-0.646, 1.000], loss: 3.970052, mean_absolute_error: 40.579628, mean_q: 54.654690
 1437857/1500000: episode: 2846, duration: 8.856s, episode steps: 510, steps per second: 58, episode reward: 228.871, mean reward: 0.449 [-20.094, 100.000], mean action: 0.771 [0.000, 3.000], mean observation: 0.131 [-0.612, 1.000], loss: 3.916057, mean_absolute_error: 40.341782, mean_q: 54.322376
 1438452/1500000: episode: 2847, duration: 11.153s, episode steps: 595, steps per second: 53, episode reward: 244.091, mean reward: 0.410 [-18.368, 100.000], mean action: 0.706 [0.000, 3.000], mean observation: 0.162 [-0.708, 1.000], loss: 4.043631, mean_absolute_error: 40.372849, mean_q: 54.339657
 1438669/1500000: episode: 2848, duration: 3.425s, episode steps: 217, steps per second: 63, episode reward: 185.210, mean reward: 0.854 [-2.934, 100.000], mean action: 1.507 [0.000, 3.000], mean observation: 0.037 [-0.481, 1.000], loss: 4.726206, mean_absolute_error: 40.531715, mean_q: 54.565460
 1438945/1500000: episode: 2849, duration: 6.861s, episode steps: 276, steps per second: 40, episode reward: 234.173, mean reward: 0.848 [-2.723, 100.000], mean action: 1.333 [0.000, 3.000], mean observation: 0.063 [-0.627, 1.000], loss: 2.622624, mean_absolute_error: 40.272835, mean_q: 54.201584
 1439237/1500000: episode: 2850, duration: 5.717s, episode steps: 292, steps per second: 51, episode reward: 244.391, mean reward: 0.837 [-3.336, 100.000], mean action: 1.373 [0.000, 3.000], mean observation: 0.090 [-0.528, 1.058], loss: 5.032917, mean_absolute_error: 40.245914, mean_q: 54.194508
 1439693/1500000: episode: 2851, duration: 6.908s, episode steps: 456, steps per second: 66, episode reward: 167.197, mean reward: 0.367 [-18.843, 100.000], mean action: 2.250 [0.000, 3.000], mean observation: 0.125 [-0.594, 1.000], loss: 3.782850, mean_absolute_error: 40.347923, mean_q: 54.330952
 1439935/1500000: episode: 2852, duration: 5.931s, episode steps: 242, steps per second: 41, episode reward: 203.286, mean reward: 0.840 [-12.438, 100.000], mean action: 1.269 [0.000, 3.000], mean observation: 0.061 [-0.488, 1.000], loss: 2.929028, mean_absolute_error: 40.276878, mean_q: 54.244930
 1440388/1500000: episode: 2853, duration: 7.691s, episode steps: 453, steps per second: 59, episode reward: 234.419, mean reward: 0.517 [-18.125, 100.000], mean action: 0.958 [0.000, 3.000], mean observation: 0.148 [-0.567, 1.000], loss: 3.904305, mean_absolute_error: 40.363888, mean_q: 54.353657
 1440787/1500000: episode: 2854, duration: 8.625s, episode steps: 399, steps per second: 46, episode reward: 251.471, mean reward: 0.630 [-20.985, 100.000], mean action: 1.085 [0.000, 3.000], mean observation: 0.140 [-0.708, 1.000], loss: 3.184859, mean_absolute_error: 40.313946, mean_q: 54.273888
 1441071/1500000: episode: 2855, duration: 7.059s, episode steps: 284, steps per second: 40, episode reward: 189.938, mean reward: 0.669 [-17.705, 100.000], mean action: 1.310 [0.000, 3.000], mean observation: 0.048 [-0.506, 1.000], loss: 3.874707, mean_absolute_error: 40.490070, mean_q: 54.486603
 1441368/1500000: episode: 2856, duration: 7.576s, episode steps: 297, steps per second: 39, episode reward: 204.781, mean reward: 0.689 [-9.388, 100.000], mean action: 1.481 [0.000, 3.000], mean observation: 0.027 [-0.526, 1.000], loss: 4.362967, mean_absolute_error: 40.427353, mean_q: 54.403370
 1441857/1500000: episode: 2857, duration: 12.385s, episode steps: 489, steps per second: 39, episode reward: 210.007, mean reward: 0.429 [-18.828, 100.000], mean action: 0.716 [0.000, 3.000], mean observation: 0.140 [-0.571, 1.000], loss: 4.587143, mean_absolute_error: 40.588047, mean_q: 54.620029
 1442183/1500000: episode: 2858, duration: 9.155s, episode steps: 326, steps per second: 36, episode reward: 227.796, mean reward: 0.699 [-8.326, 100.000], mean action: 1.221 [0.000, 3.000], mean observation: 0.111 [-0.467, 1.000], loss: 4.987565, mean_absolute_error: 40.513092, mean_q: 54.509010
 1442471/1500000: episode: 2859, duration: 7.331s, episode steps: 288, steps per second: 39, episode reward: 185.317, mean reward: 0.643 [-9.858, 100.000], mean action: 0.993 [0.000, 3.000], mean observation: 0.034 [-0.496, 1.000], loss: 2.759240, mean_absolute_error: 40.589424, mean_q: 54.672276
 1442731/1500000: episode: 2860, duration: 6.379s, episode steps: 260, steps per second: 41, episode reward: 258.925, mean reward: 0.996 [-17.376, 100.000], mean action: 1.162 [0.000, 3.000], mean observation: 0.111 [-0.780, 1.000], loss: 5.103675, mean_absolute_error: 40.462528, mean_q: 54.493458
 1442952/1500000: episode: 2861, duration: 5.528s, episode steps: 221, steps per second: 40, episode reward: 183.032, mean reward: 0.828 [-18.959, 100.000], mean action: 1.005 [0.000, 3.000], mean observation: 0.032 [-0.598, 1.000], loss: 3.828718, mean_absolute_error: 40.476536, mean_q: 54.519115
 1443182/1500000: episode: 2862, duration: 5.630s, episode steps: 230, steps per second: 41, episode reward: 251.357, mean reward: 1.093 [-11.733, 100.000], mean action: 1.548 [0.000, 3.000], mean observation: -0.004 [-0.838, 1.000], loss: 3.830551, mean_absolute_error: 40.546440, mean_q: 54.591484
 1443536/1500000: episode: 2863, duration: 8.920s, episode steps: 354, steps per second: 40, episode reward: 215.549, mean reward: 0.609 [-2.855, 100.000], mean action: 1.065 [0.000, 3.000], mean observation: 0.107 [-0.487, 1.000], loss: 3.129386, mean_absolute_error: 40.545280, mean_q: 54.581215
 1443884/1500000: episode: 2864, duration: 9.387s, episode steps: 348, steps per second: 37, episode reward: 176.685, mean reward: 0.508 [-19.238, 100.000], mean action: 1.836 [0.000, 3.000], mean observation: 0.058 [-0.567, 1.000], loss: 4.032294, mean_absolute_error: 40.518993, mean_q: 54.548706
 1444151/1500000: episode: 2865, duration: 6.601s, episode steps: 267, steps per second: 40, episode reward: 201.150, mean reward: 0.753 [-11.033, 100.000], mean action: 1.281 [0.000, 3.000], mean observation: 0.058 [-0.502, 1.000], loss: 2.618558, mean_absolute_error: 40.506855, mean_q: 54.551182
 1444449/1500000: episode: 2866, duration: 7.567s, episode steps: 298, steps per second: 39, episode reward: 224.567, mean reward: 0.754 [-12.296, 100.000], mean action: 1.054 [0.000, 3.000], mean observation: 0.095 [-0.951, 1.000], loss: 4.169217, mean_absolute_error: 40.474766, mean_q: 54.474979
 1444718/1500000: episode: 2867, duration: 6.871s, episode steps: 269, steps per second: 39, episode reward: 227.952, mean reward: 0.847 [-2.880, 100.000], mean action: 1.312 [0.000, 3.000], mean observation: 0.033 [-0.504, 1.000], loss: 4.569233, mean_absolute_error: 40.416008, mean_q: 54.400269
 1445046/1500000: episode: 2868, duration: 8.269s, episode steps: 328, steps per second: 40, episode reward: 249.908, mean reward: 0.762 [-11.171, 100.000], mean action: 1.427 [0.000, 3.000], mean observation: 0.093 [-0.562, 1.053], loss: 4.029109, mean_absolute_error: 40.496582, mean_q: 54.503643
 1445504/1500000: episode: 2869, duration: 11.834s, episode steps: 458, steps per second: 39, episode reward: 202.448, mean reward: 0.442 [-11.481, 100.000], mean action: 0.921 [0.000, 3.000], mean observation: 0.109 [-0.435, 1.000], loss: 3.717690, mean_absolute_error: 40.794521, mean_q: 54.912083
 1445872/1500000: episode: 2870, duration: 9.542s, episode steps: 368, steps per second: 39, episode reward: 187.216, mean reward: 0.509 [-17.450, 100.000], mean action: 1.139 [0.000, 3.000], mean observation: 0.113 [-0.543, 1.000], loss: 3.100528, mean_absolute_error: 40.503246, mean_q: 54.551498
 1446227/1500000: episode: 2871, duration: 8.880s, episode steps: 355, steps per second: 40, episode reward: 233.707, mean reward: 0.658 [-9.301, 100.000], mean action: 1.499 [0.000, 3.000], mean observation: 0.096 [-0.655, 1.028], loss: 5.256221, mean_absolute_error: 40.646706, mean_q: 54.723427
 1446621/1500000: episode: 2872, duration: 10.173s, episode steps: 394, steps per second: 39, episode reward: 224.329, mean reward: 0.569 [-9.650, 100.000], mean action: 1.033 [0.000, 3.000], mean observation: 0.084 [-0.688, 1.000], loss: 5.616466, mean_absolute_error: 40.811405, mean_q: 54.931728
 1446958/1500000: episode: 2873, duration: 8.088s, episode steps: 337, steps per second: 42, episode reward: 242.626, mean reward: 0.720 [-17.767, 100.000], mean action: 1.303 [0.000, 3.000], mean observation: 0.109 [-0.537, 1.000], loss: 4.674450, mean_absolute_error: 40.825359, mean_q: 54.979855
 1447291/1500000: episode: 2874, duration: 8.954s, episode steps: 333, steps per second: 37, episode reward: 224.854, mean reward: 0.675 [-3.250, 100.000], mean action: 1.462 [0.000, 3.000], mean observation: 0.092 [-0.554, 1.000], loss: 4.104330, mean_absolute_error: 40.748661, mean_q: 54.868771
 1448291/1500000: episode: 2875, duration: 27.075s, episode steps: 1000, steps per second: 37, episode reward: 97.277, mean reward: 0.097 [-19.029, 22.533], mean action: 1.862 [0.000, 3.000], mean observation: 0.173 [-0.796, 1.000], loss: 4.561194, mean_absolute_error: 40.717686, mean_q: 54.867832
 1448727/1500000: episode: 2876, duration: 11.765s, episode steps: 436, steps per second: 37, episode reward: 198.970, mean reward: 0.456 [-19.484, 100.000], mean action: 1.032 [0.000, 3.000], mean observation: 0.123 [-0.491, 1.000], loss: 3.201838, mean_absolute_error: 40.752750, mean_q: 54.885822
 1449056/1500000: episode: 2877, duration: 9.103s, episode steps: 329, steps per second: 36, episode reward: 187.333, mean reward: 0.569 [-10.016, 100.000], mean action: 1.334 [0.000, 3.000], mean observation: 0.089 [-0.500, 1.000], loss: 3.305675, mean_absolute_error: 40.692188, mean_q: 54.825844
 1449452/1500000: episode: 2878, duration: 10.992s, episode steps: 396, steps per second: 36, episode reward: 186.698, mean reward: 0.471 [-9.304, 100.000], mean action: 1.354 [0.000, 3.000], mean observation: 0.092 [-0.380, 1.000], loss: 3.960097, mean_absolute_error: 40.909058, mean_q: 55.102066
 1449809/1500000: episode: 2879, duration: 10.008s, episode steps: 357, steps per second: 36, episode reward: 258.248, mean reward: 0.723 [-17.712, 100.000], mean action: 0.989 [0.000, 3.000], mean observation: 0.105 [-0.824, 1.000], loss: 3.512924, mean_absolute_error: 41.038807, mean_q: 55.251534
 1450104/1500000: episode: 2880, duration: 7.825s, episode steps: 295, steps per second: 38, episode reward: 229.488, mean reward: 0.778 [-9.686, 100.000], mean action: 1.339 [0.000, 3.000], mean observation: 0.036 [-0.674, 1.000], loss: 3.808507, mean_absolute_error: 40.864124, mean_q: 55.006409
 1450410/1500000: episode: 2881, duration: 7.738s, episode steps: 306, steps per second: 40, episode reward: 213.498, mean reward: 0.698 [-2.832, 100.000], mean action: 1.327 [0.000, 3.000], mean observation: 0.059 [-0.468, 1.000], loss: 2.530003, mean_absolute_error: 40.951591, mean_q: 55.138378
 1450684/1500000: episode: 2882, duration: 6.711s, episode steps: 274, steps per second: 41, episode reward: 226.932, mean reward: 0.828 [-2.918, 100.000], mean action: 1.084 [0.000, 3.000], mean observation: 0.064 [-0.532, 1.014], loss: 5.789670, mean_absolute_error: 40.725567, mean_q: 54.855854
 1451094/1500000: episode: 2883, duration: 10.497s, episode steps: 410, steps per second: 39, episode reward: 219.178, mean reward: 0.535 [-19.832, 100.000], mean action: 0.971 [0.000, 3.000], mean observation: 0.100 [-0.546, 1.028], loss: 3.265200, mean_absolute_error: 40.812309, mean_q: 54.978703
 1451496/1500000: episode: 2884, duration: 10.082s, episode steps: 402, steps per second: 40, episode reward: 224.798, mean reward: 0.559 [-17.421, 100.000], mean action: 1.030 [0.000, 3.000], mean observation: 0.131 [-0.533, 1.000], loss: 2.316647, mean_absolute_error: 40.921329, mean_q: 55.130978
 1451848/1500000: episode: 2885, duration: 8.873s, episode steps: 352, steps per second: 40, episode reward: 208.438, mean reward: 0.592 [-18.946, 100.000], mean action: 1.216 [0.000, 3.000], mean observation: 0.125 [-0.482, 1.000], loss: 4.203490, mean_absolute_error: 40.825562, mean_q: 54.960228
 1452161/1500000: episode: 2886, duration: 7.968s, episode steps: 313, steps per second: 39, episode reward: 183.288, mean reward: 0.586 [-8.716, 100.000], mean action: 1.224 [0.000, 3.000], mean observation: 0.039 [-0.558, 1.000], loss: 3.460798, mean_absolute_error: 40.816963, mean_q: 54.955017
 1452587/1500000: episode: 2887, duration: 10.948s, episode steps: 426, steps per second: 39, episode reward: 178.643, mean reward: 0.419 [-17.224, 100.000], mean action: 2.146 [0.000, 3.000], mean observation: 0.120 [-0.539, 1.000], loss: 4.473864, mean_absolute_error: 40.711864, mean_q: 54.799763
 1452909/1500000: episode: 2888, duration: 8.559s, episode steps: 322, steps per second: 38, episode reward: 224.699, mean reward: 0.698 [-2.694, 100.000], mean action: 1.106 [0.000, 3.000], mean observation: 0.108 [-0.412, 1.000], loss: 4.241086, mean_absolute_error: 40.783531, mean_q: 54.898849
 1453188/1500000: episode: 2889, duration: 7.478s, episode steps: 279, steps per second: 37, episode reward: 210.116, mean reward: 0.753 [-10.023, 100.000], mean action: 1.541 [0.000, 3.000], mean observation: 0.085 [-0.491, 1.000], loss: 3.020649, mean_absolute_error: 40.665188, mean_q: 54.748322
 1453657/1500000: episode: 2890, duration: 12.624s, episode steps: 469, steps per second: 37, episode reward: 216.049, mean reward: 0.461 [-18.888, 100.000], mean action: 1.017 [0.000, 3.000], mean observation: 0.109 [-0.491, 1.000], loss: 2.933779, mean_absolute_error: 40.769321, mean_q: 54.879707
 1453961/1500000: episode: 2891, duration: 7.926s, episode steps: 304, steps per second: 38, episode reward: 173.352, mean reward: 0.570 [-17.415, 100.000], mean action: 2.135 [0.000, 3.000], mean observation: 0.088 [-0.521, 1.000], loss: 3.655816, mean_absolute_error: 40.716015, mean_q: 54.800724
 1454307/1500000: episode: 2892, duration: 8.836s, episode steps: 346, steps per second: 39, episode reward: 201.663, mean reward: 0.583 [-11.530, 100.000], mean action: 0.974 [0.000, 3.000], mean observation: 0.116 [-0.475, 1.000], loss: 3.713000, mean_absolute_error: 40.571873, mean_q: 54.602989
 1454556/1500000: episode: 2893, duration: 6.229s, episode steps: 249, steps per second: 40, episode reward: 173.775, mean reward: 0.698 [-9.467, 100.000], mean action: 1.028 [0.000, 3.000], mean observation: 0.065 [-0.478, 1.000], loss: 4.446429, mean_absolute_error: 40.517712, mean_q: 54.545727
 1454776/1500000: episode: 2894, duration: 5.466s, episode steps: 220, steps per second: 40, episode reward: 228.267, mean reward: 1.038 [-10.401, 100.000], mean action: 1.214 [0.000, 3.000], mean observation: 0.059 [-0.565, 1.000], loss: 4.947944, mean_absolute_error: 40.590405, mean_q: 54.665226
 1454927/1500000: episode: 2895, duration: 3.678s, episode steps: 151, steps per second: 41, episode reward: 21.294, mean reward: 0.141 [-100.000, 25.099], mean action: 1.742 [0.000, 3.000], mean observation: 0.070 [-0.765, 1.428], loss: 6.154700, mean_absolute_error: 40.458344, mean_q: 54.450138
 1455184/1500000: episode: 2896, duration: 6.368s, episode steps: 257, steps per second: 40, episode reward: 210.070, mean reward: 0.817 [-5.743, 100.000], mean action: 1.397 [0.000, 3.000], mean observation: 0.082 [-0.575, 1.000], loss: 4.863741, mean_absolute_error: 40.714462, mean_q: 54.798222
 1455425/1500000: episode: 2897, duration: 5.985s, episode steps: 241, steps per second: 40, episode reward: 220.652, mean reward: 0.916 [-9.092, 100.000], mean action: 1.440 [0.000, 3.000], mean observation: 0.059 [-0.540, 1.014], loss: 5.383079, mean_absolute_error: 40.634644, mean_q: 54.663685
 1456368/1500000: episode: 2898, duration: 24.632s, episode steps: 943, steps per second: 38, episode reward: 175.195, mean reward: 0.186 [-18.975, 100.000], mean action: 1.116 [0.000, 3.000], mean observation: 0.185 [-0.543, 1.000], loss: 4.209386, mean_absolute_error: 40.877136, mean_q: 55.005444
 1456787/1500000: episode: 2899, duration: 10.450s, episode steps: 419, steps per second: 40, episode reward: 254.777, mean reward: 0.608 [-18.739, 100.000], mean action: 0.792 [0.000, 3.000], mean observation: 0.115 [-0.797, 1.000], loss: 3.785291, mean_absolute_error: 40.714096, mean_q: 54.815147
 1457041/1500000: episode: 2900, duration: 6.371s, episode steps: 254, steps per second: 40, episode reward: 221.315, mean reward: 0.871 [-2.814, 100.000], mean action: 1.402 [0.000, 3.000], mean observation: 0.052 [-0.606, 1.000], loss: 4.451800, mean_absolute_error: 40.671688, mean_q: 54.745266
 1457379/1500000: episode: 2901, duration: 8.609s, episode steps: 338, steps per second: 39, episode reward: 219.737, mean reward: 0.650 [-9.493, 100.000], mean action: 1.654 [0.000, 3.000], mean observation: 0.086 [-0.573, 1.000], loss: 4.245472, mean_absolute_error: 40.835194, mean_q: 54.993576
 1458277/1500000: episode: 2902, duration: 23.861s, episode steps: 898, steps per second: 38, episode reward: 208.259, mean reward: 0.232 [-18.848, 100.000], mean action: 1.927 [0.000, 3.000], mean observation: 0.187 [-0.594, 1.028], loss: 4.093419, mean_absolute_error: 40.687519, mean_q: 54.790997
 1458495/1500000: episode: 2903, duration: 5.360s, episode steps: 218, steps per second: 41, episode reward: 190.578, mean reward: 0.874 [-3.056, 100.000], mean action: 1.032 [0.000, 3.000], mean observation: 0.060 [-0.834, 1.000], loss: 4.298218, mean_absolute_error: 40.706497, mean_q: 54.842907
 1458777/1500000: episode: 2904, duration: 7.040s, episode steps: 282, steps per second: 40, episode reward: 222.618, mean reward: 0.789 [-9.561, 100.000], mean action: 1.298 [0.000, 3.000], mean observation: 0.080 [-0.615, 1.000], loss: 3.773119, mean_absolute_error: 40.990490, mean_q: 55.166065
 1459040/1500000: episode: 2905, duration: 6.768s, episode steps: 263, steps per second: 39, episode reward: 229.797, mean reward: 0.874 [-2.902, 100.000], mean action: 1.384 [0.000, 3.000], mean observation: 0.073 [-0.664, 1.000], loss: 3.514640, mean_absolute_error: 40.741444, mean_q: 54.846401
 1459267/1500000: episode: 2906, duration: 5.735s, episode steps: 227, steps per second: 40, episode reward: 228.578, mean reward: 1.007 [-9.322, 100.000], mean action: 1.555 [0.000, 3.000], mean observation: 0.013 [-0.810, 1.000], loss: 3.606715, mean_absolute_error: 40.782303, mean_q: 54.892929
 1459525/1500000: episode: 2907, duration: 6.743s, episode steps: 258, steps per second: 38, episode reward: 180.803, mean reward: 0.701 [-2.600, 100.000], mean action: 1.016 [0.000, 3.000], mean observation: 0.068 [-0.487, 1.000], loss: 3.425610, mean_absolute_error: 40.912445, mean_q: 55.112617
 1459839/1500000: episode: 2908, duration: 8.265s, episode steps: 314, steps per second: 38, episode reward: 184.170, mean reward: 0.587 [-2.777, 100.000], mean action: 1.201 [0.000, 3.000], mean observation: 0.082 [-0.463, 1.000], loss: 2.554269, mean_absolute_error: 40.794960, mean_q: 54.932350
 1460103/1500000: episode: 2909, duration: 6.480s, episode steps: 264, steps per second: 41, episode reward: 215.521, mean reward: 0.816 [-18.692, 100.000], mean action: 1.371 [0.000, 3.000], mean observation: 0.069 [-0.576, 1.000], loss: 5.533790, mean_absolute_error: 41.057129, mean_q: 55.218956
 1460468/1500000: episode: 2910, duration: 9.251s, episode steps: 365, steps per second: 39, episode reward: 195.510, mean reward: 0.536 [-10.566, 100.000], mean action: 1.041 [0.000, 3.000], mean observation: 0.097 [-0.481, 1.000], loss: 3.812556, mean_absolute_error: 41.243008, mean_q: 55.496681
 1460714/1500000: episode: 2911, duration: 6.224s, episode steps: 246, steps per second: 40, episode reward: 255.871, mean reward: 1.040 [-8.948, 100.000], mean action: 1.028 [0.000, 3.000], mean observation: -0.005 [-0.880, 1.504], loss: 3.398706, mean_absolute_error: 40.942455, mean_q: 55.088421
 1461146/1500000: episode: 2912, duration: 11.006s, episode steps: 432, steps per second: 39, episode reward: 244.012, mean reward: 0.565 [-18.779, 100.000], mean action: 0.921 [0.000, 3.000], mean observation: 0.129 [-0.731, 1.000], loss: 4.764236, mean_absolute_error: 41.348881, mean_q: 55.617874
 1461389/1500000: episode: 2913, duration: 5.893s, episode steps: 243, steps per second: 41, episode reward: 197.307, mean reward: 0.812 [-9.709, 100.000], mean action: 1.016 [0.000, 3.000], mean observation: 0.069 [-0.488, 1.000], loss: 2.625693, mean_absolute_error: 41.091694, mean_q: 55.355255
 1462389/1500000: episode: 2914, duration: 26.843s, episode steps: 1000, steps per second: 37, episode reward: 47.518, mean reward: 0.048 [-17.753, 14.643], mean action: 1.708 [0.000, 3.000], mean observation: 0.179 [-0.580, 1.000], loss: 3.986419, mean_absolute_error: 41.066383, mean_q: 55.287991
 1462714/1500000: episode: 2915, duration: 8.089s, episode steps: 325, steps per second: 40, episode reward: 200.643, mean reward: 0.617 [-21.259, 100.000], mean action: 1.151 [0.000, 3.000], mean observation: 0.062 [-0.443, 1.000], loss: 2.658586, mean_absolute_error: 40.763012, mean_q: 54.905716
 1463024/1500000: episode: 2916, duration: 7.787s, episode steps: 310, steps per second: 40, episode reward: 247.922, mean reward: 0.800 [-19.367, 100.000], mean action: 1.184 [0.000, 3.000], mean observation: 0.113 [-0.588, 1.000], loss: 5.264282, mean_absolute_error: 40.801525, mean_q: 54.953503
 1463369/1500000: episode: 2917, duration: 8.829s, episode steps: 345, steps per second: 39, episode reward: 198.790, mean reward: 0.576 [-13.468, 100.000], mean action: 1.586 [0.000, 3.000], mean observation: 0.093 [-0.514, 1.000], loss: 3.422238, mean_absolute_error: 40.861698, mean_q: 55.051052
 1463671/1500000: episode: 2918, duration: 7.521s, episode steps: 302, steps per second: 40, episode reward: 260.692, mean reward: 0.863 [-20.717, 100.000], mean action: 1.278 [0.000, 3.000], mean observation: 0.126 [-0.718, 1.000], loss: 5.380677, mean_absolute_error: 40.932377, mean_q: 55.083508
 1464281/1500000: episode: 2919, duration: 16.090s, episode steps: 610, steps per second: 38, episode reward: 166.841, mean reward: 0.274 [-17.650, 100.000], mean action: 2.005 [0.000, 3.000], mean observation: 0.162 [-0.469, 1.000], loss: 4.092389, mean_absolute_error: 40.803310, mean_q: 54.951370
 1464514/1500000: episode: 2920, duration: 6.188s, episode steps: 233, steps per second: 38, episode reward: 197.348, mean reward: 0.847 [-17.044, 100.000], mean action: 1.549 [0.000, 3.000], mean observation: -0.004 [-0.597, 1.000], loss: 4.058205, mean_absolute_error: 40.659737, mean_q: 54.725357
 1464768/1500000: episode: 2921, duration: 6.194s, episode steps: 254, steps per second: 41, episode reward: 208.614, mean reward: 0.821 [-17.553, 100.000], mean action: 1.319 [0.000, 3.000], mean observation: 0.054 [-0.489, 1.000], loss: 3.672248, mean_absolute_error: 40.997810, mean_q: 55.188538
 1465087/1500000: episode: 2922, duration: 7.907s, episode steps: 319, steps per second: 40, episode reward: 240.501, mean reward: 0.754 [-12.218, 100.000], mean action: 1.094 [0.000, 3.000], mean observation: 0.081 [-0.735, 1.000], loss: 3.652079, mean_absolute_error: 40.965893, mean_q: 55.143974
 1465438/1500000: episode: 2923, duration: 10.265s, episode steps: 351, steps per second: 34, episode reward: 241.359, mean reward: 0.688 [-3.168, 100.000], mean action: 1.194 [0.000, 3.000], mean observation: 0.116 [-0.586, 1.000], loss: 3.314731, mean_absolute_error: 41.121387, mean_q: 55.353634
 1465977/1500000: episode: 2924, duration: 14.312s, episode steps: 539, steps per second: 38, episode reward: 239.878, mean reward: 0.445 [-19.576, 100.000], mean action: 0.829 [0.000, 3.000], mean observation: 0.163 [-0.584, 1.000], loss: 3.222542, mean_absolute_error: 41.068134, mean_q: 55.273197
 1466256/1500000: episode: 2925, duration: 7.051s, episode steps: 279, steps per second: 40, episode reward: 197.707, mean reward: 0.709 [-19.314, 100.000], mean action: 1.330 [0.000, 3.000], mean observation: 0.082 [-0.522, 1.000], loss: 4.634566, mean_absolute_error: 40.906979, mean_q: 55.087273
 1466548/1500000: episode: 2926, duration: 7.276s, episode steps: 292, steps per second: 40, episode reward: 225.583, mean reward: 0.773 [-8.452, 100.000], mean action: 1.425 [0.000, 3.000], mean observation: 0.085 [-0.533, 1.000], loss: 4.871675, mean_absolute_error: 41.025803, mean_q: 55.197834
 1466801/1500000: episode: 2927, duration: 6.117s, episode steps: 253, steps per second: 41, episode reward: 176.444, mean reward: 0.697 [-19.190, 100.000], mean action: 0.941 [0.000, 3.000], mean observation: 0.055 [-0.492, 1.000], loss: 3.165911, mean_absolute_error: 41.040760, mean_q: 55.254810
 1467078/1500000: episode: 2928, duration: 6.792s, episode steps: 277, steps per second: 41, episode reward: 192.357, mean reward: 0.694 [-8.984, 100.000], mean action: 1.130 [0.000, 3.000], mean observation: 0.074 [-0.488, 1.000], loss: 4.361840, mean_absolute_error: 41.012829, mean_q: 55.200905
 1467372/1500000: episode: 2929, duration: 7.501s, episode steps: 294, steps per second: 39, episode reward: 256.603, mean reward: 0.873 [-9.810, 100.000], mean action: 1.296 [0.000, 3.000], mean observation: 0.040 [-0.750, 1.009], loss: 3.070737, mean_absolute_error: 41.024021, mean_q: 55.270363
 1467721/1500000: episode: 2930, duration: 8.758s, episode steps: 349, steps per second: 40, episode reward: 252.322, mean reward: 0.723 [-11.428, 100.000], mean action: 1.461 [0.000, 3.000], mean observation: 0.103 [-0.568, 1.023], loss: 4.362193, mean_absolute_error: 41.025951, mean_q: 55.240917
 1468118/1500000: episode: 2931, duration: 9.963s, episode steps: 397, steps per second: 40, episode reward: 233.317, mean reward: 0.588 [-17.360, 100.000], mean action: 1.081 [0.000, 3.000], mean observation: 0.100 [-0.699, 1.000], loss: 3.233301, mean_absolute_error: 41.024612, mean_q: 55.210575
 1468436/1500000: episode: 2932, duration: 8.221s, episode steps: 318, steps per second: 39, episode reward: 228.714, mean reward: 0.719 [-20.234, 100.000], mean action: 1.038 [0.000, 3.000], mean observation: 0.058 [-0.733, 1.000], loss: 4.120976, mean_absolute_error: 40.954948, mean_q: 55.116943
 1468675/1500000: episode: 2933, duration: 6.022s, episode steps: 239, steps per second: 40, episode reward: 225.083, mean reward: 0.942 [-2.746, 100.000], mean action: 1.418 [0.000, 3.000], mean observation: 0.028 [-0.600, 1.000], loss: 5.185623, mean_absolute_error: 41.097088, mean_q: 55.304306
 1468914/1500000: episode: 2934, duration: 5.721s, episode steps: 239, steps per second: 42, episode reward: 234.655, mean reward: 0.982 [-17.589, 100.000], mean action: 1.331 [0.000, 3.000], mean observation: 0.040 [-0.723, 1.000], loss: 5.541711, mean_absolute_error: 41.072266, mean_q: 55.346455
 1469114/1500000: episode: 2935, duration: 5.162s, episode steps: 200, steps per second: 39, episode reward: -21.111, mean reward: -0.106 [-100.000, 15.741], mean action: 1.600 [0.000, 3.000], mean observation: 0.052 [-0.551, 1.000], loss: 3.007423, mean_absolute_error: 40.955021, mean_q: 55.155521
 1469370/1500000: episode: 2936, duration: 6.096s, episode steps: 256, steps per second: 42, episode reward: 220.786, mean reward: 0.862 [-9.593, 100.000], mean action: 1.156 [0.000, 3.000], mean observation: 0.070 [-0.564, 1.006], loss: 3.293736, mean_absolute_error: 41.084167, mean_q: 55.316837
 1469778/1500000: episode: 2937, duration: 10.406s, episode steps: 408, steps per second: 39, episode reward: 165.235, mean reward: 0.405 [-19.128, 100.000], mean action: 2.245 [0.000, 3.000], mean observation: 0.101 [-0.544, 1.000], loss: 4.044713, mean_absolute_error: 41.049950, mean_q: 55.242992
 1470152/1500000: episode: 2938, duration: 9.435s, episode steps: 374, steps per second: 40, episode reward: 243.822, mean reward: 0.652 [-9.633, 100.000], mean action: 1.243 [0.000, 3.000], mean observation: 0.082 [-0.680, 1.020], loss: 2.785203, mean_absolute_error: 41.027393, mean_q: 55.229393
 1470516/1500000: episode: 2939, duration: 9.453s, episode steps: 364, steps per second: 39, episode reward: 229.046, mean reward: 0.629 [-18.875, 100.000], mean action: 0.970 [0.000, 3.000], mean observation: 0.086 [-0.777, 1.000], loss: 3.636893, mean_absolute_error: 40.869892, mean_q: 55.018436
 1470800/1500000: episode: 2940, duration: 7.018s, episode steps: 284, steps per second: 40, episode reward: 251.680, mean reward: 0.886 [-3.404, 100.000], mean action: 1.261 [0.000, 3.000], mean observation: 0.048 [-0.721, 1.000], loss: 3.461894, mean_absolute_error: 40.854656, mean_q: 54.988232
 1471081/1500000: episode: 2941, duration: 7.036s, episode steps: 281, steps per second: 40, episode reward: 238.788, mean reward: 0.850 [-18.770, 100.000], mean action: 1.060 [0.000, 3.000], mean observation: 0.109 [-0.648, 1.000], loss: 5.812869, mean_absolute_error: 40.855122, mean_q: 54.992416
 1471350/1500000: episode: 2942, duration: 6.930s, episode steps: 269, steps per second: 39, episode reward: 231.251, mean reward: 0.860 [-6.765, 100.000], mean action: 1.528 [0.000, 3.000], mean observation: 0.085 [-0.509, 1.000], loss: 4.246167, mean_absolute_error: 40.824669, mean_q: 54.973312
 1471616/1500000: episode: 2943, duration: 6.893s, episode steps: 266, steps per second: 39, episode reward: 197.102, mean reward: 0.741 [-2.935, 100.000], mean action: 1.485 [0.000, 3.000], mean observation: 0.053 [-0.470, 1.000], loss: 3.795452, mean_absolute_error: 40.836475, mean_q: 54.948982
 1471980/1500000: episode: 2944, duration: 9.736s, episode steps: 364, steps per second: 37, episode reward: 212.408, mean reward: 0.584 [-12.352, 100.000], mean action: 0.920 [0.000, 3.000], mean observation: 0.118 [-0.617, 1.000], loss: 6.833580, mean_absolute_error: 40.969147, mean_q: 55.108688
 1472299/1500000: episode: 2945, duration: 7.947s, episode steps: 319, steps per second: 40, episode reward: 208.090, mean reward: 0.652 [-18.849, 100.000], mean action: 1.003 [0.000, 3.000], mean observation: 0.062 [-0.695, 1.000], loss: 4.855484, mean_absolute_error: 41.201263, mean_q: 55.478119
 1472681/1500000: episode: 2946, duration: 10.212s, episode steps: 382, steps per second: 37, episode reward: 170.019, mean reward: 0.445 [-17.704, 100.000], mean action: 0.987 [0.000, 3.000], mean observation: 0.114 [-0.567, 1.000], loss: 3.523229, mean_absolute_error: 41.228706, mean_q: 55.506294
 1472985/1500000: episode: 2947, duration: 7.700s, episode steps: 304, steps per second: 39, episode reward: 229.591, mean reward: 0.755 [-11.110, 100.000], mean action: 1.309 [0.000, 3.000], mean observation: 0.096 [-0.428, 1.000], loss: 3.775435, mean_absolute_error: 40.983189, mean_q: 55.167912
 1473368/1500000: episode: 2948, duration: 9.716s, episode steps: 383, steps per second: 39, episode reward: 206.365, mean reward: 0.539 [-18.075, 100.000], mean action: 1.003 [0.000, 3.000], mean observation: 0.112 [-0.591, 1.000], loss: 3.794427, mean_absolute_error: 41.254528, mean_q: 55.525784
 1473630/1500000: episode: 2949, duration: 6.417s, episode steps: 262, steps per second: 41, episode reward: 244.690, mean reward: 0.934 [-3.369, 100.000], mean action: 1.485 [0.000, 3.000], mean observation: 0.109 [-0.590, 1.000], loss: 3.839565, mean_absolute_error: 41.378677, mean_q: 55.663605
 1474000/1500000: episode: 2950, duration: 9.482s, episode steps: 370, steps per second: 39, episode reward: 250.742, mean reward: 0.678 [-18.728, 100.000], mean action: 1.019 [0.000, 3.000], mean observation: 0.102 [-0.765, 1.000], loss: 4.543901, mean_absolute_error: 41.323769, mean_q: 55.611504
 1474254/1500000: episode: 2951, duration: 6.292s, episode steps: 254, steps per second: 40, episode reward: 251.452, mean reward: 0.990 [-17.452, 100.000], mean action: 1.299 [0.000, 3.000], mean observation: 0.036 [-0.773, 1.000], loss: 3.875040, mean_absolute_error: 41.381767, mean_q: 55.677486
 1474821/1500000: episode: 2952, duration: 14.509s, episode steps: 567, steps per second: 39, episode reward: 233.989, mean reward: 0.413 [-19.872, 100.000], mean action: 0.720 [0.000, 3.000], mean observation: 0.169 [-0.664, 1.000], loss: 4.481174, mean_absolute_error: 41.455097, mean_q: 55.748947
 1475065/1500000: episode: 2953, duration: 6.354s, episode steps: 244, steps per second: 38, episode reward: 182.594, mean reward: 0.748 [-12.028, 100.000], mean action: 1.824 [0.000, 3.000], mean observation: 0.043 [-0.452, 1.000], loss: 4.164814, mean_absolute_error: 41.728603, mean_q: 56.109558
 1475569/1500000: episode: 2954, duration: 12.961s, episode steps: 504, steps per second: 39, episode reward: 229.265, mean reward: 0.455 [-20.674, 100.000], mean action: 0.938 [0.000, 3.000], mean observation: 0.126 [-0.756, 1.000], loss: 3.223451, mean_absolute_error: 41.584000, mean_q: 55.940399
 1475787/1500000: episode: 2955, duration: 5.523s, episode steps: 218, steps per second: 39, episode reward: 237.461, mean reward: 1.089 [-9.258, 100.000], mean action: 1.550 [0.000, 3.000], mean observation: 0.026 [-0.942, 1.000], loss: 3.932264, mean_absolute_error: 41.573017, mean_q: 55.913425
 1476091/1500000: episode: 2956, duration: 7.295s, episode steps: 304, steps per second: 42, episode reward: 203.593, mean reward: 0.670 [-9.017, 100.000], mean action: 1.303 [0.000, 3.000], mean observation: 0.082 [-0.629, 1.000], loss: 4.633718, mean_absolute_error: 41.584972, mean_q: 55.937469
 1476361/1500000: episode: 2957, duration: 7.159s, episode steps: 270, steps per second: 38, episode reward: 207.658, mean reward: 0.769 [-2.980, 100.000], mean action: 1.096 [0.000, 3.000], mean observation: 0.088 [-0.580, 1.000], loss: 3.341238, mean_absolute_error: 41.606968, mean_q: 55.983955
 1476790/1500000: episode: 2958, duration: 11.235s, episode steps: 429, steps per second: 38, episode reward: 207.974, mean reward: 0.485 [-18.664, 100.000], mean action: 0.837 [0.000, 3.000], mean observation: 0.108 [-0.516, 1.000], loss: 3.150708, mean_absolute_error: 41.588863, mean_q: 55.962250
 1477181/1500000: episode: 2959, duration: 10.419s, episode steps: 391, steps per second: 38, episode reward: 185.535, mean reward: 0.475 [-17.393, 100.000], mean action: 1.312 [0.000, 3.000], mean observation: 0.090 [-0.627, 1.000], loss: 3.008463, mean_absolute_error: 41.692604, mean_q: 56.101063
 1477488/1500000: episode: 2960, duration: 7.667s, episode steps: 307, steps per second: 40, episode reward: 250.394, mean reward: 0.816 [-12.996, 100.000], mean action: 1.397 [0.000, 3.000], mean observation: 0.042 [-0.767, 1.000], loss: 2.918927, mean_absolute_error: 41.391823, mean_q: 55.698696
 1477615/1500000: episode: 2961, duration: 3.323s, episode steps: 127, steps per second: 38, episode reward: -1.923, mean reward: -0.015 [-100.000, 17.355], mean action: 1.756 [0.000, 3.000], mean observation: 0.072 [-0.802, 1.000], loss: 2.334956, mean_absolute_error: 41.365208, mean_q: 55.649487
 1478051/1500000: episode: 2962, duration: 11.345s, episode steps: 436, steps per second: 38, episode reward: 214.534, mean reward: 0.492 [-17.927, 100.000], mean action: 0.773 [0.000, 3.000], mean observation: 0.118 [-0.501, 1.000], loss: 3.029767, mean_absolute_error: 41.530289, mean_q: 55.872810
 1478338/1500000: episode: 2963, duration: 7.197s, episode steps: 287, steps per second: 40, episode reward: 197.852, mean reward: 0.689 [-18.184, 100.000], mean action: 0.902 [0.000, 3.000], mean observation: 0.097 [-0.534, 1.000], loss: 3.890186, mean_absolute_error: 41.600006, mean_q: 55.959946
 1478830/1500000: episode: 2964, duration: 12.942s, episode steps: 492, steps per second: 38, episode reward: 202.138, mean reward: 0.411 [-18.303, 100.000], mean action: 0.882 [0.000, 3.000], mean observation: 0.138 [-0.468, 1.000], loss: 3.078846, mean_absolute_error: 41.434299, mean_q: 55.743935
 1479060/1500000: episode: 2965, duration: 5.704s, episode steps: 230, steps per second: 40, episode reward: 198.218, mean reward: 0.862 [-12.613, 100.000], mean action: 0.996 [0.000, 3.000], mean observation: 0.086 [-0.835, 1.000], loss: 4.285184, mean_absolute_error: 41.436600, mean_q: 55.783085
 1479396/1500000: episode: 2966, duration: 8.652s, episode steps: 336, steps per second: 39, episode reward: 185.501, mean reward: 0.552 [-19.315, 100.000], mean action: 0.952 [0.000, 3.000], mean observation: 0.072 [-0.664, 1.000], loss: 3.941649, mean_absolute_error: 41.488613, mean_q: 55.854622
 1479733/1500000: episode: 2967, duration: 8.537s, episode steps: 337, steps per second: 39, episode reward: 220.297, mean reward: 0.654 [-12.005, 100.000], mean action: 1.481 [0.000, 3.000], mean observation: 0.093 [-0.552, 1.000], loss: 4.829270, mean_absolute_error: 41.437176, mean_q: 55.812778
 1480036/1500000: episode: 2968, duration: 3.412s, episode steps: 303, steps per second: 89, episode reward: 204.367, mean reward: 0.674 [-2.825, 100.000], mean action: 1.449 [0.000, 3.000], mean observation: 0.061 [-0.507, 1.000], loss: 2.657146, mean_absolute_error: 41.490261, mean_q: 55.872719
 1480362/1500000: episode: 2969, duration: 3.866s, episode steps: 326, steps per second: 84, episode reward: 248.536, mean reward: 0.762 [-17.621, 100.000], mean action: 1.107 [0.000, 3.000], mean observation: 0.122 [-0.710, 1.000], loss: 3.237086, mean_absolute_error: 41.568909, mean_q: 55.962040
 1480634/1500000: episode: 2970, duration: 3.173s, episode steps: 272, steps per second: 86, episode reward: 250.160, mean reward: 0.920 [-9.706, 100.000], mean action: 1.118 [0.000, 3.000], mean observation: 0.093 [-0.746, 1.000], loss: 4.757788, mean_absolute_error: 41.532673, mean_q: 55.905182
 1480921/1500000: episode: 2971, duration: 3.189s, episode steps: 287, steps per second: 90, episode reward: 221.045, mean reward: 0.770 [-9.568, 100.000], mean action: 0.965 [0.000, 3.000], mean observation: 0.112 [-0.663, 1.000], loss: 4.571444, mean_absolute_error: 41.584709, mean_q: 55.975483
 1481272/1500000: episode: 2972, duration: 4.124s, episode steps: 351, steps per second: 85, episode reward: 203.328, mean reward: 0.579 [-11.656, 100.000], mean action: 0.875 [0.000, 3.000], mean observation: 0.103 [-0.457, 1.000], loss: 3.491221, mean_absolute_error: 41.563766, mean_q: 55.948830
 1481602/1500000: episode: 2973, duration: 3.839s, episode steps: 330, steps per second: 86, episode reward: 223.661, mean reward: 0.678 [-2.784, 100.000], mean action: 1.185 [0.000, 3.000], mean observation: 0.110 [-0.397, 1.000], loss: 3.438225, mean_absolute_error: 41.574120, mean_q: 55.968311
 1481858/1500000: episode: 2974, duration: 2.834s, episode steps: 256, steps per second: 90, episode reward: 223.146, mean reward: 0.872 [-7.619, 100.000], mean action: 1.383 [0.000, 3.000], mean observation: 0.024 [-0.511, 1.000], loss: 3.797229, mean_absolute_error: 41.868042, mean_q: 56.327484
 1482214/1500000: episode: 2975, duration: 4.311s, episode steps: 356, steps per second: 83, episode reward: 209.480, mean reward: 0.588 [-9.274, 100.000], mean action: 1.048 [0.000, 3.000], mean observation: 0.098 [-0.559, 1.000], loss: 5.634115, mean_absolute_error: 41.663036, mean_q: 56.069214
 1482564/1500000: episode: 2976, duration: 4.173s, episode steps: 350, steps per second: 84, episode reward: 227.657, mean reward: 0.650 [-18.749, 100.000], mean action: 1.186 [0.000, 3.000], mean observation: 0.080 [-0.741, 1.000], loss: 4.409650, mean_absolute_error: 41.685665, mean_q: 56.099094
 1482893/1500000: episode: 2977, duration: 6.892s, episode steps: 329, steps per second: 48, episode reward: 189.147, mean reward: 0.575 [-10.343, 100.000], mean action: 1.395 [0.000, 3.000], mean observation: 0.108 [-0.485, 1.000], loss: 4.815454, mean_absolute_error: 41.730476, mean_q: 56.165310
 1483301/1500000: episode: 2978, duration: 4.870s, episode steps: 408, steps per second: 84, episode reward: 212.807, mean reward: 0.522 [-22.348, 100.000], mean action: 0.966 [0.000, 3.000], mean observation: 0.113 [-0.579, 1.000], loss: 3.038391, mean_absolute_error: 41.506691, mean_q: 55.881653
 1483630/1500000: episode: 2979, duration: 3.899s, episode steps: 329, steps per second: 84, episode reward: 237.887, mean reward: 0.723 [-18.337, 100.000], mean action: 1.383 [0.000, 3.000], mean observation: 0.100 [-0.597, 1.000], loss: 4.907436, mean_absolute_error: 41.618874, mean_q: 55.994385
 1483882/1500000: episode: 2980, duration: 5.753s, episode steps: 252, steps per second: 44, episode reward: 240.398, mean reward: 0.954 [-17.337, 100.000], mean action: 1.274 [0.000, 3.000], mean observation: 0.043 [-0.653, 1.000], loss: 3.712048, mean_absolute_error: 41.591213, mean_q: 56.010876
 1484171/1500000: episode: 2981, duration: 3.789s, episode steps: 289, steps per second: 76, episode reward: 203.946, mean reward: 0.706 [-17.621, 100.000], mean action: 1.010 [0.000, 3.000], mean observation: 0.086 [-0.484, 1.000], loss: 2.972687, mean_absolute_error: 41.430332, mean_q: 55.789818
 1484412/1500000: episode: 2982, duration: 2.757s, episode steps: 241, steps per second: 87, episode reward: 212.271, mean reward: 0.881 [-2.688, 100.000], mean action: 1.062 [0.000, 3.000], mean observation: 0.088 [-0.513, 1.000], loss: 4.754650, mean_absolute_error: 41.416393, mean_q: 55.763626
 1484739/1500000: episode: 2983, duration: 5.387s, episode steps: 327, steps per second: 61, episode reward: 217.344, mean reward: 0.665 [-17.438, 100.000], mean action: 1.171 [0.000, 3.000], mean observation: 0.105 [-0.454, 1.000], loss: 4.213946, mean_absolute_error: 41.442734, mean_q: 55.839211
 1485018/1500000: episode: 2984, duration: 5.264s, episode steps: 279, steps per second: 53, episode reward: 188.672, mean reward: 0.676 [-17.792, 100.000], mean action: 1.161 [0.000, 3.000], mean observation: 0.072 [-0.520, 1.000], loss: 3.718624, mean_absolute_error: 41.671589, mean_q: 56.129879
 1485290/1500000: episode: 2985, duration: 3.041s, episode steps: 272, steps per second: 89, episode reward: 205.999, mean reward: 0.757 [-3.982, 100.000], mean action: 1.114 [0.000, 3.000], mean observation: 0.054 [-0.474, 1.000], loss: 2.438861, mean_absolute_error: 41.492676, mean_q: 55.904266
 1485603/1500000: episode: 2986, duration: 4.575s, episode steps: 313, steps per second: 68, episode reward: 226.073, mean reward: 0.722 [-7.869, 100.000], mean action: 1.291 [0.000, 3.000], mean observation: 0.047 [-0.498, 1.000], loss: 5.613648, mean_absolute_error: 41.472595, mean_q: 55.831856
 1485937/1500000: episode: 2987, duration: 6.775s, episode steps: 334, steps per second: 49, episode reward: 210.691, mean reward: 0.631 [-17.332, 100.000], mean action: 1.147 [0.000, 3.000], mean observation: 0.073 [-0.510, 1.000], loss: 3.435837, mean_absolute_error: 41.249989, mean_q: 55.537964
 1486225/1500000: episode: 2988, duration: 3.460s, episode steps: 288, steps per second: 83, episode reward: 228.672, mean reward: 0.794 [-3.093, 100.000], mean action: 1.458 [0.000, 3.000], mean observation: 0.047 [-0.664, 1.000], loss: 3.586110, mean_absolute_error: 41.659927, mean_q: 56.064655
 1486594/1500000: episode: 2989, duration: 6.666s, episode steps: 369, steps per second: 55, episode reward: 239.168, mean reward: 0.648 [-17.478, 100.000], mean action: 1.038 [0.000, 3.000], mean observation: 0.110 [-0.705, 1.000], loss: 3.331747, mean_absolute_error: 41.489105, mean_q: 55.853123
 1487001/1500000: episode: 2990, duration: 6.509s, episode steps: 407, steps per second: 63, episode reward: 245.378, mean reward: 0.603 [-17.880, 100.000], mean action: 0.850 [0.000, 3.000], mean observation: 0.173 [-0.650, 1.403], loss: 3.786363, mean_absolute_error: 41.366428, mean_q: 55.672993
 1487335/1500000: episode: 2991, duration: 4.402s, episode steps: 334, steps per second: 76, episode reward: 200.536, mean reward: 0.600 [-21.666, 100.000], mean action: 0.874 [0.000, 3.000], mean observation: 0.127 [-0.538, 1.000], loss: 4.666848, mean_absolute_error: 41.532719, mean_q: 55.885174
 1487804/1500000: episode: 2992, duration: 9.250s, episode steps: 469, steps per second: 51, episode reward: 217.170, mean reward: 0.463 [-18.805, 100.000], mean action: 1.004 [0.000, 3.000], mean observation: 0.150 [-0.655, 1.000], loss: 3.795452, mean_absolute_error: 41.665646, mean_q: 56.073101
 1488229/1500000: episode: 2993, duration: 6.977s, episode steps: 425, steps per second: 61, episode reward: 252.108, mean reward: 0.593 [-18.157, 100.000], mean action: 0.976 [0.000, 3.000], mean observation: 0.114 [-0.786, 1.000], loss: 2.723747, mean_absolute_error: 41.556038, mean_q: 55.974174
 1488536/1500000: episode: 2994, duration: 6.109s, episode steps: 307, steps per second: 50, episode reward: 192.141, mean reward: 0.626 [-17.475, 100.000], mean action: 0.922 [0.000, 3.000], mean observation: 0.091 [-0.530, 1.000], loss: 3.202386, mean_absolute_error: 41.633415, mean_q: 56.070122
 1488841/1500000: episode: 2995, duration: 3.475s, episode steps: 305, steps per second: 88, episode reward: 182.331, mean reward: 0.598 [-8.791, 100.000], mean action: 1.131 [0.000, 3.000], mean observation: 0.039 [-0.845, 1.000], loss: 3.562654, mean_absolute_error: 41.524326, mean_q: 55.898315
 1489067/1500000: episode: 2996, duration: 3.931s, episode steps: 226, steps per second: 57, episode reward: 245.302, mean reward: 1.085 [-11.056, 100.000], mean action: 1.531 [0.000, 3.000], mean observation: 0.030 [-0.702, 1.000], loss: 5.168108, mean_absolute_error: 41.444965, mean_q: 55.797138
 1489450/1500000: episode: 2997, duration: 7.714s, episode steps: 383, steps per second: 50, episode reward: 191.314, mean reward: 0.500 [-11.933, 100.000], mean action: 0.841 [0.000, 3.000], mean observation: 0.107 [-0.476, 1.000], loss: 2.585515, mean_absolute_error: 41.428417, mean_q: 55.772835
 1489706/1500000: episode: 2998, duration: 2.968s, episode steps: 256, steps per second: 86, episode reward: 192.305, mean reward: 0.751 [-3.185, 100.000], mean action: 1.207 [0.000, 3.000], mean observation: 0.054 [-0.520, 1.000], loss: 3.189583, mean_absolute_error: 41.585121, mean_q: 55.967293
 1489973/1500000: episode: 2999, duration: 4.754s, episode steps: 267, steps per second: 56, episode reward: 214.498, mean reward: 0.803 [-17.454, 100.000], mean action: 1.352 [0.000, 3.000], mean observation: 0.084 [-0.515, 1.000], loss: 4.869856, mean_absolute_error: 41.534973, mean_q: 55.865917
 1490501/1500000: episode: 3000, duration: 9.442s, episode steps: 528, steps per second: 56, episode reward: 198.715, mean reward: 0.376 [-19.597, 100.000], mean action: 0.850 [0.000, 3.000], mean observation: 0.124 [-0.482, 1.000], loss: 3.367860, mean_absolute_error: 41.532864, mean_q: 55.864605
 1491139/1500000: episode: 3001, duration: 12.272s, episode steps: 638, steps per second: 52, episode reward: 214.673, mean reward: 0.336 [-20.172, 100.000], mean action: 0.683 [0.000, 3.000], mean observation: 0.134 [-0.600, 1.000], loss: 3.660430, mean_absolute_error: 41.576584, mean_q: 55.942966
 1491478/1500000: episode: 3002, duration: 3.991s, episode steps: 339, steps per second: 85, episode reward: 230.104, mean reward: 0.679 [-21.430, 100.000], mean action: 1.171 [0.000, 3.000], mean observation: 0.070 [-0.741, 1.000], loss: 5.306987, mean_absolute_error: 41.952290, mean_q: 56.472614
 1491749/1500000: episode: 3003, duration: 5.193s, episode steps: 271, steps per second: 52, episode reward: 235.732, mean reward: 0.870 [-3.017, 100.000], mean action: 1.166 [0.000, 3.000], mean observation: 0.053 [-0.708, 1.000], loss: 3.015866, mean_absolute_error: 41.720509, mean_q: 56.205517
 1492106/1500000: episode: 3004, duration: 7.224s, episode steps: 357, steps per second: 49, episode reward: 161.548, mean reward: 0.453 [-15.623, 100.000], mean action: 1.232 [0.000, 3.000], mean observation: 0.043 [-0.854, 1.000], loss: 3.325440, mean_absolute_error: 41.737309, mean_q: 56.183495
 1492423/1500000: episode: 3005, duration: 3.824s, episode steps: 317, steps per second: 83, episode reward: 172.249, mean reward: 0.543 [-18.188, 100.000], mean action: 1.022 [0.000, 3.000], mean observation: 0.110 [-0.520, 1.000], loss: 2.508862, mean_absolute_error: 41.678486, mean_q: 56.104076
 1492786/1500000: episode: 3006, duration: 8.412s, episode steps: 363, steps per second: 43, episode reward: 216.839, mean reward: 0.597 [-18.456, 100.000], mean action: 1.507 [0.000, 3.000], mean observation: 0.118 [-1.126, 1.039], loss: 4.097492, mean_absolute_error: 41.610817, mean_q: 55.984116
 1493346/1500000: episode: 3007, duration: 7.675s, episode steps: 560, steps per second: 73, episode reward: 162.078, mean reward: 0.289 [-24.288, 100.000], mean action: 1.116 [0.000, 3.000], mean observation: 0.128 [-0.550, 1.000], loss: 3.089437, mean_absolute_error: 41.434914, mean_q: 55.781471
 1493583/1500000: episode: 3008, duration: 5.942s, episode steps: 237, steps per second: 40, episode reward: 180.681, mean reward: 0.762 [-10.472, 100.000], mean action: 1.304 [0.000, 3.000], mean observation: 0.054 [-0.499, 1.000], loss: 2.951782, mean_absolute_error: 41.361385, mean_q: 55.689144
 1493824/1500000: episode: 3009, duration: 4.298s, episode steps: 241, steps per second: 56, episode reward: 217.059, mean reward: 0.901 [-9.449, 100.000], mean action: 1.137 [0.000, 3.000], mean observation: 0.073 [-0.538, 1.000], loss: 2.959625, mean_absolute_error: 41.472050, mean_q: 55.876881
 1494094/1500000: episode: 3010, duration: 3.399s, episode steps: 270, steps per second: 79, episode reward: 192.973, mean reward: 0.715 [-17.337, 100.000], mean action: 0.944 [0.000, 3.000], mean observation: 0.091 [-0.518, 1.000], loss: 3.315850, mean_absolute_error: 41.671021, mean_q: 56.114605
 1494576/1500000: episode: 3011, duration: 11.564s, episode steps: 482, steps per second: 42, episode reward: 228.131, mean reward: 0.473 [-18.243, 100.000], mean action: 0.934 [0.000, 3.000], mean observation: 0.151 [-0.508, 1.000], loss: 4.703267, mean_absolute_error: 41.519878, mean_q: 55.887566
 1494828/1500000: episode: 3012, duration: 2.858s, episode steps: 252, steps per second: 88, episode reward: 197.253, mean reward: 0.783 [-2.626, 100.000], mean action: 1.135 [0.000, 3.000], mean observation: 0.044 [-0.504, 1.000], loss: 2.501806, mean_absolute_error: 41.675735, mean_q: 56.111771
 1495226/1500000: episode: 3013, duration: 8.081s, episode steps: 398, steps per second: 49, episode reward: 216.713, mean reward: 0.545 [-17.955, 100.000], mean action: 1.143 [0.000, 3.000], mean observation: 0.085 [-0.535, 1.003], loss: 3.582796, mean_absolute_error: 41.555866, mean_q: 55.927036
 1495561/1500000: episode: 3014, duration: 5.881s, episode steps: 335, steps per second: 57, episode reward: 262.151, mean reward: 0.783 [-18.566, 100.000], mean action: 1.167 [0.000, 3.000], mean observation: 0.137 [-0.705, 1.000], loss: 4.562434, mean_absolute_error: 41.635799, mean_q: 56.023037
 1495737/1500000: episode: 3015, duration: 1.994s, episode steps: 176, steps per second: 88, episode reward: -73.725, mean reward: -0.419 [-100.000, 8.280], mean action: 1.273 [0.000, 3.000], mean observation: -0.043 [-0.894, 1.000], loss: 4.194488, mean_absolute_error: 41.698986, mean_q: 56.133690
 1496049/1500000: episode: 3016, duration: 6.224s, episode steps: 312, steps per second: 50, episode reward: 208.830, mean reward: 0.669 [-3.302, 100.000], mean action: 1.064 [0.000, 3.000], mean observation: 0.071 [-0.537, 1.000], loss: 2.885745, mean_absolute_error: 41.421169, mean_q: 55.789387
 1496326/1500000: episode: 3017, duration: 6.160s, episode steps: 277, steps per second: 45, episode reward: 195.416, mean reward: 0.705 [-17.893, 100.000], mean action: 1.148 [0.000, 3.000], mean observation: 0.091 [-0.567, 1.000], loss: 7.466290, mean_absolute_error: 41.744389, mean_q: 56.145634
 1496699/1500000: episode: 3018, duration: 4.570s, episode steps: 373, steps per second: 82, episode reward: 213.621, mean reward: 0.573 [-18.033, 100.000], mean action: 1.043 [0.000, 3.000], mean observation: 0.086 [-0.509, 1.000], loss: 3.022298, mean_absolute_error: 41.471512, mean_q: 55.812862
 1496961/1500000: episode: 3019, duration: 6.685s, episode steps: 262, steps per second: 39, episode reward: 208.789, mean reward: 0.797 [-17.466, 100.000], mean action: 1.225 [0.000, 3.000], mean observation: 0.076 [-0.482, 1.000], loss: 3.753108, mean_absolute_error: 41.450653, mean_q: 55.795952
 1497338/1500000: episode: 3020, duration: 6.836s, episode steps: 377, steps per second: 55, episode reward: 203.840, mean reward: 0.541 [-21.772, 100.000], mean action: 1.066 [0.000, 3.000], mean observation: 0.108 [-0.460, 1.000], loss: 3.716546, mean_absolute_error: 41.485340, mean_q: 55.833244
 1497675/1500000: episode: 3021, duration: 4.857s, episode steps: 337, steps per second: 69, episode reward: 206.381, mean reward: 0.612 [-17.726, 100.000], mean action: 1.086 [0.000, 3.000], mean observation: 0.110 [-0.506, 1.000], loss: 3.126210, mean_absolute_error: 41.468464, mean_q: 55.817230
 1498014/1500000: episode: 3022, duration: 8.389s, episode steps: 339, steps per second: 40, episode reward: 258.453, mean reward: 0.762 [-9.640, 100.000], mean action: 1.168 [0.000, 3.000], mean observation: 0.088 [-0.728, 1.000], loss: 2.705396, mean_absolute_error: 41.414448, mean_q: 55.750183
 1498327/1500000: episode: 3023, duration: 4.028s, episode steps: 313, steps per second: 78, episode reward: 217.513, mean reward: 0.695 [-3.270, 100.000], mean action: 1.335 [0.000, 3.000], mean observation: 0.083 [-0.548, 1.000], loss: 2.981061, mean_absolute_error: 41.264027, mean_q: 55.541916
 1498620/1500000: episode: 3024, duration: 5.206s, episode steps: 293, steps per second: 56, episode reward: 234.341, mean reward: 0.800 [-17.725, 100.000], mean action: 0.891 [0.000, 3.000], mean observation: 0.114 [-0.590, 1.000], loss: 5.517426, mean_absolute_error: 41.325493, mean_q: 55.590542
 1498960/1500000: episode: 3025, duration: 8.132s, episode steps: 340, steps per second: 42, episode reward: 202.289, mean reward: 0.595 [-18.697, 100.000], mean action: 1.165 [0.000, 3.000], mean observation: 0.108 [-0.442, 1.000], loss: 4.324111, mean_absolute_error: 41.357788, mean_q: 55.642315
 1499388/1500000: episode: 3026, duration: 4.961s, episode steps: 428, steps per second: 86, episode reward: 220.483, mean reward: 0.515 [-18.260, 100.000], mean action: 0.876 [0.000, 3.000], mean observation: 0.150 [-0.505, 1.000], loss: 4.448371, mean_absolute_error: 41.443077, mean_q: 55.753605
 1499739/1500000: episode: 3027, duration: 9.100s, episode steps: 351, steps per second: 39, episode reward: 225.357, mean reward: 0.642 [-9.528, 100.000], mean action: 1.365 [0.000, 3.000], mean observation: 0.072 [-0.791, 1.000], loss: 2.759908, mean_absolute_error: 41.426605, mean_q: 55.758381
done, took 35224.546 seconds
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Time-taken-to-train-the-model-(in-hours)-:-9-(35224.546sec)">Time taken to train the model (in hours) : 9 (35224.546sec)<a class="anchor-link" href="#Time-taken-to-train-the-model-(in-hours)-:-9-(35224.546sec)">&#182;</a></h5>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Plot-the-rewards-for-the-training-steps">Plot the rewards for the training steps<a class="anchor-link" href="#Plot-the-rewards-for-the-training-steps">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[42]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;nb_steps&#39;</span><span class="p">],</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;episode_reward&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training steps vs Rewards&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;number of steps&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Rewards&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAl8AAAFkCAYAAAAe6l7uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Evaluate-the-performace-of-the-DQ-model">Evaluate the performace of the DQ model<a class="anchor-link" href="#Evaluate-the-performace-of-the-DQ-model">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[38]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Evaluate our algorithm for a few episodes.</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">dqn</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">nb_episodes</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">visualize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Testing for 200 episodes ...
Episode 1: reward: 216.942, steps: 324
Episode 2: reward: 83.155, steps: 1000
Episode 3: reward: 183.849, steps: 281
Episode 4: reward: 204.916, steps: 229
Episode 5: reward: 198.087, steps: 251
Episode 6: reward: 219.701, steps: 256
Episode 7: reward: 224.746, steps: 241
Episode 8: reward: 185.734, steps: 825
Episode 9: reward: 208.339, steps: 241
Episode 10: reward: 201.815, steps: 241
Episode 11: reward: 241.577, steps: 296
Episode 12: reward: 76.641, steps: 1000
Episode 13: reward: 227.537, steps: 271
Episode 14: reward: 232.063, steps: 272
Episode 15: reward: 253.205, steps: 283
Episode 16: reward: 192.391, steps: 227
Episode 17: reward: 208.460, steps: 242
Episode 18: reward: 228.817, steps: 219
Episode 19: reward: 197.935, steps: 274
Episode 20: reward: 207.445, steps: 248
Episode 21: reward: 70.890, steps: 1000
Episode 22: reward: 235.744, steps: 287
Episode 23: reward: 257.392, steps: 323
Episode 24: reward: -46.984, steps: 230
Episode 25: reward: 229.574, steps: 305
Episode 26: reward: 203.560, steps: 265
Episode 27: reward: 215.689, steps: 279
Episode 28: reward: 227.881, steps: 253
Episode 29: reward: 220.972, steps: 280
Episode 30: reward: 222.567, steps: 269
Episode 31: reward: 242.879, steps: 296
Episode 32: reward: 250.737, steps: 248
Episode 33: reward: 205.555, steps: 282
Episode 34: reward: 243.953, steps: 280
Episode 35: reward: 203.450, steps: 262
Episode 36: reward: 212.346, steps: 253
Episode 37: reward: 218.783, steps: 218
Episode 38: reward: 264.067, steps: 247
Episode 39: reward: 236.424, steps: 225
Episode 40: reward: 102.485, steps: 1000
Episode 41: reward: 245.812, steps: 315
Episode 42: reward: 240.564, steps: 239
Episode 43: reward: 228.978, steps: 279
Episode 44: reward: 237.560, steps: 310
Episode 45: reward: 192.391, steps: 268
Episode 46: reward: 231.215, steps: 276
Episode 47: reward: 218.594, steps: 273
Episode 48: reward: 220.757, steps: 311
Episode 49: reward: 201.681, steps: 247
Episode 50: reward: 200.012, steps: 295
Episode 51: reward: 254.190, steps: 282
Episode 52: reward: 178.041, steps: 234
Episode 53: reward: 227.817, steps: 252
Episode 54: reward: 208.785, steps: 272
Episode 55: reward: 193.023, steps: 215
Episode 56: reward: 244.868, steps: 298
Episode 57: reward: 235.397, steps: 293
Episode 58: reward: 228.400, steps: 259
Episode 59: reward: 171.225, steps: 287
Episode 60: reward: 218.550, steps: 301
Episode 61: reward: 223.629, steps: 335
Episode 62: reward: -62.985, steps: 170
Episode 63: reward: 181.756, steps: 229
Episode 64: reward: 216.992, steps: 238
Episode 65: reward: 172.637, steps: 237
Episode 66: reward: 234.122, steps: 272
Episode 67: reward: 213.976, steps: 270
Episode 68: reward: 232.826, steps: 267
Episode 69: reward: 225.415, steps: 231
Episode 70: reward: 259.510, steps: 352
Episode 71: reward: 258.767, steps: 225
Episode 72: reward: 236.624, steps: 235
Episode 73: reward: 204.478, steps: 305
Episode 74: reward: 180.778, steps: 243
Episode 75: reward: 196.517, steps: 258
Episode 76: reward: 155.180, steps: 334
Episode 77: reward: 217.933, steps: 275
Episode 78: reward: 251.205, steps: 264
Episode 79: reward: 221.943, steps: 270
Episode 80: reward: 222.499, steps: 257
Episode 81: reward: 223.064, steps: 283
Episode 82: reward: 214.730, steps: 243
Episode 83: reward: 210.859, steps: 237
Episode 84: reward: 213.037, steps: 230
Episode 85: reward: 218.360, steps: 256
Episode 86: reward: 193.969, steps: 230
Episode 87: reward: -81.046, steps: 169
Episode 88: reward: 214.026, steps: 221
Episode 89: reward: 213.000, steps: 274
Episode 90: reward: 192.579, steps: 235
Episode 91: reward: 179.152, steps: 295
Episode 92: reward: 234.019, steps: 253
Episode 93: reward: 214.245, steps: 285
Episode 94: reward: 210.108, steps: 313
Episode 95: reward: 193.787, steps: 273
Episode 96: reward: 201.227, steps: 270
Episode 97: reward: 229.510, steps: 238
Episode 98: reward: 171.233, steps: 219
Episode 99: reward: 187.437, steps: 243
Episode 100: reward: 227.536, steps: 233
Episode 101: reward: 170.611, steps: 248
Episode 102: reward: 212.944, steps: 279
Episode 103: reward: 213.595, steps: 262
Episode 104: reward: 196.849, steps: 225
Episode 105: reward: 175.851, steps: 262
Episode 106: reward: 218.958, steps: 266
Episode 107: reward: 231.623, steps: 262
Episode 108: reward: 224.503, steps: 259
Episode 109: reward: 213.415, steps: 272
Episode 110: reward: 233.642, steps: 285
Episode 111: reward: 212.150, steps: 219
Episode 112: reward: 197.715, steps: 300
Episode 113: reward: 212.844, steps: 280
Episode 114: reward: 169.311, steps: 456
Episode 115: reward: 236.224, steps: 225
Episode 116: reward: 228.857, steps: 245
Episode 117: reward: 227.251, steps: 269
Episode 118: reward: 197.661, steps: 250
Episode 119: reward: 191.897, steps: 196
Episode 120: reward: 215.229, steps: 309
Episode 121: reward: 243.504, steps: 281
Episode 122: reward: 209.911, steps: 262
Episode 123: reward: 201.699, steps: 219
Episode 124: reward: 198.038, steps: 243
Episode 125: reward: 238.864, steps: 313
Episode 126: reward: 218.087, steps: 258
Episode 127: reward: 195.890, steps: 257
Episode 128: reward: 209.501, steps: 258
Episode 129: reward: 227.173, steps: 290
Episode 130: reward: 214.944, steps: 283
Episode 131: reward: 157.345, steps: 557
Episode 132: reward: 262.424, steps: 247
Episode 133: reward: 213.979, steps: 261
Episode 134: reward: 209.560, steps: 277
Episode 135: reward: 171.200, steps: 294
Episode 136: reward: 245.744, steps: 256
Episode 137: reward: 252.882, steps: 238
Episode 138: reward: 202.035, steps: 239
Episode 139: reward: 175.840, steps: 244
Episode 140: reward: 259.613, steps: 239
Episode 141: reward: 237.391, steps: 303
Episode 142: reward: 193.594, steps: 240
Episode 143: reward: 219.448, steps: 257
Episode 144: reward: 213.906, steps: 252
Episode 145: reward: 171.869, steps: 236
Episode 146: reward: 228.895, steps: 288
Episode 147: reward: 218.192, steps: 267
Episode 148: reward: 241.607, steps: 256
Episode 149: reward: 249.111, steps: 244
Episode 150: reward: 227.719, steps: 297
Episode 151: reward: 235.511, steps: 270
Episode 152: reward: 241.235, steps: 269
Episode 153: reward: 188.911, steps: 242
Episode 154: reward: 205.994, steps: 241
Episode 155: reward: 210.200, steps: 253
Episode 156: reward: 186.867, steps: 246
Episode 157: reward: 206.406, steps: 281
Episode 158: reward: 191.606, steps: 244
Episode 159: reward: 237.869, steps: 258
Episode 160: reward: 243.104, steps: 232
Episode 161: reward: 183.905, steps: 236
Episode 162: reward: 200.475, steps: 215
Episode 163: reward: 223.230, steps: 253
Episode 164: reward: 211.872, steps: 272
Episode 165: reward: 180.424, steps: 273
Episode 166: reward: 203.103, steps: 216
Episode 167: reward: 252.200, steps: 265
Episode 168: reward: 207.867, steps: 227
Episode 169: reward: 198.035, steps: 243
Episode 170: reward: 187.967, steps: 237
Episode 171: reward: 228.035, steps: 238
Episode 172: reward: 190.653, steps: 508
Episode 173: reward: 183.613, steps: 295
Episode 174: reward: 201.086, steps: 218
Episode 175: reward: 203.878, steps: 245
Episode 176: reward: 89.740, steps: 1000
Episode 177: reward: 221.592, steps: 291
Episode 178: reward: 178.473, steps: 255
Episode 179: reward: 252.493, steps: 296
Episode 180: reward: 220.629, steps: 248
Episode 181: reward: 199.587, steps: 274
Episode 182: reward: 239.171, steps: 295
Episode 183: reward: 248.096, steps: 255
Episode 184: reward: 207.468, steps: 257
Episode 185: reward: 186.984, steps: 241
Episode 186: reward: 180.698, steps: 257
Episode 187: reward: 233.749, steps: 293
Episode 188: reward: 233.429, steps: 276
Episode 189: reward: 197.588, steps: 253
Episode 190: reward: 226.212, steps: 321
Episode 191: reward: 221.011, steps: 285
Episode 192: reward: 206.413, steps: 244
Episode 193: reward: 249.627, steps: 237
Episode 194: reward: 237.778, steps: 301
Episode 195: reward: 239.598, steps: 310
Episode 196: reward: 249.073, steps: 268
Episode 197: reward: 223.723, steps: 308
Episode 198: reward: 183.690, steps: 237
Episode 199: reward: 214.170, steps: 298
Episode 200: reward: 247.215, steps: 292
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[40]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># mean reward in 200 episodes</span>
<span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;episode_reward&#39;</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[40]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>207.88579554642783</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Plot-the-reward-distribution-for-200-episodes">Plot the reward distribution for 200 episodes<a class="anchor-link" href="#Plot-the-reward-distribution-for-200-episodes">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[39]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;episode_reward&#39;</span><span class="p">])),</span><span class="n">test</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;episode_reward&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA2sAAAFkCAYAAAC+fKNsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Saving-the-weights-and-the-models">Saving the weights and the models<a class="anchor-link" href="#Saving-the-weights-and-the-models">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # After training is done, we save the final weights.</span>
<span class="n">dqn</span><span class="o">.</span><span class="n">save_weights</span><span class="p">(</span><span class="s1">&#39;dqn_</span><span class="si">{}</span><span class="s1">_weights_rl.h5f&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ENV_NAME</span><span class="p">),</span> <span class="n">overwrite</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># save model</span>
<span class="n">filepath</span> <span class="o">=</span> <span class="s2">&quot;player_RL.mod&quot;</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
    </div>
  </div>
</body>

 


</html>
